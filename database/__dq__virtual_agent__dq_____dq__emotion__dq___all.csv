,paperId,abstract,authors,citationCount,citationStyles,citations,journal,referenceCount,title,year
0,0008a082e953c180482e38fb7fc40220d598a099,"Social intelligence in robots has a quite recent history in artificial intelligence and robotics. However, it has become increasingly apparent that social and interactive skills are necessary requirements in many application areas and contexts where robots need to interact and collaborate with other robots or humans. Research on human–robot interaction (HRI) poses many challenges regarding the nature of interactivity and ‘social behaviour’ in robot and humans. The first part of this paper addresses dimensions of HRI, discussing requirements on social skills for robots and introducing the conceptual space of HRI studies. In order to illustrate these concepts, two examples of HRI research are presented. First, research is surveyed which investigates the development of a cognitive robot companion. The aim of this work is to develop social rules for robot behaviour (a ‘robotiquette’) that is comfortable and acceptable to humans. Second, robots are discussed as possible educational or therapeutic toys for children with autism. The concept of interactive emergence in human–child interactions is highlighted. Different types of play among children are discussed in the light of their potential investigation in human–robot experiments. The paper concludes by examining different paradigms regarding ‘social relationships’ of robots and people interacting with them.","[{'authorId': '1724361', 'name': 'K. Dautenhahn'}]",981.0,"{'bibtex': '@Article{Dautenhahn2007SociallyIR,\n author = {K. Dautenhahn},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n pages = {679 - 704},\n title = {Socially intelligent robots: dimensions of human–robot interaction},\n volume = {362},\n year = {2007}\n}\n'}",,"{'volume': '362', 'pages': '679 - 704', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",127.0,Socially intelligent robots: dimensions of human–robot interaction,2007.0
1,001616a2661b20b92387ef40a63bb95ab9883a87,"ABSTRACT Primary objective: Patients with traumatic brain injury (TBI) have difficulty dealing with the social world and may display inappropriate social behavior that negatively affects their social and occupational rehabilitation. This difficulty may be explained by a social problem-solving (SPS) impairment, but little is yet known about the cognitive processes involved in the ability to solve social problems. Several publications have demonstrated that executive functions are related to social problem solving, but the role of social cognition needs to be confirmed. The present pilot study examined the expected relationships between SPS ability and both social cognition and social behavioral skills. Research design: We compared the performances of 15 patients with TBI on SPS, theory-of-mind and social behavior tasks with those of 25 matched healthy controls. Main outcomes and results: Our results showed for the first time that impaired social problem solving is associated with a theory-of-mind deficit, but surprisingly not with executive impairment. There was no evidence that SPS deficits predict social behavioral disorders. Conclusions: Studying social problem solving in patients with TBI may inform the design of more appropriate methods of social rehabilitation.","[{'authorId': '1486122642', 'name': 'Mathilde Saint-Jean'}, {'authorId': '153541817', 'name': 'P. Allain'}, {'authorId': '35312629', 'name': 'Jérémy Besnard'}]",6.0,"{'bibtex': '@Article{Saint-Jean2018ASA,\n author = {Mathilde Saint-Jean and P. Allain and Jérémy Besnard},\n journal = {Brain Injury},\n pages = {40 - 47},\n title = {A sociocognitive approach to social problem solving in patients with traumatic brain injury: a pilot study},\n volume = {33},\n year = {2018}\n}\n'}",,"{'volume': '33', 'pages': '40 - 47', 'name': 'Brain Injury'}",56.0,A sociocognitive approach to social problem solving in patients with traumatic brain injury: a pilot study,2018.0
2,0037e73960b4c9d19b8928c8f82d82ce315d4fe7,"ABSTRACT Social cognition is widely regarded as an essential skill with which to understand the social world. Despite this, the role that social cognition plays in outcome, and whether deficits are remediable after traumatic brain injury (TBI), are not yet well known. The current review examines the construct of social cognition and presents a conceptual biopsychosocial model with which to understand the social cognitive process. This is related to the literature on social cognitive deficits in TBI and we discuss relevant treatment developments to date within this population. We then review social cognition treatment programmes researched in other clinical populations in order to advise and inform approaches for those living with TBI. Whilst treatments have focused on emotion perception skills in the TBI literature, programmes developed for other clinical populations have had broader targets, focusing on Theory of Mind skills and/or modifying interpretational cognitive biases. Moreover, they have largely proven to be efficacious. Programmes that are contextualised, collaborative, and experiential seem optimal in enabling generalisation relevant to individuals’ everyday social lives. We argue that there is therefore scope to improve the evidence-based social cognitive treatment options available for those with TBI, taking into account specific adaptations necessary for this population.","[{'authorId': '10273454', 'name': 'Anneli Cassel'}, {'authorId': '143982280', 'name': 'S. McDonald'}, {'authorId': '8060857', 'name': 'M. Kelly'}, {'authorId': '4720305', 'name': 'L. Togher'}]",62.0,"{'bibtex': '@Article{Cassel2019LearningFT,\n author = {Anneli Cassel and S. McDonald and M. Kelly and L. Togher},\n journal = {Neuropsychological Rehabilitation},\n pages = {22 - 55},\n title = {Learning from the minds of others: A review of social cognition treatments and their relevance to traumatic brain injury},\n volume = {29},\n year = {2019}\n}\n'}",,"{'volume': '29', 'pages': '22 - 55', 'name': 'Neuropsychological Rehabilitation'}",237.0,Learning from the minds of others: A review of social cognition treatments and their relevance to traumatic brain injury,2019.0
3,00398dcea6a23be322066a48317edf74a9a816fd,,"[{'authorId': '2248388724', 'name': 'Karina Blair'}, {'authorId': '2249063022', 'name': 'Bruce W. Smith'}, {'authorId': '2245628705', 'name': 'Derek G.V. Mitchell'}, {'authorId': '2237561890', 'name': 'John J. L. Morton'}, {'authorId': '2532122', 'name': 'M. Vythilingam'}, {'authorId': '2248384945', 'name': 'Luiz Pessoa'}, {'authorId': '2248384108', 'name': 'Daniel J. Fridberg'}, {'authorId': '2245353586', 'name': 'Alan J. Zametkin'}, {'authorId': '2149913124', 'name': 'E. Nelson'}, {'authorId': '2248385032', 'name': 'Wayne C. Drevets'}, {'authorId': '2242549569', 'name': 'Daniel S. Pine'}, {'authorId': '2249076797', 'name': 'Alex Martin'}, {'authorId': '2237561509', 'name': 'R. Blair'}]",384.0,"{'bibtex': '@Article{Blair2007ModulationOE,\n author = {Karina Blair and Bruce W. Smith and Derek G.V. Mitchell and John J. L. Morton and M. Vythilingam and Luiz Pessoa and Daniel J. Fridberg and Alan J. Zametkin and E. Nelson and Wayne C. Drevets and Daniel S. Pine and Alex Martin and R. Blair},\n journal = {NeuroImage},\n pages = {430-440},\n title = {Modulation of emotion by cognition and cognition by emotion},\n volume = {35},\n year = {2007}\n}\n'}",,"{'volume': '35', 'pages': '430-440', 'name': 'NeuroImage'}",48.0,Modulation of emotion by cognition and cognition by emotion,2007.0
4,0059dab998f4e1bb35c1ff609e90533aae104b24,"The past decade has seen substantial research on exact infer- ence for contingency tables, both in terms of developing new analyses and developing efficient algorithms for computations. Coupled with concomitant improvements in computer power, this research has re- sulted in a greater variety of exact procedures becoming feasible for practical use and a considerable increase in the size of data sets to which the procedures can be applied. For some basic analyses of contin- gency tables, it is unnecessary to use large-sample approximations to sampling distributions when their adequacy is in doubt. This article surveys the current theoretical and computational developments of exact methods for contingency tables. Primary attention is given to the exact conditional approach, which eliminates nuisance parameters by conditioning on their sufficient statistics. The presentation of various exact inferences is unified by expressing them in terms of parameters and their sufficient statistics in loglinear models. Exact approaches for many inferences are not yet addressed in the literature, particularly for multidimensional contingency tables, and this article also suggests additional research for the next decade that would make exact methods yet more widely applicable.","[{'authorId': '2817272', 'name': 'A. Agresti'}]",1178.0,"{'bibtex': '@Article{Agresti1992ASO,\n author = {A. Agresti},\n journal = {Statistical Science},\n pages = {131-153},\n title = {A Survey of Exact Inference for Contingency Tables},\n volume = {7},\n year = {1992}\n}\n'}",,"{'volume': '7', 'pages': '131-153', 'name': 'Statistical Science'}",162.0,A Survey of Exact Inference for Contingency Tables,1992.0
5,006fdeff6e1a81c404317ee4056d6cc72f9c0e50,"Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.","[{'authorId': '144802290', 'name': 'Amir Zadeh'}, {'authorId': '28130078', 'name': 'P. Liang'}, {'authorId': '1746416', 'name': 'Soujanya Poria'}, {'authorId': '49943757', 'name': 'E. Cambria'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",586.0,"{'bibtex': '@Inproceedings{Zadeh2018MultimodalLA,\n author = {Amir Zadeh and P. Liang and Soujanya Poria and E. Cambria and Louis-Philippe Morency},\n pages = {2236-2246},\n title = {Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph},\n year = {2018}\n}\n'}",,{'pages': '2236-2246'},64.0,Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,2018.0
7,00aa9dd045d081630e68f5539b7e339b88828837,,"[{'authorId': '46409823', 'name': 'Mariam Chammat'}, {'authorId': '2078664017', 'name': 'A. Foucher'}, {'authorId': '2433022', 'name': 'J. Nadel'}, {'authorId': '3440983', 'name': 'S. Dubal'}]",25.0,"{'bibtex': '@Article{Chammat2010ReadingSB,\n author = {Mariam Chammat and A. Foucher and J. Nadel and S. Dubal},\n journal = {Brain Research},\n pages = {95-104},\n title = {Reading sadness beyond human faces},\n volume = {1348},\n year = {2010}\n}\n'}",,"{'volume': '1348', 'pages': '95-104', 'name': 'Brain Research'}",83.0,Reading sadness beyond human faces,2010.0
8,00ae2a7affaaaeaf4e1ee2c804e3fd00f779f519,,"[{'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '33694882', 'name': 'M. Harbers'}, {'authorId': '145495942', 'name': 'Willem-Paul Brinkman'}, {'authorId': '1689001', 'name': 'C. Jonker'}, {'authorId': '1747540', 'name': 'K. Bosch'}, {'authorId': '1691228', 'name': 'J. Meyer'}]",60.0,"{'bibtex': '@Inproceedings{Broekens2012VirtualRN,\n author = {J. Broekens and M. Harbers and Willem-Paul Brinkman and C. Jonker and K. Bosch and J. Meyer},\n pages = {218-230},\n title = {Virtual Reality Negotiation Training Increases Negotiation Knowledge and Skill},\n year = {2012}\n}\n'}",,{'pages': '218-230'},36.0,Virtual Reality Negotiation Training Increases Negotiation Knowledge and Skill,2012.0
9,00c17b14a9618544af90e2089e831ebefbc864b1,,"[{'authorId': '3237926', 'name': 'M. Courgeon'}, {'authorId': '1724799', 'name': 'C. Clavel'}]",35.0,"{'bibtex': '@Article{Courgeon2013MARCAF,\n author = {M. Courgeon and C. Clavel},\n journal = {Journal on Multimodal User Interfaces},\n pages = {311-319},\n title = {MARC: a framework that features emotion models for facial animation during human–computer interaction},\n volume = {7},\n year = {2013}\n}\n'}",,"{'volume': '7', 'pages': '311-319', 'name': 'Journal on Multimodal User Interfaces'}",49.0,MARC: a framework that features emotion models for facial animation during human–computer interaction,2013.0
10,00d3d6a56a1d989da8235019903e0415fd158771,,"[{'authorId': '73507079', 'name': 'J. Lutterbie'}]",59.0,"{'bibtex': '@Inproceedings{Lutterbie2011TowardAG,\n author = {J. Lutterbie},\n title = {Toward a General Theory of Acting: Cognitive Science and Performance},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Toward a General Theory of Acting: Cognitive Science and Performance,2011.0
11,00da50ad44be09f058550ac27e964882a09b1940,,"[{'authorId': '1806598', 'name': 'Hector Geffner'}]",5.0,"{'bibtex': '@Inproceedings{Geffner2010HeuristicsPA,\n author = {Hector Geffner},\n title = {Heuristics, Planning and Cognition},\n year = {2010}\n}\n'}",,,54.0,"Heuristics, Planning and Cognition",2010.0
12,00ee7355bd289a0e70e5090e0b31b685bb39d695,"To recognize and understand a person's emotion has been known as one of the most important issue in human-computer interaction. In this paper, we present a multimodal system that supports emotion recognition from both visual and acoustic feature analysis. Our main achievement is that with this bimodal method, we can effectively extend the recognized emotion categories compared to when only visual or acoustic feature analysis works alone. We also show that by carefully cooperating bimodal features, the recognition precision of each emotion category will exceed the limit set up by the single modality, both visual and acoustic. Moreover, we believe our system is closer to real human perception and experience and hence will make emotion recognition closer to practical application in the future","[{'authorId': '3288141', 'name': 'Cheng-Yao Chen'}, {'authorId': '2108715669', 'name': 'Yue Huang'}, {'authorId': '1716507', 'name': 'P. Cook'}]",32.0,"{'bibtex': '@Article{Chen2005VisualAcousticER,\n author = {Cheng-Yao Chen and Yue Huang and P. Cook},\n journal = {2005 IEEE International Conference on Multimedia and Expo},\n pages = {1468-1471},\n title = {Visual/Acoustic Emotion Recognition},\n year = {2005}\n}\n'}",,"{'pages': '1468-1471', 'name': '2005 IEEE International Conference on Multimedia and Expo'}",11.0,Visual/Acoustic Emotion Recognition,2005.0
13,00fa43776e890fbef5e1074b0b22e14b3b608e1d,,"[{'authorId': '143607713', 'name': 'Christine L. Lisetti'}, {'authorId': '2348728', 'name': 'E. Hudlicka'}]",12.0,"{'bibtex': '@Inproceedings{Lisetti2015WhyAH,\n author = {Christine L. Lisetti and E. Hudlicka},\n title = {Why and How to Build Emotion-Based Agent Architectures},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Why and How to Build Emotion-Based Agent Architectures,2015.0
15,01005b0cee748cefff2074912d23d6df46339463,"The results of a gene therapy trial for a disease called spinal muscular atrophy that gradually paralyzes infants have blown away gene therapy researchers, marking one of the once-troubled field9s most dramatic successes yet. All 15 babies treated, expected to die by age 2, are alive at 20 months or older, and most can sit up, according to a report this week. The news adds to the rising fortunes of gene therapy. But the study also breaks ground because it demonstrates the power of a virus carrying a therapeutic gene that, infused into a vein, can carry its genetic cargo straight to the central nervous system, across the so-called blood-brain barrier. The new treatment9s apparent safety and success is emboldening other researchers to use gene therapy delivered into a vein or the spine to treat rare childhood neurological and muscular diseases, and even common adult disorders such as Parkinson9s.","[{'authorId': '144662861', 'name': 'J. Kaiser'}]",15.0,"{'bibtex': '@Article{Kaiser2017ASC,\n author = {J. Kaiser},\n journal = {Science},\n pages = {\n          582-585\n        },\n title = {A second chance.},\n volume = {358 6363},\n year = {2017}\n}\n'}",,"{'volume': '358 6363', 'pages': '\n          582-585\n        ', 'name': 'Science'}",0.0,A second chance.,2017.0
16,010a73b65480dd48ac90b831630fb96c061ffff7,"We describe the creation of a film library designed for researchers interested in positive (amusing), negative (repulsive), mixed (amusing and repulsive) and neutral emotional states. Three hundred 20- to 33-second film clips videotaped by amateurs were selected from video-hosting websites and screened in laboratory studies by 75 female participants on self-reported amusement and repulsion (Experiments 1 and 2). On the basis of pre-defined cut-off values, 51 positive, 39 negative, 59 mixed and 50 neutral film clips were selected. These film clips were then presented to 411 male and female participants in a large online study to identify film clips that reliably induced the target emotions (Experiment 3). Depending on the goal of the study, researchers may choose positive, negative, mixed or neutral emotional film clips on the basis of Experiments 1 and 2 or Experiment 3 ratings.","[{'authorId': '2255000395', 'name': 'Andrea C. Samson'}, {'authorId': '3279362', 'name': 'Sylvia D. Kreibig'}, {'authorId': '47465771', 'name': 'Blake Soderstrom'}, {'authorId': '2255248464', 'name': 'A. Ayanna Wade'}, {'authorId': '2254966886', 'name': 'James J. Gross'}]",100.0,"{'bibtex': '@Article{Samson2016ElicitingPN,\n author = {Andrea C. Samson and Sylvia D. Kreibig and Blake Soderstrom and A. Ayanna Wade and James J. Gross},\n journal = {Cognition and Emotion},\n pages = {827 - 856},\n title = {Eliciting positive, negative and mixed emotional states: A film library for affective scientists},\n volume = {30},\n year = {2016}\n}\n'}",,"{'volume': '30', 'pages': '827 - 856', 'name': 'Cognition and Emotion'}",35.0,"Eliciting positive, negative and mixed emotional states: A film library for affective scientists",2016.0
17,013589f4feac0ce46856c67e4f7cd2125376e68c,"We describe an initial prototype of a holodeck- like environment that we have created for the Mission Rehearsal Exercise Project. The goal of the project is to create an experience learning system where the participants are immersed in an environment where they can encounter the sights, sounds, and circumstances of real-world scenarios. Virtual humans act as characters and coaches in an interactive story with pedagogical goals.","[{'authorId': '1812270', 'name': 'R. Hill'}, {'authorId': '69014762', 'name': 'J. Gratch'}, {'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '3074189', 'name': 'C. Kyriakakis'}, {'authorId': '2407818', 'name': 'C. LaBore'}, {'authorId': '2178316', 'name': 'Richard Lindheim'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '35080984', 'name': 'D. Miraglia'}, {'authorId': '118985925', 'name': 'B. Moore'}, {'authorId': '30625490', 'name': 'Jackie Morie'}, {'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '2096971', 'name': 'M. Thiébaux'}, {'authorId': '1879270', 'name': 'Larry Tuch'}, {'authorId': '143660353', 'name': 'R. Whitney'}, {'authorId': '47722403', 'name': 'J. Douglas'}, {'authorId': '1684040', 'name': 'W. Swartout'}]",280.0,"{'bibtex': '@Inproceedings{Hill2001TowardTH,\n author = {R. Hill and J. Gratch and W. Johnson and C. Kyriakakis and C. LaBore and Richard Lindheim and S. Marsella and D. Miraglia and B. Moore and Jackie Morie and J. Rickel and M. Thiébaux and Larry Tuch and R. Whitney and J. Douglas and W. Swartout},\n pages = {409-416},\n title = {Toward the holodeck: integrating graphics, sound, character and story},\n year = {2001}\n}\n'}",,{'pages': '409-416'},17.0,"Toward the holodeck: integrating graphics, sound, character and story",2001.0
18,01396f643a02a6a04749da068adc266d9c23de99,,"[{'authorId': '1422621837', 'name': 'Martin Gérin-Lajoie'}, {'authorId': '1852074', 'name': 'C. Richards'}, {'authorId': '40228054', 'name': 'J. Fung'}, {'authorId': '1862105', 'name': 'B. McFadyen'}]",134.0,"{'bibtex': '@Article{Gérin-Lajoie2008CharacteristicsOP,\n author = {Martin Gérin-Lajoie and C. Richards and J. Fung and B. McFadyen},\n journal = {Gait & posture},\n pages = {\n          239-47\n        },\n title = {Characteristics of personal space during obstacle circumvention in physical and virtual environments.},\n volume = {27 2},\n year = {2008}\n}\n'}",,"{'volume': '27 2', 'pages': '\n          239-47\n        ', 'name': 'Gait & posture'}",20.0,Characteristics of personal space during obstacle circumvention in physical and virtual environments.,2008.0
20,014391a96dac9a6b637ff2a0570bd30fae4a8d49,"The design, development and testing of intelligent disaster detection and alerting systems pose a set of non-trivial problems. Not only are such systems difficult to design as they need to accurately predict real-world outcomes using a distributed sensing of various parameters, they also need to generate an optimal number of timely alerts when the actual disaster strikes. In this paper, we propose the SimConnector Emulator, a novel approach for the testing of real-world systems using agent-based simulations as a means of validation. The basic idea is to use agent-based simulations to generate event data to allow the testing of responses of the software system to real-time events. As proof of concept, we have developed a Forest Fire Disaster Detection and Alerting System, which uses Intelligent Decision Support based on an internationally recognized Fire rating index, namely the Fire Weather Index (FWI). Results of extensive testing demonstrate the effectiveness of the SimCon-nector approach for the development and testing of real-time applications, in general and disaster detection/alerting systems, in particular.","[{'authorId': '1795560', 'name': 'M. Niazi'}, {'authorId': '2073401', 'name': 'Qasim Siddique'}, {'authorId': '144664815', 'name': 'A. Hussain'}, {'authorId': '1691577', 'name': 'G. Fortino'}]",4.0,"{'bibtex': '@Article{Niazi2011SimConnectorAA,\n author = {M. Niazi and Qasim Siddique and A. Hussain and G. Fortino},\n journal = {2011 Federated Conference on Computer Science and Information Systems (FedCSIS)},\n pages = {659-665},\n title = {SimConnector: An approach to testing disaster-alerting systems using agent based simulation models},\n year = {2011}\n}\n'}",,"{'pages': '659-665', 'name': '2011 Federated Conference on Computer Science and Information Systems (FedCSIS)'}",31.0,SimConnector: An approach to testing disaster-alerting systems using agent based simulation models,2011.0
21,01b36f49b6bb03a5cc51ca4deee2de9be88fc274,,"[{'authorId': '1399283111', 'name': 'H. Kose-Bagci'}, {'authorId': '2529764', 'name': 'R. Yorganci'}, {'authorId': '2652930', 'name': 'Esra H. Algan'}, {'authorId': '1700812', 'name': 'D. Syrdal'}]",40.0,"{'bibtex': '@Article{Kose-Bagci2012EvaluationOT,\n author = {H. Kose-Bagci and R. Yorganci and Esra H. Algan and D. Syrdal},\n journal = {International Journal of Social Robotics},\n pages = {273-283},\n title = {Evaluation of the Robot Assisted Sign Language Tutoring Using Video-Based Studies},\n volume = {4},\n year = {2012}\n}\n'}",,"{'volume': '4', 'pages': '273-283', 'name': 'International Journal of Social Robotics'}",20.0,Evaluation of the Robot Assisted Sign Language Tutoring Using Video-Based Studies,2012.0
22,01b9736851e561a3140b5f9e365cbbc3956ae221,,"[{'authorId': '145074235', 'name': 'John Hart'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",17.0,"{'bibtex': '@Inproceedings{Hart2013HowVR,\n author = {John Hart and J. Gratch and S. Marsella},\n title = {How virtual reality training can win friends and influence people},\n year = {2013}\n}\n'}",,"{'volume': '', 'name': ''}",1.0,How virtual reality training can win friends and influence people,2013.0
23,021183a31462a141c276d14a884ac7ccccc80fa7,,"[{'authorId': '104928470', 'name': 'Jay Hall'}, {'authorId': '143792320', 'name': 'W. Watson'}]",311.0,"{'bibtex': '@Article{Hall1970TheEO,\n author = {Jay Hall and W. Watson},\n journal = {Human Relations},\n pages = {299 - 317},\n title = {The Effects of a Normative Intervention on Group Decision-Making Performance},\n volume = {23},\n year = {1970}\n}\n'}",,"{'volume': '23', 'pages': '299 - 317', 'name': 'Human Relations'}",7.0,The Effects of a Normative Intervention on Group Decision-Making Performance,1970.0
24,025ae915106e92d397dbc1f1486b56fe0212b339,"It will not be possible to apply exactly the same teaching process to the machine as to a normal child. It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle. Possibly it might not have eyes. But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it. —Alan Turing, ""Computing Machinery and Intelligence,"" 1950","[{'authorId': '145431806', 'name': 'Justine Cassell'}]",284.0,"{'bibtex': '@Inproceedings{Cassell2001NudgeNW,\n author = {Justine Cassell},\n pages = {1-27},\n title = {Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '1-27', 'name': ''}",55.0,Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents,2001.0
25,025fe77b606e2c78bd4ba0865de0070d784f9420,"Le modele Satisfaction-Altruisme se situe a la rencontre du domaine des systemes multi-agents reactifs (SMAr) et de la robotique mobile. Il permet d'introduire des comportements cooperatifs individuels dans l'approche de resolution collective de probleme. Apres avoir defini ces differents concepts, nous analysons les situations de cooperation. Nous en deduisons deux etats de satisfaction de l'agent: la satisfaction personnelle, qui est une mesure de la progression des actions, et la satisfaction interactive qui evalue les interactions avec le voisinage (gene, aide). Les agents peuvent alors s'influencer en propageant des signaux repulsifs ou attractifs declencheurs de reactions altruistes. Cette architecture est evaluee en simulation sur des problemes distribues (conflits spatiaux, robots fourrageurs) montrant le caractere emergent et auto-adaptatif des solutions. Le modele est ensuite etendu par l'introduction d'un module d'apprentissage social. Dans le cadre d'une implementation reelle, nous proposons un protocole de communication generique dedie aux SMA situes. Enfin, le modele est valide experimentalement par la resolution de conflits spatiaux entre robots mobiles autonomes.","[{'authorId': '1807441', 'name': 'Olivier Simonin'}]",1.0,"{'bibtex': '@Inproceedings{Simonin2010LEMS,\n author = {Olivier Simonin},\n title = {LE MODELE SATISFACTION-ALTRUISME},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,LE MODELE SATISFACTION-ALTRUISME,2010.0
26,025fed19700b79839ca32385b26ea55b55fdbcb3,,"[{'authorId': '2993393', 'name': 'Valentin Lungu'}]",6.0,"{'bibtex': '@Inproceedings{Lungu2013ArtificialES,\n author = {Valentin Lungu},\n pages = {207-221},\n title = {Artificial Emotion Simulation Model and Agent Architecture: Extended},\n year = {2013}\n}\n'}","[{'paperId': '7a3df4d5094a5d113c45334204abccd3c92be8bc', 'title': 'An approach for emotions and behavior modeling in a crowd in the presence of rare events'}, {'paperId': 'a86e3d64cb51bffb8c3760888a7b474d967ac3e0', 'title': 'Approaches to Modeling the Emotional Aspects of a Crowd'}, {'paperId': '7bd21054c359904a9db40511af409d0385e43654', 'title': 'Fuzzy rule based voice emotion control for user demand speech generation of emotion robot'}, {'paperId': 'fc632427b9a69bad70f4a81df720d7df058e0a9a', 'title': 'Personality and Emotion in Strong-Story Narrative Planning'}, {'paperId': '65f707c3ef8b0929d3e9f8d3ce384f71bee2a6f2', 'title': 'Could Emotions Be Modelled through Information Processing'}, {'paperId': '1f456e15207119c77025f32b78dafb7da2133dbc', 'title': 'Using Emotion as Motivation in the Newtonian Emotion System'}]","{'name': '', 'pages': '207-221', 'volume': ''}",9.0,Artificial Emotion Simulation Model and Agent Architecture: Extended,2013.0
27,02b0555613f38ffa99ce219509c0836d48cb546a,,"[{'authorId': '3098701', 'name': 'Peter A. M. Ruijten'}, {'authorId': '3026039', 'name': 'C. Midden'}, {'authorId': '145960497', 'name': 'Jaap Ham'}]",9.0,"{'bibtex': ""@Article{Ruijten2013IDK,\n author = {Peter A. M. Ruijten and C. Midden and Jaap Ham},\n booktitle = {International Conference on Persuasive Technology},\n pages = {192-197},\n title = {I Didn't Know That Virtual Agent Was Angry at Me: Investigating Effects of Gaze Direction on Emotion Recognition and Evaluation},\n year = {2013}\n}\n""}","[{'paperId': 'f1ef8b6b5fc64a0387be4514c45a15a7f02bb306', 'title': 'A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI: Behaviour Generation, User Interaction and Perception'}, {'paperId': '8732f63151f515bee72c9852f8380e43095e9062', 'title': 'Enabling motivated believable agents with reinforcement learning'}, {'paperId': '126de4fd87129139cfab52f35f344d0ef7158c00', 'title': 'The Ultimatum Game as Measurement Tool for Anthropomorphism in Human-Robot Interaction'}, {'paperId': '3cf7a808c56d9b2dc4542d191c0ba2b954b61c8f', 'title': 'Midden Persuasive technology for a sustainable society'}, {'paperId': 'f9f6ee937ef3abeae5eb62d6b37f9e8492a97508', 'title': 'Learned Behavior: Enabling Believable Virtual Characters through Reinforcement'}, {'paperId': '6f3d1a6f3af6e49b891c8fa148cd39cdce00651b', 'title': 'Persuasive technology for a sustainable society'}, {'paperId': '0334445198de9ab4545c0647169eb93ae73a9ae6', 'title': 'Persuasive technology for a sustainable society Midden'}, {'paperId': '8f65b1d7b7a7b7d62815e3fec5455090bb7c734b', 'title': 'The Influence of Oxytocin on Older Adults’ Emotion Processing'}, {'paperId': 'db9cc9595ce5dd2e48e0e1ef2a1437ec86bd8eb9', 'title': 'Reinforcement learning with motivations for realistic agents'}]",{'pages': '192-197'},10.0,I Didn't Know That Virtual Agent Was Angry at Me: Investigating Effects of Gaze Direction on Emotion Recognition and Evaluation,2013.0
28,02b508a34619732053f83fa122205c1c83bd97a7,,"[{'authorId': '3235367', 'name': 'W. Westera'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '145255182', 'name': 'P. A. Santos'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '28004507', 'name': 'Manuel Guimarães'}, {'authorId': '48884082', 'name': 'K. Georgiadis'}, {'authorId': '3260155', 'name': 'E. Nyamsuren'}, {'authorId': '2565070', 'name': 'Kiavash Bahreini'}, {'authorId': '1730934', 'name': 'Zerrin Yumak'}, {'authorId': '73337627', 'name': 'C. Christyowidiasmoro'}, {'authorId': '151505823', 'name': 'M. Dascalu'}, {'authorId': '1410082815', 'name': 'Gabriel Gutu-Robu'}, {'authorId': '3164109', 'name': 'Stefan Ruseti'}]",39.0,"{'bibtex': '@Article{Westera2019ArtificialIM,\n author = {W. Westera and R. Prada and S. Mascarenhas and P. A. Santos and João Dias and Manuel Guimarães and K. Georgiadis and E. Nyamsuren and Kiavash Bahreini and Zerrin Yumak and C. Christyowidiasmoro and M. Dascalu and Gabriel Gutu-Robu and Stefan Ruseti},\n journal = {Education and Information Technologies},\n pages = {351-380},\n title = {Artificial intelligence moving serious gaming: Presenting reusable game AI components},\n volume = {25},\n year = {2019}\n}\n'}",,"{'volume': '25', 'pages': '351-380', 'name': 'Education and Information Technologies'}",97.0,Artificial intelligence moving serious gaming: Presenting reusable game AI components,2019.0
29,02d4f7b476e2e4f2055505e0efed4f665bb42df2,,"[{'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '1742930', 'name': 'E. André'}]",26.0,"{'bibtex': '@Inproceedings{Rehm2006FromAM,\n author = {M. Rehm and E. André},\n pages = {1-17},\n title = {From Annotated Multimodal Corpora to Simulated Human-Like Behaviors},\n year = {2006}\n}\n'}",,{'pages': '1-17'},41.0,From Annotated Multimodal Corpora to Simulated Human-Like Behaviors,2006.0
30,02e0421c0b986d9893b91322d1d50747b56e35ab,,"[{'authorId': '2321433', 'name': 'I. Hupont'}, {'authorId': '1680828', 'name': 'M. Chetouani'}]",13.0,"{'bibtex': '@Article{Hupont2019RegionbasedFR,\n author = {I. Hupont and M. Chetouani},\n journal = {Pattern Analysis and Applications},\n pages = {477-489},\n title = {Region-based facial representation for real-time Action Units intensity detection across datasets},\n volume = {22},\n year = {2019}\n}\n'}",,"{'volume': '22', 'pages': '477-489', 'name': 'Pattern Analysis and Applications'}",46.0,Region-based facial representation for real-time Action Units intensity detection across datasets,2019.0
31,02fb992ba57e3178cc405ac1226221dcbfd639ef,,"[{'authorId': '9760697', 'name': 'D. Chiffi'}]",20.0,"{'bibtex': '@Article{Chiffi2019SpringerHO,\n author = {D. Chiffi},\n journal = {International Studies in the Philosophy of Science},\n pages = {65 - 67},\n title = {Springer Handbook of Model-based Science},\n volume = {32},\n year = {2019}\n}\n'}",,"{'volume': '32', 'pages': '65 - 67', 'name': 'International Studies in the Philosophy of Science'}",4.0,Springer Handbook of Model-based Science,2019.0
32,031acf0078a4e1b5343939ce07acda6a7d795a07,,"[{'authorId': '2460061', 'name': 'G. Rizzolatti'}, {'authorId': '1824336', 'name': 'L. Fadiga'}, {'authorId': '2914469', 'name': 'V. Gallese'}, {'authorId': '2419400', 'name': 'L. Fogassi'}]",4538.0,"{'bibtex': '@Article{Rizzolatti1996PremotorCA,\n author = {G. Rizzolatti and L. Fadiga and V. Gallese and L. Fogassi},\n journal = {Brain research. Cognitive brain research},\n pages = {\n          131-41\n        },\n title = {Premotor cortex and the recognition of motor actions.},\n volume = {3 2},\n year = {1996}\n}\n'}",,"{'volume': '3 2', 'pages': '\n          131-41\n        ', 'name': 'Brain research. Cognitive brain research'}",59.0,Premotor cortex and the recognition of motor actions.,1996.0
33,0337bddb120d16c2a6a0793745e3dab142a1d728,,"[{'authorId': '3134697', 'name': 'C. Creed'}, {'authorId': '144189909', 'name': 'R. Beale'}]",31.0,"{'bibtex': '@Article{Creed2008PsychologicalRT,\n author = {C. Creed and R. Beale},\n journal = {Interact. Comput.},\n pages = {225-239},\n title = {Psychological responses to simulated displays of mismatched emotional expressions},\n volume = {20},\n year = {2008}\n}\n'}",,"{'volume': '20', 'pages': '225-239', 'name': 'Interact. Comput.'}",68.0,Psychological responses to simulated displays of mismatched emotional expressions,2008.0
34,0374b1b732ce2e5fba5b3d8b5ec50dbe2f02c64f,"This study sought to develop and test a measure of social presence. The networked minds battery is proposed for a broad self-report measure of social presence. An experiment was conducted to test the internal consistency and criterion validity of the six constructs as determined by theory, specifically the ability of the measure to distinguish levels of social presence between (1) face-to-face interaction and mediated interaction, and (2) different levels of mediated interaction. The confirmatory factor analysis supports a model based upon a structure six distinct factors. In criterion validity tests the measure was generally sensitive to predicted differences between face-to-face and mediated interaction. On the other hand the measure was less sensitive to differences among low affordance and high affordance media, although the differences suggesting that text rated higher on perceived message and emotional understanding may provide some insight into the communication effectiveness of print media.","[{'authorId': '34303570', 'name': 'Chad Harms'}, {'authorId': '1726689', 'name': 'F. Biocca'}]",93.0,"{'bibtex': '@Inproceedings{Harms2006InternalCA,\n author = {Chad Harms and F. Biocca},\n title = {Internal Consistency and Reliability of the Networked Minds Social Presence Measure},\n year = {2006}\n}\n'}",,,5.0,Internal Consistency and Reliability of the Networked Minds Social Presence Measure,2006.0
35,037af053a367777c6d4560a5680b91d4cbcdd57b,,"[{'authorId': '20955552', 'name': 'P. Bourgeois'}, {'authorId': '3067657', 'name': 'U. Hess'}]",401.0,"{'bibtex': '@Article{Bourgeois2008TheIO,\n author = {P. Bourgeois and U. Hess},\n journal = {Biological Psychology},\n pages = {343-352},\n title = {The impact of social context on mimicry},\n volume = {77},\n year = {2008}\n}\n'}",,"{'volume': '77', 'pages': '343-352', 'name': 'Biological Psychology'}",70.0,The impact of social context on mimicry,2008.0
36,037b59ab816c00d55bd4f03c4efd61d316a6e3b9,"Facial expressions provide an important behavioral measure for the study of emotion, cognitive processes, and social interaction. The Facial Action Coding System (Ekman & Friesen, 1978) is an objective method for quantifying facial movement in terms of component actions. We applied computer image analysis to the problem of automatically detecting facial actions in sequences of images. Three approaches were compared: holistic spatial analysis, explicit measurement of features such as wrinkles, and estimation of motion flow fields. The three methods were combined in a hybrid system that classified six upper facial actions with 91% accuracy. The hybrid system outperformed human nonexperts on this task and performed as well as highly trained experts. An automated system would make facial expression measurement more widely accessible as a research tool in behavioral science and investigations of the neural substrates of emotion.","[{'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '2072657855', 'name': 'Joseph C. Hager'}, {'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '1714528', 'name': 'T. Sejnowski'}, {'authorId': '1714528', 'name': 'T. Sejnowski'}]",442.0,"{'bibtex': '@Article{Bartlett1999MeasuringFE,\n author = {M. Bartlett and M. Bartlett and Joseph C. Hager and P. Ekman and T. Sejnowski and T. Sejnowski},\n journal = {Psychophysiology},\n pages = {\n          253-63\n        },\n title = {Measuring facial expressions by computer image analysis.},\n volume = {36 2},\n year = {1999}\n}\n'}",,"{'volume': '36 2', 'pages': '\n          253-63\n        ', 'name': 'Psychophysiology'}",66.0,Measuring facial expressions by computer image analysis.,1999.0
37,037e97f35db0d3a4730ece11a9078a1f9ee2c913,,"[{'authorId': '2268731', 'name': 'Jamil Zaki'}, {'authorId': '2076737', 'name': 'J. Davis'}, {'authorId': '2669604', 'name': 'K. Ochsner'}]",386.0,"{'bibtex': '@Article{Zaki2012OverlappingAI,\n author = {Jamil Zaki and J. Davis and K. Ochsner},\n journal = {NeuroImage},\n pages = {493-499},\n title = {Overlapping activity in anterior insula during interoception and emotional experience},\n volume = {62},\n year = {2012}\n}\n'}",,"{'volume': '62', 'pages': '493-499', 'name': 'NeuroImage'}",53.0,Overlapping activity in anterior insula during interoception and emotional experience,2012.0
38,038251b7c6ffee94181dd6584ae2a923a6b0be07,"We observed that recent state-of-the-art results on single image human pose estimation were achieved by multistage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations.1","[{'authorId': '2118199949', 'name': 'Yue Luo'}, {'authorId': '145335572', 'name': 'Jimmy S. J. Ren'}, {'authorId': '29988001', 'name': 'Zhouxia Wang'}, {'authorId': '8397576', 'name': 'Wenxiu Sun'}, {'authorId': '9416881', 'name': 'Jin-shan Pan'}, {'authorId': '2124809722', 'name': 'Jianbo Liu'}, {'authorId': '33520260', 'name': 'Jiahao Pang'}, {'authorId': '1737218', 'name': 'Liang Lin'}]",101.0,"{'bibtex': '@Article{Luo2017LSTMPM,\n author = {Yue Luo and Jimmy S. J. Ren and Zhouxia Wang and Wenxiu Sun and Jin-shan Pan and Jianbo Liu and Jiahao Pang and Liang Lin},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {5207-5215},\n title = {LSTM Pose Machines},\n year = {2017}\n}\n'}",,"{'pages': '5207-5215', 'name': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition'}",40.0,LSTM Pose Machines,2017.0
39,03c656b7010b3a13d5c50a538903141f338b08b5,"We address the difficult open problem of emulating the rich complexity of real pedestrians in urban environments. Our artificial life approach integrates motor, perceptual, behavioral, and cognitive components within a model of pedestrians as individuals. Our comprehensive model feature innovations in these components, as well as in their combination, yielding results of unprecedented fidelity and complexity for fully autonomous multi-human simulation in a large urban environment. We represent the environment using hierarchical data structures, which efficiently support the perceptual queries of the autonomous pedestrians that drive their behavioral responses and sustain their ability to plan their actions on local and global scales.","[{'authorId': '89267134', 'name': 'W. Shao'}, {'authorId': '1750924', 'name': 'Demetri Terzopoulos'}]",634.0,"{'bibtex': '@Inproceedings{Shao2005AutonomousP,\n author = {W. Shao and Demetri Terzopoulos},\n pages = {19-28},\n title = {Autonomous pedestrians},\n year = {2005}\n}\n'}",,{'pages': '19-28'},44.0,Autonomous pedestrians,2005.0
42,03d43a1f53889b26edbd65776d9939846833a5e5,"Over the past years, extensive research has been dedicated to developing robust platforms and data-driven dialog models to support long-term human-robot interactions. However, little is known about how people's perception of robots and engagement with them develop over time and how these can be accurately assessed through implicit and continuous measurement techniques. In this paper, we explore this by involving participants in three interaction sessions with multiple days of zero exposure in between. Each session consists of a joint task with a robot as well as two short social chats with it before and after the task. We measure participants' gaze patterns with a wearable eye-tracker and gauge their perception of the robot and engagement with it and the joint task using questionnaires. Results disclose that aversion of gaze in a social chat is an indicator of a robot's uncanniness and that the more people gaze at the robot in a joint task, the worse they perform. In contrast with most HRI literature, our results show that gaze toward an object of shared attention, rather than gaze toward a robotic partner, is the most meaningful predictor of engagement in a joint task. Furthermore, the analyses of gaze patterns in repeated interactions disclose that people's mutual gaze in a social chat develops congruently with their perceptions of the robot over time. These are key findings for the HRI community as they entail that gaze behavior can be used as an implicit measure of people's perception of robots in a social chat and of their engagement and task performance in a joint task.","[{'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '2047241817', 'name': 'Maike Paetzel-Prüsmann'}, {'authorId': '2047245362', 'name': 'Madelene Alanenpää'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",9.0,"{'bibtex': '@Article{Perugia2021ICS,\n author = {G. Perugia and Maike Paetzel-Prüsmann and Madelene Alanenpää and Ginevra Castellano},\n journal = {Frontiers in Robotics and AI},\n title = {I Can See It in Your Eyes: Gaze as an Implicit Cue of Uncanniness and Task Performance in Repeated Interactions With Robots},\n volume = {8},\n year = {2021}\n}\n'}",,"{'volume': '8', 'name': 'Frontiers in Robotics and AI'}",67.0,I Can See It in Your Eyes: Gaze as an Implicit Cue of Uncanniness and Task Performance in Repeated Interactions With Robots,2021.0
43,03d61a33796234b8bae5ac38de9b26c1c5ed9e2f,"An anatomical parcellation of the spatially normalized single-subject high-resolution T1 volume provided by the Montreal Neurological Institute (MNI) (D. L. Collins et al., 1998, Trans. Med. Imag. 17, 463-468) was performed. The MNI single-subject main sulci were first delineated and further used as landmarks for the 3D definition of 45 anatomical volumes of interest (AVOI) in each hemisphere. This procedure was performed using a dedicated software which allowed a 3D following of the sulci course on the edited brain. Regions of interest were then drawn manually with the same software every 2 mm on the axial slices of the high-resolution MNI single subject. The 90 AVOI were reconstructed and assigned a label. Using this parcellation method, three procedures to perform the automated anatomical labeling of functional studies are proposed: (1) labeling of an extremum defined by a set of coordinates, (2) percentage of voxels belonging to each of the AVOI intersected by a sphere centered by a set of coordinates, and (3) percentage of voxels belonging to each of the AVOI intersected by an activated cluster. An interface with the Statistical Parametric Mapping package (SPM, J. Ashburner and K. J. Friston, 1999, Hum. Brain Mapp. 7, 254-266) is provided as a freeware to researchers of the neuroimaging community. We believe that this tool is an improvement for the macroscopical labeling of activated area compared to labeling assessed using the Talairach atlas brain in which deformations are well known. However, this tool does not alleviate the need for more sophisticated labeling strategies based on anatomical or cytoarchitectonic probabilistic maps.","[{'authorId': '1398099420', 'name': 'N. Tzourio-Mazoyer'}, {'authorId': '2043846', 'name': 'B. Landeau'}, {'authorId': '3310831', 'name': 'D. Papathanassiou'}, {'authorId': '2442129', 'name': 'F. Crivello'}, {'authorId': '48751540', 'name': 'O. Etard'}, {'authorId': '2580282', 'name': 'N. Delcroix'}, {'authorId': '2946115', 'name': 'B. Mazoyer'}, {'authorId': '2188749', 'name': 'M. Joliot'}]",14324.0,"{'bibtex': '@Article{Tzourio-Mazoyer2002AutomatedAL,\n author = {N. Tzourio-Mazoyer and B. Landeau and D. Papathanassiou and F. Crivello and O. Etard and N. Delcroix and B. Mazoyer and M. Joliot},\n journal = {NeuroImage},\n pages = {273-289},\n title = {Automated Anatomical Labeling of Activations in SPM Using a Macroscopic Anatomical Parcellation of the MNI MRI Single-Subject Brain},\n volume = {15},\n year = {2002}\n}\n'}",,"{'volume': '15', 'pages': '273-289', 'name': 'NeuroImage'}",45.0,Automated Anatomical Labeling of Activations in SPM Using a Macroscopic Anatomical Parcellation of the MNI MRI Single-Subject Brain,2002.0
44,03d6502c0889b34f37a63869fe76ef6f0c8c974b,,"[{'authorId': '46646879', 'name': 'C. Keysers'}, {'authorId': '144109100', 'name': 'B. Wicker'}, {'authorId': '2091067', 'name': 'V. Gazzola'}, {'authorId': '31885367', 'name': 'J. Anton'}, {'authorId': '2419400', 'name': 'L. Fogassi'}, {'authorId': '2914469', 'name': 'V. Gallese'}]",821.0,"{'bibtex': '@Article{Keysers2004ATS,\n author = {C. Keysers and B. Wicker and V. Gazzola and J. Anton and L. Fogassi and V. Gallese},\n journal = {Neuron},\n pages = {335-346},\n title = {A Touching Sight SII/PV Activation during the Observation and Experience of Touch},\n volume = {42},\n year = {2004}\n}\n'}",,"{'volume': '42', 'pages': '335-346', 'name': 'Neuron'}",40.0,A Touching Sight SII/PV Activation during the Observation and Experience of Touch,2004.0
45,040792d23a489391dcc45be1aa0107cfae17b116,"Beyond backchannels: co-construction of dyadic stance by reciprocal reinforcement of smiles between virtual agents. Ken Prepin (ken.prepin@telecom-paristech.fr) LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau 75014 Paris, France Magalie Ochs (magalie.ochs@telecom-paristech.fr) LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau 75014 Paris, France Catherine Pelachaud (catherine.pelachaud@telecom-paristech.fr) LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau 75014 Paris, France Abstract dyadic stances can be inferred (Prepin, Ochs, & Pelachaud, 2012) from diachronic alignment between interactants. The effort of interlocutors to linguistically and non-verbally align through time is a marker of stance: it convey stance of mu- tual understanding, attention, agreement, interest and pleas- antness (Louwerse, Dale, Bard, & Jeuniaux, 2012). When two persons participate in a discussion, they not only exchange the concepts and ideas they are discussing, they also express attitudes, feelings and commitments regarding their partner: they express interpersonal stances. Endowed with backchannel model, several virtual agents are able to react to their partners’ behaviour through their non-verbal behaviour. In this paper, we go beyond this approach, proposing and test- ing a model that enables agents to express a dyadic stance, marker of effective communication: agents will naturally co- construct a shared dyadic stance if and only if their interper- sonal stance is reciprocally positive. We focus on smile, which conveys interpersonal stance and is a particularly efficient sig- nal for co-regulation of communication. With this model, a virtual agent, only capable to control its own individual pa- rameters, can, in fact, modulate and control the dyadic stance appearing when it interacts with its partner. The evaluation of the model through a user perceptive study has enabled us to validate that the dyadic stance is significantly perceived as more positive (mutual understanding, attention, agreement, in- terest, pleasantness) when reinforcement of smile is reciprocal. Keywords: dyadic interaction; interactive behaviours; dynam- ical systems; dyadic stance; smile; virtual agent; The description of stance has not only evolved toward a distinction between individual and co-constructed stance. It has also evolved from a uniquely linguistic description (DuBois, 2007; Kielsing, 2009) to a description implying in- teractants’ Non-Verbal Behaviours (NVBs) (Scherer, 2005; Prepin et al., 2012). The non-verbal behaviours participate in maintaining contact between interactants and facilitate ver- bal exchange: they are an integral part of the communication process (Paradowski, 2011). NVBs actively convey stances through paralinguistic features (such as tone of voice, dura- tion, loudness or prosody), facial expressions, and postures (Chindamo et al., 2012). Introduction When we consider verbal communication, interlocutors not only exchange the concepts and ideas which constitute the subject of their discussion, they also express feelings, judge- ments or commitments regarding this subject. This “atti- tude which, for some time, is expressed and sustained in- teractively in communication, in a unimodal or multi-modal manner” corresponds to the stance: Chindamo, Allwood, and Ahls´en (2012) review the existing definitions and descriptions of stance; they show how these definitions have evolved from a focus on individual expression of stance to a more interac- tive and social description. Individual stance refers to two types of stance: epistemic and interpersonal stance (Kielsing, 2009). The epistemic stance is the expression of the rela- tionship of a person to his/her own talk (for instance “cer- tain”). The interpersonal stances convey the relationship of a person to the interlocutor (for example “warm” or “polite”). Moreover, during an interaction, “stances are constructed across turns rather than being the product of a single turn” (Chindamo et al., 2012). When interactants with individ- ual epistemic and interpersonal stances are put in presence, Models of interactive agents have mainly explored the au- tomatic generation of virtual agent’s behaviour aligned on the interlocutor’s behaviour. Buschmeier, S., and Kopp (2010) combine a model of lexical alignment with a model gener- ating behaviours based on linguistic information. Bailenson and Yee (2005) model the NVBs alignment of a speaking virtual agent to a listening human. They propose a Digital Chameleon (in reference to the Chameleon effect described by Chartrand and Bargh (1999)). Bevacqua, Hyniewska, and Pelachaud (2010) model the NVBs alignment of a listen- ing agent to a speaking human: they propose a model of backchannels, i.e. NVBs aligned in time and nature, to fa- cilitate human users to tell a story. All these models focus on the adaptation of the virtual agent to its interlocutor, but do not take into account the recip- rocal adaptation of this interlocutor: behaviours are computed in reaction to partner’s behaviour, but not in interaction with partner’s behaviour; the dynamical coupling associated to the mutual engagement of interactants is not modelled, and crit- ical parameters of interaction such as synchrony and align- ment which appear as side effects of this coupling (Paolo,","[{'authorId': '2181003', 'name': 'K. Prepin'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",23.0,"{'bibtex': '@Article{Prepin2013BeyondBC,\n author = {K. Prepin and M. Ochs and C. Pelachaud},\n journal = {Cognitive Science},\n title = {Beyond backchannels: co-construction of dyadic stancce by reciprocal reinforcement of smiles between virtual agents},\n volume = {35},\n year = {2013}\n}\n'}",,"{'volume': '35', 'name': 'Cognitive Science'}",23.0,Beyond backchannels: co-construction of dyadic stancce by reciprocal reinforcement of smiles between virtual agents,2013.0
46,041326c202655cd60df276bf7a148f2ecddfc479,,"[{'authorId': '1713919', 'name': 'P. Langley'}, {'authorId': '1715438', 'name': 'J. Laird'}, {'authorId': '144476811', 'name': 'Seth Rogers'}]",710.0,"{'bibtex': '@Article{Langley2009CognitiveAR,\n author = {P. Langley and J. Laird and Seth Rogers},\n journal = {Cognitive Systems Research},\n pages = {141-160},\n title = {Cognitive architectures: Research issues and challenges},\n volume = {10},\n year = {2009}\n}\n'}",,"{'volume': '10', 'pages': '141-160', 'name': 'Cognitive Systems Research'}",104.0,Cognitive architectures: Research issues and challenges,2009.0
48,043248200e65e6be90de7136512dab672a526eba,,"[{'authorId': '35033593', 'name': 'M. Fabri'}, {'authorId': '2111741', 'name': 'S. Y. A. Elzouki'}, {'authorId': '71268802', 'name': 'D. Moore'}]",67.0,"{'bibtex': '@Inproceedings{Fabri2007EmotionallyEA,\n author = {M. Fabri and S. Y. A. Elzouki and D. Moore},\n pages = {275-285},\n title = {Emotionally Expressive Avatars for Chatting, Learning and Therapeutic Intervention},\n year = {2007}\n}\n'}",,{'pages': '275-285'},40.0,"Emotionally Expressive Avatars for Chatting, Learning and Therapeutic Intervention",2007.0
49,046795f142bfe550d53de6056458d7f4ec962d4f,"Objectives: Immersive virtual reality has tremendous potential to improve cognition in populations with cognitive impairment. We conducted a feasibility and proof-of-concept study to assess the potential of virtual reality and electroencephalography, with or without an intelligent agent, that adapts the presented material to the emotions elicited by the environment. Method: Older adults with subjective cognitive decline recruited from the community received a virtual reality-based intervention taking place in one of two virtual environments, a train (Part 1, N = 19) or a music theatre, complemented by the intelligent agent (Part 2, N = 19). A comparative control group (N = 19) receiving no intervention was also included. All participants completed measures of affect and cognition before and after the intervention. The intervention groups completed measures of cybersickness and user experience after the intervention. Results: Participants did not suffer from increased cybersickness following either intervention. They also reported a positive to highly positive user experience concerning the following aspects: attractivity, hedonic quality-identity and hedonic quality-stimulation. The measures of affect showed no pre-post change when comparing either intervention to the control condition. However, a reduction of negative affect was observed following the train intervention for participants with a high self-reported negative affect at baseline. Finally, there was a significant improvement in working memory when comparing either intervention group to the control condition. Conclusion: Our results support the feasibility and tolerability of the technology, and a positive impact on cognition, paving the way for a larger-scale randomized clinical trial to confirm efficacy.","[{'authorId': '47258254', 'name': 'M. Cuesta'}, {'authorId': '2017917021', 'name': 'Lynn Valeyry Verty'}, {'authorId': '28987363', 'name': 'H. Abdessalem'}, {'authorId': '68990793', 'name': 'A. Byrns'}, {'authorId': '34277926', 'name': 'M. Bruneau'}, {'authorId': '1788058', 'name': 'C. Frasson'}, {'authorId': '145580293', 'name': 'S. Belleville'}]",1.0,"{'bibtex': '@Article{Cuesta2022VirtualRA,\n author = {M. Cuesta and Lynn Valeyry Verty and H. Abdessalem and A. Byrns and M. Bruneau and C. Frasson and S. Belleville},\n booktitle = {Frontiers in Virtual Reality},\n title = {Virtual Reality and EEG-Based Intelligent Agent in Older Adults With Subjective Cognitive Decline: A Feasibility Study for Effects on Emotion and Cognition},\n volume = {2},\n year = {2022}\n}\n'}","[{'paperId': '429e7a6f11b9d65f67ca3df4250baf2f9ee02d17', 'title': 'In too deep? A systematic literature review of fully-immersive virtual reality and cybersickness among older adults.'}]",{'volume': '2'},45.0,Virtual Reality and EEG-Based Intelligent Agent in Older Adults With Subjective Cognitive Decline: A Feasibility Study for Effects on Emotion and Cognition,2022.0
50,0470202ebe4fc43048f729a3aa4a90805f4bd8ea,"Providing interactive control is a hot topic in crowd-navigation research. Here, the authors propose a simple but effective way for authoring a crowd scene. With their governing tool, users can easily drive the flow of crowds by sketching velocities on anchor points in the scene. This approach is fast enough to allow on-the-fly modification of vector fields.","[{'authorId': '144240366', 'name': 'Xiaogang Jin'}, {'authorId': '2154719265', 'name': 'Jiayi Xu'}, {'authorId': '50097074', 'name': 'Charlie C. L. Wang'}, {'authorId': '2110204524', 'name': 'Shengsheng Huang'}, {'authorId': '2155661615', 'name': 'Jun Zhang'}]",59.0,"{'bibtex': '@Article{Jin2008InteractiveCO,\n author = {Xiaogang Jin and Jiayi Xu and Charlie C. L. Wang and Shengsheng Huang and Jun Zhang},\n journal = {IEEE Computer Graphics and Applications},\n title = {Interactive Control of Large-Crowd Navigation in Virtual Environments Using Vector Fields},\n volume = {28},\n year = {2008}\n}\n'}",,"{'volume': '28', 'name': 'IEEE Computer Graphics and Applications'}",16.0,Interactive Control of Large-Crowd Navigation in Virtual Environments Using Vector Fields,2008.0
51,0475cbed8c8fa313f69d6c673502308cde35c9d1,"Environmental psychology, though a fast-growing field, is one of the most difficult to fit into the confines of scientific inquiry. Measuring such subjective data as reactions to color, heat, light, and sound would seem to be an almost impossible task; indeed, until now there has been no theory around which the research in this field could be organized. This volume represents a preliminary effort to identify the relevant variables involved and fit them into a systematic framework. Furthermore, it presents extensive sets of measures for investigating the theory and implementing it in a variety of everyday environments.Basically, the framework outlined here proposes that environmental stimuli are linked to behavioral responses by the primary emotional responses of arousal, pleasure, and dominance. By considering the impact of the environment on these basic emotional responses, the effects of diverse stimulus components within or across sense modalities can be readily compared. An additional concept, information rate, is used to compare the effects of different environments, each with stimulation in many sense modalities. In the final chapters the authors present a series of hypotheses which relate the emotional response variables to a diversity of behaviors such as physical approach, performance, affiliation, and verbally or nonverbally expressed preference.","[{'authorId': '144102217', 'name': 'A. Mehrabian'}, {'authorId': '46367714', 'name': 'J. Russell'}]",5903.0,"{'bibtex': '@Inproceedings{Mehrabian1974AnAT,\n author = {A. Mehrabian and J. Russell},\n title = {An approach to environmental psychology},\n year = {1974}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,An approach to environmental psychology,1974.0
52,05078d92622357cbaec551dcde8b8f57facc9ffb,,"[{'authorId': '3065464', 'name': 'A. V. D. Pütten'}, {'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '3689632', 'name': 'Laura Hoffmann'}, {'authorId': '3163828', 'name': 'Sabrina Sobieraj'}, {'authorId': '2374750', 'name': 'S. Eimler'}]",221.0,"{'bibtex': '@Article{Pütten2013AnES,\n author = {A. V. D. Pütten and N. Krämer and Laura Hoffmann and Sabrina Sobieraj and S. Eimler},\n journal = {International Journal of Social Robotics},\n pages = {17-34},\n title = {An Experimental Study on Emotional Reactions Towards a Robot},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '17-34', 'name': 'International Journal of Social Robotics'}",102.0,An Experimental Study on Emotional Reactions Towards a Robot,2013.0
53,050bfb1e475de1de425949800d25d3e0cbbcca62,"The context of this work is the search for realism and believability of Virtual Humans. Our contribution to achieve this goal is to enable Virtual Humans (VH) to react to spontaneous events in virtual environments (VE). In order to reflect the individuality of each VH, these reactions have to be expressive and unique. In this paper we present firstly a model of reaction based on personality traits. The model was defined using statistical analysis of real people reacting to unexpected events. We also consider that the emotional state is involved in the modulation of reactions, thus we integrate a model of emotion update. Secondly, we present a semantic-based methodology to compose reactive animation sequences using inverse kinematics (IK) and key frame (KF) interpolation animation techniques. Finally, we present an application that demonstrates how Virtual Humans can produce different movements as reaction to unexpected stimuli, depending on their personality traits and emotional state.","[{'authorId': '66173005', 'name': 'Alejandra García-Rojas'}, {'authorId': '144901415', 'name': 'M. Gutiérrez'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}]",16.0,"{'bibtex': '@Inproceedings{García-Rojas2008SimulationOI,\n author = {Alejandra García-Rojas and M. Gutiérrez and D. Thalmann},\n pages = {143-150},\n title = {Simulation of individual spontaneous reactive behavior},\n year = {2008}\n}\n'}",,{'pages': '143-150'},31.0,Simulation of individual spontaneous reactive behavior,2008.0
54,051722a89ea75583cfe58f6d218db373776b4c6e,"(1941) argues that the sole criterion for excellent research is that the researcher produces "" beauty. "" While seemingly ineffable and frustratingly imprecise, Hardy instead suggests that creating beauty is straightforward. First, the work must be accurate: erroneous results are useless. Second, one's peers must recognize the work to be interesting, exciting, elegant, and "" cool. "" While this second criterion might seem arbitrary, there is generally good agreement between scholars in a given community about "" interesting "" work (see Cole and Cole 1973 for a discussion), so one need not survey numerous researchers to ensure research is beautiful; asking a couple is equivalent to asking them all. With certain caveats, the work in embodied conversational agents (ECA) can make claims to beauty. ECAs are phenomenologically "" accurate "" to the extent that the agent's outward appearance objectively matches the appearance, language, attitudes and behavior of humans. Thus, questions that address manifestation accuracy include "" Does the agent walk like a person walk? "" and "" Does the agent use language and make grammatical errors the same way a person does? "" An alternative approach to accuracy, generally associated more with the artificial intelligence literature than with the ECA literature, assesses the extent to which the processes that produce aspects of the ECA are the same as the processes in humans. For example, "" Does the muscle model of the character match how human muscles work? "" or","[{'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '1740889', 'name': 'K. Isbister'}, {'authorId': '47266165', 'name': 'Eun-Ju Lee'}]",266.0,"{'bibtex': '@Inproceedings{Nass2001TruthIB,\n author = {C. Nass and K. Isbister and Eun-Ju Lee},\n pages = {374-402},\n title = {Truth is beauty: researching embodied conversational agents},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '374-402', 'name': ''}",36.0,Truth is beauty: researching embodied conversational agents,2001.0
56,051c598c616a533be31edae7481b97f20194895a,"The ability to detect agency is fundamental for understanding the social world. Underlying this capacity are neural circuits that respond to patterns of intentional biological motion in the superior temporal sulcus and temporoparietal junction. Here we show that the brain's blood oxygenation level dependent (BOLD) response to such motion is modulated by the representation of the actor. Dynamic social interactions were portrayed by either live-action agents or computer-animated agents, enacting the exact same patterns of biological motion. Using an event-related design, we found that the BOLD response associated with the perception and interpretation of agency was greater when identical physical movements were performed by real rather than animated agents. This finding has important implications for previous work on biological motion that has relied upon computer-animated stimuli and demonstrates that the neural substrates of social perception are finely tuned toward real-world agents. In addition, the response in lateral temporal areas was observed in the absence of instructions to make mental inferences, thus demonstrating the spontaneous implementation of the intentional stance.","[{'authorId': '1829071', 'name': 'R. Mar'}, {'authorId': '144179765', 'name': 'W. M. Kelley'}, {'authorId': '2681278', 'name': 'T. Heatherton'}, {'authorId': '145929749', 'name': 'C. Macrae'}]",88.0,"{'bibtex': '@Article{Mar2007DetectingAF,\n author = {R. Mar and W. M. Kelley and T. Heatherton and C. Macrae},\n journal = {Social cognitive and affective neuroscience},\n pages = {\n          199-205\n        },\n title = {Detecting agency from the biological motion of veridical vs animated agents.},\n volume = {2 3},\n year = {2007}\n}\n'}",,"{'volume': '2 3', 'pages': '\n          199-205\n        ', 'name': 'Social cognitive and affective neuroscience'}",48.0,Detecting agency from the biological motion of veridical vs animated agents.,2007.0
57,0534432de675f5a44335a3727a15a1f69fed09b9,"Background: The facial affective behavior of traumatized patients and of a healthy control group was compared. Sampling and Methods: Data of 15 videotaped clinical interviews of traumatized inpatients and of 15 healthy women (absence of mental/psychiatric disorder according to ICD-10) were ascertained. The affective facial expression of both groups was coded with the Emotional Facial Acting Coding System. Afterwards, the mimic analysis was correlated with gazing behavior and the emotional experience. The patients reported their traumatic experiences and the healthy women their main complaints. Results: The traumatized patients showed neither a reduction of overall facial expressions nor a reduced frequency of facial affects in comparison to the healthy control group. The control group, however, showed significantly more ‘genuine joy’. The traumatized patients showed significantly more anger. Conclusions: The traumatized patients did not show a significant reduction of overall facial expression. A more detailed analysis showed that on the one hand, stabilizing elements of relationships, such as genuine joy, appear significantly less on the face of traumatized patients as compared with the healthy women. On the other hand, the expression of anger was brought into the relationship significantly more often by the traumatized patients during face-to-face interaction (clinical interview). This indicates the importance of distance regulation interaction patterns of traumatized patients.","[{'authorId': '40005661', 'name': 'A. Kirsch'}, {'authorId': '48572086', 'name': 'S. Brunnhuber'}]",34.0,"{'bibtex': '@Article{Kirsch2007FacialEA,\n author = {A. Kirsch and S. Brunnhuber},\n journal = {Psychopathology},\n pages = {296 - 302},\n title = {Facial Expression and Experience of Emotions in Psychodynamic Interviews with Patients with PTSD in Comparison to Healthy Subjects},\n volume = {40},\n year = {2007}\n}\n'}",,"{'volume': '40', 'pages': '296 - 302', 'name': 'Psychopathology'}",36.0,Facial Expression and Experience of Emotions in Psychodynamic Interviews with Patients with PTSD in Comparison to Healthy Subjects,2007.0
58,053eab53e12c4c65f97d3fc41bdb8f71bc6334a1,"Zara, or ‘Zara the Supergirl’, is a virtual robot that can show empathy while interacting with an user, and at the end of a 5-10 minute conversation, it can give a personality analysis based on the user responses. It can display and share emotions with the aid of its built in sentiment analysis, facial and emotion recognition, and speech module. Being the first of its kind, it has successfully integrated an empathetic system along with the human emotion recognition and sharing, into an augmented humanrobot interaction system. Zara was also displayed at the World Economic Forum held at Dalian in September 2015.","[{'authorId': '1683412', 'name': 'Pascale Fung'}, {'authorId': '2198200', 'name': 'Anik Dey'}, {'authorId': '3407465', 'name': 'Farhad Bin Siddique'}, {'authorId': '2068166503', 'name': 'Ruixi Lin'}, {'authorId': '2152916959', 'name': 'Yang Yang'}, {'authorId': '2075389340', 'name': 'Yan Wan'}, {'authorId': '1748955', 'name': 'R. Chan'}]",2.0,"{'bibtex': '@Article{Fung2016ZaraAE,\n author = {Pascale Fung and Anik Dey and Farhad Bin Siddique and Ruixi Lin and Yang Yang and Yan Wan and R. Chan},\n booktitle = {Interspeech},\n pages = {1176-1177},\n title = {Zara: An Empathetic Interactive Virtual Agent},\n year = {2016}\n}\n'}","[{'paperId': '704ba417c38aff41a942a1aceee1bf5db8cba0ac', 'title': 'Exploring Perceived Emotional Intelligence of Personality-Driven Virtual Agents in Handling User Challenges'}, {'paperId': '8eb761f8569245e0a8bab78542cd898b3ed3a764', 'title': 'Empathy and virtual agents for learning applications in symbiotic systems'}]",{'pages': '1176-1177'},13.0,Zara: An Empathetic Interactive Virtual Agent,2016.0
59,053f4a06847ba6ea551eaf7ef1fb5bda9f64108f,,"[{'authorId': '144427848', 'name': 'D. McColl'}, {'authorId': '32418659', 'name': 'A. Hong'}, {'authorId': '2094257416', 'name': 'Naoaki Hatakeyama'}, {'authorId': '2497882', 'name': 'G. Nejat'}, {'authorId': '1719617', 'name': 'B. Benhabib'}]",86.0,"{'bibtex': '@Article{McColl2016ASO,\n author = {D. McColl and A. Hong and Naoaki Hatakeyama and G. Nejat and B. Benhabib},\n journal = {Journal of Intelligent & Robotic Systems},\n pages = {101-133},\n title = {A Survey of Autonomous Human Affect Detection Methods for Social Robots Engaged in Natural HRI},\n volume = {82},\n year = {2016}\n}\n'}",,"{'volume': '82', 'pages': '101-133', 'name': 'Journal of Intelligent & Robotic Systems'}",221.0,A Survey of Autonomous Human Affect Detection Methods for Social Robots Engaged in Natural HRI,2016.0
60,0559485c0b5eff8e7c4f3395b655b96cbc8ef884,"Current evidence suggests that older adults are less accurate than young adults in their ability to identify facial expressions of emotion. In the present study, young and older adults' ability to correctly recognize facial affect representative of 6 different emotions (happiness, surprise, disgust, fear, anger, and sadness) was examined in 3 conditions varying in difficulty. Task difficulty was measured by varying the number of labels available in a forced choice recognition task to 2, 4, and 6. Results showed that age differences were present in the 2 more difficult conditions for fear and sadness. Older adults were impaired in recognizing facial expressions of surprise only in the 4-label condition. Current findings suggest that task difficulty moderates age differences in emotion labeling. The present study has contributed to previous research by illuminating the conditions under which age differences in the accuracy of labeling of facial affect are more likely to be observed.","[{'authorId': '4828478', 'name': 'V. Orgeta'}]",46.0,"{'bibtex': '@Article{Orgeta2010EffectsOA,\n author = {V. Orgeta},\n journal = {The journals of gerontology. Series B, Psychological sciences and social sciences},\n pages = {\n          323-7\n        },\n title = {Effects of age and task difficulty on recognition of facial affect.},\n volume = {65B 3},\n year = {2010}\n}\n'}",,"{'volume': '65B 3', 'pages': '\n          323-7\n        ', 'name': 'The journals of gerontology. Series B, Psychological sciences and social sciences'}",16.0,Effects of age and task difficulty on recognition of facial affect.,2010.0
61,0597ad1f89504f1239da80588e70619d93f3dced,,"[{'authorId': '1398652551', 'name': 'J. Marco-Pallarés'}, {'authorId': '46360447', 'name': 'T. Münte'}, {'authorId': '1381777734', 'name': 'A. Rodríguez-Fornells'}]",97.0,"{'bibtex': '@Article{Marco-Pallarés2015TheRO,\n author = {J. Marco-Pallarés and T. Münte and A. Rodríguez-Fornells},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {1-7},\n title = {The role of high-frequency oscillatory activity in reward processing and learning},\n volume = {49},\n year = {2015}\n}\n'}",,"{'volume': '49', 'pages': '1-7', 'name': 'Neuroscience & Biobehavioral Reviews'}",90.0,The role of high-frequency oscillatory activity in reward processing and learning,2015.0
62,05e0f92d03b7c15d888a2160b20c69f0964da725,"Anthropomorphism describes the tendency to imbue the real or imagined behavior of nonhuman agents with humanlike characteristics, motivations, intentions, or emotions. Although surprisingly common, anthropomorphism is not invariant. This article describes a theory to explain when people are likely to anthropomorphize and when they are not, focused on three psychological determinants--the accessibility and applicability of anthropocentric knowledge (elicited agent knowledge), the motivation to explain and understand the behavior of other agents (effectance motivation), and the desire for social contact and affiliation (sociality motivation). This theory predicts that people are more likely to anthropomorphize when anthropocentric knowledge is accessible and applicable, when motivated to be effective social agents, and when lacking a sense of social connection to other humans. These factors help to explain why anthropomorphism is so variable; organize diverse research; and offer testable predictions about dispositional, situational, developmental, and cultural influences on anthropomorphism. Discussion addresses extensions of this theory into the specific psychological processes underlying anthropomorphism, applications of this theory into robotics and human-computer interaction, and the insights offered by this theory into the inverse process of dehumanization.","[{'authorId': '7007014', 'name': 'Nicholas Epley'}, {'authorId': '3377580', 'name': 'A. Waytz'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}]",1980.0,"{'bibtex': '@Article{Epley2007OnSH,\n author = {Nicholas Epley and A. Waytz and J. Cacioppo},\n journal = {Psychological review},\n pages = {\n          864-86\n        },\n title = {On seeing human: a three-factor theory of anthropomorphism.},\n volume = {114 4},\n year = {2007}\n}\n'}",,"{'volume': '114 4', 'pages': '\n          864-86\n        ', 'name': 'Psychological review'}",286.0,On seeing human: a three-factor theory of anthropomorphism.,2007.0
63,06041d2a5e0fdc38d38a0b5c93f5fb4a125698ec,"The goal of the present review is to explain how immersive virtual environment technology (IVET) can be used for the study of social interactions and how the use of virtual humans in immersive virtual environments can advance research and application in many different fields. Researchers studying individual differences in social interactions are typically interested in keeping the behavior and the appearance of the interaction partner constant across participants. With IVET researchers have full control over the interaction partners, can standardize them while still keeping the simulation realistic. Virtual simulations are valid: growing evidence shows that indeed studies conducted with IVET can replicate some well-known findings of social psychology. Moreover, IVET allows researchers to subtly manipulate characteristics of the environment (e.g., visual cues to prime participants) or of the social partner (e.g., his/her race) to investigate their influences on participants’ behavior and cognition. Furthermore, manipulations that would be difficult or impossible in real life (e.g., changing participants’ height) can be easily obtained with IVET. Beside the advantages for theoretical research, we explore the most recent training and clinical applications of IVET, its integration with other technologies (e.g., social sensing) and future challenges for researchers (e.g., making the communication between virtual humans and participants smoother).","[{'authorId': '5983047', 'name': 'Dario Bombari'}, {'authorId': '2284257', 'name': 'M. Schmid Mast'}, {'authorId': '4036594', 'name': 'Elena Cañadas'}, {'authorId': '2056606805', 'name': 'Manuel Bachmann'}]",102.0,"{'bibtex': '@Article{Bombari2015StudyingSI,\n author = {Dario Bombari and M. Schmid Mast and Elena Cañadas and Manuel Bachmann},\n journal = {Frontiers in Psychology},\n title = {Studying social interactions through immersive virtual environment technology: virtues, pitfalls, and future challenges},\n volume = {6},\n year = {2015}\n}\n'}",,"{'volume': '6', 'name': 'Frontiers in Psychology'}",83.0,"Studying social interactions through immersive virtual environment technology: virtues, pitfalls, and future challenges",2015.0
64,062360fa270e2b30739363f5a219114d9a1c2a9f,,"[{'authorId': '82301100', 'name': 'A. Winkel'}]",146.0,"{'bibtex': '@Inproceedings{Winkel2016EyeTM,\n author = {A. Winkel},\n title = {Eye Tracking Methodology Theory And Practice},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Eye Tracking Methodology Theory And Practice,2016.0
65,0629b3489ad2e67211691ed9d07c0fd0ca1b5a73,"This study aims to investigate cultural differences in recognition accuracy as well as the in-group advantage hypothesis for emotion recognition among sub-Saharan African, Chinese, and French Canadian individuals living in Canada. The participants viewed expressions of happiness, anger, sadness, fear, disgust, and shame selected from the Montreal Set of Facial Displays of Emotion. These data did not support the in-group advantage hypothesis under the condition of stimulus equivalence. However, both encoder and decoder effects were found. Specifically, French Canadians were more accurate for the decoding of expressions of shame and sadness. Moreover, fear expressions were best recognized when shown by sub-Saharan Africans, suggesting an effect of salience of expressive cues due to morphological features of the face.","[{'authorId': '5868984', 'name': 'Martin G. Beaupré'}, {'authorId': '3067657', 'name': 'U. Hess'}]",285.0,"{'bibtex': '@Article{Beaupré2005CrossCulturalER,\n author = {Martin G. Beaupré and U. Hess},\n journal = {Journal of Cross-Cultural Psychology},\n pages = {355 - 370},\n title = {Cross-Cultural Emotion Recognition among Canadian Ethnic Groups},\n volume = {36},\n year = {2005}\n}\n'}",,"{'volume': '36', 'pages': '355 - 370', 'name': 'Journal of Cross-Cultural Psychology'}",35.0,Cross-Cultural Emotion Recognition among Canadian Ethnic Groups,2005.0
66,062be145b254cc7b41f92fbf32df0de09de2e2a9,"Animated characters are frequently used in television programs, movies, and video games, but relatively little is known about how their characteristics affect attention and viewer opinions. We used eyetracking and questionnaires to examine the role of visual complexity and animation style on viewing patterns and ratings of video-recorded and animated movie clips. We created videos of an actress performing and describing a series of actions with blocks. Of the videos, one set included regular HD recordings of the actress. The remaining video sets were animated using motion capture data from that actress for three characters: realistic, cartoon, and robot. Increased facial looking time correlated with unpleasantness ratings for individual characters and clips, determining that animation styles have an effect on both viewing patterns and audience members' subjective opinions of characters. In addition, the method described in this paper can expand future research on character animation.","[{'authorId': '32134175', 'name': 'E. Carter'}, {'authorId': '30303590', 'name': 'Moshe Mahler'}, {'authorId': '1788773', 'name': 'J. Hodgins'}]",20.0,"{'bibtex': '@Article{Carter2013UnpleasantnessOA,\n author = {E. Carter and Moshe Mahler and J. Hodgins},\n journal = {Proceedings of the ACM Symposium on Applied Perception},\n title = {Unpleasantness of animated characters corresponds to increased viewer attention to faces},\n year = {2013}\n}\n'}",,{'name': 'Proceedings of the ACM Symposium on Applied Perception'},26.0,Unpleasantness of animated characters corresponds to increased viewer attention to faces,2013.0
67,062ccad7d8882c662525b884c9fcd83f46839999,We present an adaptive data-driven algorithm for interactive crowd simulation. Our approach combines realistic trajectory behaviors extracted from videos with synthetic multi-agent algorithms to generate plausible simulations. We use statistical techniques to compute the movement patterns and motion dynamics from noisy 2D trajectories extracted from crowd videos. These learned pedestrian dynamic characteristics are used to generate collision-free trajectories of virtual pedestrians in slightly different environments or situations. The overall approach is robust and can generate perceptually realistic crowd movements at interactive rates in dynamic environments. We also present results from preliminary user studies that evaluate the trajectory behaviors generated by our algorithm.,"[{'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '10817944', 'name': 'A. Best'}, {'authorId': '3428200', 'name': 'Rohan Chabra'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",45.0,"{'bibtex': '@Article{Kim2016InteractiveAA,\n author = {Sujeong Kim and Aniket Bera and A. Best and Rohan Chabra and Dinesh Manocha},\n journal = {2016 IEEE Virtual Reality (VR)},\n pages = {29-38},\n title = {Interactive and adaptive data-driven crowd simulation},\n year = {2016}\n}\n'}",,"{'pages': '29-38', 'name': '2016 IEEE Virtual Reality (VR)'}",48.0,Interactive and adaptive data-driven crowd simulation,2016.0
68,0632ed2d7a9830bb3b09d8b2124b08a00b34adc3,,"[{'authorId': '1809661', 'name': 'A. Kołakowska'}, {'authorId': '2414357', 'name': 'A. Landowska'}, {'authorId': '3271448', 'name': 'M. Szwoch'}, {'authorId': '3175073', 'name': 'W. Szwoch'}, {'authorId': '34487194', 'name': 'M. Wróbel'}]",98.0,"{'bibtex': '@Article{Kołakowska2014EmotionRA,\n author = {A. Kołakowska and A. Landowska and M. Szwoch and W. Szwoch and M. Wróbel},\n journal = {Advances in intelligent systems and computing},\n pages = {51-62},\n title = {Emotion Recognition and Its Applications},\n volume = {300},\n year = {2014}\n}\n'}",,"{'volume': '300', 'pages': '51-62', 'name': 'Advances in intelligent systems and computing'}",20.0,Emotion Recognition and Its Applications,2014.0
69,064887d3ca4532602c1212387d63e741a32ea909,,"[{'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '1699200', 'name': 'N. Badler'}]",21.0,"{'bibtex': '@Inproceedings{Allbeck2000TowardsBC,\n author = {J. Allbeck and N. Badler},\n pages = {191-205},\n title = {Towards Behavioral Consistency in Animated Agents},\n year = {2000}\n}\n'}",,{'pages': '191-205'},39.0,Towards Behavioral Consistency in Animated Agents,2000.0
70,0651fe99cc587614c660d9d962584b8265c22807,"This paper presents work on an artificial anthropomorphic agent with multimodal interaction abilitities. It focuses on the development of a markup language, MURML, that bridges between the planning and the animation tasks in the production of multimodal utterances. This hierarchically structured notation provides flexible means of describing gestures in a form-based way and of explicitly experessing their relations to accompanying speech.","[{'authorId': '2165508', 'name': 'Alfred Kranstedt'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",109.0,"{'bibtex': '@Inproceedings{Kranstedt2002MURMLAM,\n author = {Alfred Kranstedt and S. Kopp and I. Wachsmuth},\n title = {MURML: A Multimodal Utterance Representation Markup Language for Conversational Agents},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,MURML: A Multimodal Utterance Representation Markup Language for Conversational Agents,2002.0
71,0672092ecc507fb41d81e82d2986cf86c4bff14f,"Traditional models of decision making do not take into account many critical aspects of operational settings, as described in Chapter 1. Deci­ sion makers in operational settings are usually very experienced, in contrast to the naive subjects used in laboratory studies. In this chap­ ter I present a recognitional model of decision making that shows how people can use experience to avoid some of the limitations of analytical strategies. This model explains how people can make decisions without having to compare options. It fuses two processes-situation assess· ment and mental simulation-and asserts that people Wle situation assessment to generate a plausible course of action and use mental simulation to evaluate that course of action. I believe this recognition. al model describes how decision making is usually carried out in real­ world settings. This conclusion is based on a series of studies in which it was found that recognitional decision malting is much more common than analytical decision making. Finally, I contrast the strengths and weaknesses of recognitional and analytical decision strategies.","[{'authorId': '145320331', 'name': 'Gary Klein'}]",1172.0,"{'bibtex': '@Inproceedings{Klein1993ARD,\n author = {Gary Klein},\n title = {A recognition-primed decision (RPD) model of rapid decision making.},\n year = {1993}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,A recognition-primed decision (RPD) model of rapid decision making.,1993.0
72,0693cecba234b591169fdee7718116b4eb76fdca,"The last decade has witnessed considerable interest in the investigation of the affective dimensions of learning and in the development of advanced learning technologies that automatically detect and respond to student affect. Identifying the affective states that students experience in technology-enhanced learning contexts is a fundamental question in this area. This article provides an initial attempt to answer this question with a selective meta-analysis of 24 studies that utilized a mixture of methodologies (online self-reports, online observations, emote-aloud protocols, cued recall) and affect judges (students themselves, untrained peers, trained judges) for fine-grained monitoring of 14 discrete affective states of 1,740 middle school, high school, college, and adult students in 5 countries. Affective states occurred over the course of interactions with a range of learning technologies, including intelligent tutoring systems, serious games, simulation environments, and simple computer interfaces. Standardized effect sizes of relative frequency, computed by comparing the proportional occurrence of each affective state to the other states in each study, were modeled with random-effects models. Engagement/flow was consistently found to be relatively frequent (d+ = 2.5), and contempt, anger, disgust, sadness, anxiety, delight, fear, and surprise were consistently infrequent, with d+ ranging from −6.5 to −0.78. Effects for boredom (d+ = 0.19), confusion (d+ = 0.12), curiosity (d+ = −0.10), happiness (d+ = −0.13), and frustration (d+ = −2.5) varied substantially across studies. Mixed-effects models indicated that the source of the affect judgments (self vs. observers) and the authenticity of the learning contexts (classroom vs. laboratory) accounted for greater heterogeneity than the use of advanced learning technologies and training time. Theoretical and applied implications of the findings are discussed.","[{'authorId': '1383996606', 'name': 'S. D’Mello'}]",238.0,"{'bibtex': '@Article{D’Mello2013ASM,\n author = {S. D’Mello},\n journal = {Journal of Educational Psychology},\n pages = {1082-1099},\n title = {A selective meta-analysis on the relative incidence of discrete affective states during learning with technology},\n volume = {105},\n year = {2013}\n}\n'}",,"{'volume': '105', 'pages': '1082-1099', 'name': 'Journal of Educational Psychology'}",102.0,A selective meta-analysis on the relative incidence of discrete affective states during learning with technology,2013.0
73,069443f2bbeb2deedefb600c82ed59cde2137e60,"The interaction between human beings and computers will be more natural if computers are able to perceive and respond to human non-verbal communication such as emotions. Although several approaches have been proposed to recognize human emotions based on facial expressions or speech, relatively limited work has been done to fuse these two, and other, modalities to improve the accuracy and robustness of the emotion recognition system. This paper analyzes the strengths and the limitations of systems based only on facial expressions or acoustic information. It also discusses two approaches used to fuse these two modalities: decision level and feature level integration. Using a database recorded from an actress, four emotions were classified: sadness, anger, happiness, and neutral state. By the use of markers on her face, detailed facial motions were captured with motion capture, in conjunction with simultaneous speech recordings. The results reveal that the system based on facial expression gave better performance than the system based on just acoustic information for the emotions considered. Results also show the complementarily of the two modalities and that when these two modalities are fused, the performance and the robustness of the emotion recognition system improve measurably.","[{'authorId': '2106794', 'name': 'C. Busso'}, {'authorId': '145140508', 'name': 'Z. Deng'}, {'authorId': '1936287', 'name': 'S. Yıldırım'}, {'authorId': '38816202', 'name': 'M. Bulut'}, {'authorId': '2118656349', 'name': 'C. Lee'}, {'authorId': '1764265', 'name': 'Ebrahim (Abe) Kazemzadeh'}, {'authorId': '2108057415', 'name': 'Sungbok Lee'}, {'authorId': '143840663', 'name': 'U. Neumann'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",880.0,"{'bibtex': '@Inproceedings{Busso2004AnalysisOE,\n author = {C. Busso and Z. Deng and S. Yıldırım and M. Bulut and C. Lee and Ebrahim (Abe) Kazemzadeh and Sungbok Lee and U. Neumann and Shrikanth S. Narayanan},\n pages = {205-211},\n title = {Analysis of emotion recognition using facial expressions, speech and multimodal information},\n year = {2004}\n}\n'}",,{'pages': '205-211'},24.0,"Analysis of emotion recognition using facial expressions, speech and multimodal information",2004.0
74,06952e2450d82764d0da4b641f60fcfa4cf333c6,"Recent work in behavioral animation has taken impressive steps toward autonomous, self-animating characters for use in production animation and interactive games. It remains difficult, however, to direct autonomous characters to perform specific tasks. This paper addresses the challenge by introducing cognitive modeling. Cognitive models go beyond behavioral models in that they govern what a character knows, how that knowledge is acquired, and how it can be used to plan actions. To help build cognitive models, we develop the cognitive modeling language CML. Using CML, we can imbue a character with domain knowledge, elegantly specified in terms of actions, their preconditions and their effects, and then direct the character’s behavior in terms of goals. Our approach allows behaviors to be specified more naturally and intuitively, more succinctly and at a much higher level of abstraction than would otherwise be possible. With cognitively empowered characters, the animator need only specify a behavior outline or “sketch plan” and, through reasoning, the character will automatically work out a detailed sequence of actions satisfying the specification. We exploit interval methods to integrate sensing into our underlying theoretical framework, thus enabling our autonomous characters to generate action plans even in highly complex, dynamic virtual worlds. We demonstrate cognitive modeling applications in advanced character animation and automated cinematography.","[{'authorId': '2198924', 'name': 'J. Funge'}, {'authorId': '40509745', 'name': 'Xiaoyuan Tu'}, {'authorId': '1750924', 'name': 'Demetri Terzopoulos'}]",505.0,"{'bibtex': '@Article{Funge1999CognitiveMK,\n author = {J. Funge and Xiaoyuan Tu and Demetri Terzopoulos},\n journal = {Proceedings of the 26th annual conference on Computer graphics and interactive techniques},\n title = {Cognitive modeling: knowledge, reasoning and planning for intelligent characters},\n year = {1999}\n}\n'}",,{'name': 'Proceedings of the 26th annual conference on Computer graphics and interactive techniques'},29.0,"Cognitive modeling: knowledge, reasoning and planning for intelligent characters",1999.0
75,06980c9e81382b0b479358f1423a9553c22c1dd2,,"[{'authorId': '3705092', 'name': 'L. Harris'}, {'authorId': '2751811', 'name': 'Samuel M. McClure'}, {'authorId': '39861124', 'name': 'W. Bos'}, {'authorId': '153564781', 'name': 'J. Cohen'}, {'authorId': '1885803', 'name': 'S. Fiske'}]",92.0,"{'bibtex': '@Article{Harris2007RegionsOT,\n author = {L. Harris and Samuel M. McClure and W. Bos and J. Cohen and S. Fiske},\n journal = {Cognitive, Affective, & Behavioral Neuroscience},\n pages = {309-316},\n title = {Regions of the MPFC differentially tuned to social and nonsocial affective evaluation},\n volume = {7},\n year = {2007}\n}\n'}",,"{'volume': '7', 'pages': '309-316', 'name': 'Cognitive, Affective, & Behavioral Neuroscience'}",56.0,Regions of the MPFC differentially tuned to social and nonsocial affective evaluation,2007.0
76,069ae43eab55150e2b54785cfdbe12b04b80ce9f,"Human state–of-mind (SOM; e.g.: perception, cognition, attention) constantly shifts due to internal and external demands. Mental health is influenced by the habitual use of either adaptive or maladaptive SOM. Therefore, the training of conscious regulation of SOM could be promising in self-help (e- and m-health), blended care and psychotherapy. The presented study indicates that SOM can be influenced by telling personal narratives. Furthermore, SOM and narrative sentiment (positive vs. negative) can be predicted through word use. Such results lay the groundwork for the development of applications that analyse text and speech for: i) the early detection of mental health; ii) the early detection of maladaptive changes in emotion dynamics; (iii) the use of personal narratives to improve emotion regulation skills; iv) the distribution of tailored interventions; and finally, v) the evaluation of therapy outcome.","[{'authorId': '7957231', 'name': 'E. Rathner'}, {'authorId': '51261937', 'name': 'Y. Terhorst'}, {'authorId': '1709997', 'name': 'N. Cummins'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '2847350', 'name': 'H. Baumeister'}]",18.0,"{'bibtex': '@Inproceedings{Rathner2018StateOM,\n author = {E. Rathner and Y. Terhorst and N. Cummins and Björn Schuller and H. Baumeister},\n pages = {267-271},\n title = {State of Mind: Classification through Self-reported Affect and Word Use in Speech},\n year = {2018}\n}\n'}",,{'pages': '267-271'},42.0,State of Mind: Classification through Self-reported Affect and Word Use in Speech,2018.0
77,06b3c99865869eec4ba54bd29de109b3ba153529,,"[{'authorId': '66536451', 'name': 'Welch Bl'}]",4038.0,"{'bibtex': ""@Article{Bl1947THEGO,\n author = {Welch Bl},\n journal = {Biometrika},\n pages = {28-35},\n title = {THE GENERALIZATION OF ‘STUDENT'S’ PROBLEM WHEN SEVERAL DIFFERENT POPULATION VARLANCES ARE INVOLVED},\n volume = {34},\n year = {1947}\n}\n""}",,"{'volume': '34', 'pages': '28-35', 'name': 'Biometrika'}",7.0,THE GENERALIZATION OF ‘STUDENT'S’ PROBLEM WHEN SEVERAL DIFFERENT POPULATION VARLANCES ARE INVOLVED,1947.0
78,06c60807c9e95ce47770e1caff7f592760eeb78a,,"[{'authorId': '1797292', 'name': 'K. VanLehn'}, {'authorId': '1730942', 'name': 'Pamela W. Jordan'}, {'authorId': '35959897', 'name': 'C. Rosé'}, {'authorId': '2888266', 'name': 'Dumisizwe Bhembe'}, {'authorId': '34612911', 'name': 'Michael Böttner'}, {'authorId': '47728940', 'name': 'A. Gaydos'}, {'authorId': '2200378', 'name': 'Maxim Makatchev'}, {'authorId': '2590575', 'name': 'Umarani Pappuswamy'}, {'authorId': '2267026', 'name': 'M. Ringenberg'}, {'authorId': '145753983', 'name': 'Antonio Roque'}, {'authorId': '2508756', 'name': 'Stephanie Siler'}, {'authorId': '2066943347', 'name': 'Ramesh Srivastava'}]",249.0,"{'bibtex': '@Inproceedings{VanLehn2002TheAO,\n author = {K. VanLehn and Pamela W. Jordan and C. Rosé and Dumisizwe Bhembe and Michael Böttner and A. Gaydos and Maxim Makatchev and Umarani Pappuswamy and M. Ringenberg and Antonio Roque and Stephanie Siler and Ramesh Srivastava},\n pages = {158-167},\n title = {The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing},\n year = {2002}\n}\n'}",,{'pages': '158-167'},25.0,The Architecture of Why2-Atlas: A Coach for Qualitative Physics Essay Writing,2002.0
79,06d754c74be430f56646055fbe5e6646777a9dd9,"Tag interactions are agent interactions that complement and differ from speech act communication models. Tags are public information that agents expose to others in the system to allow two types of interactions. Tag monitoring interactions let agents observe the tags of others actively. Tag fortuitous interactions make agents realize the tag of others with unrequested and application-dependent messages. In this paper we model tag interactions based on the agent environment and computational bodies to enact, maintain, and regulate their execution. We discuss the model and we identify further issues in the current state of the research. An example application is described in detail to show the potential of introducing tag interactions.","[{'authorId': '145697442', 'name': 'E. Platon'}, {'authorId': '1731432', 'name': 'N. Sabouret'}, {'authorId': '1720917', 'name': 'S. Honiden'}]",23.0,"{'bibtex': '@Inproceedings{Platon2006TagII,\n author = {E. Platon and N. Sabouret and S. Honiden},\n pages = {270-281},\n title = {Tag Interactions in MultiAgent Systems: Environment Support},\n year = {2006}\n}\n'}",,{'pages': '270-281'},29.0,Tag Interactions in MultiAgent Systems: Environment Support,2006.0
80,06dd2fdb03e82a5aa6b00dce1f0e703f67df4f7a,,"[{'authorId': '48014697', 'name': 'Robin Read'}, {'authorId': '2301161', 'name': 'Tony Belpaeme'}]",65.0,"{'bibtex': '@Article{Read2013PeopleIR,\n author = {Robin Read and Tony Belpaeme},\n journal = {International Journal of Social Robotics},\n pages = {31 - 50},\n title = {People Interpret Robotic Non-linguistic Utterances Categorically},\n volume = {8},\n year = {2013}\n}\n'}",,"{'volume': '8', 'pages': '31 - 50', 'name': 'International Journal of Social Robotics'}",80.0,People Interpret Robotic Non-linguistic Utterances Categorically,2013.0
81,06ee8b438d144c5db1f722f5143ecaaef63bcd46,"OBJECTIVES
To evaluate the reliability and validity of the PANAS (Watson, Clark, & Tellegen, 1988b) and provide normative data.


DESIGN
Cross-sectional and correlational.


METHOD
The PANAS was administered to a non-clinical sample, broadly representative of the general adult UK population (N = 1,003). Competing models of the latent structure of the PANAS were evaluated using confirmatory factor analysis. Regression and correlational analysis were used to determine the influence of demographic variables on PANAS scores as well as the relationship between the PANAS with measures of depression and anxiety (the HADS and the DASS).


RESULTS
The best-fitting model (robust comparative fit index = .94) of the latent structure of the PANAS consisted of two correlated factors corresponding to the PA and NA scales, and permitted correlated error between items drawn from the same mood subcategories (Zevon & Tellegen, 1982). Demographic variables had only very modest influences on PANAS scores and the PANAS exhibited measurement invariance across demographic subgroups. The reliability of the PANAS was high, and the pattern of relationships between the PANAS and the DASS and HADS were consistent with tripartite theory.


CONCLUSION
The PANAS is a reliable and valid measure of the constructs it was intended to assess, although the hypothesis of complete independence between PA and NA must be rejected. The utility of this measure is enhanced by the provision of large-scale normative data.","[{'authorId': '144723673', 'name': 'J. Crawford'}, {'authorId': '2149900272', 'name': 'J. Henry'}]",2608.0,"{'bibtex': '@Article{Crawford2004ThePA,\n author = {J. Crawford and J. Henry},\n journal = {The British journal of clinical psychology},\n pages = {\n          245-65\n        },\n title = {The positive and negative affect schedule (PANAS): construct validity, measurement properties and normative data in a large non-clinical sample.},\n volume = {43 Pt 3},\n year = {2004}\n}\n'}",,"{'volume': '43 Pt 3', 'pages': '\n          245-65\n        ', 'name': 'The British journal of clinical psychology'}",49.0,"The positive and negative affect schedule (PANAS): construct validity, measurement properties and normative data in a large non-clinical sample.",2004.0
82,07052d964fb858cf4b982100a4c7d8b00add3efc,"Virtual human expressions can shape user behavior [1, 2, 3], yet in negotiation, findings have been underwhelming. For example, human negotiators can use anger to claim value (i.e., extract concessions) [4], but anger has no effect when exhibited by a virtual human [5]. Other psychological work suggests that emotions can create value (e.g., happy negotiators can better discover tradeoffs across issues that ""grow the pie""), but little research has examined how virtual human expressions shape value creation. Here we present an agent architecture and pilot study that examines differences between how the emotional expressions of human and virtual-human opponents shape value claiming and value creation. We replicate the finding that virtual human anger fails to influence value claiming but discover counter-intuitive findings on value creation. We argue these findings highlight the potential for intelligent virtual humans to yield insight into human psychology.","[{'authorId': '2149209253', 'name': 'Eugene Lee'}, {'authorId': '2132855226', 'name': 'Zachary McNulty'}, {'authorId': '2183389468', 'name': 'Alex Gentle'}, {'authorId': '2183390749', 'name': 'Prerak Tusharkumar Pradhan'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",0.0,"{'bibtex': '@Article{Lee2022ExaminingTI,\n author = {Eugene Lee and Zachary McNulty and Alex Gentle and Prerak Tusharkumar Pradhan and J. Gratch},\n booktitle = {International Conference on Intelligent Virtual Agents},\n journal = {Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents},\n title = {Examining the impact of emotion and agency on negotiator behavior},\n year = {2022}\n}\n'}",[],{'name': 'Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents'},10.0,Examining the impact of emotion and agency on negotiator behavior,2022.0
83,07333fe6d1afbaef762faf444c3d976cef8ab794,"Expressive behaviour is a vital aspect of human interaction. A model for adaptive emotion expression was developed for the Nao robot. The robot has an internal arousal and valence value, which are influenced by the emotional state of its interaction partner and emotional occurrences such as winning a game. It expresses these emotions through its voice, posture, whole body poses, eye colour and gestures. An experiment with 18 children (mean age 9) and two Nao robots was conducted to study the influence of adaptive emotion expression on the interaction behaviour and opinions of children. In a within-subjects design the children played a quiz with both an affective robot using the model for adaptive emotion expression and a non-affective robot without this model. The affective robot reacted to the emotions of the child using the implementation of the model, the emotions of the child were interpreted by a Wizard of Oz. The dependent variables, namely the behaviour and opinions of the children, were measured through video analysis and questionnaires. The results show that children react more expressively and more positively to a robot which adaptively expresses itself than to a robot which does not. The feedback of the children in the questionnaires further suggests that showing emotion through movement is considered a very positive trait for a robot. From their positive reactions we can conclude that children enjoy interacting with a robot which adaptively expresses itself through emotion and gesture more than with a robot which does not do this. Categories and Subject Descriptors H.1 [Information Systems Models and Principles]: User / Machine Systems","[{'authorId': '3252848', 'name': 'M. Tielman'}, {'authorId': '1784286', 'name': 'Mark Antonius Neerincx'}, {'authorId': '1691228', 'name': 'J. Meyer'}, {'authorId': '1786270', 'name': 'R. Looije'}]",139.0,"{'bibtex': '@Article{Tielman2014AdaptiveEE,\n author = {M. Tielman and Mark Antonius Neerincx and J. Meyer and R. Looije},\n journal = {2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {407-414},\n title = {Adaptive Emotional Expression in Robot-Child Interaction},\n year = {2014}\n}\n'}",,"{'pages': '407-414', 'name': '2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",34.0,Adaptive Emotional Expression in Robot-Child Interaction,2014.0
84,073973452ca9198831f0be1be0b8398005c13e7d,"The task of planning trajectories for a mobile robot has received considerable attention in the research literature. Most of the work assumes the robot has a complete and accurate model of its environment before it begins to move; less attention has been paid to the problem of partially known environments. This situation occurs for an exploratory robot or one that must move to a goal location without the benefit of a floorplan or terrain map. Existing approaches plan an initial path based on known information and then modify the plan locally or replan the entire path as the robot discovers obstacles with its sensors, sacrificing optimality or computational efficiency respectively. This paper introduces a new algorithm, D*, capable of planning paths in unknown, partially known, and changing environments in an efficient, optimal, and complete manner.<<ETX>>","[{'authorId': '1722938', 'name': 'A. Stentz'}]",1597.0,"{'bibtex': '@Article{Stentz1994OptimalAE,\n author = {A. Stentz},\n journal = {Proceedings of the 1994 IEEE International Conference on Robotics and Automation},\n pages = {3310-3317 vol.4},\n title = {Optimal and efficient path planning for partially-known environments},\n year = {1994}\n}\n'}",,"{'pages': '3310-3317 vol.4', 'name': 'Proceedings of the 1994 IEEE International Conference on Robotics and Automation'}",14.0,Optimal and efficient path planning for partially-known environments,1994.0
85,074e8a7bece079f6c8b4110273f3de1614f5075d,,"[{'authorId': '2063522518', 'name': 'Abhisek Tiwari'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",16.0,"{'bibtex': '@Article{Tiwari2022AKI,\n author = {Abhisek Tiwari and S. Saha and P. Bhattacharyya},\n journal = {Knowl. Based Syst.},\n pages = {108292},\n title = {A knowledge infused context driven dialogue agent for disease diagnosis using hierarchical reinforcement learning},\n volume = {242},\n year = {2022}\n}\n'}",,"{'volume': '242', 'pages': '108292', 'name': 'Knowl. Based Syst.'}",16.0,A knowledge infused context driven dialogue agent for disease diagnosis using hierarchical reinforcement learning,2022.0
86,077ceadb36ffee8bca90a1980742538721784c7c,"Spontaneous mimicry is a marker of empathy. Conditions characterized by reduced spontaneous mimicry (e.g., autism) also display deficits in sensitivity to social rewards. We tested if spontaneous mimicry of socially rewarding stimuli (happy faces) depends on the reward value of stimuli in 32 typical participants. An evaluative conditioning paradigm was used to associate different reward values with neutral target faces. Subsequently, electromyographic activity over the Zygomaticus Major was measured whilst participants watched video clips of the faces making happy expressions. Higher Zygomaticus Major activity was found in response to happy faces conditioned with high reward versus low reward. Moreover, autistic traits in the general population modulated the extent of spontaneous mimicry of happy faces. This suggests a link between reward and spontaneous mimicry and provides a possible underlying mechanism for the reduced response to social rewards seen in autism.","[{'authorId': '2786578', 'name': 'Thomas B. Sims'}, {'authorId': '79538426', 'name': 'C. V. van Reekum'}, {'authorId': '30361732', 'name': 'T. Johnstone'}, {'authorId': '3102450', 'name': 'B. Chakrabarti'}]",81.0,"{'bibtex': '@Article{Sims2012HowRM,\n author = {Thomas B. Sims and C. V. van Reekum and T. Johnstone and B. Chakrabarti},\n journal = {Psychophysiology},\n pages = {\n          998-1004\n        },\n title = {How reward modulates mimicry: EMG evidence of greater facial mimicry of more rewarding happy faces.},\n volume = {49 7},\n year = {2012}\n}\n'}",,"{'volume': '49 7', 'pages': '\n          998-1004\n        ', 'name': 'Psychophysiology'}",62.0,How reward modulates mimicry: EMG evidence of greater facial mimicry of more rewarding happy faces.,2012.0
87,078316fb9d89b243b04e5497bd2ce36d1ddb567c,"This article investigates the role of interaction kinesics in human–robot interaction (HRI). We adopted a bottom-up, synthetic approach towards interactive competencies in robots using simple, minimal computational models underlying the robot's interaction dynamics. We present two empirical, exploratory studies investigating a drumming experience with a humanoid robot (KASPAR) and a human. In the first experiment, the turn-taking behaviour of the humanoid is deterministic and the non-verbal gestures of the robot accompany its drumming to assess the impact of non-verbal gestures on the interaction. The second experiment studies a computational framework that facilitates emergent turn-taking dynamics, whereby the particular dynamics of turn-taking emerge from the social interaction between the human and the humanoid. The results from the HRI experiments are presented and analysed qualitatively (in terms of the participants’ subjective experiences) and quantitatively (concerning the drumming performance of the human–robot pair). The results point out a trade-off between the subjective evaluation of the drumming experience from the perspective of the participants and the objective evaluation of the drumming performance. A certain number of gestures was preferred as a motivational factor in the interaction. The participants preferred the models underlying the robot's turn-taking which enable the robot and human to interact more and provide turn-taking closer to ‘natural’ human–human conversations, despite differences in objective measures of drumming behaviour. The results are consistent with the temporal behaviour matching hypothesis previously proposed in the literature which concerns the effect that the participants adapt their own interaction dynamics to the robot's.","[{'authorId': '1399283111', 'name': 'H. Kose-Bagci'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}, {'authorId': '1700812', 'name': 'D. Syrdal'}, {'authorId': '1718528', 'name': 'Chrystopher L. Nehaniv'}]",40.0,"{'bibtex': '@Article{Kose-Bagci2010DrummateID,\n author = {H. Kose-Bagci and K. Dautenhahn and D. Syrdal and Chrystopher L. Nehaniv},\n journal = {Connection Science},\n pages = {103 - 134},\n title = {Drum-mate: interaction dynamics and gestures in human–humanoid drumming experiments},\n volume = {22},\n year = {2010}\n}\n'}",,"{'volume': '22', 'pages': '103 - 134', 'name': 'Connection Science'}",78.0,Drum-mate: interaction dynamics and gestures in human–humanoid drumming experiments,2010.0
88,078765aee88112d417c355b2887b23538da468cd,"Emotional behaviour in the context of crowd simulationis a topic that is gaining particular interest in the area of artificial intelligence. Recent efforts in this domain havelooked for the modelling of emotional emergence and socialinteraction inside a crowd of virtual agents, but further investigation is still needed in aspects such as simulation of emotional awareness and emotion contagion. Also, in relation to perception of emotions, many questions remain about perception of emotional behaviour in the context of virtual crowds.This thesis investigates the current state-of-the-art of emotional characters in virtual crowds and presents the implementation of a computational model able to generate expressive full-body motion behaviour and emotion contagion in a crowd of virtual agents. Also, as a second part of the thesis, this project presents a perceptual study in which the perception of emotional behaviour is investigated in the context of virtual crowds. The results of this thesis reveal some interesting findings in relation to the perception and modelling of virtual crowds, including some relevant effectsin relation to the influence of emotional crowd behaviourin viewers, specially when virtual crowds are not the mainfocus of a particular scene. These results aim to contribute for the further development of this interdisciplinary area of computer graphics, artificial intelligence and psychology.","[{'authorId': '38188266', 'name': 'M. R. Carretero'}]",3.0,"{'bibtex': '@Inproceedings{Carretero2014ExpressionOE,\n author = {M. R. Carretero},\n title = {Expression of Emotion in Virtual Crowds:Investigating Emotion Contagion and Perception of Emotional Behaviour in Crowd Simulation},\n year = {2014}\n}\n'}",,"{'volume': '', 'name': ''}",42.0,Expression of Emotion in Virtual Crowds:Investigating Emotion Contagion and Perception of Emotional Behaviour in Crowd Simulation,2014.0
89,078c02fd5dac303021884c378dc766edb49e8583,"Brain research indicates that repetition is of vital importance in the learning process. Repetition is an especially useful tool in the area of music education. The success of repetition can be enhanced by accurate and timely feedback. From “simple repetition” to “repetition with the addition or subtraction of degrees of freedom,” there are many forms of repetition that can be successfully adapted to music education. Descriptions of each form of repetition are provided, along with accompanying rehearsal strategies that can be implemented in the classroom. Music teachers can avoid the pitfalls of boredom and mindless repetition by constantly shifting teaching strategies and including new goals and framing techniques. Using these strategies wisely, music educators can provide meaningful, refreshed, and powerful teaching and learning opportunities for both themselves and their students.","[{'authorId': '1453347563', 'name': 'Kirt Saville'}]",28.0,"{'bibtex': '@Article{Saville2011StrategiesFU,\n author = {Kirt Saville},\n journal = {Music Educators Journal},\n pages = {69 - 75},\n title = {Strategies for Using Repetition as a Powerful Teaching Tool},\n volume = {98},\n year = {2011}\n}\n'}",,"{'volume': '98', 'pages': '69 - 75', 'name': 'Music Educators Journal'}",0.0,Strategies for Using Repetition as a Powerful Teaching Tool,2011.0
90,07989cb0093eeaa0185eea50ac49a9dac8bda820,,"[{'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '32964910', 'name': 'Yanghee Kim'}]",240.0,"{'bibtex': '@Inproceedings{Baylor2004PedagogicalAD,\n author = {A. L. Baylor and Yanghee Kim},\n pages = {592-603},\n title = {Pedagogical Agent Design: The Impact of Agent Realism, Gender, Ethnicity, and Instructional Role},\n year = {2004}\n}\n'}",,{'pages': '592-603'},48.0,"Pedagogical Agent Design: The Impact of Agent Realism, Gender, Ethnicity, and Instructional Role",2004.0
91,07a525836aab4ee18708b2a029e95a9b83e1ec7e,,"[{'authorId': '2065109', 'name': 'M. Just'}]",193.0,"{'bibtex': '@Misc{None,\n author = {M. Just},\n title = {From the Selectedworks of Marcel Adam Just the Organization of Thinking: What Functional Brain Imaging Reveals about the Neuroarchitecture of Complex Cognition}\n}\n'}",,,122.0,From the Selectedworks of Marcel Adam Just the Organization of Thinking: What Functional Brain Imaging Reveals about the Neuroarchitecture of Complex Cognition,
92,07c4dab8f604cf56172ed931c329c56aa07f15e5,"This article describes current work in selective attention within a framework derived from important findings extending back over a century. The contributions of Danders, Helmholtz, Pavlov, Sokolov, and Wundt, for example, are deeply embedded in current methods for studying selectivity. The cumulative nature of work on attention is not widely appreciated, in part because of a failure to recognize that the methods used in current studies arose in empirical findings of the past and also because attention is a concept that can be studied at many levels. There is evidence that findings at the level of performance, subjective experience, and neural systems can be linked, even though they are not yet reducible to a single theory. Studies to date reveal some properties of a complex neural mechanism involved in our awareness of a stimulus. The time course of operation of this mechanism can be studied objectively and shown to be related both to changes in performance and to subjective experience. This attentional mechanism is involved in the skilled performance of daily life, but many other systems are also important in determining the degree to which natural tasks can be time shared. The goal of every science is a cumulative development of its theoretical structure so that a larger part of its subject matter is explicable in terms of simpler principles. This traditional view of science has been challenged in psychology from many sources. One argument has been that it is better to view psychology in terms of shifting paradigms (Kuhn, 1962). It often seems to be accepted, almost as a matter of course, that in psychology no cumulative development will take place. A different challenge to the view of psychology as a cumulative science is the notion that nothing new is discovered while the views of Helmholtz, Wundt, or some other elder of the field are being reworked, with no apparent gain in either insight or scope. These two challenges to the cumulative nature of psychological theory are persuasive, but they are hot consistent. If we shift from paradigm to paradigm, it seems puzzling that the current paradigm would so exactly mirror that of 100 years ago. On the other hand, if the solutions of 100 years ago remain, what has happened to paradigm shifts? Another criticism that has been applied to the 168 • FEBRUARY 1982 • AMERICAN PSYCHOLOGIST study of attention is that psychological theories are sterile, in that they do not illuminate important natural behavior or provide a perspective on the nature of mind (Neisser, 1976). The contention in this article is that one can see emerging from psychological research in the area of attention a cumulative development of theoretical concepts that rely on principles, some over 100 years old, that are now elaborated in ways that were essentially unavailable to earlier researchers. Moreover, taken as a whole these ideas do provide insight into the skills of daily life. If this contention is correct, why is it that the cumulative development of psychological theories of attention arfe so obscure, even to/ researchers in the field? I believe that several facts about the nature of psychological inquiry make its cumulative development obscure even to those who read the psychological literature. The first difficulty in perceiving the cumulative nature of theories arises because much work in psychology is fueled by tests between complex theoretical views that differ in only subtle ways. These theories often have common assumptions, but similarities between them that amount to a common core of agreed principles are overlooked. The view of experiments as tests among competing, wellspecified theories can be contrasted with the more cumulative theoretical approach outlined by Broadbent (1958): The proper road for progress then is to set up theories which are not at first detailed, although they must be capable of disproof. As research advances the theory will become continually more detailed, until one reaches the stage at which further advance is made by giving exact values to constants previously left unspecified in equaThis article was presented as a Distinguished Scientific Contribution Award address at the meeting of the' American Psychological Association, Los Angeles, September 1981. This work was supported by a series of National Science Foundation grants to the University of Oregon. I am most grateful to the many students and colleagues who have contributed to this work, and to Mary R'othbart for assistance in writing it. Requests for reprints should be sent to Michael I. Posner, Department of Psychology, University of Oregon, Eugene, Or-","[{'authorId': '2262729', 'name': 'M. Posner'}]",328.0,"{'bibtex': '@Article{Posner1982CumulativeDO,\n author = {M. Posner},\n journal = {American Psychologist},\n pages = {168-179},\n title = {Cumulative Development of Attentional Theory.},\n volume = {37},\n year = {1982}\n}\n'}",,"{'volume': '37', 'pages': '168-179', 'name': 'American Psychologist'}",54.0,Cumulative Development of Attentional Theory.,1982.0
93,07cc4371c8d3ddf7de0189f73227c3f0d896d66d,"Based on a rigorous mathematical analysis, the authors present a systematic overview and a critical discussion of the inherent problems of potential field methods (PFMs). The authors previously (1989) developed a PFM called the virtual force field (VFF) method. Much insight has been gained into the strengths and weaknesses of this method. Four distinct drawbacks with PFMs are identified. Because of these drawbacks, the authors abandoned potential field methods and developed a new method for fast obstacle avoidance. This method, called the vector field histogram method, produces smooth, nonoscillatory motion, while sampling time and hardware are identical to those used in the VFF method.<<ETX>>","[{'authorId': '1687718', 'name': 'Y. Koren'}, {'authorId': '34767446', 'name': 'J. Borenstein'}]",1754.0,"{'bibtex': '@Article{Koren1991PotentialFM,\n author = {Y. Koren and J. Borenstein},\n journal = {Proceedings. 1991 IEEE International Conference on Robotics and Automation},\n pages = {1398-1404 vol.2},\n title = {Potential field methods and their inherent limitations for mobile robot navigation},\n year = {1991}\n}\n'}",,"{'pages': '1398-1404 vol.2', 'name': 'Proceedings. 1991 IEEE International Conference on Robotics and Automation'}",12.0,Potential field methods and their inherent limitations for mobile robot navigation,1991.0
94,07ccd119ff7ace675f1f9075708f32a634d542a5,,"[{'authorId': '47985333', 'name': 'A. Kendon'}]",312.0,"{'bibtex': '@Inproceedings{Kendon1988HowGC,\n author = {A. Kendon},\n title = {How gestures can become like words},\n year = {1988}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,How gestures can become like words,1988.0
95,07df67b4938bd28578b5d155b70aa132bc305b9a,"Children will increasingly come of age with personified robots and potentially form social and even moral relationships with them. What will such relationships look like? To address this question, 90 children (9-, 12-, and 15-year-olds) initially interacted with a humanoid robot, Robovie, in 15-min sessions. Each session ended when an experimenter interrupted Robovie's turn at a game and, against Robovie's stated objections, put Robovie into a closet. Each child was then engaged in a 50-min structural-developmental interview. Results showed that during the interaction sessions, all of the children engaged in physical and verbal social behaviors with Robovie. The interview data showed that the majority of children believed that Robovie had mental states (e.g., was intelligent and had feelings) and was a social being (e.g., could be a friend, offer comfort, and be trusted with secrets). In terms of Robovie's moral standing, children believed that Robovie deserved fair treatment and should not be harmed psychologically but did not believe that Robovie was entitled to its own liberty (Robovie could be bought and sold) or civil rights (in terms of voting rights and deserving compensation for work performed). Developmentally, while more than half the 15-year-olds conceptualized Robovie as a mental, social, and partly moral other, they did so to a lesser degree than the 9- and 12-year-olds. Discussion focuses on how (a) children's social and moral relationships with future personified robots may well be substantial and meaningful and (b) personified robots of the future may emerge as a unique ontological category.","[{'authorId': '144674612', 'name': 'P. Kahn'}, {'authorId': '48309591', 'name': 'T. Kanda'}, {'authorId': '1687808', 'name': 'H. Ishiguro'}, {'authorId': '2947158', 'name': 'Nathan G. Freier'}, {'authorId': '2667695', 'name': 'Rachel L. Severson'}, {'authorId': '2582208', 'name': 'Brian T. Gill'}, {'authorId': '2517956', 'name': 'Jolina H. Ruckert'}, {'authorId': '36425853', 'name': 'Solace Shen'}]",314.0,"{'bibtex': '@Article{Kahn2012RobovieYH,\n author = {P. Kahn and T. Kanda and H. Ishiguro and Nathan G. Freier and Rachel L. Severson and Brian T. Gill and Jolina H. Ruckert and Solace Shen},\n journal = {Developmental psychology},\n pages = {\n          303-14\n        },\n title = {""Robovie, you\'ll have to go into the closet now"": children\'s social and moral relationships with a humanoid robot.},\n volume = {48 2},\n year = {2012}\n}\n'}",,"{'volume': '48 2', 'pages': '\n          303-14\n        ', 'name': 'Developmental psychology'}",58.0,"""Robovie, you'll have to go into the closet now"": children's social and moral relationships with a humanoid robot.",2012.0
96,07f2fdd1851146dcd954d643703b6adce3882692,"In this abstract we introduce the design of an experiment aimed at investigating how users' impressions of an embodied conversational agent are influenced by agent's non-verbal behaviour. We focus on impressions of warmth and competence, the two fundamental dimensions of social perception. Agent's gestures, arms rest poses and smile frequency are manipulated, as well as users' expectations about agent's competence. We hypothesize that user's judgments will differ according to his expectations, by following the Expectancy Violation Theory proposed by Burgoon and colleagues. We also hypothesize to replicate the results found in our previous study concerning human-human interaction, for example high frequency of smiles will elicit higher warmth and lower competence impressions compared to low frequency of smiles, while arms crossed will elicit low competence and low warmth impressions.","[{'authorId': '23567239', 'name': 'Béatrice Biancardi'}, {'authorId': '40123315', 'name': 'Angelo Cafaro'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",10.0,"{'bibtex': ""@Article{Biancardi2017CouldAV,\n author = {Béatrice Biancardi and Angelo Cafaro and C. Pelachaud},\n journal = {Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents},\n title = {Could a virtual agent be warm and competent? investigating user's impressions of agent's non-verbal behaviours},\n year = {2017}\n}\n""}",,{'name': 'Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents'},16.0,Could a virtual agent be warm and competent? investigating user's impressions of agent's non-verbal behaviours,2017.0
97,07fd19b3b9aa930f0a85776d9e1b94b37f7fc318,"This program of experiments examined heart rate responses to mental arithmetic and a video game. Attention first focused on their metabolic relevance. Comparison with heart rate/oxygen consumption regression equations generated from isotonic exercise data revealed that the heart rate increases of certain individuals were considerably in excess of those necessitated by contemporary metabolic demand. Both temporal and intertask consistency of reaction were explored, and supportive evidence was obtained. The relationship between laboratory and real-world reactions was investigated, and preliminary evidence found suggesting that in-laboratory responses are indicative of responses to more naturalistic stressors. Finally, twin studies examining the genetic and environmental determinants of individual differences in heart rate change during the tasks revealed a substantial genetic component for these responses.","[{'authorId': '2151553', 'name': 'J. Turner'}]",49.0,"{'bibtex': '@Article{Turner1989ForDE,\n author = {J. Turner},\n journal = {Psychophysiology},\n pages = {497-505},\n title = {For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1988},\n volume = {26},\n year = {1989}\n}\n'}",,"{'volume': '26', 'pages': '497-505', 'name': 'Psychophysiology'}",39.0,"For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1988",1989.0
98,0800357049444fb80588f264d68e0c23f9b12a19,"The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the ""animator."" The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.","[{'authorId': '30613168', 'name': 'Craig W. Reynolds'}]",8228.0,"{'bibtex': '@Article{Reynolds1987FlocksHA,\n author = {Craig W. Reynolds},\n journal = {Seminal graphics: pioneering efforts that shaped the field},\n title = {Flocks, herds, and schools: a distributed behavioral model},\n year = {1987}\n}\n'}",,{'name': 'Seminal graphics: pioneering efforts that shaped the field'},42.0,"Flocks, herds, and schools: a distributed behavioral model",1987.0
99,086b5f9c21bf0af825bb9e98a42edbbc3e559a46,"&NA; In movies, robots are often extremely humanlike. Although these robots are not yet reality, robots are currently being used in healthcare, education, and business. Robots provide benefits such as relieving loneliness and enabling communication. Engineers are trying to build robots that look and behave like humans and thus need comprehensive knowledge not only of technology but also of human cognition, emotion, and behavior. This need is driving engineers to study human behavior toward other humans and toward robots, leading to greater understanding of how humans think, feel, and behave in these contexts, including our tendencies for mindless social behaviors, anthropomorphism, uncanny feelings toward robots, and the formation of emotional attachments. However, in considering the increased use of robots, many people have concerns about deception, privacy, job loss, safety, and the loss of human relationships. Human–robot interaction is a fascinating field and one in which psychologists have much to contribute, both to the development of robots and to the study of human behavior.","[{'authorId': '145095677', 'name': 'E. Broadbent'}]",315.0,"{'bibtex': '@Article{Broadbent2017InteractionsWR,\n author = {E. Broadbent},\n journal = {Annual Review of Psychology},\n pages = {627–652},\n title = {Interactions With Robots: The Truths We Reveal About Ourselves},\n volume = {68},\n year = {2017}\n}\n'}",,"{'volume': '68', 'pages': '627–652', 'name': 'Annual Review of Psychology'}",104.0,Interactions With Robots: The Truths We Reveal About Ourselves,2017.0
100,086ef3944e7ca805d9f8bf8af7f4b66e43495c0c,"Recent eye tracking studies of face processing have produced differing accounts of how and whether children with autism differ from their typically developing peers. The two groups' gaze patterns appear to differ for dynamic videos of social scenes, but not for static photos of isolated individuals. The present study replicated and extended previous research by comparing the gaze patterns of individuals with and without autism for four types of stimuli: social dynamic, social static, isolated dynamic, and isolated static. Participants with autism differed from their typically developing peers only for social-dynamic stimuli; fixation durations were decreased for eye regions and increased for body regions. Further, these fixation durations predicted scores on a measure of social responsiveness. These findings reconcile differences in previous reports by identifying the specific social and dynamic task components associated with autism-related face processing impairments.","[{'authorId': '33434026', 'name': 'Leslie L. Speer'}, {'authorId': '2622551', 'name': 'A. E. Cook'}, {'authorId': '2241753', 'name': 'W. McMahon'}, {'authorId': '2072399105', 'name': 'E. Clark'}]",345.0,"{'bibtex': '@Article{Speer2007FacePI,\n author = {Leslie L. Speer and A. E. Cook and W. McMahon and E. Clark},\n journal = {Autism},\n pages = {265 - 277},\n title = {Face processing in children with autism},\n volume = {11},\n year = {2007}\n}\n'}",,"{'volume': '11', 'pages': '265 - 277', 'name': 'Autism'}",23.0,Face processing in children with autism,2007.0
101,0894e07867170b3994140b70b8ad3d6fd0ab213c,Detecting cooperative partners in situations that have financial stakes is crucial to successful social exchange. The authors tested whether humans are sensitive to subtle facial dynamics of counterparts when deciding whether to trust and cooperate. Participants played a 2-person trust game before which the facial dynamics of the other player were manipulated using brief (<6 s) but highly realistic facial animations. Results showed that facial dynamics significantly influenced participants' (a) choice of with whom to play the game and (b) decisions to cooperate. It was also found that inferences about the other player's trustworthiness mediated these effects of facial dynamics on cooperative behavior.,"[{'authorId': '50755536', 'name': 'Eva G. Krumhuber'}, {'authorId': '92736978', 'name': 'A. Manstead'}, {'authorId': '1792288', 'name': 'D. Cosker'}, {'authorId': '34839995', 'name': 'D. Marshall'}, {'authorId': '1734823', 'name': 'Paul L. Rosin'}, {'authorId': '1742554', 'name': 'Arvid Kappas'}]",403.0,"{'bibtex': '@Article{Krumhuber2007FacialDA,\n author = {Eva G. Krumhuber and A. Manstead and D. Cosker and D. Marshall and Paul L. Rosin and Arvid Kappas},\n journal = {Emotion},\n pages = {\n          730-5\n        },\n title = {Facial dynamics as indicators of trustworthiness and cooperative behavior.},\n volume = {7 4},\n year = {2007}\n}\n'}",,"{'volume': '7 4', 'pages': '\n          730-5\n        ', 'name': 'Emotion'}",31.0,Facial dynamics as indicators of trustworthiness and cooperative behavior.,2007.0
102,0897c6b842ecd5e511b434407c35ff32724a9978,"Virtual environments are a promising milieu for education and training, because they allow students to practice their skills in 3D simulations of work settings. Autonomous agents can improve the eeectiveness of such e n vironments by assisting and collaborating with students as appropriate. This paper describes an autonomous pedagogical agent called Steve that can support the training of procedural skills such as operating or repairing complex equipment. Steve's architecture permits him to sense and manipulate dynamic virtual worlds. The architecture also enables Steve to assume alternative realizations, either as a full, articulated, human gure or as abstract pointers and disembodied hands. Steve employs a combination of intelligent capabilities in his interactions with students and the environment: plan revision and execution, explanation, and student monitoring. These capabilities are employed in multiple ways in order to support alternative pedagogical styles. Steve's knowledge representation is designed so that agent capabilities can be authored without detailed knowledge of agent architectures and languages.",[],163.0,"{'bibtex': '@Inproceedings{None,\n pages = {30-38},\n title = {Integrating pedagogical capabilities in a virtual environment agent},\n year = {1997}\n}\n'}",,{'pages': '30-38'},26.0,Integrating pedagogical capabilities in a virtual environment agent,1997.0
103,08a005912a72800e4cc632be3d2e559547f48ac2,"We present an end-to-end voice-based conversational agent that is able to engage in naturalistic multi-turn dialogue and align with the interlocutor's conversational style. The system uses a series of deep neural network components for speech recognition, dialogue generation, prosodic analysis and speech synthesis to generate language and prosodic expression with qualities that match those of the user. We conducted a user study (N=30) in which participants talked with the agent for 15 to 20 minutes, resulting in over 8 hours of natural interaction data. Users with high consideration conversational styles reported the agent to be more trustworthy when it matched their conversational style. Whereas, users with high involvement conversational styles were indifferent. Finally, we provide design guidelines for multi-turn dialogue interactions using conversational style adaptation.","[{'authorId': '2065815350', 'name': 'Rens Hoegen'}, {'authorId': '2494850', 'name': 'Deepali Aneja'}, {'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '1817251', 'name': 'M. Czerwinski'}]",46.0,"{'bibtex': '@Article{Hoegen2019AnEC,\n author = {Rens Hoegen and Deepali Aneja and Daniel J. McDuff and M. Czerwinski},\n journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},\n title = {An End-to-End Conversational Style Matching Agent},\n year = {2019}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents'},43.0,An End-to-End Conversational Style Matching Agent,2019.0
104,08c42c913a168ccaf496f2f0db0332b7496c512e,"Public spaces are created to be used, and large crowds gather in many buildings and external spaces. Maintaining a high level of safety for these people is of utmost importance. Cameras are used for security reasons by control room personnel, who also monitor crowd movements in case of emergency. Crowd modelling can be used to detect and analyse time dependent and space dependent crowd behaviour. Despite the large amount of raw visual information being processed, crowd modelling has not been dedicated yet to evacuation decision making. Predictive information can assist the decision maker in assessing the situation in the early stages, potentially preventing the need for an evacuation. If evacuation is inescapable, a decision maker can use crowd modelling to define the quickest and safest evacuation routes. This kind of decision support will reduce the number of deaths that occur before and during an evacuation.","[{'authorId': '3436358', 'name': 'B. Vreugdenhil'}, {'authorId': '2698741', 'name': 'N. Bellomo'}, {'authorId': '47154685', 'name': 'P. Townsend'}]",8.0,"{'bibtex': '@Inproceedings{Vreugdenhil2015UsingCM,\n author = {B. Vreugdenhil and N. Bellomo and P. Townsend},\n title = {Using Crowd Modelling in Evacuation Decision Making},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",16.0,Using Crowd Modelling in Evacuation Decision Making,2015.0
105,0908779b9ec4233739a5297e56b36f91896e7d04,,"[{'authorId': '4506321', 'name': 'Selda Ozdemir'}]",67.0,"{'bibtex': '@Article{Ozdemir2008TheEO,\n author = {Selda Ozdemir},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {1689-1696},\n title = {The Effectiveness of Social Stories on Decreasing Disruptive Behaviors of Children with Autism: Three Case Studies},\n volume = {38},\n year = {2008}\n}\n'}",,"{'volume': '38', 'pages': '1689-1696', 'name': 'Journal of Autism and Developmental Disorders'}",26.0,The Effectiveness of Social Stories on Decreasing Disruptive Behaviors of Children with Autism: Three Case Studies,2008.0
106,092bdb6b5e61a81465c2413733efa0dec8f8908c,"How can simulation be made more compelling and effective as a tool for learning? This is the question thatthe Institute for Creative Technologies (ICT) set out to answer when it was formed at the University ofSouthern California in 1999, to serve as a nexus between the simulation and entertainment communities.The ultimate goal of the ICT is to create the Experience Learning System (ELS), which will advance thestate of the art in virtual reality immersion through use of high-resolution graphics, immersive audio,virtual humans and story-based scenarios. Once fully realized, ELS will make it possible for participants toenter places in time and space where they can interact with believable characters capable of conversationand action, and where they can observe and participate in events that are accessible only throughsimulation.","[{'authorId': '1812270', 'name': 'R. Hill'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '1684040', 'name': 'W. Swartout'}, {'authorId': '144518646', 'name': 'D. Traum'}]",155.0,"{'bibtex': '@Article{Hill2003VirtualHI,\n author = {R. Hill and J. Gratch and S. Marsella and J. Rickel and W. Swartout and D. Traum},\n journal = {Künstliche Intell.},\n pages = {5-},\n title = {Virtual Humans in the Mission Rehearsal Exercise System},\n volume = {17},\n year = {2003}\n}\n'}",,"{'volume': '17', 'pages': '5-', 'name': 'Künstliche Intell.'}",41.0,Virtual Humans in the Mission Rehearsal Exercise System,2003.0
107,093d7bdce1f2270b6e0d69ad5680fce6c5c98e2f,"The aim of this paper is to review data from my laboratory, which were collected in an attempt to determine whether the facial EMG response is a general component of the emotional reaction. In a number of studies it was found that facial reactions: first, are spontaneously elicited and differ according to the kind of emotional stimuli to which subjects are exposed; second, are sensitive to learning; third, are consistent with how the subjects perceive the stimuli and their own specific emotions; fourth, are congruent with autonomic responses; fifth, are more pronounced for females than for males; and finally, differ among subjects with specific fears. These data converge to indicate that facial muscle activity is a general component of the emotional reaction and demonstrate that the facial EMG technique is a sensitive tool for measuring emotional reactions.","[{'authorId': '4583182', 'name': 'U. Dimberg'}]",355.0,"{'bibtex': '@Article{Dimberg1990FacialEA,\n author = {U. Dimberg},\n journal = {Psychophysiology},\n pages = {\n          481-94\n        },\n title = {Facial electromyography and emotional reactions.},\n volume = {27 5},\n year = {1990}\n}\n'}",,"{'volume': '27 5', 'pages': '\n          481-94\n        ', 'name': 'Psychophysiology'}",32.0,Facial electromyography and emotional reactions.,1990.0
108,0950e823eef8f5632bfcb3fb879d5e4b4b3ac285,,"[{'authorId': '29224904', 'name': 'H. H. Clark'}, {'authorId': '118315489', 'name': 'Deanna Wilkes-Gibbs'}]",1241.0,"{'bibtex': '@Article{Clark1986ReferringAA,\n author = {H. H. Clark and Deanna Wilkes-Gibbs},\n journal = {Cognition},\n pages = {1-39},\n title = {Referring as a collaborative process},\n volume = {22},\n year = {1986}\n}\n'}",,"{'volume': '22', 'pages': '1-39', 'name': 'Cognition'}",50.0,Referring as a collaborative process,1986.0
109,0956a3c628959afcf870f5d7ec581160a4aa5221,"This paper presents the LIFEisGAME prototype-Ipad version – a serious game that proposes to enhance facial and emotional recognition skills in children with Autism Spectrum Disorders (ASD). We assess the prototype game regarding motivation to play and game usability, and also participants’ emotional recognition abilities and technology usage. People with autism are less likely to gaze at faces and are also impaired in face discrimination tasks. Recently, technology plays an active role in helping these individuals to understand emotions and recognise facial expressions. LIFEisGAME prototype was played during a 15 minute game session by 11 children with ASD, with ages varying from 5-15 years old (M=9.27, SD=2.97), 91% were male and 9% were female, 82% were verbal ASD and 18% were non-verbal ASD. We video recorded each child and the footage was analysed according to game usability and motivation to play. Parents (n=11) filled out a parental consent form and a questionnaire about their child ́s technology usage and their emotional understanding. Therapists' opinions (n=8) about the game were given during an unstructured interview. The game was presented on an Ipad 4 (9.7 inches, 2048x1536). Participants enjoyed the prototype but it still needs to be simplified. All participants had experience with computer games. Fear, disgust and surprise were the most challenging emotions to recognise. Parents suggested adding musical stimuli to promote motivation and therapists recommended to include visual game instructions. Technology is a useful resource for autism and LIFEisGAME utilises technology to promote emotional understanding, bringing positive outcomes to quality of life for children with autism. Keywords: autism, emotions, prototype-game, children, Ipad Paper Received 26/06/2013; received in revised form 29/11/2013; accepted 20/12/2013. Cite as: Alves, S., Marques, A., Queirós, C. & Orvalho, V. (2013). LIFEisGAME Prototype: A Serious Game about Emotions for Children with Autism Spectrum Disorders. PsychNology Journal, 11(3), 191 – 211. Retrieved [month] [day], [year], from www.psychnology.org. Corresponding Author: Cristina Queirós Faculty of Psychology and Educational Sciences, Porto University Psychosocial Rehabilitation Laboratory. Rua Alfredo Allen, 4200-135 Porto, Portugal E-mail: cqueiros@fpce.up.pt S.	  Alves,	  A.	  Marques,	  C.	  Queirós,	  V.	  Orvalho","[{'authorId': '40423538', 'name': 'Samanta Alves'}, {'authorId': '50453251', 'name': 'A. Marques'}, {'authorId': '1897745560', 'name': 'C. Queirós'}, {'authorId': '2087332', 'name': 'V. Orvalho'}]",48.0,"{'bibtex': '@Article{Alves2013LIFEisGAMEPA,\n author = {Samanta Alves and A. Marques and C. Queirós and V. Orvalho},\n journal = {PsychNology J.},\n pages = {191-211},\n title = {LIFEisGAME Prototype: A Serious Game about Emotions for Children with Autism Spectrum Disorders},\n volume = {11},\n year = {2013}\n}\n'}",,"{'volume': '11', 'pages': '191-211', 'name': 'PsychNology J.'}",53.0,LIFEisGAME Prototype: A Serious Game about Emotions for Children with Autism Spectrum Disorders,2013.0
110,099609b75d3454b6a6c298e745ae8d8b5143eac5,"Approach action tendencies toward positive stimuli and avoidance tendencies from negative stimuli are widely seen to foster survival. Many studies have shown that approach and avoidance arm movements are facilitated by positive and negative affect, respectively. There is considerable debate whether positively and negatively valenced stimuli prime approach and avoidance movements directly (i.e., immediate, unintentional, implicit, automatic, and stimulus-based), or indirectly (i.e., after conscious or non-conscious interpretation of the situation). The direction and size of these effects were often found to depend on the instructions referring to the stimulus object or the self, and on explicit vs. implicit stimulus evaluation. We present a meta-analysis of 29 studies included for their use of strongly positive and negative stimuli, with 81 effect sizes derived solely from the means and standard deviations (combined N = 1538), to examine the automaticity of the link between affective information processing and approach and avoidance, and to test whether it depends on instruction, type of approach-avoidance task, and stimulus type. Results show a significant small to medium-sized effect after correction for publication bias. The strongest arguments for an indirect link between affect and approach-avoidance were the absence of evidence for an effect with implicit evaluation, and the opposite directions of the effect with self and object-related interpretations. The link appears to be influenced by conscious or non-conscious intentions to deal with affective stimuli.","[{'authorId': '2545816', 'name': 'R. Phaf'}, {'authorId': '115670276', 'name': 'Sören E. Mohr'}, {'authorId': '4837582', 'name': 'M. Rotteveel'}, {'authorId': '2748685', 'name': 'J. Wicherts'}]",227.0,"{'bibtex': '@Article{Phaf2014ApproachAA,\n author = {R. Phaf and Sören E. Mohr and M. Rotteveel and J. Wicherts},\n journal = {Frontiers in Psychology},\n title = {Approach, avoidance, and affect: a meta-analysis of approach-avoidance tendencies in manual reaction time tasks},\n volume = {5},\n year = {2014}\n}\n'}",,"{'volume': '5', 'name': 'Frontiers in Psychology'}",81.0,"Approach, avoidance, and affect: a meta-analysis of approach-avoidance tendencies in manual reaction time tasks",2014.0
111,099e8d93a9fad83a5eb06c6fe39c72ab10a71f2a,,"[{'authorId': '1399253964', 'name': 'Rinat B. Rosenberg-Kima'}, {'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '145202321', 'name': 'E. Plant'}, {'authorId': '2977014', 'name': 'Celeste E. Doerr'}]",142.0,"{'bibtex': ""@Article{Rosenberg-Kima2008InterfaceAA,\n author = {Rinat B. Rosenberg-Kima and A. L. Baylor and E. Plant and Celeste E. Doerr},\n journal = {Comput. Hum. Behav.},\n pages = {2741-2756},\n title = {Interface agents as social models for female students: The effects of agent visual presence and appearance on female students' attitudes and beliefs},\n volume = {24},\n year = {2008}\n}\n""}",,"{'volume': '24', 'pages': '2741-2756', 'name': 'Comput. Hum. Behav.'}",50.0,Interface agents as social models for female students: The effects of agent visual presence and appearance on female students' attitudes and beliefs,2008.0
112,09c36dbb41ed1abd1b07b531c78515bbed487d25,"Evolutionary theory lacks a term for a crucial concept—a feature, now useful to an organism, that did not arise as an adaptation for its present role, but was subsequently coopted for its current function. I call such features “exaptations” and show that they are neither rare nor arcane, but dominant features of evolution—though previously unappreciated in the context of the overly adaptationist neo-Darwinian theory. This article argues that exaptation overcomes the fallacy of human sociobiology, helps us to understand the major patterns of flexibility and contingency in life's history, revises the roles of structure and function in evolutionary theory, serves as a centerpiece for grasping the origin and meaning of brain size in human evolution, and thereby cries out for recognition as a key to evolutionary psychology. Historical origin and current utility are distinct concepts and must never be conflated.","[{'authorId': '49384810', 'name': 'S. Gould'}]",500.0,"{'bibtex': '@Article{Gould1991ExaptationAC,\n author = {S. Gould},\n journal = {Journal of Social Issues},\n pages = {43-65},\n title = {Exaptation: A Crucial Tool for an Evolutionary Psychology},\n volume = {47},\n year = {1991}\n}\n'}",,"{'volume': '47', 'pages': '43-65', 'name': 'Journal of Social Issues'}",16.0,Exaptation: A Crucial Tool for an Evolutionary Psychology,1991.0
113,09f1a254c78ba0fefb01a75be971340d29d2c036,Mother Nature knows best--How engineered organizations of the future will resemble natural-born systems.,"[{'authorId': '2263157709', 'name': 'James Kalbach'}]",4421.0,"{'bibtex': '@Article{Kalbach2002PersuasiveTU,\n author = {James Kalbach},\n journal = {Ubiquity},\n pages = {5},\n title = {Persuasive technology: using computers to change what we think and do},\n volume = {2002},\n year = {2002}\n}\n'}",,"{'volume': '2002', 'pages': '5', 'name': 'Ubiquity'}",139.0,Persuasive technology: using computers to change what we think and do,2002.0
114,0a0e51a1430efa890b299a40c7b313b6ed6d62e2,,"[{'authorId': '145440944', 'name': 'M. Morris'}, {'authorId': '3990536', 'name': 'D. Keltner'}]",424.0,"{'bibtex': '@Article{Morris2000HowEW,\n author = {M. Morris and D. Keltner},\n journal = {Research in Organizational Behavior},\n pages = {1-50},\n title = {How Emotions Work: The Social Functions of Emotional Expression in Negotiations},\n volume = {22},\n year = {2000}\n}\n'}",,"{'volume': '22', 'pages': '1-50', 'name': 'Research in Organizational Behavior'}",164.0,How Emotions Work: The Social Functions of Emotional Expression in Negotiations,2000.0
115,0a1812c6b149ea0f1239f3a2d48f2e71fd5eb607,"Agent-based modeling of human social behavior is an increasingly important research area. A key factor in human social interaction is our beliefs about others, a theory of mind. Whether we believe a message depends not only on its content but also on our model of the communicator. How we act depends not only on the immediate effect but also on how we believe others will react. In this paper, we discuss PsychSim, an implemented multiagent-based simulation tool for modeling interactions and influence. While typical approaches to such modeling have used first-order logic, Psych-Sim agents have their own decision-theoretic model of the world, including beliefs about its environment and recursive models of other agents. Using these quantitative models of uncertainty and preferences, we have translated existing psychological theories into a decision-theoretic semantics that allow the agents to reason about degrees of believability in a novel way. We discuss PsychSim's underlying architecture and describe its application to a school violence scenario for illustration.","[{'authorId': '1748597', 'name': 'D. Pynadath'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",209.0,"{'bibtex': '@Inproceedings{Pynadath2005PsychSimMT,\n author = {D. Pynadath and S. Marsella},\n pages = {1181-1186},\n title = {PsychSim: Modeling Theory of Mind with Decision-Theoretic Agents},\n year = {2005}\n}\n'}",,{'pages': '1181-1186'},16.0,PsychSim: Modeling Theory of Mind with Decision-Theoretic Agents,2005.0
116,0a4d9913a15355816818bef68ec78af2aeff6fbc,"Judgments, preferences, and other cognitive tasks entail an emotional foundation and cannot function in an emotional vacuum. This essential emotional component how- ever, needs to be continuously monitored. Emotion regulation strategies target the potential risk of having inappropriate level of emotions in the process of decision making. This study is a follow-up on a formerly proposed computational model for emotion regulation strategies based on Gross theory and applies several enhancements to it. In particular, we extend the dynamism and realism of the original model by considering a dynamic environment in which we study the effect of emotion eliciting events such as psychiatric therapies or traumas occurring during the simulation period. Furthermore, the new model uses an emotion-dependent regulation process based on the mood of individuals. This approach is consistent with human behavior in the real life. In addition, some key pa- rameters in our proposed computational model, such as emotion persistence factor were made adaptive. Results obtained from the simulation experiments using our proposed model show further consistency with the base theory.","[{'authorId': '9458014', 'name': 'Ahmad Soleimani'}, {'authorId': '1763459', 'name': 'Ziad Kobti'}]",6.0,"{'bibtex': '@Inproceedings{Soleimani2012AMD,\n author = {Ahmad Soleimani and Ziad Kobti},\n title = {A Mood Driven Computational Model for Gross Emotion Regulation Process Paradigm},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",22.0,A Mood Driven Computational Model for Gross Emotion Regulation Process Paradigm,2012.0
117,0aafeae87b3691314407fbbb7ccf4829f94e20df,"In the last decade and a half, the amount of work on affect in general and emotion in particular has grown, in empirical psychology, cognitive science and AI, both for scientific purposes and for the purpose of designing synthetic characters, e.g. in games and entertainments. Such work understandably starts from concepts of ordinary language (e.g. “emotion”, “feeling”, “mood”, etc.). However, these concepts can be deceptive: the words appear to have clear meanings but are used in very imprecise and systematically ambiguous ways. This is often because of explicit or implicit pre-scientific theories about mental states and process. More sophisticated theories can provide a basis for deeper and more precise concepts, as has happened in physics and chemistry. In the Cognition and Affect project we have been attempting to explore the benefits of developing architecture-based concepts, i.e. starting with specifications of architectures for complete agents and then finding out what sorts of states and processes are supported by those architectures. So, instead of presupposing one theory of the architecture and explicitly or implicitly basing concepts on that, we define a space of architectures generated by the CogAff architecture schema, where each supports different collections of concepts. In that space we focus on one architecture H-Cogaff, a particularly rich instance of the CogAff architecture schema, conjectured as a theory of normal adult human information processing. The architecture-based concepts that it supports provide a framework for defining with greater precision than previously a host of mental concepts, including affective concepts. We then find that these map more or less loosely onto various pre-theoretical concepts, such as “emotion”, etc. We indicate some of the variety of emotion concepts generated by the H-Cogaff architecture A different architecture, supporting a different range of mental concepts might be appropriate for exploring affective states of other animals, for instance insects, reptiles, or other mammals, and young children.","[{'authorId': '145788442', 'name': 'A. Sloman'}]",117.0,"{'bibtex': '@Inproceedings{Sloman2001VarietiesOA,\n author = {A. Sloman},\n title = {Varieties of Affect and the CogAff Architecture Schema},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",41.0,Varieties of Affect and the CogAff Architecture Schema,2001.0
118,0abd78ca51a0c8fee6c1ed4756360961a1419a13,"Why do some students seek, while others avoid, second language (L2) communication? Many language teachers have encountered students high in linguistic competence who are unwilling to use their L2 for communication whereas other students, with only minimal linguistic knowledge, seem to communicate in the L2 whenever possible. Despite excellent communicative competence, spontaneous and sustained use of the L2 is not ensured. A colleague, who teaches a L2 and whose L2 competence is excellent, is well known to avoid “like the plague” L2 communication in social settings. A related observation is that many learners have noticed that their willingness to communicate (WTC) varies considerably over time and across situations. Our aim in this article is twofold. First we wish to provide an account of the linguistic, communicative, and social psychological variables that might affect one's “willingness to communicate.” As demonstrated in the text below, and examination of WTC offers the opportunity to integrate psychological, linguistic, and communicative approaches to L2 research that typically have been independent of each other. We view the WTC model as having the potential to provide a useful interface between these disparate lines of inquiry. Our second goal is to suggest potential relations among these variables by outlining a comprehensive conceptual model that may be useful in describing, explaining, and predicting L2 communication. In an effort to move beyond linguistic or communicative competence as the primary goal of language instruction, this article represents an overt attempt to combine these disparate approaches in a common theme, that is, proposing WTC as the primary goal of language instruction.","[{'authorId': '40447483', 'name': 'P. MacIntyre'}, {'authorId': '8142683', 'name': 'Z. Dörnyei'}, {'authorId': '144637030', 'name': 'R. Clément'}, {'authorId': '2018388', 'name': 'K. Noels'}]",1672.0,"{'bibtex': '@Article{MacIntyre1998ConceptualizingWT,\n author = {P. MacIntyre and Z. Dörnyei and R. Clément and K. Noels},\n journal = {The Modern Language Journal},\n pages = {545-562},\n title = {Conceptualizing Willingness to Communicate in a L2: A Situational Model of L2 Confidence and Affiliation},\n volume = {82},\n year = {1998}\n}\n'}",,"{'volume': '82', 'pages': '545-562', 'name': 'The Modern Language Journal'}",92.0,Conceptualizing Willingness to Communicate in a L2: A Situational Model of L2 Confidence and Affiliation,1998.0
119,0acbef813ca104634a8147962285ea45ed28ff48,,"[{'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2230689684', 'name': 'A.E. Yoram Tap'}, {'authorId': '4837582', 'name': 'M. Rotteveel'}, {'authorId': '2545816', 'name': 'R. Phaf'}]",7.0,"{'bibtex': '@Article{Heerebout2013GammaFE,\n author = {B. T. Heerebout and A.E. Yoram Tap and M. Rotteveel and R. Phaf},\n journal = {Consciousness and Cognition},\n pages = {281-289},\n title = {Gamma flicker elicits positive affect without awareness},\n volume = {22},\n year = {2013}\n}\n'}",,"{'volume': '22', 'pages': '281-289', 'name': 'Consciousness and Cognition'}",33.0,Gamma flicker elicits positive affect without awareness,2013.0
120,0ae7e526fd50102fbad4f781105afac0cb5c06e4,"This article considers tools to support remote gesture in video systems being used to complete collaborative physical tasks-tasks in which two or more individuals work together manipulating three-dimensional objects in the real world. We first discuss the process of conversational grounding during collaborative physical tasks, particularly the role of two types of gestures in the grounding process: pointing gestures, which are used to refer to task objects and locations, and representational gestures, which are used to represent the form of task objects and the nature of actions to be used with those objects. We then consider ways in which both pointing and representational gestures can be instantiated in systems for remote collaboration on physical tasks. We present the results of two studies that use a ""surrogate"" approach to remote gesture, in which images are intended to express the meaning of gestures through visible embodiments, rather than direct views of the hands. In Study 1, we compare performance with a cursor-based pointing device that allows remote partners to point to objects in a video feed of the work area to performance side-by-side or with the video system alone. In Study 2, we compare performance with two variations of a pen-based drawing tool that allows for both pointing and representational gestures to performance with video alone. The results suggest that simple surrogate gesture tools can be used to convey gestures from remote sites, but that the tools need to be able to convey representational as well as pointing gestures to be effective. The results further suggest that an automatic erasure function, in which drawings disappear a few seconds after they were created, is more beneficial for collaboration than tools requiring manual erasure. We conclude with a discussion of the theoretical and practical implications of the results, as well as several areas for future research.","[{'authorId': '1692772', 'name': 'Susan R. Fussell'}, {'authorId': '3190175', 'name': 'Leslie D. Setlock'}, {'authorId': '2118579343', 'name': 'Jie Yang'}, {'authorId': '1962473', 'name': 'Jiazhi Ou'}, {'authorId': '2075606633', 'name': 'Elizabeth Mauer'}, {'authorId': '1845400', 'name': 'Adam D. I. Kramer'}]",329.0,"{'bibtex': '@Article{Fussell2004GesturesOV,\n author = {Susan R. Fussell and Leslie D. Setlock and Jie Yang and Jiazhi Ou and Elizabeth Mauer and Adam D. I. Kramer},\n journal = {Human–Computer Interaction},\n pages = {273 - 309},\n title = {Gestures Over Video Streams to Support Remote Collaboration on Physical Tasks},\n volume = {19},\n year = {2004}\n}\n'}",,"{'volume': '19', 'pages': '273 - 309', 'name': 'Human–Computer Interaction'}",54.0,Gestures Over Video Streams to Support Remote Collaboration on Physical Tasks,2004.0
121,0b33a4f813b07ab62ad05b3fc0b54972a00343bf,"Often the AI techniques for decision making used in commercial games are predictable and unadaptive. Arguably, this causes a lack of realism for the players. We believe that emotions are a vital part in the creation of interesting and believable non-player characters for games. In this paper, we present an extension to behavior trees that incorporates emotions into the decision making. Specifically, we introduce a new type of priority selector whose priorities are dynamically evaluated to allow for emotional influence. This selector takes into account time-discounting, risk perception, and planning as relevant factors of the decision making that emotions greatly influence. The objective of our work is to provide game developers with a technique to model gaming scenarios using emotional behavior trees.","[{'authorId': '39799288', 'name': 'Anja Johansson'}, {'authorId': '1403410455', 'name': ""Pierangelo Dell'Acqua""}]",28.0,"{'bibtex': ""@Article{Johansson2012EmotionalBT,\n author = {Anja Johansson and Pierangelo Dell'Acqua},\n journal = {2012 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {355-362},\n title = {Emotional behavior trees},\n year = {2012}\n}\n""}",,"{'pages': '355-362', 'name': '2012 IEEE Conference on Computational Intelligence and Games (CIG)'}",47.0,Emotional behavior trees,2012.0
122,0b5696a31de431b0f8f86b2ff9c5ef17ef5a97b8,,"[{'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '8112579', 'name': 'I. D. Kok'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",162.0,"{'bibtex': '@Article{Morency2009APM,\n author = {Louis-Philippe Morency and I. D. Kok and J. Gratch},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {70-84},\n title = {A probabilistic multimodal approach for predicting listener backchannels},\n volume = {20},\n year = {2009}\n}\n'}",,"{'volume': '20', 'pages': '70-84', 'name': 'Autonomous Agents and Multi-Agent Systems'}",40.0,A probabilistic multimodal approach for predicting listener backchannels,2009.0
123,0bb0ffdd3e6f1fbf1df15de84b88ff788ec77400,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}, {'authorId': '2114900598', 'name': 'Andrew Young'}, {'authorId': '152639768', 'name': 'S. Sato'}]",210.0,"{'bibtex': '@Article{Mehrabian1988EmotionalEA,\n author = {A. Mehrabian and Andrew Young and S. Sato},\n journal = {Current Psychology},\n pages = {221-240},\n title = {Emotional empathy and associated individual differences},\n volume = {7},\n year = {1988}\n}\n'}",,"{'volume': '7', 'pages': '221-240', 'name': 'Current Psychology'}",67.0,Emotional empathy and associated individual differences,1988.0
124,0bb35c411a2c1d2ba4cb295cc158c2f7b101d097,,"[{'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '2349057', 'name': 'K. Kipp'}]",45.0,"{'bibtex': '@Inproceedings{Gebhard2006AreCE,\n author = {Patrick Gebhard and K. Kipp},\n pages = {343-356},\n title = {Are Computer-Generated Emotions and Moods Plausible to Humans?},\n year = {2006}\n}\n'}",,{'pages': '343-356'},30.0,Are Computer-Generated Emotions and Moods Plausible to Humans?,2006.0
126,0bfbd54c2ed38d580fd8c6fb2baf5ee59525dabc,"Developing intelligent virtual characters has attracted a lot of attention in the recent years. The process of creating such characters often involves a team of creative authors who describe different aspects of the characters in natural language, and planning experts that translate this description into a planning domain. This can be quite challenging as the team of creative authors should diligently define every aspect of the character especially if it contains complex human-like behavior. Also a team of engineers has to manually translate the natural language description of a character's personality into the planning domain knowledge. This can be extremely time and resource demanding and can be an obstacle to author's creativity. The goal of this paper is to introduce an authoring assistant tool to automate the process of domain generation from natural language description of virtual characters, thus bridging between the creative authoring team and the planning domain experts. More- over, the proposed tool also identifies possible missing information in the domain description and iteratively makes suggestions to the author.","[{'authorId': '51057369', 'name': 'Sepehr Janghorbani'}, {'authorId': '2477939', 'name': 'Ashutosh Modi'}, {'authorId': '51147349', 'name': 'Jakob Buhmann'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",14.0,"{'bibtex': '@Article{Janghorbani2019DomainAA,\n author = {Sepehr Janghorbani and Ashutosh Modi and Jakob Buhmann and Mubbasir Kapadia},\n journal = {ArXiv},\n title = {Domain Authoring Assistant for Intelligent Virtual Agents},\n volume = {abs/1904.03266},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1904.03266', 'name': 'ArXiv'}",46.0,Domain Authoring Assistant for Intelligent Virtual Agents,2019.0
127,0bfff1ee8f14815d847fe7ecb9d7d2c0e52e6e38,"As in most of secondary schools in Indonesia tend to focus on words list in vocabulary lesson which is easily forgotten by learners, providing retrieval opportunity to strengthen their memory about words can be effective since the retrieval is one of the psychological processes to make the students remember the words. Accordingly, providing retrieval opportunity during vocabulary learning is encouraged in this essay. After reviewing some literature related to retrieval process, repetition is one effective way to promote retrieval process. Since repeated encounters of the words facilitate the students to access the words-related information in their mind, repetition activity will be the effective solution for this context. The spaced repetition as proposed by Baddley (1990) can be appropriate for this context since learners will not be bored about the repetition. This idea is then developed by providing students with Southeast Sulawesi folklore as the reading material in order to obtain target words. It is believed that this kind of reading material can enhance the students’ active engagement in the classroom as well.","[{'authorId': '82317834', 'name': 'D. Atikah'}, {'authorId': '84164742', 'name': 'Anita Rezki'}]",5.0,"{'bibtex': '@Article{Atikah2018RepetitionFR,\n author = {D. Atikah and Anita Rezki},\n journal = {IOP Conference Series: Earth and Environmental Science},\n title = {Repetition Facilitates Retrieval Opportunity in Vocabulary Learning},\n volume = {175},\n year = {2018}\n}\n'}",,"{'volume': '175', 'name': 'IOP Conference Series: Earth and Environmental Science'}",16.0,Repetition Facilitates Retrieval Opportunity in Vocabulary Learning,2018.0
128,0c2756bf713fdbf216e89cdaf96cc993619b56b8,The following article presents a collaborative VR system designed to assess social inclusion among neurodiverse students. Students can join virtual environments to communicate and collaborate in a virtual supermarket shopping trip. A virtual classroom is developed in which students can be introduced to their task and the basic concepts of shopping. An overview of related literature in inclusion research is presented and used to discuss how the VR system can be used in future studies to measure social inclusion.,"[{'authorId': '30995887', 'name': 'L. Thomsen'}, {'authorId': '20410106', 'name': 'Ali Adjorlu'}]",2.0,"{'bibtex': '@Article{Thomsen2021DesigningAC,\n author = {L. Thomsen and Ali Adjorlu},\n journal = {2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},\n pages = {353-357},\n title = {Designing a collaborative virtual reality system to assess social inclusion among neurodiverse students},\n year = {2021}\n}\n'}",,"{'pages': '353-357', 'name': '2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)'}",28.0,Designing a collaborative virtual reality system to assess social inclusion among neurodiverse students,2021.0
129,0c29e1319bbbacf1ea3821eccf95aabb3d21c9e3,,"[{'authorId': '10129966', 'name': 'D. Schacter'}]",753.0,"{'bibtex': '@Inproceedings{Schacter2001TheSS,\n author = {D. Schacter},\n title = {The Seven Sins of Memory: How the Mind Forgets and Remembers},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Seven Sins of Memory: How the Mind Forgets and Remembers,2001.0
130,0c3f276c4857e30f70854f5ed215502dba7d2bf7,,"[{'authorId': '3259367', 'name': 'J. Albus'}, {'authorId': '145667472', 'name': 'A. Barbera'}]",44.0,"{'bibtex': '@Article{Albus2004RCSAC,\n author = {J. Albus and A. Barbera},\n journal = {Annual Reviews in Control},\n pages = {87-99},\n title = {RCS: A cognitive architecture for intelligent multi-agent systems☆},\n volume = {29},\n year = {2004}\n}\n'}",,"{'volume': '29', 'pages': '87-99', 'name': 'Annual Reviews in Control'}",32.0,RCS: A cognitive architecture for intelligent multi-agent systems☆,2004.0
131,0c5afb209b647456e99ce42a6d9d177764f9a0dd,"Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentions are more often communicated by changes in one or a few discrete facial features. In this paper, we develop an Automatic Face Analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes in facial expression into action units (AUs) of the Facial Action Coding System (FACS), instead of a few prototypic expressions. Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and furrows. During tracking, detailed parametric descriptions of the facial features are extracted. With these parameters as the inputs, a group of action units (neutral expression, six upper face AUs and 10 lower face AUs) are recognized whether they occur alone or in combinations. The system has achieved average recognition rates of 96.4 percent (95.4 percent if neutral expressions are excluded) for upper face AUs and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face AUs. The generalizability of the system has been tested by using independent image databases collected and FACS-coded for ground-truth by different research teams.","[{'authorId': '40383812', 'name': 'Ying-li Tian'}, {'authorId': '1733113', 'name': 'T. Kanade'}, {'authorId': '1737918', 'name': 'J. Cohn'}]",1837.0,"{'bibtex': '@Article{Tian2001RecognizingAU,\n author = {Ying-li Tian and T. Kanade and J. Cohn},\n journal = {IEEE transactions on pattern analysis and machine intelligence},\n pages = {\n          97-115\n        },\n title = {Recognizing Action Units for Facial Expression Analysis},\n volume = {23 2},\n year = {2001}\n}\n'}",,"{'volume': '23 2', 'pages': '\n          97-115\n        ', 'name': 'IEEE transactions on pattern analysis and machine intelligence'}",54.0,Recognizing Action Units for Facial Expression Analysis,2001.0
132,0c7492a870c73eed22c19fb4b827dd581be27d3e,,"[{'authorId': '4834966', 'name': 'N. Racine'}, {'authorId': '116630031', 'name': 'J. Cooke'}, {'authorId': '66852066', 'name': 'Rachel Eirich'}, {'authorId': '48503019', 'name': 'D. Korczak'}, {'authorId': '39578140', 'name': 'B. McArthur'}, {'authorId': '4017256', 'name': 'S. Madigan'}]",252.0,"{'bibtex': '@Article{Racine2020ChildAA,\n author = {N. Racine and J. Cooke and Rachel Eirich and D. Korczak and B. McArthur and S. Madigan},\n journal = {Psychiatry Research},\n pages = {113307 - 113307},\n title = {Child and adolescent mental illness during COVID-19: A rapid review},\n volume = {292},\n year = {2020}\n}\n'}",,"{'volume': '292', 'pages': '113307 - 113307', 'name': 'Psychiatry Research'}",10.0,Child and adolescent mental illness during COVID-19: A rapid review,2020.0
133,0c77e3e0058361eef0f277793a1d32b2cee823e5,,"[{'authorId': '118374367', 'name': 'P. H. Lodhi'}, {'authorId': '114335645', 'name': 'S. Deo'}, {'authorId': '102571978', 'name': 'Vivek Belhekar'}]",259.0,"{'bibtex': '@Inproceedings{Lodhi2002TheFM,\n author = {P. H. Lodhi and S. Deo and Vivek Belhekar},\n pages = {227-248},\n title = {The Five-Factor Model of Personality},\n year = {2002}\n}\n'}",,"{'volume': '', 'pages': '227-248', 'name': ''}",54.0,The Five-Factor Model of Personality,2002.0
134,0ca31ced65b55f47f7cec53456376dff8ac6643f,,"[{'authorId': '153492041', 'name': 'Matt C. Howard'}]",297.0,"{'bibtex': '@Article{Howard2017AMA,\n author = {Matt C. Howard},\n journal = {Comput. Hum. Behav.},\n pages = {317-327},\n title = {A meta-analysis and systematic literature review of virtual reality rehabilitation programs},\n volume = {70},\n year = {2017}\n}\n'}",,"{'volume': '70', 'pages': '317-327', 'name': 'Comput. Hum. Behav.'}",79.0,A meta-analysis and systematic literature review of virtual reality rehabilitation programs,2017.0
135,0cab531faa123491630b49da3290eb2b7eceeb17,"ABSTRACT This special issue describes a number of applications that utilize lifelike characters that teach indirectly, by playing some role in a social interaction with a user. The design of such systems reflects a compromise between competing, sometimes unarticulated, demands: They must realistically exhibit the behaviors and characteristics of their role, they must facilitate the desired learning, and they must work within the limitations of current technology, and there is little theoretical or empirical guidance on the impact of these compromises on learning. Our perspective on this problem is shaped by our interest in the role of emotion and emotional behaviors in such forms of learning. In recent years, there has been an explosion of interest in the role of emotion in the design of virtual humans. The techniques and motivations underlying these various efforts can seem, from an outsider's perspective, as bewildering and multifaceted as the concept of emotion itself is generally accused of being. Drawing on insights from emotion psychology, this article attempts to clarify for the designers of educational agents the various theoretical perspectives on the concept of emotion with the aim of giving guidance to designers of educational agents.","[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",96.0,"{'bibtex': '@Article{Gratch2005LESSONSFE,\n author = {J. Gratch and S. Marsella},\n journal = {Applied Artificial Intelligence},\n pages = {215 - 233},\n title = {LESSONS FROM EMOTION PSYCHOLOGY FOR THE DESIGN OF LIFELIKE CHARACTERS},\n volume = {19},\n year = {2005}\n}\n'}",,"{'volume': '19', 'pages': '215 - 233', 'name': 'Applied Artificial Intelligence'}",88.0,LESSONS FROM EMOTION PSYCHOLOGY FOR THE DESIGN OF LIFELIKE CHARACTERS,2005.0
136,0cc6ca8e9980232e922eba0d0b888b2a3d90bbc1,"It is widely believed that certain emotions are universally recognized in facial expressions. Recent evidence indicates that Western perceptions (e.g., scowls as anger) depend on cues to U.S. emotion concepts embedded in experiments. Because such cues are standard features in methods used in cross-cultural experiments, we hypothesized that evidence of universality depends on this conceptual context. In our study, participants from the United States and the Himba ethnic group from the Keunene region of northwestern Namibia sorted images of posed facial expressions into piles by emotion type. Without cues to emotion concepts, Himba participants did not show the presumed ""universal"" pattern, whereas U.S. participants produced a pattern with presumed universal features. With cues to emotion concepts, participants in both cultures produced sorts that were closer to the presumed ""universal"" pattern, although substantial cultural variation persisted. Our findings indicate that perceptions of emotion are not universal, but depend on cultural and conceptual contexts.",[],297.0,"{'bibtex': '@Article{None,\n journal = {Emotion},\n pages = {\n          251-62\n        },\n title = {Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture.},\n volume = {14 2},\n year = {2014}\n}\n'}",,"{'volume': '14 2', 'pages': '\n          251-62\n        ', 'name': 'Emotion'}",61.0,Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture.,2014.0
137,0cf9c7bd93a7256717245e799b36eb5e689be3de,"his paper argues that, despite the changes in philosophies and techniques that have occurred since ITS research began, there are continuous threads running through this research which define its essential and distinctive nature. In particular, ITSs are computer-based learning systems which attempt to adapt to the needs of learners and are therefore the only such systems which attempt to 'care' about learners in that sense. Also, ITS research is the only part of the general IT and education field which has as its scientific goal to make computationally precise and explicit forms of educational, psychological and social knowledge which are often left implicit. (http://aied.inf.ed.ac.uk/members99/archive/vol_10/self_paper/full.html)","[{'authorId': '1773413', 'name': 'J. Self'}]",273.0,"{'bibtex': '@Inproceedings{Self1998TheDC,\n author = {J. Self},\n pages = {350-364},\n title = {The defining characteristics of intelligent tutoring systems research: ITSs care, precisely},\n volume = {10},\n year = {1998}\n}\n'}",,"{'volume': '10', 'pages': '350-364', 'name': ''}",22.0,"The defining characteristics of intelligent tutoring systems research: ITSs care, precisely",1998.0
138,0d02686296773fe5c70cff03eed434e816eea836,,"[{'authorId': '2739604', 'name': 'Scott Brave'}, {'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '114107135', 'name': 'Kevin Hutchinson'}]",458.0,"{'bibtex': '@Article{Brave2005ComputersTC,\n author = {Scott Brave and C. Nass and Kevin Hutchinson},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {161-178},\n title = {Computers that care: investigating the effects of orientation of emotion exhibited by an embodied computer agent},\n volume = {62},\n year = {2005}\n}\n'}",,"{'volume': '62', 'pages': '161-178', 'name': 'Int. J. Hum. Comput. Stud.'}",45.0,Computers that care: investigating the effects of orientation of emotion exhibited by an embodied computer agent,2005.0
139,0d091a0d217f4a4f9d744135f179efc24a776625,"Impairments in using eye gaze to establish joint attention and to comprehend the mental states and intentions of other people are striking features of autism. Here, using event-related functional MRI (fMRI), we show that in autism, brain regions involved in gaze processing, including the superior temporal sulcus (STS) region, are not sensitive to intentions conveyed by observed gaze shifts. On congruent trials, subjects watched as a virtual actor looked towards a checkerboard that appeared in her visual field, confirming the subject's expectation regarding what the actor 'ought to do' in this context. On incongruent trials, she looked towards empty space, violating the subject's expectation. Consistent with a prior report from our laboratory that used this task in neurologically normal subjects, 'errors' (incongruent trials) evoked more activity in the STS and other brain regions linked to social cognition, indicating a strong effect of intention in typically developing subjects (n = 9). The same brain regions were activated during observation of gaze shifts in subjects with autism (n = 10), but did not differentiate congruent and incongruent trials, indicating that activity in these regions was not modulated by the context of the perceived gaze shift. These results demonstrate a difference in the response of brain regions underlying eye gaze processing in autism. We conclude that lack of modulation of the STS region by gaze shifts that convey different intentions contributes to the eye gaze processing deficits associated with autism.","[{'authorId': '9765768', 'name': 'K. Pelphrey'}, {'authorId': '2118077239', 'name': 'James P. Morris'}, {'authorId': '145984907', 'name': 'G. McCarthy'}]",435.0,"{'bibtex': '@Article{Pelphrey2005NeuralBO,\n author = {K. Pelphrey and James P. Morris and G. McCarthy},\n journal = {Brain : a journal of neurology},\n pages = {\n          1038-48\n        },\n title = {Neural basis of eye gaze processing deficits in autism.},\n volume = {128 Pt 5},\n year = {2005}\n}\n'}",,"{'volume': '128 Pt 5', 'pages': '\n          1038-48\n        ', 'name': 'Brain : a journal of neurology'}",46.0,Neural basis of eye gaze processing deficits in autism.,2005.0
140,0d0efe3c8aa2f9e530a11b8c7ba9e681cc296ade,"Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.","[{'authorId': '1693399289', 'name': 'Aimée Sousa Calepso'}, {'authorId': '41159925', 'name': 'Natalie Hube'}, {'authorId': '2164042241', 'name': 'Noah Berenguel Senn'}, {'authorId': '2163479960', 'name': 'Vincent Brandt'}, {'authorId': '3020591', 'name': 'M. Sedlmair'}]",2.0,"{'bibtex': '@Article{Calepso2022cARdLearnerUE,\n author = {Aimée Sousa Calepso and Natalie Hube and Noah Berenguel Senn and Vincent Brandt and M. Sedlmair},\n booktitle = {CHI Extended Abstracts},\n journal = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},\n title = {cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality},\n year = {2022}\n}\n'}","[{'paperId': '734c0cfedea4488753cbed0a3aed4aae8e5176ae', 'title': 'MoveToCode: An Embodied Augmented Reality Visual Programming Language with an Autonomous Robot Tutor for Promoting Student Programming Curiosity'}, {'paperId': '676dd72ff7972c92f4ff1f588635bae120377168', 'title': 'The Effectiveness of Augmented Reality Using Flash Card in Education to Learn Simple English Words as a Secondary Language'}]",{'name': 'CHI Conference on Human Factors in Computing Systems Extended Abstracts'},31.0,cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality,2022.0
141,0d1a655f31c0f66bdb533b7f0b1dbaf1ff95f532,"People commonly anthropomorphize nonhuman agents, imbuing everything from computers to pets to gods with humanlike capacities and mental experiences. Although widely observed, the determinants of anthropomorphism are poorly understood and rarely investigated. We propose that people anthropomorphize, in part, to satisfy effectance motivation-the basic and chronic motivation to attain mastery of one's environment. Five studies demonstrated that increasing effectance motivation by manipulating the perceived unpredictability of a nonhuman agent or by increasing the incentives for mastery increases anthropomorphism. Neuroimaging data demonstrated that the neural correlates of this process are similar to those engaged when mentalizing other humans. A final study demonstrated that anthropomorphizing a stimulus makes it appear more predictable and understandable, suggesting that anthropomorphism satisfies effectance motivation. Anthropomorphizing nonhuman agents seems to satisfy the basic motivation to make sense of an otherwise uncertain environment.","[{'authorId': '3377580', 'name': 'A. Waytz'}, {'authorId': '2295910', 'name': 'Carey K. Morewedge'}, {'authorId': '7007014', 'name': 'Nicholas Epley'}, {'authorId': '1969782', 'name': 'George Monteleone'}, {'authorId': '48441202', 'name': 'Jianfeng Gao'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}]",391.0,"{'bibtex': '@Article{Waytz2010MakingSB,\n author = {A. Waytz and Carey K. Morewedge and Nicholas Epley and George Monteleone and Jianfeng Gao and J. Cacioppo},\n journal = {Journal of personality and social psychology},\n pages = {\n          410-35\n        },\n title = {Making sense by making sentient: effectance motivation increases anthropomorphism.},\n volume = {99 3},\n year = {2010}\n}\n'}",,"{'volume': '99 3', 'pages': '\n          410-35\n        ', 'name': 'Journal of personality and social psychology'}",174.0,Making sense by making sentient: effectance motivation increases anthropomorphism.,2010.0
142,0d3fb665cbf4a9e2d633e445c6f6fbb38dfb026a,,"[{'authorId': '2063522518', 'name': 'Abhisek Tiwari'}, {'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '2062808558', 'name': 'Shubhashis Sengupta'}, {'authorId': '40585053', 'name': 'Anutosh Maitra'}, {'authorId': '3040439', 'name': 'Roshni Ramnani'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",3.0,"{'bibtex': '@Article{Tiwari2022APA,\n author = {Abhisek Tiwari and Tulika Saha and S. Saha and Shubhashis Sengupta and Anutosh Maitra and Roshni Ramnani and P. Bhattacharyya},\n journal = {Expert Syst. Appl.},\n pages = {116303},\n title = {A persona aware persuasive dialogue policy for dynamic and co-operative goal setting},\n volume = {195},\n year = {2022}\n}\n'}",,"{'volume': '195', 'pages': '116303', 'name': 'Expert Syst. Appl.'}",38.0,A persona aware persuasive dialogue policy for dynamic and co-operative goal setting,2022.0
143,0d67f1542f86a1cde792701a9ae5c824a5fad3f9,OBJECTIVES: To examine the temporal association between depressive symptoms and cognitive functioning and estimate the effect measure modification of the apolipoprotein E (APOE) ɛ4 allele on this relationship.,"[{'authorId': '145551891', 'name': 'S. Köhler'}, {'authorId': '34697200', 'name': 'M. V. van Boxtel'}, {'authorId': '2257662561', 'name': 'J. van Os'}, {'authorId': '2224732178', 'name': 'Alan Thomas'}, {'authorId': '2255183798', 'name': ""John T. O'Brien""}, {'authorId': '144438320', 'name': 'J. Jolles'}, {'authorId': '2253142888', 'name': 'Frans R J Verhey'}, {'authorId': '2073448603', 'name': 'J. Allardyce'}]",123.0,"{'bibtex': ""@Article{Köhler2010DepressiveSA,\n author = {S. Köhler and M. V. van Boxtel and J. van Os and Alan Thomas and John T. O'Brien and J. Jolles and Frans R J Verhey and J. Allardyce},\n journal = {Journal of the American Geriatrics Society},\n title = {Depressive Symptoms and Cognitive Decline in Community‐Dwelling Older Adults},\n volume = {58},\n year = {2010}\n}\n""}",,"{'volume': '58', 'name': 'Journal of the American Geriatrics Society'}",57.0,Depressive Symptoms and Cognitive Decline in Community‐Dwelling Older Adults,2010.0
144,0d7bd0c278162bfb69621439a3b8dd217478e76e,,"[{'authorId': '7788375', 'name': 'M. Novack'}, {'authorId': '115377287', 'name': 'S. Goldin‐Meadow'}]",100.0,"{'bibtex': '@Article{Novack2015LearningFG,\n author = {M. Novack and S. Goldin‐Meadow},\n journal = {Educational Psychology Review},\n pages = {405 - 412},\n title = {Learning from Gesture: How Our Hands Change Our Minds},\n volume = {27},\n year = {2015}\n}\n'}",,"{'volume': '27', 'pages': '405 - 412', 'name': 'Educational Psychology Review'}",46.0,Learning from Gesture: How Our Hands Change Our Minds,2015.0
145,0d9658d9e1ba9adc9e0f691347a251ed8335e030,,"[{'authorId': '144627382', 'name': 'John Short'}, {'authorId': '30111315', 'name': 'Ederyn Williams'}, {'authorId': '117015815', 'name': 'B. Christie'}]",5278.0,"{'bibtex': '@Inproceedings{Short1976TheSP,\n author = {John Short and Ederyn Williams and B. Christie},\n title = {The social psychology of telecommunications},\n year = {1976}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The social psychology of telecommunications,1976.0
146,0dac8ec705eb251de270b39673f2752728bbaff0,,"[{'authorId': '8089863', 'name': 'R. Cole'}, {'authorId': '1403911433', 'name': 'Cindy Buchenroth-Martin'}, {'authorId': '3035147', 'name': 'T. Weston'}, {'authorId': '2072332505', 'name': 'Liam Devine'}, {'authorId': '1408443187', 'name': 'Jeannine Myatt'}, {'authorId': '3026361', 'name': 'Brandon Helding'}, {'authorId': '1735131', 'name': 'Sameer Pradhan'}, {'authorId': '1935567', 'name': 'Margaret G. McKeown'}, {'authorId': '47165157', 'name': 'Samantha Messier'}, {'authorId': '120772509', 'name': 'Jennifer Borum'}, {'authorId': '1866226', 'name': 'Wayne H. Ward'}]",8.0,"{'bibtex': '@Article{Cole2018OneononeAS,\n author = {R. Cole and Cindy Buchenroth-Martin and T. Weston and Liam Devine and Jeannine Myatt and Brandon Helding and Sameer Pradhan and Margaret G. McKeown and Samantha Messier and Jennifer Borum and Wayne H. Ward},\n journal = {Comput. Speech Lang.},\n pages = {157-174},\n title = {One-on-one and small group conversations with an intelligent virtual science tutor},\n volume = {50},\n year = {2018}\n}\n'}",,"{'volume': '50', 'pages': '157-174', 'name': 'Comput. Speech Lang.'}",80.0,One-on-one and small group conversations with an intelligent virtual science tutor,2018.0
147,0dfbd4e53b90a3969e56119333521af5bfad0cb6,,"[{'authorId': '2250292983', 'name': 'Nicola Bellomo'}, {'authorId': '2250292983', 'name': 'Nicola Bellomo'}, {'authorId': '34287161', 'name': 'D. Clarke'}, {'authorId': '2004264', 'name': 'L. Gibelli'}, {'authorId': '2250293861', 'name': 'Philippa Townsend'}, {'authorId': '3436358', 'name': 'B. Vreugdenhil'}]",114.0,"{'bibtex': '@Article{Bellomo2016HumanBI,\n author = {Nicola Bellomo and Nicola Bellomo and D. Clarke and L. Gibelli and Philippa Townsend and B. Vreugdenhil},\n journal = {Physics of life reviews},\n pages = {\n          1-21\n        },\n title = {Human behaviours in evacuation crowd dynamics: From modelling to ""big data"" toward crisis management.},\n volume = {18},\n year = {2016}\n}\n'}",,"{'volume': '18', 'pages': '\n          1-21\n        ', 'name': 'Physics of life reviews'}",95.0,"Human behaviours in evacuation crowd dynamics: From modelling to ""big data"" toward crisis management.",2016.0
148,0e0fc36255b7129b5a03a52bc1df55722e02033c,"Although product recommendation virtual agents (PRVAs) are used in a large number of online shopping websites, the optimal types of agents in this context remain unclear. In the present study, we tested whether agent appearance affects people's buying motivations and analyzed the key factors in persuading people to buy products. The experimental results confirmed that recommendation effects vary according to agent appearance. Furthermore, we obtained a partial order ranking of the agent types, representing the effectiveness of their recommendations. The factor analysis results indicated that the perceptions of familiarity and intelligence in relation to appearance are the key factors in persuading people to buy products.","[{'authorId': '145990834', 'name': 'K. Terada'}, {'authorId': '2113523161', 'name': 'Liang Jing'}, {'authorId': '1679243', 'name': 'S. Yamada'}]",29.0,"{'bibtex': '@Article{Terada2015EffectsOA,\n author = {K. Terada and Liang Jing and S. Yamada},\n journal = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},\n title = {Effects of Agent Appearance on Customer Buying Motivations on Online Shopping Sites},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems'},11.0,Effects of Agent Appearance on Customer Buying Motivations on Online Shopping Sites,2015.0
150,0e2c10aaf5274c662c9d35831b1180ff47ba097b,"This paper essentially tries to examine all the roles that Virtual Humans can play in empowering human expression, and the research challenges we have to face to make this possible. It starts with a short history of Virtual Humans and how we contribute to the foundations of this field. We then define six typical Virtual Humans: the Performing Virtual Human, the Physiological Virtual Human, the Learning Virtual Human, the Connected Virtual Human, the Secure Virtual Human, and the Anthropometric Virtual Human. For each category, we provide a definition and a few possible scenarios, then we try to identify the research challenges, the past experiences, and some unsolved core issues.","[{'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}]",3.0,"{'bibtex': '@Inproceedings{Magnenat-Thalmann2012VirtualHB,\n author = {N. Magnenat-Thalmann and D. Thalmann},\n pages = {1-8},\n title = {Virtual humans: back to the future},\n year = {2012}\n}\n'}",,{'pages': '1-8'},23.0,Virtual humans: back to the future,2012.0
151,0e33eca0541b5d5fd59532239006ae1c26729dca,"Although the adverse consequences of changes in social behavior following traumatic brain injury (TBI) are well documented, relatively little is known about possible underlying neuropsychological deficits. Following a model originally developed for social behavior deficits in schizophrenia, we investigated whether impairments in emotion recognition, understanding of other people's intentions (“theory of mind”), and cognitive flexibility soon after first TBI or 1 year later were associated with self and proxy ratings of behavior following TBI. Each of the three functions was assessed with two separate tests, and ratings of behavior were collected on three questionnaires. Patients with TBI (n = 33) were impaired in emotion recognition, “theory of mind,” and cognitive flexibility compared with matched orthopedic controls (n = 34). Proxy ratings showed increases in behavioral problems 1 year following injury in the TBI group but not in the control group. However, test performance was not associated with questionnaire data. Severity of the impairments in emotion recognition, understanding intention, and flexibility were unrelated to the severity of behavioral problems following TBI. These findings failed to confirm the used model for social behavior deficits and may cast doubt on the alleged link between deficits in emotion recognition or theory of mind and social functioning. (JINS, 2008, 14, 318–326.)","[{'authorId': '3748173', 'name': 'M. Milders'}, {'authorId': '6088500', 'name': 'M. Ietswaart'}, {'authorId': '144723673', 'name': 'J. Crawford'}, {'authorId': '3496915', 'name': 'D. Currie'}]",144.0,"{'bibtex': '@Article{Milders2008SocialBF,\n author = {M. Milders and M. Ietswaart and J. Crawford and D. Currie},\n journal = {Journal of the International Neuropsychological Society},\n pages = {318 - 326},\n title = {Social behavior following traumatic brain injury and its association with emotion recognition, understanding of intentions, and cognitive flexibility},\n volume = {14},\n year = {2008}\n}\n'}",,"{'volume': '14', 'pages': '318 - 326', 'name': 'Journal of the International Neuropsychological Society'}",65.0,"Social behavior following traumatic brain injury and its association with emotion recognition, understanding of intentions, and cognitive flexibility",2008.0
152,0e41b0698fd13d3216598767d6767860ba0cd7ae,"The internet is rife with theories about the identity of Miquela Sousa. Some say the computer generated 'it-girl', better known as Lil Miquela, is the alter-ego of another recording artist, a marketing strategy, or simply a piece of digital performance art. Her creators are keeping us in the dark - with 1.2 million Instagram followers at stake, they want to sustain the mystery.","[{'authorId': '39169675', 'name': 'Claire Hubble'}]",3.0,"{'bibtex': '@Article{Hubble2018MiquelaSA,\n author = {Claire Hubble},\n journal = {Eureka street},\n pages = {20},\n title = {Miquela Sousa and the rise of fake influencers},\n volume = {28},\n year = {2018}\n}\n'}",,"{'volume': '28', 'pages': '20', 'name': 'Eureka street'}",0.0,Miquela Sousa and the rise of fake influencers,2018.0
153,0e6f5abd7e4738b765cd48f4c272093ecb5fd0bc,,"[{'authorId': '2389763', 'name': 'Angela Di Serio'}, {'authorId': '1447342257', 'name': 'M. Ibáñez-Espiga'}, {'authorId': '118042932', 'name': 'C. D. Kloos'}]",845.0,"{'bibtex': ""@Article{Serio2013ImpactOA,\n author = {Angela Di Serio and M. Ibáñez-Espiga and C. D. Kloos},\n journal = {Comput. Educ.},\n pages = {586-596},\n title = {Impact of an augmented reality system on students' motivation for a visual art course},\n volume = {68},\n year = {2013}\n}\n""}",,"{'volume': '68', 'pages': '586-596', 'name': 'Comput. Educ.'}",71.0,Impact of an augmented reality system on students' motivation for a visual art course,2013.0
154,0eb5b4c065999d8191202429e665e97124634c80,"Individuals living with serious mental illness are often difficult to engage in ongoing treatment, with high dropout rates. Poor engagement may lead to worse clinical outcomes, with symptom relapse and rehospitalization. Numerous variables may affect level of treatment engagement, including therapeutic alliance, accessibility of care, and a client's trust that the treatment will address his/her own unique goals. As such, we have found that the concept of recovery‐oriented care, which prioritizes autonomy, empowerment and respect for the person receiving services, is a helpful framework in which to view tools and techniques to enhance treatment engagement. Specifically, person‐centered care, including shared decision making, is a treatment approach that focuses on an individual's unique goals and life circumstances. Use of person‐centered care in mental health treatment models has promising outcomes for engagement. Particular populations of people have historically been difficult to engage, such as young adults experiencing a first episode of psychosis, individuals with coexisting psychotic and substance use disorders, and those who are homeless. We review these populations and outline how various evidence‐based, recovery‐oriented treatment techniques have been shown to enhance engagement. Our review then turns to emerging treatment strategies that may improve engagement. We focus on use of electronics and Internet, involvement of peer providers in mental health treatment, and incorporation of the Cultural Formulation Interview to provide culturally competent, person‐centered care. Treatment engagement is complex and multifaceted, but optimizing recovery‐oriented skills and attitudes is essential in delivery of services to those with serious mental illness.","[{'authorId': '1871022', 'name': 'L. Dixon'}, {'authorId': '5632834', 'name': 'Y. Holoshitz'}, {'authorId': '5569736', 'name': 'Ilana Nossel'}]",356.0,"{'bibtex': '@Article{Dixon2016TreatmentEO,\n author = {L. Dixon and Y. Holoshitz and Ilana Nossel},\n journal = {World Psychiatry},\n title = {Treatment engagement of individuals experiencing mental illness: review and update},\n volume = {15},\n year = {2016}\n}\n'}",,"{'volume': '15', 'name': 'World Psychiatry'}",77.0,Treatment engagement of individuals experiencing mental illness: review and update,2016.0
155,0ed8ea9cf028e0871a1e9313d1fdec1247394087,"Lifelike animated agents for knowledge-based learning environments can provide timely, customized advice to support learners' problem-solving activities. By drawing on a rich repertoire of emotive behaviors to exhibit contextually appropriate facial expressions and emotive gestures, these agents could exploit the visual channel to more effectively communicate with learners. To address these issues, this article proposes the emotive-kinesthetic behavior sequencing framework for dynamically sequencing lifelike pedagogical agents' full-body emotive expression. By exploiting a rich behavior space populated with emotive behaviors and structured by pedagogical speech act categories, a behavior sequencing engine operates in realtime to select and assemble contextually appropriate expressive behaviors. This framework has been implemented in a lifelike pedagogical agent, C OSMO, who exhibits full-body emotive behaviors in response to learners' problem-solving activities.","[{'authorId': '1717955', 'name': 'James C. Lester'}, {'authorId': '71992438', 'name': 'Stuart G. Towns'}, {'authorId': '1803396955', 'name': 'Patrick J. Fitzgerald'}]",156.0,"{'bibtex': '@Inproceedings{Lester1999AchievingAI,\n author = {James C. Lester and Stuart G. Towns and Patrick J. Fitzgerald},\n pages = {278-291},\n title = {Achieving Affective Impact: Visual Emotive Communication in Lifelike Pedagogical Agents},\n volume = {10},\n year = {1999}\n}\n'}",,"{'volume': '10', 'pages': '278-291', 'name': ''}",44.0,Achieving Affective Impact: Visual Emotive Communication in Lifelike Pedagogical Agents,1999.0
156,0ee23730654fae0ce41a57db4e28becb272d3c6c,,"[{'authorId': '46593222', 'name': 'N. Adamo'}, {'authorId': '34367660', 'name': 'H. Dib'}, {'authorId': '48235202', 'name': 'N. Villani'}]",7.0,"{'bibtex': ""@Article{Adamo2019AnimatedAF,\n author = {N. Adamo and H. Dib and N. Villani},\n booktitle = {International Conference on Augmented and Virtual Reality},\n pages = {10-25},\n title = {Animated Agents' Facial Emotions: Does the Agent Design Make a Difference?},\n year = {2019}\n}\n""}","[{'paperId': 'f4b86c06bc6981a18bb6397073cb169afae31edb', 'title': 'Twenty-five Years of Learning with Pedagogical Agents: History, Barriers, and Opportunities'}, {'paperId': '655eda5abfb976af2a9821a2517bb9f1a6791888', 'title': 'Microexpressions in digital humans: perceived affect, sincerity, and trustworthiness'}, {'paperId': '3c8338d4101cc6710d677103663409256b187099', 'title': 'Picturing It!: The Effect of Image Styles on User Perceptions of Personas'}, {'paperId': '103b0c14857f72e2a9bdf5053e4084317d165e73', 'title': 'Near and Dear: Designing Relatable VR Agents for Training Games'}, {'paperId': '6ffb9d080a7c494dec05c8840884c310a4fc0c83', 'title': 'Towards Designing Agent Based Virtual Reality Applications for Cybersecurity Training'}, {'paperId': '3c51fd5b56ecd2f7ec8e702ec9716745c63094dd', 'title': 'Attention and Communication in Virtual Worlds: Interacting with Non-Player Characters in Virtual Reality'}, {'paperId': '4dbe4a7febf151f99868bbf2d28efd71984168cd', 'title': 'Why are you using a screwdriver? Exploring virtual process documentation assistance agents in the maker community'}]",{'pages': '10-25'},54.0,Animated Agents' Facial Emotions: Does the Agent Design Make a Difference?,2019.0
157,0ef3fb7984291549367f7b6d8b8eb8e9dd63d6a5,,"[{'authorId': '1783919', 'name': 'M. Lim'}, {'authorId': '1732377', 'name': 'R. Aylett'}]",39.0,"{'bibtex': '@Inproceedings{Lim2007FeelTD,\n author = {M. Lim and R. Aylett},\n pages = {317-330},\n title = {Feel the Difference: A Guide with Attitude!},\n year = {2007}\n}\n'}",,{'pages': '317-330'},19.0,Feel the Difference: A Guide with Attitude!,2007.0
158,0ef40bead0e160939e97bc6104c5461c23bc7057,,"[{'authorId': '49158771', 'name': 'J. Susskind'}, {'authorId': '2724380', 'name': 'G. Littlewort'}, {'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '1741200', 'name': 'J. Movellan'}, {'authorId': '5040426', 'name': 'A. Anderson'}]",160.0,"{'bibtex': '@Article{Susskind2007HumanAC,\n author = {J. Susskind and G. Littlewort and M. Bartlett and J. Movellan and A. Anderson},\n journal = {Neuropsychologia},\n pages = {152-162},\n title = {Human and computer recognition of facial expressions of emotion},\n volume = {45},\n year = {2007}\n}\n'}",,"{'volume': '45', 'pages': '152-162', 'name': 'Neuropsychologia'}",81.0,Human and computer recognition of facial expressions of emotion,2007.0
159,0f0604f991797c2eb234cd6c6a42db035146abf8,"Whatever culture we come from, emotions play a huge role in our lives and in every relationship within them. Whether anger, joy, fear or sorrow, they can be incredibly powerful things - but can be equally hard to understand or control. In ""Emotions Revealed"", Paul Ekman draws on a lifetime's study to take the reader on a complete tour of the emotional self. Against a background of specially commissioned photographs and forceful news images from around the world, he examines and explains how, when and why we become emotional and how far we can change what we get emotional about; why we sometimes get emotional when others don't; how to recognise and understand the subtlest signs of emotion both in ourselves and other people; and much more. This stunning volume contains a test to find out how good you are at spotting emotions, exercises to improve your awareness of the bodily sensations involved in each emotion and an explanation of how to apply all this information to enhance your daily life.","[{'authorId': '21451088', 'name': 'P. Ekman'}]",249.0,"{'bibtex': '@Inproceedings{Ekman2003EmotionsR,\n author = {P. Ekman},\n title = {Emotions Revealed : Understanding Faces and Feelings},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotions Revealed : Understanding Faces and Feelings,2003.0
160,0f3a546c62487e20d9aadd8d869f2ca7f0d68c7d,,"[{'authorId': '1748636', 'name': 'Elizabeth S. Kim'}, {'authorId': '39841151', 'name': 'Lauren D. Berkovits'}, {'authorId': '34255085', 'name': 'Emily P. Bernier'}, {'authorId': '2928756', 'name': 'Dan Leyzberg'}, {'authorId': '1693018', 'name': 'F. Shic'}, {'authorId': '72901767', 'name': 'R. Paul'}, {'authorId': '1792053', 'name': 'B. Scassellati'}]",392.0,"{'bibtex': '@Article{Kim2013SocialRA,\n author = {Elizabeth S. Kim and Lauren D. Berkovits and Emily P. Bernier and Dan Leyzberg and F. Shic and R. Paul and B. Scassellati},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {1038-1049},\n title = {Social Robots as Embedded Reinforcers of Social Behavior in Children with Autism},\n volume = {43},\n year = {2013}\n}\n'}",,"{'volume': '43', 'pages': '1038-1049', 'name': 'Journal of Autism and Developmental Disorders'}",55.0,Social Robots as Embedded Reinforcers of Social Behavior in Children with Autism,2013.0
161,0f455b48022bb74eea54f5aafad43016805a2db3,,"[{'authorId': '50053632', 'name': 'K. Glanz'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '4702987', 'name': 'K. Graap'}]",153.0,"{'bibtex': '@Article{Glanz2003VirtualRF,\n author = {K. Glanz and A. Rizzo and K. Graap},\n journal = {Psychotherapy},\n pages = {55-67},\n title = {Virtual reality for psychotherapy: Current reality and future possibilities.},\n volume = {40},\n year = {2003}\n}\n'}",,"{'volume': '40', 'pages': '55-67', 'name': 'Psychotherapy'}",42.0,Virtual reality for psychotherapy: Current reality and future possibilities.,2003.0
162,0f5ba457b6f44cb582d90f753334a474a403c60f,"This paper presents an experiment investigating the impact of behavior and responsiveness on social responses to virtual humans in an immersive virtual environment (IVE). A number of responses are investigated, including presence, copresence, and two physiological responsesheart rate and electrodermal activity (EDA). Our findings suggest that increasing agents' responsiveness even on a simple level can have a significant impact on certain aspects of people's social responses to human-oid agents. Despite being aware that the agents were computer-generated, participants with higher levels of social anxiety were significantly more likely to avoid disturbing them. This suggests that on some level people can respond to virtual humans as social actors even in the absence of complex interaction. Responses appear to be shaped both by the agents' behaviors and by people's expectations of the technology. Participants experienced a significantly higher sense of personal contact when the agents were visually responsive to them, as opposed to static or simply moving. However, this effect diminished with experienced computer users. Our preliminary analysis of objective heart-rate data reveals an identical pattern of responses.","[{'authorId': '11554704', 'name': 'Maia Garau'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '1921915', 'name': 'David-Paul Pertaub'}, {'authorId': '50590753', 'name': 'Sharif Razzaque'}]",196.0,"{'bibtex': '@Article{Garau2005TheRO,\n author = {Maia Garau and M. Slater and David-Paul Pertaub and Sharif Razzaque},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {104-116},\n title = {The Responses of People to Virtual Humans in an Immersive Virtual Environment},\n volume = {14},\n year = {2005}\n}\n'}",,"{'volume': '14', 'pages': '104-116', 'name': 'Presence: Teleoperators & Virtual Environments'}",30.0,The Responses of People to Virtual Humans in an Immersive Virtual Environment,2005.0
164,0f94b1b699f1320a5fadbb34d29e8e255da8942f,"Abstract Automaticspeechrecognitionsystemstypicallymodeltherela-tionship between the acoustic speech signal and the phones intwo separate steps: feature extraction and classier training. Inourrecentworks, wehaveshownthat, intheframeworkofcon-volutionalneuralnetworks(CNN),therelationshipbetweentheraw speech signal and the phones can be directly modeled andASR systems competitive to standard approach can be built. Inthis paper, we rst analyze and show that, between the rst twoconvolutional layers, the CNN learns (in parts) and models thephone-specic spectral envelope information of 2-4 ms speech.Given that we show that the CNN-based approach yields ASRtrends similar to standard short-term spectral based ASR sys-tem under mismatched (noisy) conditions, with the CNN-basedapproach being more robust.Index Terms: automatic speech recognition, convolutionalneural networks, raw signal, robust speech recognition. 1. Introduction State-of-the-art automatic speech recognition (ASR) systemstypically model the relationship between the acoustic speechsignal and the phones in two separate steps, which are op-timized in an independent manner [1]. In a rst step, thespeech signal is transformed into features, usually composed ofa dimensionality reduction phase and an information selectionphase, based on the task-specic knowledge of the phenomena.These two phases have been carefully hand-crafted, leading tostate-of-the-art features such as Mel frequency cepstral coef-cients(MFCCs)orperceptuallinearpredictioncepstralfeatures(PLPs). In a second step, the likelihood of subword units suchas, phonemes is estimated using generative models or discrimi-native models.In recent years, in the hybrid HMM/ANN framework [1],there has been growing interests in using intermediate rep-resentations instead of conventional features, such as cepstral-based features, as input for neural networks-based systems.ANNs with deep learning architectures, more precisely, deepneural networks (DNNs) [2, 3], which can yield better systemthan a single hidden layer MLP have been proposed to addressvarious aspects of acoustic modeling. More specically, useof context-dependent phonemes [4, 5]; use of spectral featuresas opposed to cepstral features [6, 7]; CNN-based system withMel lter bank energies as input [8, 9, 10]; combination of dif-ferent features [11], to name a few. Features learning from therawspeechsignalusingneuralnetworks-basedsystemshasalsobeen investigated in [12]. In all these approaches, the features","[{'authorId': '2922874', 'name': 'Dimitri Palaz'}, {'authorId': '1398480065', 'name': 'M. Magimai.-Doss'}, {'authorId': '2939803', 'name': 'Ronan Collobert'}]",271.0,"{'bibtex': '@Inproceedings{Palaz2015AnalysisOC,\n author = {Dimitri Palaz and M. Magimai.-Doss and Ronan Collobert},\n pages = {11-15},\n title = {Analysis of CNN-based speech recognition system using raw speech as input},\n year = {2015}\n}\n'}",,{'pages': '11-15'},27.0,Analysis of CNN-based speech recognition system using raw speech as input,2015.0
165,0f979f91c1e9a85b1695cc52c75a156b24b33441,,"[{'authorId': '10101422', 'name': 'A. Hirsh'}, {'authorId': '3186088', 'name': 'S. George'}, {'authorId': '144011759', 'name': 'M. Robinson'}]",94.0,"{'bibtex': '@Article{Hirsh2009PainAA,\n author = {A. Hirsh and S. George and M. Robinson},\n journal = {Pain},\n pages = {106-113},\n title = {Pain assessment and treatment disparities: A virtual human technology investigation},\n volume = {143},\n year = {2009}\n}\n'}",,"{'volume': '143', 'pages': '106-113', 'name': 'Pain'}",71.0,Pain assessment and treatment disparities: A virtual human technology investigation,2009.0
166,0fb3f03e5e7f90665b9a5e1c65597116d872745e,"Virtual reality exposure therapy (VRET) is a promising intervention for the treatment of the anxiety disorders. The main objective of this meta‐analysis is to compare the efficacy of VRET, used in a behavioral or cognitive‐behavioral framework, with that of the classical evidence‐based treatments, in anxiety disorders. A comprehensive search of the literature identified 23 studies (n = 608) that were included in the final analysis. The results show that in the case of anxiety disorders, (1) VRET does far better than the waitlist control; (2) the post‐treatment results show similar efficacy between the behavioral and the cognitive behavioral interventions incorporating a virtual reality exposure component and the classical evidence‐based interventions, with no virtual reality exposure component; (3) VRET has a powerful real‐life impact, similar to that of the classical evidence‐based treatments; (4) VRET has a good stability of results over time, similar to that of the classical evidence‐based treatments; (5) there is a dose–response relationship for VRET; and (6) there is no difference in the dropout rate between the virtual reality exposure and the in vivo exposure. Implications are discussed. Depression and Anxiety 0:1–9, 2011.  © 2011 Wiley Periodicals, Inc.","[{'authorId': '40594619', 'name': 'David Opriş'}, {'authorId': '2413277', 'name': 'S. Pintea'}, {'authorId': '1404807497', 'name': 'A. García-Palacios'}, {'authorId': '145945543', 'name': 'C. Botella'}, {'authorId': '16096003', 'name': 'Ștefan Szamosközi'}, {'authorId': '144000444', 'name': 'D. David'}]",496.0,"{'bibtex': '@Article{Opriş2012VirtualRE,\n author = {David Opriş and S. Pintea and A. García-Palacios and C. Botella and Ștefan Szamosközi and D. David},\n journal = {Depression and Anxiety},\n title = {Virtual reality exposure therapy in anxiety disorders: a quantitative meta‐analysis},\n volume = {29},\n year = {2012}\n}\n'}",,"{'volume': '29', 'name': 'Depression and Anxiety'}",44.0,Virtual reality exposure therapy in anxiety disorders: a quantitative meta‐analysis,2012.0
167,0fc334e7f5a4738fdd1e150db0c729851b0399e7,"In this paper, we describe the elaboration and the validation of a body and face database1, of 96 videos of 1 to 2 seconds of duration, expressing 4 emotions (i.e., anger, happiness, fear, and sadness) elicited through 4 platforms of increased visual complexity and level of embodiment. The final aim of this database is to develop an individualized training program designed for individuals suffering of autism in order to help them recognize various emotions on different test platforms: two robots, a virtual agent, and a human. Before assessing the recognition capabilities of individuals with ASD, we validated our video database on typically developed individuals (TD). Moreover, we also looked at the relationship between the recognition rate and their personality traits (extroverted (EX) vs. introverted (IN)). We found that the personality of our TD participants did not lead to a different recognition behavior. However, introverted individuals better recognized emotions from less visually complex characters than extroverted individuals.","[{'authorId': '40648373', 'name': 'P. Chevalier'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}, {'authorId': '2950863', 'name': 'B. Isableu'}, {'authorId': '1738469', 'name': 'A. Tapus'}]",15.0,"{'bibtex': '@Article{Chevalier2015ImpactOP,\n author = {P. Chevalier and Jean-Claude Martin and B. Isableu and A. Tapus},\n booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},\n journal = {2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {229-234},\n title = {Impact of personality on the recognition of emotion expressed via human, virtual, and robotic embodiments},\n year = {2015}\n}\n'}","[{'paperId': 'bdf2a28934f5291af7f75e0b9811acbd58ee1829', 'title': 'A personality-guided affective brain—computer interface for implementation of emotional intelligence in machines'}, {'paperId': 'f4640719d539f11d9547c204ef0a9842ba619f3e', 'title': 'Investigation of Perception Towards Robot Expressions Considering Attitude and Personality'}, {'paperId': '2b50250b07632526a00142109e0aba3a11383563', 'title': 'A Meta-Analysis of Human Personality and Robot Acceptance in Human-Robot Interaction'}, {'paperId': 'c9e7369ec757c2847a3c0426198c0ae388a35192', 'title': 'A Review of Personality in Human‒Robot Interactions'}, {'paperId': '70998e14f795d82e367d890aae5476a91472f268', 'title': 'The Attribution of Emotional State - How Embodiment Features and Social Traits Affect the Perception of an Artificial Agent'}, {'paperId': '15d761ed20ca0a9c8daa32f5b04153296584c398', 'title': 'A Multi-Task Cascaded Network for Prediction of Affect, Personality, Mood and Social Context Using EEG Signals'}, {'paperId': '594236967c5dc3948758a1ff38e18105106e5cf7', 'title': 'Automatic replication of teleoperator head movements and facial expressions on a humanoid robot'}, {'paperId': 'e53b6ca45ddae2c97a5545fb163d2caacd7a417d', 'title': 'AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups'}, {'paperId': 'd660abfbe5f84c1c49f1e7174eb166b8b23e53c4', 'title': 'AMIGOS: A dataset for Mood, personality and affect research on Individuals and GrOupS'}, {'paperId': '3eb9bcb2652b7b86fb1bd918217ee6bda468a038', 'title': 'Impact des préférences sensorielles chez les individus souffrant de troubles du spectre autistique sur leur interaction sociale avec un robot. (Impact of sensory preferences in individuals with autism spectrum disorderon their social interaction with a robot)'}, {'paperId': 'dc4fd971234724bca72b63848566eaabe4f35a5d', 'title': 'Intelligent management of hierarchical behaviors using a NAO robot as a vocational tutor'}, {'paperId': '8dc30a97a009cde1689f7c5c24faf1e53194ceee', 'title': 'PhyMER: Physiological Dataset for Multimodal Emotion Recognition With Personality as a Context'}, {'paperId': '05179e3dea62ca17b7b76002eddccb40bb46a2ad', 'title': 'Design and development of HRI-based intervention for ASD children using ADDIE model and ABA: A preliminary study'}, {'paperId': '7f1703f343637b17054c96b0c33121a20772fed2', 'title': 'Human-Robot Interaction Considering User Types by Sensing Human Behavior March 2020'}, {'paperId': 'a3c4761cb9638a950462df0f592116bc2bd6747e', 'title': ""Recognizing film aesthetics, spectators' affect and aesthetic emotions from multimodal signals""}]","{'name': '2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)', 'pages': '229-234'}",25.0,"Impact of personality on the recognition of emotion expressed via human, virtual, and robotic embodiments",2015.0
168,0fea4371d73960c8fcb17e6d94123aba9905a7ab,"Two experiments are reported which examined operators' trust in and use of the automation in a simulated supervisory process control task. Tests of the integrated model of human trust in machines proposed by Muir (1994) showed that models of interpersonal trust capture some important aspects of the nature and dynamics of human-machine trust. Results showed that operators' subjective ratings of trust in the automation were based mainly upon their perception of its competence. Trust was significantly reduced by any sign of incompetence in the automation, even one which had no effect on overall system performance. Operators' trust changed very little with experience, with a few notable exceptions. Distrust in one function of an automatic component spread to reduce trust in another function of the same component, but did not generalize to another independent automatic component in the same system, or to other systems. There was high positive correlation between operators' trust in and use of the automation; operators used automation they trusted and rejected automation they distrusted, preferring to do the control task manually. There was an inverse relationship between trust and monitoring of the automation. These results suggest that operators' subjective ratings of trust and the properties of the automation which determine their trust, can be used to predict and optimize the dynamic allocation of functions in automated systems.","[{'authorId': '48589865', 'name': 'B. M. Muir'}, {'authorId': '2251924', 'name': 'N. Moray'}]",892.0,"{'bibtex': '@Article{Muir1996TrustIA,\n author = {B. M. Muir and N. Moray},\n journal = {Ergonomics},\n pages = {\n          429-60\n        },\n title = {Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation.},\n volume = {39 3},\n year = {1996}\n}\n'}",,"{'volume': '39 3', 'pages': '\n          429-60\n        ', 'name': 'Ergonomics'}",10.0,Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation.,1996.0
169,100b09552f77abba945a297cbbb1dce8ee3c986e,"
 
 Sentiment classification on Twitter has attracted increasing research in recent years.Most existing work focuses on feature engineering according to the tweet content itself.In this paper, we propose a context-based neural network model for Twitter sentiment analysis, incorporating contextualized features from relevant Tweets into the model in the form of word embedding vectors.Experiments on both balanced and unbalanced datasets show that our proposed models outperform the current state-of-the-art.
 
","[{'authorId': '3350168', 'name': 'Yafeng Ren'}, {'authorId': None, 'name': 'Yue Zhang'}, {'authorId': '2678094', 'name': 'Meishan Zhang'}, {'authorId': '1719916', 'name': 'D. Ji'}]",142.0,"{'bibtex': '@Inproceedings{Ren2016ContextSensitiveTS,\n author = {Yafeng Ren and Yue Zhang and Meishan Zhang and D. Ji},\n pages = {215-221},\n title = {Context-Sensitive Twitter Sentiment Classification Using Neural Network},\n year = {2016}\n}\n'}",,{'pages': '215-221'},23.0,Context-Sensitive Twitter Sentiment Classification Using Neural Network,2016.0
170,1028d8562fec24d637292b4234a3d4b338eab602,"In order to analyze the emotional content of motions portrayed by different characters, we created real and virtual replicas of an actor exhibiting six basic emotions: sadness, happiness, surprise, fear, anger and disgust. In addition to the video of the real actor, his actions were applied to five virtual body shapes: a low and high resolution virtual counterpart, a cartoon-like character, a wooden mannequin, and a zombie-like character (Figure 1). Participants were asked to rate the actions based on a list of 41 more complex emotions. We found that the perception of emotional actions is highly robust and to the most part independent of the character's body.","[{'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '144892110', 'name': 'S. Jörg'}, {'authorId': '52093924', 'name': 'J. McHugh'}, {'authorId': '1818330', 'name': 'F. Newell'}, {'authorId': '1404017833', 'name': ""C. O'Sullivan""}]",66.0,"{'bibtex': ""@Inproceedings{Mcdonnell2008EvaluatingTE,\n author = {R. Mcdonnell and S. Jörg and J. McHugh and F. Newell and C. O'Sullivan},\n pages = {67-74},\n title = {Evaluating the emotional content of human motions on real and virtual characters},\n year = {2008}\n}\n""}",,{'pages': '67-74'},20.0,Evaluating the emotional content of human motions on real and virtual characters,2008.0
173,104a803cc09ae2b36b79d66b7e5d522d3842e784,"Why bodies? It is rather puzzling that given the massive interest in affective neuroscience in the last decade, it still seems to make sense to raise the question ‘Why bodies’ and to try to provide an answer to it, as is the goal of this article. There are now hundreds of articles on human emotion perception ranging from behavioural studies to brain imaging experiments. These experimental studies complement decades of reports on affective disorders in neurological patients and clinical studies of psychiatric populations. The most cursory glance at the literature on emotion in humans, now referred to by the umbrella term of social and affective neuroscience, shows that over 95 per cent of them have used faces as stimuli. Of the remaining 5 per cent, a few have used scenes or auditory information including human voices, music or environmental sounds. But by far the smallest number has looked into whole-body expressions. As a rough estimate, a search on PubMed today, 1 May 2009, yields 3521 hits for emotion × faces, 1003 hits for emotion × music and 339 hits for emotion × bodies. When looking in more detail, the body × emotion category in fact yields a majority of papers on well-being, nursing, sexual violence or organ donation. But the number of cognitive and affective neuroscience studies of emotional body perception as of today is lower than 20. Why then have whole bodies and bodily expressions not attracted the attention of researchers so far? The goal of this article is to contribute some elements for an answer to this question. I believe that there is something to learn from the historical neglect of bodies and bodily expressions. I will next address some historical misconceptions about whole-body perception, and in the process I intend not only to provide an impetus for this kind of work but also to contribute to a better understanding of the significance of the affective dimension of behaviour, mind and brain as seen from the vantage point of bodily communication. Subsequent sections discuss available evidence for the neurofunctional basis of facial and bodily expressions as well as neuropsychological and clinical studies of bodily expressions.","[{'authorId': '2252323751', 'name': 'Beatrice de Gelder'}]",315.0,"{'bibtex': '@Article{Gelder2009WhyBT,\n author = {Beatrice de Gelder},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n pages = {3475 - 3484},\n title = {Why bodies? Twelve reasons for including bodily expressions in affective neuroscience},\n volume = {364},\n year = {2009}\n}\n'}",,"{'volume': '364', 'pages': '3475 - 3484', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",84.0,Why bodies? Twelve reasons for including bodily expressions in affective neuroscience,2009.0
174,105ee4960a2c51b20c89be7622b112bfbae80ee1,"In entertainment applications, artificial intelligence techniques have most often been used to implement embodied agents or to automatically generate artistic content. A more recent development concerns using AI to support the user experience through new AI-based interactivity techniques. This is especially of interest for the development of artistic installations based on interactive 3D worlds. A major difficulty in developing such installations is to properly translate the artistic intention into actual elements of interactivity, which in turn determine the user experience. The starting point of this research was to facilitate the description of high-level behaviors for virtual worlds that would form part of virtual reality (VR) art installations. In our approach to interactivity, the consequences of user interaction can be dynamically computed to produce cascaded effects eliciting a specific kind of user experience. This chain of events is computed from first principles embedding elements of the artistic brief (the artist's initial conceptual description of the interactive installation and the intended user experience). In other words, AI techniques are used for their ability to represent actions and to compute analogical transformations on them to create a user experience","[{'authorId': '144315100', 'name': 'Jean-Luc Lugrin'}, {'authorId': '1696638', 'name': 'M. Cavazza'}, {'authorId': '67010268', 'name': 'Mark Palmer'}, {'authorId': '144431268', 'name': 'S. Crooks'}]",12.0,"{'bibtex': '@Article{Lugrin2006ArtificialII,\n author = {Jean-Luc Lugrin and M. Cavazza and Mark Palmer and S. Crooks},\n journal = {IEEE Intelligent Systems},\n pages = {54-62},\n title = {Artificial Intelligence-Mediated Interaction in Virtual Reality Art},\n volume = {21},\n year = {2006}\n}\n'}",,"{'volume': '21', 'pages': '54-62', 'name': 'IEEE Intelligent Systems'}",15.0,Artificial Intelligence-Mediated Interaction in Virtual Reality Art,2006.0
176,106529b222fff69c1d8cb4a26007ca3c3ddffd7e,,"[{'authorId': '2512383', 'name': 'M. Kret'}, {'authorId': '72217688', 'name': 'B. D. Gelder'}]",78.0,"{'bibtex': '@Article{Kret2013WhenAS,\n author = {M. Kret and B. D. Gelder},\n journal = {Experimental Brain Research},\n pages = {399 - 410},\n title = {When a smile becomes a fist: the perception of facial and bodily expressions of emotion in violent offenders},\n volume = {228},\n year = {2013}\n}\n'}",,"{'volume': '228', 'pages': '399 - 410', 'name': 'Experimental Brain Research'}",53.0,When a smile becomes a fist: the perception of facial and bodily expressions of emotion in violent offenders,2013.0
177,10a7fd6e6089c5eb00ab1c20fb255bb9064ab307,,"[{'authorId': '40635785', 'name': 'S. Tomkins'}]",367.0,"{'bibtex': '@Inproceedings{Tomkins1963ThePA,\n author = {S. Tomkins},\n title = {The positive affects},\n year = {1963}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The positive affects,1963.0
178,10b509bb40deb0df1dcaf76644e37768bf6d9fff,"After implementing a motivational model of action selection applied to autonomous virtual humans inspired by models of animals' decision-making, the problem consists of testing it in good conditions for validation. Indeed this model has to respect the six criteria that we define according to Tyrell's requirements for designing a mechanism of action selection. So we use the real-time framework VHD++ for advanced virtual human simulation and we define a simulated environment with many conflicting motivations. Finally a video shows the first results where the virtual human's decision-making follows the six criteria and so validates our model in the simulated environment","[{'authorId': '1761859', 'name': 'E. D. Sevin'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}]",12.0,"{'bibtex': '@Article{Sevin2004TheCO,\n author = {E. D. Sevin and D. Thalmann},\n journal = {Proceedings Computer Graphics International, 2004.},\n pages = {540-543},\n title = {The complexity of testing a motivational model of action selection for virtual humans},\n year = {2004}\n}\n'}",,"{'pages': '540-543', 'name': 'Proceedings Computer Graphics International, 2004.'}",10.0,The complexity of testing a motivational model of action selection for virtual humans,2004.0
179,10b9509d6deac5f88746e8ffbe2e3b3834dede4c,This investigation examines the dynamics associated with soliciting intimate information from consumers via computers. Experiment 1 identifies two factors--reciprocity and sequence--that affect the likelihood that people will reveal intimate information about themselves via a computer. Experiment 2 provides evidence that intimate information exchanges can affect how consumers behave in subsequent interactions. Implications for marketing research and practice are discussed. Copyright 2000 by the University of Chicago.,"[{'authorId': '33875827', 'name': 'Youngme Moon'}]",779.0,"{'bibtex': '@Article{Moon2000IntimateEU,\n author = {Youngme Moon},\n journal = {Journal of Consumer Research},\n pages = {323-339},\n title = {Intimate Exchanges: Using Computers to Elicit Self-Disclosure from Consumers},\n volume = {26},\n year = {2000}\n}\n'}",,"{'volume': '26', 'pages': '323-339', 'name': 'Journal of Consumer Research'}",66.0,Intimate Exchanges: Using Computers to Elicit Self-Disclosure from Consumers,2000.0
181,10cc2d53ff8349d3432b8f822d58e4ddee3d475e,"An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect & pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects).","[{'authorId': '3864246', 'name': 'A. Greenwald'}, {'authorId': '49986173', 'name': 'D. McGhee'}, {'authorId': '2072435656', 'name': 'Jordan L. K. Schwartz'}]",10580.0,"{'bibtex': '@Article{Greenwald1998MeasuringID,\n author = {A. Greenwald and D. McGhee and Jordan L. K. Schwartz},\n journal = {Journal of personality and social psychology},\n pages = {\n          1464-80\n        },\n title = {Measuring individual differences in implicit cognition: the implicit association test.},\n volume = {74 6},\n year = {1998}\n}\n'}",,"{'volume': '74 6', 'pages': '\n          1464-80\n        ', 'name': 'Journal of personality and social psychology'}",29.0,Measuring individual differences in implicit cognition: the implicit association test.,1998.0
182,11029a1780f31cce830faa3a9b664079e9b30a70,,"[{'authorId': '2565834', 'name': 'M. Scheeff'}, {'authorId': '2067614784', 'name': 'J. Pinto'}, {'authorId': '2587540', 'name': 'K. Rahardja'}, {'authorId': '3163335', 'name': 'Scott S. Snibbe'}, {'authorId': '2062142', 'name': 'Rob Tow'}]",177.0,"{'bibtex': '@Inproceedings{Scheeff2002ExperiencesWS,\n author = {M. Scheeff and J. Pinto and K. Rahardja and Scott S. Snibbe and Rob Tow},\n pages = {173-180},\n title = {Experiences with Sparky, a Social Robot},\n year = {2002}\n}\n'}",,"{'volume': '', 'pages': '173-180', 'name': ''}",17.0,"Experiences with Sparky, a Social Robot",2002.0
183,11036a318e450effc32e6c0db5032cd8fd6a0674,,"[{'authorId': '7021441', 'name': 'Joseph A. Mikels'}, {'authorId': '1892780', 'name': 'B. Fredrickson'}, {'authorId': '153818150', 'name': 'G. R. Larkin'}, {'authorId': '2058724130', 'name': 'Casey M. Lindberg'}, {'authorId': '1860459', 'name': 'Sam J. Maglio'}, {'authorId': '1398013984', 'name': 'P. Reuter-Lorenz'}]",601.0,"{'bibtex': '@Article{Mikels2005EmotionalCD,\n author = {Joseph A. Mikels and B. Fredrickson and G. R. Larkin and Casey M. Lindberg and Sam J. Maglio and P. Reuter-Lorenz},\n journal = {Behavior Research Methods},\n pages = {626-630},\n title = {Emotional category data on images from the international affective picture system},\n volume = {37},\n year = {2005}\n}\n'}",,"{'volume': '37', 'pages': '626-630', 'name': 'Behavior Research Methods'}",36.0,Emotional category data on images from the international affective picture system,2005.0
185,111359c407e728dc0aa09a9279e6ab013b189246,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '71825175', 'name': 'J. Hoorn'}, {'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '3347943', 'name': 'G. F. Siddiqui'}]",18.0,"{'bibtex': '@Inproceedings{Bosse2010ComparingTC,\n author = {T. Bosse and J. Gratch and J. Hoorn and M. Pontier and G. F. Siddiqui},\n pages = {175-184},\n title = {Comparing Three Computational Models of Affect},\n year = {2010}\n}\n'}",,{'pages': '175-184'},19.0,Comparing Three Computational Models of Affect,2010.0
186,1123da519480f59b11133f21c15603cad388af2c,"This study presents a Virtual Elderly Companion Agent that based on speech spectrograms and deep convolutional neural networks. The system can dynamically detect and analyze the user's emotion from the dialogue and give appropriate positive feedback. The proposed system architecture is divided into two parts. The client side supports Android operating system; the server side is implemented in python, and applied GoogleLeNet and AlexNet for emotion recognition. The system supports natural language speech input, and then analyzes the converted speech spectrogram to provide appropriate feedback.","[{'authorId': '47804134', 'name': 'Ming-Che Lee'}, {'authorId': '122305220', 'name': 'Sheng-Cheng Yeh'}, {'authorId': '1398665180', 'name': 'Sheng Yu Chiu'}, {'authorId': '2874892', 'name': 'Jia-Wei Chang'}]",4.0,"{'bibtex': '@Article{Lee2017ADC,\n author = {Ming-Che Lee and Sheng-Cheng Yeh and Sheng Yu Chiu and Jia-Wei Chang},\n booktitle = {ACM SIGMM Conference on Multimedia Systems},\n journal = {Proceedings of the 8th ACM on Multimedia Systems Conference},\n title = {A Deep Convolutional Neural Network Based Virtual Elderly Companion Agent},\n year = {2017}\n}\n'}","[{'paperId': '8f6f58da9011023c044a42bffa92310a5c55467c', 'title': 'Cross-Lingual Cross-Age Group Adaptation for Low-Resource Elderly Speech Emotion Recognition'}, {'paperId': '5cd7406be033a39f0ddadbb85b5eee1322e44e6f', 'title': 'Conversational Agents for Elderly Interaction'}, {'paperId': '2c6585945e6ac8019a6702bd3c65e4c178d759a7', 'title': 'Uso de Chatbots Personalizados para Monitoração do Desaprendizado e Esquecimento de Idosos'}, {'paperId': 'f3265d19280f459a1c99f06602009dc5dbe445b7', 'title': 'Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks'}]",{'name': 'Proceedings of the 8th ACM on Multimedia Systems Conference'},16.0,A Deep Convolutional Neural Network Based Virtual Elderly Companion Agent,2017.0
187,11422e12989048a336d86e643a4bd0b7a32e20dc,"Emotion dysregulation is not a formal criterion for the diagnosis of autism spectrum disorder (ASD). However, parents and clinicians have long noted the importance of emotional problems in individuals with ASD (e.g. tantrums and “meltdowns”). In this study, 21 high‐functioning children and adolescents with ASD and 22 age and gender group‐matched typically developing (TD) controls completed a Reactivity and Regulation Situation Task. This task assesses emotional reactivity and spontaneous use of emotion regulation strategies (problem solving, cognitive reappraisal, avoidance, distraction, venting, suppression, and relaxation) in the context of age‐appropriate ambiguous and potentially threatening negative scenarios. After the concept of cognitive reappraisal was explained, the scenarios were presented again to participants, and they were prompted to use this strategy. Results indicated that individuals with ASD exhibited the same level of reactivity to negative stimuli as TD participants. Furthermore, youth with ASD had a different emotion regulation profile than TD individuals, characterized by a less frequent use of cognitive reappraisal and more frequent use of suppression. When prompted to use cognitive reappraisal, participants with ASD were less able to implement reappraisal, but benefitted from this strategy when they were able to generate a reappraisal. Findings from this study suggest that cognitive reappraisal strategies may be useful to children and adolescents with ASD. Therefore, the development of treatment programs that focus on enhancing the use of adaptive forms of emotion regulation might decrease emotional problems and optimize long‐term outcomes in youth with ASD. Autism Res 2015, 8: 9–18. © 2014 International Society for Autism Research, Wiley Periodicals, Inc.","[{'authorId': '38707445', 'name': 'Andrea C. Samson'}, {'authorId': '6760287', 'name': 'A. Hardan'}, {'authorId': '16161768', 'name': 'Rebecca W. Podell'}, {'authorId': '2536136', 'name': 'Jennifer M. Phillips'}, {'authorId': '1775321', 'name': 'J. Gross'}]",153.0,"{'bibtex': '@Article{Samson2015EmotionRI,\n author = {Andrea C. Samson and A. Hardan and Rebecca W. Podell and Jennifer M. Phillips and J. Gross},\n journal = {Autism Research},\n title = {Emotion Regulation in Children and Adolescents With Autism Spectrum Disorder},\n volume = {8},\n year = {2015}\n}\n'}",,"{'volume': '8', 'name': 'Autism Research'}",68.0,Emotion Regulation in Children and Adolescents With Autism Spectrum Disorder,2015.0
188,1161b0ad569a846387b699d8c5190a486c4f419d,"A 7 degrees of freedom leg-wheel hybrid mobile robot, it is called KaMERo(Kaist motion expressive robot), which can conduct not only various types of locomotion but also noble body motions, is developed. The KaMERo has total three leg-wheel hybrid leg mechanisms. Basically each hybrid leg mechanism consists of 2-DOF leg which has 90 degrees bendable knee joint and one wheel at the end of the leg. Two front legs are having passive wheel, which means no actuator, and one rear leg is having active wheel. And all the wheels are center oriented wheels and individually steerable. This particular and unique configuration of KaMERo has interesting characteristics of: (1) three types of locomotion (2) various body expressions. Through this paper, three emotional expressions and behavioral expressions are proposed and realized through a real robot system. Also, to evaluate and verify the effectiveness of proposed emotional body expressions of KaMERo, human survey is conducted and its result is analyzed. This paper introduces a new leg-wheel hybrid mobile robot, KaMERo, and proposes expressive body motions and verified its emotional expressions.","[{'authorId': '9112848', 'name': 'Nam-Su Yuk'}, {'authorId': '145079887', 'name': 'D. Kwon'}]",14.0,"{'bibtex': '@Article{Yuk2008RealizationOE,\n author = {Nam-Su Yuk and D. Kwon},\n journal = {2008 International Conference on Control, Automation and Systems},\n pages = {2350-2355},\n title = {Realization of expressive body motion using leg-wheel hybrid mobile robot: KaMERo1},\n year = {2008}\n}\n'}",,"{'pages': '2350-2355', 'name': '2008 International Conference on Control, Automation and Systems'}",10.0,Realization of expressive body motion using leg-wheel hybrid mobile robot: KaMERo1,2008.0
189,118389d6db58d05e65b9cf4da3a7e94601c364aa,,"[{'authorId': '2510000', 'name': 'G. Vreeswijk'}]",394.0,"{'bibtex': '@Article{Vreeswijk1997AbstractAS,\n author = {G. Vreeswijk},\n journal = {Artif. Intell.},\n pages = {225-279},\n title = {Abstract Argumentation Systems},\n volume = {90},\n year = {1997}\n}\n'}",,"{'volume': '90', 'pages': '225-279', 'name': 'Artif. Intell.'}",49.0,Abstract Argumentation Systems,1997.0
190,11dcaddab76b153584471e6919ff25cabda92ace,"Recently, there has been a rapid growth in the use of 3D multi-modal correlative imaging for studies of the human brain. Regional cerebral blood flow (CBF) changes indicate brain areas involved in stimulus processing. These focal changes are often too small (<10%) to be discerned from a single subject and the experiment is repeated in a series of individuals. To investigate the extent of residual variability the authors have collected over 300 MRI volumetric datasets from normal individuals and transformed these datasets into stereotaxic space using a 3D linear re-sampling algorithm. The authors then generated a series of statistical measures which express this population nonlinear variability in the form of parametric volumes, e.g. mean intensity, intensity variance. A model for anatomical variability, expressed as the width of a Gaussian blurring kernel applied to an ideal single subject, was developed and tested against the observed data.<<ETX>>","[{'authorId': '144159535', 'name': 'Alan C. Evans'}, {'authorId': '2238288468', 'name': 'D. Collins'}, {'authorId': '114880105', 'name': 'S. R. Mills'}, {'authorId': '2072718024', 'name': 'E. D. Brown'}, {'authorId': '1406347926', 'name': 'R. L. Kelly'}, {'authorId': '1698185', 'name': 'T. Peters'}]",1569.0,"{'bibtex': '@Article{Evans19933DSN,\n author = {Alan C. Evans and D. Collins and S. R. Mills and E. D. Brown and R. L. Kelly and T. Peters},\n journal = {1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference},\n pages = {1813-1817 vol.3},\n title = {3D statistical neuroanatomical models from 305 MRI volumes},\n year = {1993}\n}\n'}",,"{'pages': '1813-1817 vol.3', 'name': '1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference'}",9.0,3D statistical neuroanatomical models from 305 MRI volumes,1993.0
191,11f35d63d44cc0b763f41208918230ddb6822535,"Psychologists have long explored mechanisms with which humans recognize other humans' affective states from modalities, such as voice and face display. This exploration has led to the identification of the main mechanisms, including the important role played in the recognition process by the modalities' dynamics. Constrained by the human physiology, the temporal evolution of a modality appears to be well approximated by a sequence of temporal segments called onset, apex, and offset. Stemming from these findings, computer scientists, over the past 15 years, have proposed various methodologies to automate the recognition process. We note, however, two main limitations to date. The first is that much of the past research has focused on affect recognition from single modalities. The second is that even the few multimodal systems have not paid sufficient attention to the modalities' dynamics: The automatic determination of their temporal segments, their synchronization to the purpose of modality fusion, and their role in affect recognition are yet to be adequately explored. To address this issue, this paper focuses on affective face and body display, proposes a method to automatically detect their temporal segments or phases, explores whether the detection of the temporal phases can effectively support recognition of affective states, and recognizes affective states based on phase synchronization/alignment. The experimental results obtained show the following: 1) affective face and body displays are simultaneous but not strictly synchronous; 2) explicit detection of the temporal phases can improve the accuracy of affect recognition; 3) recognition from fused face and body modalities performs better than that from the face or the body modality alone; and 4) synchronized feature-level fusion achieves better performance than decision-level fusion.","[{'authorId': '1781916', 'name': 'H. Gunes'}, {'authorId': '35153150', 'name': 'M. Piccardi'}]",210.0,"{'bibtex': '@Article{Gunes2009AutomaticTS,\n author = {H. Gunes and M. Piccardi},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {64-84},\n title = {Automatic Temporal Segment Detection and Affect Recognition From Face and Body Display},\n volume = {39},\n year = {2009}\n}\n'}",,"{'volume': '39', 'pages': '64-84', 'name': 'IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)'}",86.0,Automatic Temporal Segment Detection and Affect Recognition From Face and Body Display,2009.0
192,11f49977e4680a74451db07308c1d8057923e287,"Our research focuses on complex agents that are capable of interacting with their environments in ways that are increasingly similar to individual humans. In this article we describe a cognitive architecture for an interactive decision-making agent with emotions. The primary goal of this work is to make the decision-making process of complex agents more realistic with regard to the behavior moderators, including emotional factors that affect humans. Instead of uniform agents that rely entirely on a deterministic body of expertise to make their decisions, the decision making process of our agents will vary according to select emotional factors affecting the agent as well as the agent's parameterized emotional profile. The premise of this model is that emotions serve as a kind of automatic assessment system that can guide or otherwise influence the more deliberative decision making process. The primary components of this emotional system are pleasure/pain and clarity/confusion subsystems that differentiate between positive and negative states. These, in turn, feed into an arousal system that interfaces with the decision-making system. We are testing our model using synthetic special-forces agents in a reconnaissance simulation.","[{'authorId': '2756158', 'name': 'E. Chown'}, {'authorId': '153788834', 'name': 'Randolph M. Jones'}, {'authorId': '8993685', 'name': 'Amy E. Henninger'}]",24.0,"{'bibtex': '@Inproceedings{Chown2002AnAF,\n author = {E. Chown and Randolph M. Jones and Amy E. Henninger},\n pages = {352-353},\n title = {An architecture for emotional decision-making agents},\n year = {2002}\n}\n'}",,{'pages': '352-353'},5.0,An architecture for emotional decision-making agents,2002.0
193,11f7d9b2017abd46c756df533618d2c4326fd3cb,"The process of building Façade, a first-person, real-time, one-act interactive drama, has involved three major research efforts: designing ways to deconstruct a dramatic narrative into a hierarchy of story and behavior pieces; engineering an AI system that responds to and integrates the player's moment-by-moment interactions to reconstruct a real-time dramatic performance from those pieces; and understanding how to write an engaging, compelling story within this new organizational framework. This paper provides an overview of the process of bringing our interactive drama to life as a coherent, engaging, high agency experience, including the design and programming of thousands of joint dialog behaviors in the reactive planning language ABL, and their higher level organization into a collection of story beats sequenced by a drama manager. The process of iteratively developing the architecture, its languages, authorial idioms, and varieties of story content structures are described. These content structures are designed to intermix to offer players a high degree of responsiveness and narrative agency. We conclude with design and implementation lessons learned and future directions for creating more generative architectures.","[{'authorId': '114402462', 'name': 'Michael Mateas'}, {'authorId': '143727207', 'name': 'A. Stern'}]",266.0,"{'bibtex': '@Inproceedings{Mateas2005StructuringCI,\n author = {Michael Mateas and A. Stern},\n pages = {93-98},\n title = {Structuring Content in the Façade Interactive Drama Architecture},\n year = {2005}\n}\n'}",,{'pages': '93-98'},16.0,Structuring Content in the Façade Interactive Drama Architecture,2005.0
194,11fe5d0a8dca7810cebe0c8c1c07b1928b293999,,"[{'authorId': '2112389014', 'name': 'Hao Tang'}, {'authorId': '1719916', 'name': 'D. Ji'}, {'authorId': '92758499', 'name': 'Qiji Zhou'}]",15.0,"{'bibtex': '@Article{Tang2020EndtoendMG,\n author = {Hao Tang and D. Ji and Qiji Zhou},\n journal = {Neurocomputing},\n pages = {348-359},\n title = {End-to-end masked graph-based CRF for joint slot filling and intent detection},\n volume = {413},\n year = {2020}\n}\n'}",,"{'volume': '413', 'pages': '348-359', 'name': 'Neurocomputing'}",11.0,End-to-end masked graph-based CRF for joint slot filling and intent detection,2020.0
195,123062ea1c407f97f98e4f3337b93157890bd5a2,"Participants in manipulated emotional states played computerised movies in which facial expressions of emotion changed into categorically different expressions. The participants' task was to detect the offset of the initial expression. An effect of emotional state was observed such that individuals in happy states saw the offset of happiness (changing into sadness) at an earlier point in the movies than did those in sad states. Similarly, sad condition participants detected the offset of a sad expression changing into a happy expression earlier than did happy condition participants. This result is consistent with a proposed role of facial mimicry in the perception of change in emotional expression. The results of a second experiment provide additional evidence for the mimicry account. The Discussion focuses on the relationship between motor behaviour and perception.","[{'authorId': '1986858', 'name': 'P. Niedenthal'}, {'authorId': '40529687', 'name': 'M. Brauer'}, {'authorId': '3012562', 'name': 'J. Halberstadt'}, {'authorId': '1402996412', 'name': 'Åse Innes-Ker'}]",379.0,"{'bibtex': '@Article{Niedenthal2001WhenDH,\n author = {P. Niedenthal and M. Brauer and J. Halberstadt and Åse Innes-Ker},\n journal = {Cognition and Emotion},\n pages = {853 - 864},\n title = {When did her smile drop? Facial mimicry and the influences of emotional state on the detection of change in emotional expression},\n volume = {15},\n year = {2001}\n}\n'}",,"{'volume': '15', 'pages': '853 - 864', 'name': 'Cognition and Emotion'}",38.0,When did her smile drop? Facial mimicry and the influences of emotional state on the detection of change in emotional expression,2001.0
196,1231456cefb7302e7d82a587557d58019f5f41a1,"Motivation is an important psychology characteristic for a virtual character. An autonomous virtual characterpsilas behavior and emotion are controlled by motivation and outer stimuli, setting up the computer model of motivation of virtual characters is a hot topic in computer game and human interface. The current state of research on mental models of virtual characters is reviewed. Based on psychology theory, formalization concepts on Maslowpsilas theory are set up. A new motivation model of virtual character is proposed. The model can integrate stimuli, motivation, behavior, emotion and personality together.","[{'authorId': '2109341502', 'name': 'Zhen Liu'}, {'authorId': '2145038074', 'name': 'Yu Lu'}]",10.0,"{'bibtex': '@Article{Liu2008AMM,\n author = {Zhen Liu and Yu Lu},\n journal = {2008 International Conference on Machine Learning and Cybernetics},\n pages = {2712-2717},\n title = {A motivation model for virtual characters},\n volume = {5},\n year = {2008}\n}\n'}",,"{'volume': '5', 'pages': '2712-2717', 'name': '2008 International Conference on Machine Learning and Cybernetics'}",26.0,A motivation model for virtual characters,2008.0
198,129cbad01be98ee88a930e31898cb76be79c41c1,"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","[{'authorId': '2144634184', 'name': 'Chia-Wei Liu'}, {'authorId': '2054294', 'name': 'Ryan Lowe'}, {'authorId': '35224828', 'name': 'Iulian Serban'}, {'authorId': '38107789', 'name': 'Michael Noseworthy'}, {'authorId': '1778839', 'name': 'Laurent Charlin'}, {'authorId': '145134886', 'name': 'Joelle Pineau'}]",1168.0,"{'bibtex': '@Article{Liu2016HowNT,\n author = {Chia-Wei Liu and Ryan Lowe and Iulian Serban and Michael Noseworthy and Laurent Charlin and Joelle Pineau},\n journal = {ArXiv},\n title = {How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation},\n volume = {abs/1603.08023},\n year = {2016}\n}\n'}",,"{'volume': 'abs/1603.08023', 'name': 'ArXiv'}",48.0,How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation,2016.0
199,12c68b91d114846ab1203e46a09e2e684684a396,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}]",244.0,"{'bibtex': '@Inproceedings{Mehrabian1980BasicDF,\n author = {A. Mehrabian},\n title = {Basic dimensions for a general psychological theory : implications for personality, social, environmental, and developmental studies},\n year = {1980}\n}\n'}",,"{'volume': '', 'name': ''}",1.0,"Basic dimensions for a general psychological theory : implications for personality, social, environmental, and developmental studies",1980.0
201,12f1e9093c1799806064b7899a7682a60cefc16b,,"[{'authorId': '32532116', 'name': 'S. Anderson'}, {'authorId': '1998849', 'name': 'A. Bechara'}, {'authorId': '144027810', 'name': 'H. Damasio'}, {'authorId': '2467200', 'name': 'D. Tranel'}, {'authorId': '2656777', 'name': 'A. Damasio'}]",1468.0,"{'bibtex': '@Article{Anderson1999ImpairmentOS,\n author = {S. Anderson and A. Bechara and H. Damasio and D. Tranel and A. Damasio},\n journal = {Nature Neuroscience},\n pages = {1032-1037},\n title = {Impairment of social and moral behavior related to early damage in human prefrontal cortex},\n volume = {2},\n year = {1999}\n}\n'}",,"{'volume': '2', 'pages': '1032-1037', 'name': 'Nature Neuroscience'}",27.0,Impairment of social and moral behavior related to early damage in human prefrontal cortex,1999.0
202,12f8a302ec2cf406abfc5b710ad46a2d64b3dcb5,"How to build virtual agents that establish rapport with human? According to Tickle-Degnen and Rosenthal [4], the three essential components of rapport are mutual attentiveness, positivity and coordination. In our previous work, we designed an embodied virtual agent to establish rapport with a human speaker by providing rapid and contingent nonverbal feedback [13] [22]. How do we know that a human speaker is feeling a sense of rapport? In this paper, we focus on the positivity component of rapport by investigating the relationship of human speakers' facial expressions on the establishment of rapport. We used an automatic facial expression coding tool called CERT to analyze the human dyad interactions and human-virtual human interactions. Results show that recognizing positive facial displays alone may be insufficient and that recognized negative facial displays was more diagnostic in assessing the level of rapport between participants.","[{'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",32.0,"{'bibtex': '@Article{Wang2009RapportAF,\n author = {Ning Wang and J. Gratch},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-6},\n title = {Rapport and facial expression},\n year = {2009}\n}\n'}",,"{'pages': '1-6', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",30.0,Rapport and facial expression,2009.0
203,12ff3eaa7b23970a284a711431523ac3378a351e,,"[{'authorId': '1732377', 'name': 'R. Aylett'}, {'authorId': '2910576', 'name': 'S. Louchart'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '7306645', 'name': 'M. Vala'}]",316.0,"{'bibtex': '@Inproceedings{Aylett2005FearNotA,\n author = {R. Aylett and S. Louchart and João Dias and Ana Paiva and M. Vala},\n pages = {305-316},\n title = {FearNot! - An Experiment in Emergent Narrative},\n year = {2005}\n}\n'}",,{'pages': '305-316'},22.0,FearNot! - An Experiment in Emergent Narrative,2005.0
205,1347f1516a698d862bf8d9bb53be14f9f493193b,"The claim that computer personalities can be human personalities was tested by demonstrating that (1) computer personalities can be easily created using a minimal set of cues, and (2) that people will respond to these personalities in the same way they would respond to similar human personalities. The present study focused on the ""similarity-attraction hypothesis,"" which predicts that people will prefer to interact with others who are similar in personality. In a 2 × 2, balanced, between-subjects experiment (n = 48), dominant and submissive subjects were randomly matched with a computer that was endowed with the properties associated with dominance or submissiveness. Subjects recognized the computer's personality type, distinct from friendliness and competence. In addition, subjects not only preferred the similar computer, but they were more satisfied with the interaction. The findings demonstrate that personality does not require richly defined agents, sophisticated pictorial representations, nautral language processing, or artificial intelligence. Rather, even the most superficial manipulations are sufficient to exhibit personality, with powerful effects.","[{'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '33875827', 'name': 'Youngme Moon'}, {'authorId': '145469150', 'name': 'B. Fogg'}, {'authorId': '143923082', 'name': 'Byron Reeves'}, {'authorId': '2223729202', 'name': 'Chris Dryer'}]",756.0,"{'bibtex': '@Article{Nass1995CanCP,\n author = {C. Nass and Youngme Moon and B. Fogg and Byron Reeves and Chris Dryer},\n journal = {Conference Companion on Human Factors in Computing Systems},\n title = {Can computer personalities be human personalities?},\n year = {1995}\n}\n'}",,{'name': 'Conference Companion on Human Factors in Computing Systems'},5.0,Can computer personalities be human personalities?,1995.0
207,1348eb116429d2b284825607c26f5a7f7236d2fd,"The third Audio-Visual Emotion Challenge and workshop AVEC 2013 will be held in conjunction ACM Multimedia'13. Like the 2012 edition of AVEC, the workshop/challenge addresses the interpretation of social signals represented in both audio and video in terms of the high-level continuous dimensions arousal and valence, but importantly this year the data is that of a large number of clinically depressed patients and controls, with a sub-challenge in self-reported severity of depression estimation. Like both previous AVECs, the aim is to bring together the audio and video analysis communities.","[{'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '34030213', 'name': 'J. Krajewski'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",11.0,"{'bibtex': ""@Article{Valstar2013WorkshopSF,\n author = {M. Valstar and Björn Schuller and J. Krajewski and R. Cowie and M. Pantic},\n journal = {Proceedings of the 21st ACM international conference on Multimedia},\n title = {Workshop summary for the 3rd international audio/visual emotion challenge and workshop (AVEC'13)},\n year = {2013}\n}\n""}",,{'name': 'Proceedings of the 21st ACM international conference on Multimedia'},0.0,Workshop summary for the 3rd international audio/visual emotion challenge and workshop (AVEC'13),2013.0
208,13a774cabe403a580d5f93d80919f5e85983188a,"In face-to-face work, discussion and negotiation relies strongly on non-verbal feedback, which provides important clues to negotiation states such as agreement/disagreement and understanding/confusion, as well as indicating the emotional states and reactions of those around us. With the continued rise of virtual teams, collaborative work increasingly requires tools to manage the reality of distributed and remote work, which is often hampered by a lack of social cohesion and such phenomena as participants multi-tasking rather than paying full attention. This paper discusses the use of a neural network-based emotion recognition system and describes its application to the monitoring of presence and emotional states of participants in virtual meetings. Experimental analysis shows our Emotion Tracking Agent (ETA) to have marginally better accuracy at recognising universal emotions than human subjects presented with the same data.","[{'authorId': '46549484', 'name': 'S. Redfern'}, {'authorId': '2118148111', 'name': 'Paul Smith'}]",3.0,"{'bibtex': '@Inproceedings{Redfern2010EmotionTF,\n author = {S. Redfern and Paul Smith},\n title = {Emotion Tracking for Remote Conferencing Applications using Neural Networks.},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",23.0,Emotion Tracking for Remote Conferencing Applications using Neural Networks.,2010.0
209,13df913dce520381c61acff04510a3cd16776707,"When talking about spatial domains, humans frequently accompany their explanations with iconic gestures to depict what they are referring to. For example, when giving directions, it is common to see people making gestures that indicate the shape of buildings, or outline a route to be taken by the listener, and these gestures are essential to the understanding of the directions. Based on results from an ongoing study on language and gesture in direction-giving, we propose a framework to analyze such gestural images into semantic units (image description features), and to link these units to morphological features (hand shape, trajectory, etc.). This feature-based framework allows us to generate novel iconic gestures for embodied conversational agents, without drawing on a lexicon of canned gestures. We present an integrated microplanner that derives the form of both coordinated natural language and iconic gesture directly from given communicative goals, and serves as input to the speech and gesture realization engine in our NUMACK project.","[{'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '46580865', 'name': 'Paul Tepper'}, {'authorId': '145431806', 'name': 'Justine Cassell'}]",127.0,"{'bibtex': '@Inproceedings{Kopp2004TowardsIM,\n author = {S. Kopp and Paul Tepper and Justine Cassell},\n pages = {97-104},\n title = {Towards integrated microplanning of language and iconic gesture for multimodal output},\n year = {2004}\n}\n'}",,{'pages': '97-104'},30.0,Towards integrated microplanning of language and iconic gesture for multimodal output,2004.0
210,13e5c0c234ebf17bf17145d13a3d27c177dfbffe,"In order for robots to be socially accepted and generate empathy it is necessary that they display rich emotions. For robots such as Nao, body language is the best medium available given their inability to convey facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should significantly improve its sociability. This research investigates the creation of an Affect Space for the generation of emotional body language to be displayed by robots. To create an Affect Space for body language, one has to establish the contribution of the different positions of the joints to the emotional expression. The experiment reported in this paper investigated the effect of varying a robot's head position on the interpretation, Valence, Arousal and Stance of emotional key poses. It was found that participants were better than chance level in interpreting the key poses. This finding confirms that body language is an appropriate medium for robot to express emotions. Moreover, the results of this study support the conclusion that Head Position is an important body posture variable. Head Position up increased correct identification for some emotion displays (pride, happiness, and excitement), whereas Head Position down increased correct identification for other displays (anger, sadness). Fear, however, was identified well regardless of Head Position. Head up was always evaluated as more highly Aroused than Head straight or down. Evaluations of Valence (degree of negativity to positivity) and Stance (degree to which the robot was aversive to approaching), however, depended on both Head Position and the emotion displayed. The effects of varying this single body posture variable were complex.","[{'authorId': '2609243', 'name': 'Aryel Beck'}, {'authorId': '1713009', 'name': 'L. Cañamero'}, {'authorId': '2063584', 'name': 'K. Bard'}]",151.0,"{'bibtex': '@Article{Beck2010TowardsAA,\n author = {Aryel Beck and L. Cañamero and K. Bard},\n journal = {19th International Symposium in Robot and Human Interactive Communication},\n pages = {464-469},\n title = {Towards an Affect Space for robots to display emotional body language},\n year = {2010}\n}\n'}",,"{'pages': '464-469', 'name': '19th International Symposium in Robot and Human Interactive Communication'}",17.0,Towards an Affect Space for robots to display emotional body language,2010.0
211,13fdb8341f43ff606cd90a16b71d0e198ad28c57,,"[{'authorId': '47355165', 'name': 'Joanna Hall'}, {'authorId': '2465359', 'name': 'T. Tritton'}, {'authorId': '145781549', 'name': 'A. Rowe'}, {'authorId': '3859511', 'name': 'A. Pipe'}, {'authorId': '1730409', 'name': 'C. Melhuish'}, {'authorId': '2219529', 'name': 'U. Leonards'}]",46.0,"{'bibtex': '@Article{Hall2014PerceptionOO,\n author = {Joanna Hall and T. Tritton and A. Rowe and A. Pipe and C. Melhuish and U. Leonards},\n journal = {Robotics Auton. Syst.},\n pages = {392-399},\n title = {Perception of own and robot engagement in human-robot interactions and their dependence on robotics knowledge},\n volume = {62},\n year = {2014}\n}\n'}",,"{'volume': '62', 'pages': '392-399', 'name': 'Robotics Auton. Syst.'}",35.0,Perception of own and robot engagement in human-robot interactions and their dependence on robotics knowledge,2014.0
212,140045e50443c216c63a7d6d23afbe464735c26c,"Humans continuously assess one another's situational context, modify their own affective state, and then respond based on these outcomes through empathetic expression. Virtual agents should be capable of similarly empathizing with users in interactive environments. A key challenge posed by empathetic reasoning in virtual agents is determining whether to respond with parallel or reactive empathy. Parallel empathy refers to mere replication of another's affective state, whereas reactive empathy exhibits greater cognitive awareness and may lead to incongruent emotional responses (i.e., emotions different from the recipient's and perhaps intended to alter negative affect). Because empathy is not yet sufficiently well understood, it is unclear as to which type of empathy is most effective and under what circumstances they should be applied. Devising empirically informed models of empathy from observations of ""empathy in action"" may lead to virtual agents that can accurately respond in social situations. 
 
This paper proposes a unified inductive framework for modeling parallel and reactive empathy. First, in training sessions, a trainer guides a virtual agent through a series of problem-solving tasks in a learning environment and encounters empathetic characters. The proposed inductive architecture tracks situational data including actions, visited locations, intentions, and the trainer's physiological responses to generate models of empathy. Empathy models are used to drive runtime situation-appropriate empathetic behaviors by selecting suitable parallel or reactive empathetic expressions. An empirical evaluation of the approach in an interactive learning environment suggests that the induced empathy models can accurately assess social contexts and generate appropriate empathetic responses for virtual agent control.","[{'authorId': '2779835', 'name': 'Scott W. McQuiggan'}, {'authorId': '31942647', 'name': 'J. Robison'}, {'authorId': '2069956407', 'name': 'R. Phillips'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",68.0,"{'bibtex': '@Inproceedings{McQuiggan2008ModelingPA,\n author = {Scott W. McQuiggan and J. Robison and R. Phillips and James C. Lester},\n pages = {167-174},\n title = {Modeling parallel and reactive empathy in virtual agents: an inductive approach},\n year = {2008}\n}\n'}",,{'pages': '167-174'},29.0,Modeling parallel and reactive empathy in virtual agents: an inductive approach,2008.0
213,14229e6b265a45a58ddc78fc3cffbb4cf7e61173,,"[{'authorId': '2150357386', 'name': 'Chao Li'}, {'authorId': '121153870', 'name': 'Zhongtian Bao'}, {'authorId': '2128198994', 'name': 'Linhao Li'}, {'authorId': '143889734', 'name': 'Ziping Zhao'}]",110.0,"{'bibtex': '@Article{Li2020ExploringTR,\n author = {Chao Li and Zhongtian Bao and Linhao Li and Ziping Zhao},\n journal = {Inf. Process. Manag.},\n pages = {102185},\n title = {Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition},\n volume = {57},\n year = {2020}\n}\n'}",,"{'volume': '57', 'pages': '102185', 'name': 'Inf. Process. Manag.'}",32.0,Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition,2020.0
214,1433b65ae3a7d452c8232efc97d00c92c0774f67,"Reinforcement learning (RL) in the context of artificial agents is typically used to produce behavioral responses as a function of the reward obtained by interaction with the environment. When the problem consists of learning the shortest path to a goal, it is common to use reward functions yielding a fixed value after each decision, for example a positive value if the target location has been attained and a negative value at each intermediate step. However, this fixed strategy may be overly simplistic for agents to adapt to dynamic environments, in which resources may vary from time to time. By contrast, there is significant evidence that most living beings internally modulate reward value as a function of their context to expand their range of adaptivity. Inspired by the potential of this operation, we present a review of its underlying processes and we introduce a simplified formalization for artificial agents. The performance of this formalism is tested by monitoring the adaptation of an agent endowed with a model of motivated actor–critic, embedded with our formalization of value and constrained by physiological stability, to environments with different resource distribution. Our main result shows that the manner in which reward is internally processed as a function of the agent’s motivational state, strongly influences adaptivity of the behavioral cycles generated and the agent’s physiological stability.","[{'authorId': '1403846662', 'name': 'I. Cos-Aguilera'}, {'authorId': '1713009', 'name': 'L. Cañamero'}, {'authorId': '30668475', 'name': 'G. Hayes'}, {'authorId': '2070601979', 'name': 'A. Gillies'}]",54.0,"{'bibtex': '@Article{Cos-Aguilera2013HedonicVE,\n author = {I. Cos-Aguilera and L. Cañamero and G. Hayes and A. Gillies},\n journal = {Adaptive Behavior},\n pages = {465 - 483},\n title = {Hedonic value: enhancing adaptation for motivated agents},\n volume = {21},\n year = {2013}\n}\n'}",,"{'volume': '21', 'pages': '465 - 483', 'name': 'Adaptive Behavior'}",85.0,Hedonic value: enhancing adaptation for motivated agents,2013.0
215,143d950f9d2db4207f0ba2b96d9c420ab59fcb35,"Aiming to address the safety issue for the uncontrolled intersection, the existing schemes including design optimization of the intersection structure and additional traffic signal layout will waste lots of resources. With the rapid development of intelligent transportation, the technology of vehicle-vehicle communication provides a new way for this problem. The paper proposed a set of rules to clarify the sequence of vehicles to pass through uncontrolled intersection. The rules are planned based on the law of road traffic safety. According to the rules, each approaching car makes decision for preempting or yielding other cars based on the information from vehicle-vehicle communication. If the approaching car needs to yield other cars, we propose an algorithm to find a proper deceleration value to do yielding. The car brakes automatically using this deceleration value to avoid collision with other cars. After all, the tests were done to verify the effectiveness of the algorithm and demonstrate the function among the connected vehicles in intersection. The rule based collision avoidance algorithm can provide real-time collision detection and make safe deceleration for the cars crossing the uncontrolled intersection.","[{'authorId': '31459383', 'name': 'G. Lu'}, {'authorId': '8446882', 'name': 'Lumiao Li'}, {'authorId': '47903840', 'name': 'Yunpeng Wang'}, {'authorId': '2110044811', 'name': 'Ran Zhang'}, {'authorId': '8303550', 'name': 'Zewen Bao'}, {'authorId': '7314523', 'name': 'Haichong Chen'}]",32.0,"{'bibtex': '@Article{Lu2014ARB,\n author = {G. Lu and Lumiao Li and Yunpeng Wang and Ran Zhang and Zewen Bao and Haichong Chen},\n journal = {17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},\n pages = {115-120},\n title = {A rule based control algorithm of connected vehicles in uncontrolled intersection},\n year = {2014}\n}\n'}",,"{'pages': '115-120', 'name': '17th International IEEE Conference on Intelligent Transportation Systems (ITSC)'}",25.0,A rule based control algorithm of connected vehicles in uncontrolled intersection,2014.0
216,14602fa04a8fdf6b2d554d59647136699be3e275,,"[{'authorId': '1894052', 'name': 'Nicolas Habonneau'}, {'authorId': '2986533', 'name': 'Urs Richle'}, {'authorId': '1691377', 'name': 'N. Szilas'}, {'authorId': '12904171', 'name': 'J. Dumas'}]",20.0,"{'bibtex': '@Inproceedings{Habonneau20123DSI,\n author = {Nicolas Habonneau and Urs Richle and N. Szilas and J. Dumas},\n pages = {174-182},\n title = {3D Simulated Interactive Drama for Teenagers Coping with a Traumatic Brain Injury in a Parent},\n year = {2012}\n}\n'}",,{'pages': '174-182'},14.0,3D Simulated Interactive Drama for Teenagers Coping with a Traumatic Brain Injury in a Parent,2012.0
217,14624f972b1c14ecec3a3b1cebae8147943c0fa1,"Emotions are psychological traits which are associated with an individuals' thoughts, feelings, behavioral responses, and experiences of pleasure and displeasure. The ability to recognise a conversational partner's emotional state from their speech (and respond accordingly) is a longstanding requirement of a fully capable intelligent virtual agent. However, despite the fact that current approaches to emotion recognition primarily depend upon supervised machine learning models, there are no comprehensive guidelines for emotion label annotation of the corpora used to train such models. We present comprehensive guidelines for consistent and effective annotation of text corpora with emotion labels. In particular, our proposal directly addresses the requirements of multi-label emotion recognition, and we demonstrate how an implementation of our proposed guidelines led to substantially (30%) higher agreement score among human annotators.","[{'authorId': '7484275', 'name': 'Md. Adnanul Islam'}, {'authorId': '1806836', 'name': 'Md. Saddam Hossain Mukta'}, {'authorId': '145171812', 'name': 'P. Olivier'}, {'authorId': '2183393804', 'name': 'Md. Mahbubur Rahman'}]",5.0,"{'bibtex': '@Book{Islam2022ComprehensiveGF,\n author = {Md. Adnanul Islam and Md. Saddam Hossain Mukta and P. Olivier and Md. Mahbubur Rahman},\n booktitle = {International Conference on Intelligent Virtual Agents},\n journal = {Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents},\n title = {Comprehensive guidelines for emotion annotation},\n year = {2022}\n}\n'}","[{'paperId': '971ade660c2785ea5306720272a52d8c51cddb21', 'title': 'Context based Emotion Recognition from Bengali Text using Transformers'}, {'paperId': '06a89240de40e0bdf68a252cb04dc527cdedc9f6', 'title': 'Degrees of Anger Prediction from Speech'}, {'paperId': '3bffc9b4b60d88d562509caf6ecb78136e191bb8', 'title': 'Punctuation Prediction in Bangla Text'}, {'paperId': 'fd1e917dabadd533dba35bf73f91c808c1571404', 'title': 'Enhanced neurologic concept recognition using a named entity recognition model based on transformers'}, {'paperId': '60f0fe6af7cb1dd7329075c67044dd11ad687179', 'title': 'The impact of task complexity and translating self-efficacy belief on students’ translation performance: Evidence from process and product data'}]",{'name': 'Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents'},35.0,Comprehensive guidelines for emotion annotation,2022.0
218,1485b5a20f4f876c46f5dfa4e1bf3caf2452e26f,,"[{'authorId': '1783919', 'name': 'M. Lim'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '1732377', 'name': 'R. Aylett'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",68.0,"{'bibtex': '@Article{Lim2012CreatingAA,\n author = {M. Lim and João Dias and R. Aylett and Ana Paiva},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {287-311},\n title = {Creating adaptive affective autonomous NPCs},\n volume = {24},\n year = {2012}\n}\n'}",,"{'volume': '24', 'pages': '287-311', 'name': 'Autonomous Agents and Multi-Agent Systems'}",63.0,Creating adaptive affective autonomous NPCs,2012.0
220,1486815229ab8ae405ece93aff2c82ba234258e2,"In seems there are two dimensions that underlie most judgments of traits, people, groups, and cultures. Although the definitions vary, the first makes reference to attributes such as competence, agency, and individualism, and the second to warmth, communality, and collectivism. But the relationship between the two dimensions seems unclear. In trait and person judgment, they are often positively related; in group and cultural stereotypes, they are often negatively related. The authors report 4 studies that examine the dynamic relationship between these two dimensions, experimentally manipulating the location of a target of judgment on one and examining the consequences for the other. In general, the authors' data suggest a negative dynamic relationship between the two, moderated by factors the impact of which they explore.","[{'authorId': '5672179', 'name': 'C. Judd'}, {'authorId': '1387239301', 'name': 'Laurie James-Hawkins'}, {'authorId': '4779221', 'name': 'V. Yzerbyt'}, {'authorId': '1996561', 'name': 'Y. Kashima'}]",903.0,"{'bibtex': '@Article{Judd2005FundamentalDO,\n author = {C. Judd and Laurie James-Hawkins and V. Yzerbyt and Y. Kashima},\n journal = {Journal of personality and social psychology},\n pages = {\n          899-913\n        },\n title = {Fundamental dimensions of social judgment: understanding the relations between judgments of competence and warmth.},\n volume = {89 6},\n year = {2005}\n}\n'}",,"{'volume': '89 6', 'pages': '\n          899-913\n        ', 'name': 'Journal of personality and social psychology'}",55.0,Fundamental dimensions of social judgment: understanding the relations between judgments of competence and warmth.,2005.0
221,14904551131486eb17ad823b6c4308fea0655681,,"[{'authorId': '1742930', 'name': 'E. André'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",63.0,"{'bibtex': '@Inproceedings{André2010InteractingWE,\n author = {E. André and C. Pelachaud},\n pages = {123-149},\n title = {Interacting with Embodied Conversational Agents},\n year = {2010}\n}\n'}",,"{'volume': '', 'pages': '123-149', 'name': ''}",104.0,Interacting with Embodied Conversational Agents,2010.0
222,14caf1d247a450960efcabaa934f8ec404b08714,,"[{'authorId': '28987363', 'name': 'H. Abdessalem'}, {'authorId': '1788058', 'name': 'C. Frasson'}]",19.0,"{'bibtex': '@Inproceedings{Abdessalem2017RealtimeBA,\n author = {H. Abdessalem and C. Frasson},\n pages = {133-143},\n title = {Real-time Brain Assessment for Adaptive Virtual Reality Game : A Neurofeedback Approach},\n year = {2017}\n}\n'}",,{'pages': '133-143'},17.0,Real-time Brain Assessment for Adaptive Virtual Reality Game : A Neurofeedback Approach,2017.0
223,14e14f724401e65f6613a1ece9892429c2d5dabc,"Past studies on emotion classification focus on the writerpsilas emotional state. This research addresses the reader aspect instead. The classification of documents into reader-emotion categories has several applications. One of them is to integrate reader-emotion classification into a Web search engine to allow users to retrieve documents that contain relevant contents and at the same time instill proper emotions. In this paper, we automatically classify documents into reader-emotion categories, and examine classification performance under different feature settings. Experiments show that certain feature combinations achieve good accuracy. We also compare the best classifierpsilas classification results with the emotional distributions of documents to determine how closely the classifier models the underlying reader behavior. Finally, we investigate the feasibility of emotion ranking.","[{'authorId': '29828959', 'name': 'K. Lin'}, {'authorId': '1782222', 'name': 'Changhua Yang'}, {'authorId': '153924342', 'name': 'Hsin-Hsi Chen'}]",134.0,"{'bibtex': ""@Article{Lin2008EmotionCO,\n author = {K. Lin and Changhua Yang and Hsin-Hsi Chen},\n journal = {2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},\n pages = {220-226},\n title = {Emotion Classification of Online News Articles from the Reader's Perspective},\n volume = {1},\n year = {2008}\n}\n""}",,"{'volume': '1', 'pages': '220-226', 'name': '2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology'}",12.0,Emotion Classification of Online News Articles from the Reader's Perspective,2008.0
224,14e53403a0055dbe5faaf9f1f3be96ca0e692a4d,,"[{'authorId': '1716301', 'name': 'R. Schapire'}, {'authorId': '1740765', 'name': 'Y. Singer'}]",3518.0,"{'bibtex': '@Article{Schapire1998ImprovedBA,\n author = {R. Schapire and Y. Singer},\n journal = {Machine Learning},\n pages = {297-336},\n title = {Improved Boosting Algorithms Using Confidence-rated Predictions},\n volume = {37},\n year = {1998}\n}\n'}",,"{'volume': '37', 'pages': '297-336', 'name': 'Machine Learning'}",35.0,Improved Boosting Algorithms Using Confidence-rated Predictions,1998.0
225,151fc3935a2a994c89d4261050216478fb865f40,"Why to create personalities for synthetic actors -- Dressing virtual humans -- Autonomous virtual actors based on virtual sensors -- Towards personalities for animated agents with reactive and planning behaviors -- IMPROV: A system for real-time animation of behavior-based interactive synthetic actors -- Multi-level control for animated autonomous agents: Do the right thing... Oh, not that... -- Tools for an interactive virtual cinema -- Acting in character -- Some requirements and approaches for natural language in a believable agent -- Personality parameters and programs -- What sort of control system is able to have a personality? -- Personalities for synthetic actors: Current issues and some perspectives -- Personalities for synthetic actors: A bibliography.","[{'authorId': '1764052', 'name': 'P. Petta'}, {'authorId': '1691580', 'name': 'R. Trappl'}]",115.0,"{'bibtex': '@Inproceedings{Petta1997CreatingPF,\n author = {P. Petta and R. Trappl},\n title = {Creating Personalities for Synthetic Actors: Towards Autonomous Personality Agents},\n year = {1997}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Creating Personalities for Synthetic Actors: Towards Autonomous Personality Agents,1997.0
226,1522f7812b2c7c183abcabe8e7f6710e048b3d8e,"
 
 This paper describes a social robotic game player that is able to successfully play a team card game called Sueca. The question we will address in this paper is: how can we build a social robot player that is able to balance its ability to play the card game with natural and social behaviours towards its partner and its opponents. The first challenge we faced concerned the development of a competent artificial player for a hidden information game, whose time constraint is the average human decision time. To accomplish this requirement, the Perfect Information Monte Carlo (PIMC) algorithm was used. Further, we have performed an analysis of this algorithm's possible parametrizations for games trees that cannot be fully explored in a reasonable amount of time with a MinMax search. Additionally, given the nature of the Sueca game, such robotic player must master the social interactions both as a partner and as an opponent. To do that, an emotional agent framework (FAtiMA) was used to build the emotional and social behaviours of the robot. At each moment, the robot not only plays competitively but also appraises the situation and responds emotionally in a natural manner. To test the approach, we conducted a user study and compared the levels of trust participants attributed to the robots and to human partners. Results have shown that the robot team exhibited a winning rate of 60%. Concerning the social aspects, the results also showed that human players increased their trust in the robot as their game partners (similar to the way to the trust levels change towards human partners).
 
","[{'authorId': '144106225', 'name': 'Filipa Correia'}, {'authorId': '1401670338', 'name': 'Patrícia Alves-Oliveira'}, {'authorId': '145856842', 'name': 'T. Ribeiro'}, {'authorId': '145125979', 'name': 'Francisco S. Melo'}, {'authorId': '115420343', 'name': 'A. Paiva'}]",29.0,"{'bibtex': '@Inproceedings{Correia2021ASR,\n author = {Filipa Correia and Patrícia Alves-Oliveira and T. Ribeiro and Francisco S. Melo and A. Paiva},\n pages = {23-29},\n title = {A Social Robot as a Card Game Player},\n year = {2021}\n}\n'}",,{'pages': '23-29'},15.0,A Social Robot as a Card Game Player,2021.0
227,154387fe1347664ed7433156f19f9ea29b0ceb33,,"[{'authorId': '2304832', 'name': 'C. Lamm'}, {'authorId': '47272511', 'name': 'T. Singer'}]",481.0,"{'bibtex': '@Article{Lamm2010TheRO,\n author = {C. Lamm and T. Singer},\n journal = {Brain Structure and Function},\n pages = {579-591},\n title = {The role of anterior insular cortex in social emotions},\n volume = {214},\n year = {2010}\n}\n'}",,"{'volume': '214', 'pages': '579-591', 'name': 'Brain Structure and Function'}",128.0,The role of anterior insular cortex in social emotions,2010.0
228,1566cf20e2ba91ca8857c30083419bf7c127094b,,"[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}]",4217.0,"{'bibtex': '@Inproceedings{Ekman1978FacialAC,\n author = {P. Ekman and Wallace V. Friesen},\n title = {Facial action coding system: a technique for the measurement of facial movement},\n year = {1978}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Facial action coding system: a technique for the measurement of facial movement,1978.0
229,156a361e5a0ab5378b3057a6408d49eac36b97f0,"Emotions are central to the experience of literary narrative fiction. Affect and mood can influence what book people choose, based partly on whether their goal is to change or maintain their current emotional state. Once having chosen a book, the narrative itself acts to evoke and transform emotions, both directly through the events and characters depicted and through the cueing of emotionally valenced memories. Once evoked by the story, these emotions can in turn influence a person's experience of the narrative. Lastly, emotions experienced during reading may have consequences after closing the covers of a book. This article reviews the current state of empirical research for each of these stages, providing a snapshot of what is known about the interaction between emotions and literary narrative fiction. With this, we can begin to sketch the outlines of what remains to be discovered.","[{'authorId': '1829071', 'name': 'R. Mar'}, {'authorId': '2297721', 'name': 'K. Oatley'}, {'authorId': '3646643', 'name': 'Maja Djikic'}, {'authorId': '4044114', 'name': 'J. Mullin'}]",292.0,"{'bibtex': '@Article{Mar2011EmotionAN,\n author = {R. Mar and K. Oatley and Maja Djikic and J. Mullin},\n journal = {Cognition and Emotion},\n pages = {818 - 833},\n title = {Emotion and narrative fiction: Interactive influences before, during, and after reading},\n volume = {25},\n year = {2011}\n}\n'}",,"{'volume': '25', 'pages': '818 - 833', 'name': 'Cognition and Emotion'}",89.0,"Emotion and narrative fiction: Interactive influences before, during, and after reading",2011.0
230,157ce1b1fb97dbb1c09a60f8907a5ccfe9b1fd04,"We present results from an empirical study investigating the effect of embodiment and minimal gestures in an interactive drumming game consisting of an autonomous child-sized humanoid robot (KASPAR) playing with child participants. In this study, each participant played three games with a humanoid robot that played a drum whilst simultaneously making (or not making) head gestures. The three games included the participant interacting with the real robot (physical embodiment condition), interacting with a hidden robot when only the sound of the robot is heard (disembodiment condition; note that the term 'disembodiment' is used in this paper specifically to refer to an experimental condition where a physical robot produces the sound cues, but is not visible to the participants), or interacting with a real-time image of the robot (virtual embodiment condition). We used a mixed design where repeated measures were used to evaluate embodiment effects and independent-groups measures were used to study the gestures effects. Data from the implementation of a human–robot interaction experiment with 66 children are presented, and statistically analyzed in terms of participants' subjective experiences and drumming performance of the human–robot pair. The subjective experiences showed significant differences for the different embodiment conditions when gestures were used in terms of enjoyment of the game, and perceived intelligence and appearance of the robot. The drumming performance also differed significantly within the embodiment conditions and the presence of gestures increased these differences significantly. The presence of a physical, embodied robot enabled more interaction, better drumming and turn-taking, as well as enjoyment of the interaction, especially when the robot used gestures.","[{'authorId': '1399283111', 'name': 'H. Kose-Bagci'}, {'authorId': '2083388', 'name': 'E. Ferrari'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}, {'authorId': '1700812', 'name': 'D. Syrdal'}, {'authorId': '1718528', 'name': 'Chrystopher L. Nehaniv'}]",120.0,"{'bibtex': '@Article{Kose-Bagci2009EffectsOE,\n author = {H. Kose-Bagci and E. Ferrari and K. Dautenhahn and D. Syrdal and Chrystopher L. Nehaniv},\n journal = {Advanced Robotics},\n pages = {1951 - 1996},\n title = {Effects of Embodiment and Gestures on Social Interaction in Drumming Games with a Humanoid Robot},\n volume = {23},\n year = {2009}\n}\n'}",,"{'volume': '23', 'pages': '1951 - 1996', 'name': 'Advanced Robotics'}",61.0,Effects of Embodiment and Gestures on Social Interaction in Drumming Games with a Humanoid Robot,2009.0
231,15870c7621a87c41cd8d087bbdc28ed1d9503919,"BACKGROUND
Mild behavioral impairment (MBI) is a construct that describes the emergence at ≥50 years of age of sustained and impactful neuropsychiatric symptoms (NPS), as a precursor to cognitive decline and dementia. MBI describes NPS of any severity, which are not captured by traditional psychiatric nosology, persist for at least 6 months, and occur in advance of or in concert with mild cognitive impairment. While the detection and description of MBI has been operationalized in the International Society to Advance Alzheimer's Research and Treatment - Alzheimer's Association (ISTAART-AA) research diagnostic criteria, there is no instrument that accurately reflects MBI as described.


OBJECTIVE
To develop an instrument based on ISTAART-AA MBI criteria.


METHODS
Eighteen subject matter experts participated in development using a modified Delphi process. An iterative process ensured items reflected the five MBI domains of 1) decreased motivation; 2) emotional dysregulation; 3) impulse dyscontrol; 4) social inappropriateness; and 5) abnormal perception or thought content. Instrument language was developed a priori to pertain to non-demented functionally independent older adults.


RESULTS
We present the Mild Behavioral Impairment Checklist (MBI-C), a 34-item instrument, which can easily be completed by a patient, close informant, or clinician.


CONCLUSION
The MBI-C provides the first measure specifically developed to assess the MBI construct as explicitly described in the criteria. Its utility lies in MBI case detection, and monitoring the emergence of MBI symptoms and domains over time. Studies are required to determine the prognostic value of MBI for dementia development, and for predicting different dementia subtypes.","[{'authorId': '2005479', 'name': 'Z. Ismail'}, {'authorId': '114185955', 'name': 'L. Agüera-Ortiz'}, {'authorId': '2914632', 'name': 'H. Brodaty'}, {'authorId': '40277466', 'name': 'Alicja Cieslak'}, {'authorId': '2084085', 'name': 'J. Cummings'}, {'authorId': '4004697', 'name': 'Corinne E. Fischer'}, {'authorId': '32720034', 'name': 'S. Gauthier'}, {'authorId': '3671316', 'name': 'Y. Geda'}, {'authorId': '144528209', 'name': 'N. Herrmann'}, {'authorId': '31458208', 'name': 'J. Kanji'}, {'authorId': '4797664', 'name': 'K. Lanctôt'}, {'authorId': '49971136', 'name': 'David S. Miller'}, {'authorId': '3035849', 'name': 'M. Mortby'}, {'authorId': '8474541', 'name': 'C. Onyike'}, {'authorId': '144889112', 'name': 'P. Rosenberg'}, {'authorId': '144765494', 'name': 'Eric E. Smith'}, {'authorId': '116848569', 'name': 'Gwenn S. Smith'}, {'authorId': '6007006', 'name': 'D. Sultzer'}, {'authorId': '1901398', 'name': 'C. Lyketsos'}]",251.0,"{'bibtex': ""@Article{Ismail2017TheMB,\n author = {Z. Ismail and L. Agüera-Ortiz and H. Brodaty and Alicja Cieslak and J. Cummings and Corinne E. Fischer and S. Gauthier and Y. Geda and N. Herrmann and J. Kanji and K. Lanctôt and David S. Miller and M. Mortby and C. Onyike and P. Rosenberg and Eric E. Smith and Gwenn S. Smith and D. Sultzer and C. Lyketsos},\n journal = {Journal of Alzheimer's disease : JAD},\n pages = {\n          929-938\n        },\n title = {The Mild Behavioral Impairment Checklist\xa0(MBI-C): A Rating Scale for\xa0Neuropsychiatric Symptoms in\xa0Pre-Dementia Populations.},\n volume = {56 3},\n year = {2017}\n}\n""}",,"{'volume': '56 3', 'pages': '\n          929-938\n        ', 'name': ""Journal of Alzheimer's disease : JAD""}",47.0,The Mild Behavioral Impairment Checklist (MBI-C): A Rating Scale for Neuropsychiatric Symptoms in Pre-Dementia Populations.,2017.0
232,1597d4ab946eae4f9b1e2fb1a360dd75d75dd5e7,"We present a novel computational model of emotion, personality and social relationships, implemented and evaluated in an existing commercial game (The Elder Scrolls V: Skyrim). We argue that Non Player Characters (NPCs) with such capabilities will accommodate a new experience in playing games and provide evidence supporting this. Applying the ERiSA Framework [1, 2] to the Skyrim Creation Kit, we designed a simple quest and 2 unique NPCs to interact with. When the ERiSA framework is used, players reported significant changes in their social relationship with the two NPCs compared to the baseline. Most importantly, the results further indicate that the models provide a new experience in playing games. In particular, players report enhanced emotional attachment to the NPCs and appear to forge relationships with the NPCs. Finally, the implemented models result in significant changes in the game engagement and the game immersion score.","[{'authorId': '2789205', 'name': 'Andry Chowanda'}, {'authorId': '35896384', 'name': 'P. Blanchfield'}, {'authorId': '1795102', 'name': 'Martin Flintham'}, {'authorId': '1795528', 'name': 'M. Valstar'}]",21.0,"{'bibtex': '@Inproceedings{Chowanda2016ComputationalMO,\n author = {Andry Chowanda and P. Blanchfield and Martin Flintham and M. Valstar},\n pages = {1343-1344},\n title = {Computational Models of Emotion, Personality, and Social Relationships for Interactions in Games: (Extended Abstract)},\n year = {2016}\n}\n'}",,{'pages': '1343-1344'},6.0,"Computational Models of Emotion, Personality, and Social Relationships for Interactions in Games: (Extended Abstract)",2016.0
233,15b9f61decf351ec2bbf8027010814d31cad3449,,"[{'authorId': '3161727', 'name': 'Juan R. Terven'}, {'authorId': '3262395', 'name': 'B. Raducanu'}, {'authorId': '1409474833', 'name': 'M. Meza-de-Luna'}, {'authorId': '143861895', 'name': 'Joaquin Salas'}]",23.0,"{'bibtex': '@Article{Terven2016HeadgesturesMD,\n author = {Juan R. Terven and B. Raducanu and M. Meza-de-Luna and Joaquin Salas},\n journal = {Neurocomputing},\n pages = {866-876},\n title = {Head-gestures mirroring detection in dyadic social interactions with computer vision-based wearable devices},\n volume = {175},\n year = {2016}\n}\n'}",,"{'volume': '175', 'pages': '866-876', 'name': 'Neurocomputing'}",78.0,Head-gestures mirroring detection in dyadic social interactions with computer vision-based wearable devices,2016.0
234,15c8f2332cebc1b4d5f07083e3f586508747b65b,"In this paper, we present the verification and validation of an agent-based model of forest fires. We use a combination of a Virtual Overlay Multi-Agent System (VOMAS) validation scheme with Fire Weather Index (FWI) to validate the forest fire Simulation. FWI is based on decades of real forest fire data and is now regarded as a standard index for fire probability with wide usage across Canada, New Zealand and Australia. VOMAS approach allows for flexible validation of agent-based simulation models. In the current work, it is used in the form of a simulation of a randomly deployed Wireless Sensor Network for forest monitoring. Here, each virtual ""sensor"" agent uses FWI to calculate fire probability and compares it with the simulation model. VOMAS verification and validation methodology for agent-based models allows for interactive design of Agent-Based Models involving both the Simulation Specialists as well as the Subject Matter Experts. The presented simulation model also uses weather parameters such as wind speed, rain, snow to calculate Indices such as Fire Weather Index (FWI), Build Up Index (BUI) and Initial Spread Index (ISI) in real time. We also study the effects of fires on the life of simulated VOMAS sensors. Using extensive simulations, we demonstrate the effectiveness and ease of use of VOMAS based Validation.","[{'authorId': '1795560', 'name': 'M. Niazi'}, {'authorId': '2073401', 'name': 'Qasim Siddique'}, {'authorId': '144664815', 'name': 'A. Hussain'}, {'authorId': '1796567', 'name': 'M. Kolberg'}]",36.0,"{'bibtex': '@Inproceedings{Niazi2010VerificationV,\n author = {M. Niazi and Qasim Siddique and A. Hussain and M. Kolberg},\n pages = {1},\n title = {Verification & validation of an agent-based forest fire simulation model},\n year = {2010}\n}\n'}",,{'pages': '1'},43.0,Verification & validation of an agent-based forest fire simulation model,2010.0
235,15ca9bcd5ff14f0699dde08bac5576796763238e,,"[{'authorId': '1762744', 'name': 'A. Stolcke'}, {'authorId': '1758841', 'name': 'K. Ries'}, {'authorId': '145632116', 'name': 'N. Coccaro'}, {'authorId': '70422141', 'name': 'Elizabeth Shriberg'}, {'authorId': '2057063782', 'name': 'R. Bates'}, {'authorId': '1746807', 'name': 'Dan Jurafsky'}, {'authorId': '122297135', 'name': 'P. Taylor'}, {'authorId': '46776641', 'name': 'Rachel Martin'}, {'authorId': '1403242564', 'name': 'C. V. Ess-Dykema'}, {'authorId': '3227843', 'name': 'M. Meteer'}]",968.0,"{'bibtex': '@Inproceedings{Stolcke1982SRII,\n author = {A. Stolcke and K. Ries and N. Coccaro and Elizabeth Shriberg and R. Bates and Dan Jurafsky and P. Taylor and Rachel Martin and C. V. Ess-Dykema and M. Meteer},\n title = {SRI International},\n year = {1982}\n}\n'}",,,0.0,SRI International,1982.0
236,15d0818521234bdfb661945cbff2ebc731d490d3,,"[{'authorId': '145500304', 'name': 'N. Morina'}, {'authorId': '6061751', 'name': 'Hiske Ijntema'}, {'authorId': '4257503', 'name': 'K. Meyerbröker'}, {'authorId': '2282500', 'name': 'P. Emmelkamp'}]",331.0,"{'bibtex': '@Article{Morina2015CanVR,\n author = {N. Morina and Hiske Ijntema and K. Meyerbröker and P. Emmelkamp},\n journal = {Behaviour research and therapy},\n pages = {\n          18-24\n        },\n title = {Can virtual reality exposure therapy gains be generalized to real-life? A meta-analysis of studies applying behavioral assessments.},\n volume = {74},\n year = {2015}\n}\n'}",,"{'volume': '74', 'pages': '\n          18-24\n        ', 'name': 'Behaviour research and therapy'}",31.0,Can virtual reality exposure therapy gains be generalized to real-life? A meta-analysis of studies applying behavioral assessments.,2015.0
238,15d2cee6ca6838c26bf6d53df4d8ba765b34940e,"Cultural differences exist in the use of emotion regulation (ER) strategies, but the focus to date has been on intrapersonal ER strategies such as cognitive reappraisal. An emerging literature highlights the importance of interpersonal ER, which utilizes social cues to facilitate the regulation of emotional states. In cultures that place high value on social interconnectedness as integral to their collectivistic self-construal, including East Asian cultures, interpersonal ER strategies may be particularly effective in reducing negative affect but this has not been previously tested. In this study, two groups comprising East Asian (n = 48) and Western European (n = 38) participants were randomly assigned to receive a priming narration depicting the use of either interpersonal (e.g., social modeling, perspective taking) or intrapersonal (e.g., cognitive reappraisal) ER strategies during a stressful experience. They were then instructed to utilize similar ER strategies in an emotion reactivity task during which they viewed high arousing negative pictorial stimuli while their heart rate (HR), heart rate variability (high frequency power – HF-HRV) and subjective affective states were measured. First we found that the East Asian group reported higher use of interpersonal ER strategies of social modeling and perspective taking in daily life. During the experimental interpersonal prime exposure, the East Asian group showed elevated HF-HRV (relative to baseline) compared to the Western European group, indicating more adaptive ER, but this pattern was not sustained during the reactivity or recovery phases. Instead, the East Asian group demonstrated increased HF-HRV and decreased HR across both prime conditions. The East Asian group also showed greater decreases in positive affect across the course of the experiment. Furthermore, individual differences in social modeling and individualistic self-construal moderated the effect of the ER prime in the East Asian group at trend levels, and main effects for perspective taking and reappraisal were observed in the Western European group. The findings support the notion that engaging in interpersonal ER strategies may be more beneficial for East Asian groups when immediately exposed to a stressful situation, as these strategies are congruent with cultural context and preferences, but our priming methodology may have limited the longer-term benefits.","[{'authorId': '2742204', 'name': 'B. Liddell'}, {'authorId': '2070111205', 'name': 'Emma Williams'}]",23.0,"{'bibtex': '@Article{Liddell2019CulturalDI,\n author = {B. Liddell and Emma Williams},\n journal = {Frontiers in Psychology},\n title = {Cultural Differences in Interpersonal Emotion Regulation},\n volume = {10},\n year = {2019}\n}\n'}",,"{'volume': '10', 'name': 'Frontiers in Psychology'}",76.0,Cultural Differences in Interpersonal Emotion Regulation,2019.0
239,15febdbe5e34a230f58040e9f5db776b124b5f2b,"SpaceTag is an object system on which each object called SpaceTag can be accessed only from limited locations and a limited time period. Its applications include entertainment systems, advertisement services, bulletin board systems, and personal communication systems. For one-way communication, they are broadcasted from the server; for two-way communication, users can cut and paste SpaceTags between their portable PCs and the real space. The SpaceTag system is a location-aware information system, as well as an augmented reality system because it attaches information to the real space. However, we categorize it as an overlaid virtual system, because it has no direct link to real objects. It can be realized as a public service without causing drastic change of this society, and without much cost.","[{'authorId': '1692859', 'name': 'H. Tarumi'}, {'authorId': '1989890', 'name': 'K. Morishita'}, {'authorId': '2138792', 'name': 'M. Nakao'}, {'authorId': '1736726', 'name': 'Y. Kambayashi'}]",53.0,"{'bibtex': '@Article{Tarumi1999SpaceTagAO,\n author = {H. Tarumi and K. Morishita and M. Nakao and Y. Kambayashi},\n journal = {Proceedings IEEE International Conference on Multimedia Computing and Systems},\n pages = {207-212 vol.1},\n title = {SpaceTag: an overlaid virtual system and its applications},\n volume = {1},\n year = {1999}\n}\n'}",,"{'volume': '1', 'pages': '207-212 vol.1', 'name': 'Proceedings IEEE International Conference on Multimedia Computing and Systems'}",10.0,SpaceTag: an overlaid virtual system and its applications,1999.0
240,1612a6053c7f017dd0897d26971a1dd9cd1bebc1,"Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Most existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.","[{'authorId': '145215929', 'name': 'Youngwoo Yoon'}, {'authorId': '38108552', 'name': 'Woo-Ri Ko'}, {'authorId': '145416765', 'name': 'Minsu Jang'}, {'authorId': None, 'name': 'Jaeyeon Lee'}, {'authorId': '1684726', 'name': 'Jaehong Kim'}, {'authorId': '1717371', 'name': 'Geehyuk Lee'}]",164.0,"{'bibtex': '@Article{Yoon2018RobotsLS,\n author = {Youngwoo Yoon and Woo-Ri Ko and Minsu Jang and Jaeyeon Lee and Jaehong Kim and Geehyuk Lee},\n journal = {2019 International Conference on Robotics and Automation (ICRA)},\n pages = {4303-4309},\n title = {Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots},\n year = {2018}\n}\n'}",,"{'pages': '4303-4309', 'name': '2019 International Conference on Robotics and Automation (ICRA)'}",30.0,Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots,2018.0
241,1636d468d11873743d046ce57a9547fb35075daf,,"[{'authorId': '144189909', 'name': 'R. Beale'}, {'authorId': '3134697', 'name': 'C. Creed'}]",254.0,"{'bibtex': '@Article{Beale2009AffectiveIH,\n author = {R. Beale and C. Creed},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {755-776},\n title = {Affective interaction: How emotional agents affect users},\n volume = {67},\n year = {2009}\n}\n'}",,"{'volume': '67', 'pages': '755-776', 'name': 'Int. J. Hum. Comput. Stud.'}",50.0,Affective interaction: How emotional agents affect users,2009.0
244,1644986dd336d8c7ac071fb11c91ba671cbed344,"Virtual rehabilitation supports motor training following stroke by means of tailored virtual environments. To optimize therapy outcome, virtual rehabilitation systems automatically adapt to the different patients' changing needs. Adaptation decisions should ideally be guided by both the observable performance and the hidden mind state of the user. We hypothesize that some affective aspects can be inferred from observable metrics. Here we present preliminary results of a classification exercise to decide on 4 states; tiredness, tension, pain and satisfaction. Descriptors of 3D hand movement and finger pressure were collected from 2 post-stroke participants while they practice on a virtual rehabilitation platform. Linear Support Vector Machine models were learnt to unfold a predictive relation between observation and the affective states considered. Initial results are promising (ROC Area under the curve (mean±std): 0.713 ± 0.137). Confirmation of these opens the door to incorporate surrogates of mind state into the algorithm deciding on therapy adaptation.","[{'authorId': '9364288', 'name': 'J. Rivas'}, {'authorId': '1398007604', 'name': 'F. Orihuela-Espina'}, {'authorId': '144763689', 'name': 'L. Sucar'}, {'authorId': '33402510', 'name': 'Lorena Palafox'}, {'authorId': '1405064204', 'name': 'Jorge Hernández-Franco'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}]",19.0,"{'bibtex': '@Article{Rivas2015DetectingAS,\n author = {J. Rivas and F. Orihuela-Espina and L. Sucar and Lorena Palafox and Jorge Hernández-Franco and N. Bianchi-Berthouze},\n journal = {2015 9th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth)},\n pages = {287-292},\n title = {Detecting affective states in virtual rehabilitation},\n year = {2015}\n}\n'}",,"{'pages': '287-292', 'name': '2015 9th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth)'}",26.0,Detecting affective states in virtual rehabilitation,2015.0
245,16692e28df1e81080755d6a99b604fe225c13107,"One of the main challenges faced by the video game industry is to give life to believable nonplayer characters (NPCs). Research shows that emotions play a key role in determining the behavior of individuals. In order to improve the believability of NPCs' behavior, we propose in this paper a model of the dynamics of emotions taking into account the personality and the social relations of the character. First, we present work from the literature on emotions, personality, and social relations in computer science and in human and social sciences. We focus on the influence of personality on the triggering of emotions, and the influence of emotions on the dynamics of social relations. Based on this work, we propose a dynamic model of the socioemotional state and its implementation as part of a tool for game programmers. This tool aims at the simulation of the evolution of emotions and social relations of NPCs based on their personality and roles.","[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1731432', 'name': 'N. Sabouret'}, {'authorId': '1841984', 'name': 'V. Corruble'}]",86.0,"{'bibtex': ""@Article{Ochs2009SimulationOT,\n author = {M. Ochs and N. Sabouret and V. Corruble},\n journal = {IEEE Transactions on Computational Intelligence and AI in Games},\n pages = {281-297},\n title = {Simulation of the Dynamics of Nonplayer Characters' Emotions and Social Relations in Games},\n volume = {1},\n year = {2009}\n}\n""}",,"{'volume': '1', 'pages': '281-297', 'name': 'IEEE Transactions on Computational Intelligence and AI in Games'}",59.0,Simulation of the Dynamics of Nonplayer Characters' Emotions and Social Relations in Games,2009.0
248,167136806b3a4234b76ba97515310360f7d57a71,"Marvin Minsky--one of the fathers of computer science and cofounder of the Artificial Intelligence Laboratory at MIT--gives a revolutionary answer to the age-old question: ""how does the mind work?""","[{'authorId': '1847175', 'name': 'M. Minsky'}]",1996.0,"{'bibtex': '@Inproceedings{Minsky1986TheSO,\n author = {M. Minsky},\n title = {The Society of Mind},\n year = {1986}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Society of Mind,1986.0
249,16892074764386b74b6040fe8d6946b67a246a0b,"Background: Facial expressions of emotions represent classic stimuli for the study of social cognition. Developing virtual dynamic facial expressions of emotions, however, would open-up possibilities, both for fundamental and clinical research. For instance, virtual faces allow real-time Human–Computer retroactions between physiological measures and the virtual agent. Objectives: The goal of this study was to initially assess concomitants and construct validity of a newly developed set of virtual faces expressing six fundamental emotions (happiness, surprise, anger, sadness, fear, and disgust). Recognition rates, facial electromyography (zygomatic major and corrugator supercilii muscles), and regional gaze fixation latencies (eyes and mouth regions) were compared in 41 adult volunteers (20 ♂, 21 ♀) during the presentation of video clips depicting real vs. virtual adults expressing emotions. Results: Emotions expressed by each set of stimuli were similarly recognized, both by men and women. Accordingly, both sets of stimuli elicited similar activation of facial muscles and similar ocular fixation times in eye regions from man and woman participants. Conclusion: Further validation studies can be performed with these virtual faces among clinical populations known to present social cognition difficulties. Brain–Computer Interface studies with feedback–feedforward interactions based on facial emotion expressions can also be conducted with these stimuli.","[{'authorId': '6781484', 'name': 'C. Joyal'}, {'authorId': '2071403936', 'name': 'Laurence Jacob'}, {'authorId': '40401139', 'name': 'Marie-Hélène Cigna'}, {'authorId': '50650369', 'name': 'J. Guay'}, {'authorId': '39993457', 'name': 'P. Renaud'}]",34.0,"{'bibtex': '@Article{Joyal2014VirtualFE,\n author = {C. Joyal and Laurence Jacob and Marie-Hélène Cigna and J. Guay and P. Renaud},\n journal = {Frontiers in Human Neuroscience},\n title = {Virtual Faces Expressing Emotions: An Initial Concomitant and Construct Validity Study},\n volume = {8},\n year = {2014}\n}\n'}",,"{'volume': '8', 'name': 'Frontiers in Human Neuroscience'}",59.0,Virtual Faces Expressing Emotions: An Initial Concomitant and Construct Validity Study,2014.0
250,16a76a1c16055e811a432ca831d978e6428423ee,,"[{'authorId': '4354199', 'name': 'J. Montepare'}, {'authorId': '115888540', 'name': 'Sabra B. Goldstein'}, {'authorId': '2058302184', 'name': 'Annmarie Clausen'}]",268.0,"{'bibtex': '@Article{Montepare1987TheIO,\n author = {J. Montepare and Sabra B. Goldstein and Annmarie Clausen},\n journal = {Journal of Nonverbal Behavior},\n pages = {33-42},\n title = {The identification of emotions from gait information},\n volume = {11},\n year = {1987}\n}\n'}",,"{'volume': '11', 'pages': '33-42', 'name': 'Journal of Nonverbal Behavior'}",17.0,The identification of emotions from gait information,1987.0
252,16c9368a9874f1825a140578335be72360f7725a,"Socially-aware virtual agents may guide the design and deployment of future computing systems. This paper addresses unconscious affect recognition in the context of social interaction between agents and humans. An experiment is performed in which participants interact visually and verbally with virtual embodied agents. During the interaction, both the vocal pitch and the affective facial expressions of the agent are manipulated and the consecutive vocal and facial expressions of the participants are registered. Manual and computational analyses of the expressions reveal vocal and facial imitation as a sign of unconscious affect recognition and social engagement.","[{'authorId': '153696301', 'name': 'R.J.H. Mattheij'}, {'authorId': '36365492', 'name': 'M. Postma'}, {'authorId': '1729457', 'name': 'E. Postma'}]",8.0,"{'bibtex': '@Article{Mattheij2013VocalAF,\n author = {R.J.H. Mattheij and M. Postma and E. Postma},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {815-820},\n title = {Vocal and Facial Imitation of Humans Interacting with Virtual Agents},\n year = {2013}\n}\n'}",,"{'pages': '815-820', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",32.0,Vocal and Facial Imitation of Humans Interacting with Virtual Agents,2013.0
253,1711b34c4c4596ce800da57e1c06dc59eef790ea,"In Brazil, highway transportation is responsible for 58% of cargo transport. A relevant problem associated to cargo transport are the accidents. Incompatible speed and fatigue were pointed out as main causes of accidents. The adoption of an advanced driver assistance system (ADAS) for anticipating warning of overspeed for a curve may reduce rollover risks. In other words, it would mitigate the problem by helping the driver to maintain the vehicle in a safe speed, through customized alerts just in time that allow the driver to take corrective maneuvers in case of unsafe state. In this paper, we present an agent-based approach to address this problem.","[{'authorId': '1996733', 'name': 'Willy Tiengo'}, {'authorId': '1723140', 'name': 'E. Costa'}, {'authorId': '3184282', 'name': 'J. Fechine'}]",5.0,"{'bibtex': '@Article{Tiengo2016ReducingRO,\n author = {Willy Tiengo and E. Costa and J. Fechine},\n journal = {2016 IEEE International Conference on Computer and Information Technology (CIT)},\n pages = {65-72},\n title = {Reducing Risk of Rollover in Curve for Heavy-Duty Vehicles with an Agent-Based Advanced Driver Assistance System},\n year = {2016}\n}\n'}",,"{'pages': '65-72', 'name': '2016 IEEE International Conference on Computer and Information Technology (CIT)'}",17.0,Reducing Risk of Rollover in Curve for Heavy-Duty Vehicles with an Agent-Based Advanced Driver Assistance System,2016.0
254,1762ba0802e0540cb55c335850119836c5cdc1ff,,"[{'authorId': '2185181', 'name': 'Lori Malatesta'}, {'authorId': '3346592', 'name': 'A. Raouzaiou'}, {'authorId': '1715144', 'name': 'K. Karpouzis'}, {'authorId': '1707243', 'name': 'S. Kollias'}]",36.0,"{'bibtex': '@Article{Malatesta2009TowardsME,\n author = {Lori Malatesta and A. Raouzaiou and K. Karpouzis and S. Kollias},\n journal = {Applied Intelligence},\n pages = {58-64},\n title = {Towards modeling embodied conversational agent character profiles using appraisal theory predictions in expression synthesis},\n volume = {30},\n year = {2009}\n}\n'}",,"{'volume': '30', 'pages': '58-64', 'name': 'Applied Intelligence'}",21.0,Towards modeling embodied conversational agent character profiles using appraisal theory predictions in expression synthesis,2009.0
255,1773d10515f6c7010fe6f1e5cfaa2e9dcfefc724,"In this paper, we argue for embodied corrversational charactersas the logical extension of the metaphor of human - computerinteraction as a conversation. We argue that the only way to fullymodel the richness of human I&+ to-face communication is torely on conversational analysis that describes sets ofconversational behaviors as fi~lfilling conversational functions,both interactional and propositional. We demonstrate how toimplement this approach in Rea, an embodied conversational agentthat is capable of both multimodal input understanding and outputgeneration in a limited application domain. Rea supports bothsocial and task-oriented dialogue. We discuss issues that need tobe addressed in creating embodied conversational agents, anddescribe the architecture of the Rea interface.","[{'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '1684805', 'name': 'M. Billinghurst'}, {'authorId': '2053167846', 'name': 'L. Campbell'}, {'authorId': '2107804338', 'name': 'K. Chang'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}, {'authorId': '2116562196', 'name': 'Hao Yan'}]",585.0,"{'bibtex': '@Inproceedings{Cassell1999EmbodimentIC,\n author = {Justine Cassell and T. Bickmore and M. Billinghurst and L. Campbell and K. Chang and H. Vilhjálmsson and Hao Yan},\n pages = {520-527},\n title = {Embodiment in conversational interfaces: Rea},\n year = {1999}\n}\n'}",,{'pages': '520-527'},36.0,Embodiment in conversational interfaces: Rea,1999.0
256,1781ecf58d2dc897ae8f5bbeca7c547bd6a09658,"Virtual reality is a rapidly growing technology which utilises the ever-increasing power of computers to simulate real-world and imaginary environments and situations with a high degree of realism and interactiveness. Safety in the South African mining industry is a vital issue. On average, one worker dies every working day, and about 16 are injured in mine-related accidents. Inadequate or insufficient training is often cited as a root cause for many mining fatalities. However, training outside the direct working environment provides only limited real-life opportunities and may fail to make a significant impact within the tense working environment itself. Virtual reality-based training tools can, by contrast, provide simulated exposure to real-world working conditions without the associated risks. This paper discusses contextual requirements and constraints for virtual reality application development, applied to safety training in mines. The results of the contextual analysis were applied to the design and development of several prototypes of VR training systems. The paper also reports on how realism can be enhanced in simulation training systems.","[{'authorId': '2813669', 'name': 'E. V. Wyk'}, {'authorId': '2567639', 'name': 'R. D. Villiers'}]",140.0,"{'bibtex': '@Inproceedings{Wyk2009VirtualRT,\n author = {E. V. Wyk and R. D. Villiers},\n pages = {53-63},\n title = {Virtual reality training applications for the mining industry},\n year = {2009}\n}\n'}",,{'pages': '53-63'},24.0,Virtual reality training applications for the mining industry,2009.0
259,17ab4f6a0cd5c275ca0aab4a8d3ab60704ec3b6a,"Research on gaze and eye contact was organized within the framework of Patterson's (1982) sequential functional model of nonverbal exchange. Studies were reviewed showing how gaze functions to (a) provide information, (b) regulate interaction, (c) express intimacy, (d) exercise social control, and (","[{'authorId': '4215358', 'name': 'C. Kleinke'}]",1383.0,"{'bibtex': '@Article{Kleinke1986GazeAE,\n author = {C. Kleinke},\n journal = {Psychological bulletin},\n pages = {\n          78-100\n        },\n title = {Gaze and eye contact: a research review.},\n volume = {100 1},\n year = {1986}\n}\n'}",,"{'volume': '100 1', 'pages': '\n          78-100\n        ', 'name': 'Psychological bulletin'}",326.0,Gaze and eye contact: a research review.,1986.0
260,17b8edb23479e3f0f621b3f0ac72d4483da65670,"This paper proposes a new method for designing dynamic wrinkles that appear and disappear according to the underlying deformation of tissues. The user positions and orients wrinkling tools on a mesh. During animation, geometric wrinkles are generated in real-time in the regions covered by the tools, mimicking resistance to compression of tissues. The wrinkling feature can be added to any existing animation. When the local resolution of the mesh is not sufficient, our tool refines it according to the wrinkle's finest feature. As our results show, the technique can be applied to a variety of situations such as facial expression wrinkles, joint wrinkles or garment wrinkles","[{'authorId': '1967184', 'name': 'Caroline Larboulette'}, {'authorId': '1710314', 'name': 'Marie-Paule Cani'}]",75.0,"{'bibtex': '@Article{Larboulette2004RealtimeDW,\n author = {Caroline Larboulette and Marie-Paule Cani},\n journal = {Proceedings Computer Graphics International, 2004.},\n pages = {522-525},\n title = {Real-time dynamic wrinkles},\n year = {2004}\n}\n'}",,"{'pages': '522-525', 'name': 'Proceedings Computer Graphics International, 2004.'}",18.0,Real-time dynamic wrinkles,2004.0
261,17b8f0353e470db7ccf590f40f1419d2b5e427c2,"Robots have been introduced into our society, but their social role is still unclear. A critical issue is whether the robot’s exhibition of intelligent behaviour leads to the users’ perception of the robot as being a social actor, similar to the way in which people treat computers and media as social actors. The first experiment mimicked Stanley Milgram’s obedience experiment, but on a robot. The participants were asked to administer electric shocks to a robot, and the results show that people have fewer concerns about abusing robots than about abusing other people. We refined the methodology for the second experiment by intensifying the social dilemma of the users. The participants were asked to kill the robot. In this experiment, the intelligence of the robot and the gender of the participants were the independent variables, and the users’ destructive behaviour towards the robot the dependent variable. Several practical and methodological problems compromised the acquired data, but we can conclude that the robot’s intelligence had a significant influence on the users’ destructive behaviour. We discuss the encountered problems and suggest improvements. We also speculate on whether the users’ perception of the robot as being “sort of alive” may have influenced the participants’ abusive behaviour.","[{'authorId': '1728894', 'name': 'C. Bartneck'}, {'authorId': '40705707', 'name': 'Jun Hu'}]",82.0,"{'bibtex': '@Article{Bartneck2008ExploringTA,\n author = {C. Bartneck and Jun Hu},\n journal = {Interaction Studies},\n pages = {415-433},\n title = {Exploring the abuse of robots},\n volume = {9},\n year = {2008}\n}\n'}",,"{'volume': '9', 'pages': '415-433', 'name': 'Interaction Studies'}",25.0,Exploring the abuse of robots,2008.0
262,17b9a770ad1a5def6e0f85da01a4bd4f3cf790e6,,"[{'authorId': '15102546', 'name': 'N. Eisenberg'}, {'authorId': '2041146', 'name': 'A. Morris'}]",90.0,"{'bibtex': '@Article{Eisenberg2001TheOA,\n author = {N. Eisenberg and A. Morris},\n journal = {Social Justice Research},\n pages = {95-120},\n title = {The Origins and Social Significance of Empathy-Related Responding. A Review of Empathy and Moral Development: Implications for Caring and Justice by M. L. Hoffman},\n volume = {14},\n year = {2001}\n}\n'}",,"{'volume': '14', 'pages': '95-120', 'name': 'Social Justice Research'}",100.0,The Origins and Social Significance of Empathy-Related Responding. A Review of Empathy and Moral Development: Implications for Caring and Justice by M. L. Hoffman,2001.0
263,17c2bc52b88a1ffd4953ca7b671c26c23be5df47,We present a data-driven algorithm to model and predict the socio-emotional impact of groups on observers. Psychological research finds that highly entitative i.e. cohesive and uniform groups induce threat and unease in observers. Our algorithm models realistic trajectory-level behaviors to classify and map the motion-based entitativity of crowds. This mapping is based on a statistical scheme that dynamically learns pedestrian behavior and computes the resultant entitativity induced emotion through group motion characteristics. We also present a novel interactive multi-agent simulation algorithm to model entitative groups and conduct a VR user study to validate the socio-emotional predictive power of our algorithm. We further show that model-generated high-entitativity groups do induce more negative emotions than low-entitative groups.,"[{'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '40894651', 'name': 'Emily Kubin'}, {'authorId': '51408216', 'name': 'Husam Shaik'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",8.0,"{'bibtex': '@Article{Bera2018DatadrivenMO,\n author = {Aniket Bera and Tanmay Randhavane and Emily Kubin and Husam Shaik and Kurt Gray and Dinesh Manocha},\n journal = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},\n title = {Data-driven modeling of group entitativity in virtual environments},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology'},48.0,Data-driven modeling of group entitativity in virtual environments,2018.0
264,17cd97086715c5bb266939108431e114f48b4b20,"Emotions seem to come and go as they please. However, we actually hold considerable sway over our emotions: We influence which emotions we have and how we experience and express these emotions. The process model of emotion regulation described here suggests that how we regulate our emotions matters. Regulatory strategies that act early in the emotion-generative process should have quite different outcomes than strategies that act later. This review focuses on two widely used strategies for down-regulating emotion. The first, reappraisal, comes early in the emotion-generative process. It consists of changing how we think about a situation in order to decrease its emotional impact. The second, suppression, comes later in the emotion-generative process. It involves inhibiting the outward signs of emotion. Theory and research suggest that reappraisal is more effective than suppression. Reappraisal decreases the experience and behavioral expression of emotion, and has no impact on memory. By contrast, suppression decreases behavioral expression, but fails to decrease the experience of emotion, and actually impairs memory. Suppression also increases physiological responding in both the suppressors and their social partners.","[{'authorId': '112991450', 'name': 'A. Mellers'}]",52.0,"{'bibtex': '@Inproceedings{Mellers2001EmotionRI,\n author = {A. Mellers},\n title = {Emotion Regulation in Adulthood: Timing Is Everything},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",13.0,Emotion Regulation in Adulthood: Timing Is Everything,2001.0
265,17d429b405a0951ebf3c729570a476977e412000,,"[{'authorId': '48079792', 'name': 'T. Bauer'}, {'authorId': '1388561389', 'name': 'Emre Devrim'}, {'authorId': '1388561328', 'name': 'Misha Glazunov'}, {'authorId': '1388561303', 'name': 'William Lopez Jaramillo'}, {'authorId': '152667007', 'name': 'Balaganesh Mohan'}, {'authorId': '3266578', 'name': 'Gerasimos Spanakis'}]",16.0,"{'bibtex': '@Inproceedings{Bauer2019MeTooMaastrichtBA,\n author = {T. Bauer and Emre Devrim and Misha Glazunov and William Lopez Jaramillo and Balaganesh Mohan and Gerasimos Spanakis},\n pages = {503-521},\n title = {#MeTooMaastricht: Building a chatbot to assist survivors of sexual harassment},\n year = {2019}\n}\n'}",,{'pages': '503-521'},25.0,#MeTooMaastricht: Building a chatbot to assist survivors of sexual harassment,2019.0
266,17e5f11f32f9de8263ea3fed72f677de0bd9ebed,To facilitate a multidimensional approach to empathy the Interpersonal Reactivity Index (IRI) includes 4 subscales: Perspective-Taking (PT) Fantasy (FS) Empathic Concern (EC) and Personal Distress (PD). The aim of the present study was to establish the convergent and discriminant validity of these 4 subscales. Hypothesized relationships among the IRI subscales between the subscales and measures of other psychological constructs (social functioning self-esteem emotionality and sensitivity to others) and between the subscales and extant empathy measures were examined. Study subjects included 677 male and 667 female students enrolled in undergraduate psychology classes at the University of Texas. The IRI scales not only exhibited the predicted relationships among themselves but also were related in the expected manner to other measures. Higher PT scores were consistently associated with better social functioning and higher self-esteem; in contrast Fantasy scores were unrelated to these 2 characteristics. High EC scores were positively associated with shyness and anxiety but negatively linked to egotism. The most substantial relationships in the study involved the PD scale. PD scores were strongly linked with low self-esteem and poor interpersonal functioning as well as a constellation of vulnerability uncertainty and fearfulness. These findings support a multidimensional approach to empathy by providing evidence that the 4 qualities tapped by the IRI are indeed separate constructs each related in specific ways to other psychological measures.,"[{'authorId': '47994338', 'name': 'Mark H. Davis'}]",8516.0,"{'bibtex': '@Article{Davis1983MeasuringID,\n author = {Mark H. Davis},\n journal = {Journal of Personality and Social Psychology},\n pages = {113-126},\n title = {Measuring individual differences in empathy: Evidence for a multidimensional approach.},\n volume = {44},\n year = {1983}\n}\n'}",,"{'volume': '44', 'pages': '113-126', 'name': 'Journal of Personality and Social Psychology'}",26.0,Measuring individual differences in empathy: Evidence for a multidimensional approach.,1983.0
267,17e8741ef94522e9a67dc07fcca887ff93ddf249,"An eye-tracking study of face and object recognition was conducted to clarify the character of face gaze in autistic spectrum disorders. Experimental participants were a group of individuals diagnosed with Asperger's disorder or high-functioning autistic disorder according to their medical records and confirmed by the Autism Diagnostic Interview-Revised (ADI-R). Controls were selected on the basis of age, gender, and educational level to be comparable to the experimental group. In order to maintain attentional focus, stereoscopic images were presented in a virtual reality (VR) headset in which the eye-tracking system was installed. Preliminary analyses show impairment in face recognition, in contrast with equivalent and even superior performance in object recognition among participants with autism-related diagnoses, relative to controls. Experimental participants displayed less fixation on the central face than did control-group participants. The findings, within the limitations of the small number of subjects and technical difficulties encountered in utilizing the helmet-mounted display, suggest an impairment in face processing on the part of the individuals in the experimental group. This is consistent with the hypothesis of disruption in the first months of life, a period that may be critical to typical social and cognitive development, and has important implications for selection of appropriate targets of intervention.","[{'authorId': '2280143', 'name': 'C. Trepagnier'}, {'authorId': '1718208', 'name': 'M. Sebrechts'}, {'authorId': '2061402704', 'name': 'R. Peterson'}]",108.0,"{'bibtex': '@Article{Trepagnier2002AtypicalFG,\n author = {C. Trepagnier and M. Sebrechts and R. Peterson},\n journal = {Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society},\n pages = {\n          213-7\n        },\n title = {Atypical Face Gaze in Autism},\n volume = {5 3},\n year = {2002}\n}\n'}",,"{'volume': '5 3', 'pages': '\n          213-7\n        ', 'name': 'Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society'}",20.0,Atypical Face Gaze in Autism,2002.0
268,17eb496543b004b9c4215ab51380aae2640d5695,,"[{'authorId': '2928409', 'name': 'V. Yngve'}]",940.0,"{'bibtex': '@Inproceedings{Yngve1970OnGA,\n author = {V. Yngve},\n pages = {567-578},\n title = {On getting a word in edgewise},\n year = {1970}\n}\n'}",,"{'volume': '', 'pages': '567-578', 'name': ''}",0.0,On getting a word in edgewise,1970.0
269,17f2e1534727f84ab6c053d709a0e0b097b18fa8,"Major Depressive Disorder is one of the most serious mental health concerns haunting human kind. While there have been enough global efforts to reduce clinical depression, it is still not sufficient considering the grim statistics in terms of mental health. With the lopsided distribution of Mental Health Professionals (MHPs) to patients, initiatives should be taken to develop automated systems to assist MHPs in combating this grievous mental illness. In this paper, we propose a Virtual Assistant (VA) acting as a first point of contact for depressed or discouraged users. The VA should be capable of generating motivational and affirmative responses, provide users with a safe environment to share thoughts and seek help and advice anonymously. For addressing these multiple aspects, we curate a large-scale dataset, MotiVAte, consisting of dyadic conversations between the depressed user and the VA (imparting hope and motivation). The proposed dataset is evaluated on state of the art generative models. Empirical results both automatic and human evaluation based are reported for the same. To the best of our knowledge, this is the first application of Natural Language Generation to mental health.","[{'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '121415924', 'name': 'Saraansh Chopra'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}, {'authorId': '2139310198', 'name': 'Pankaj Kumar'}]",9.0,"{'bibtex': '@Article{Saha2021ALD,\n author = {Tulika Saha and Saraansh Chopra and S. Saha and P. Bhattacharyya and Pankaj Kumar},\n journal = {2021 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {A Large-Scale Dataset for Motivational Dialogue System: An Application of Natural Language Generation to Mental Health},\n year = {2021}\n}\n'}",,"{'pages': '1-8', 'name': '2021 International Joint Conference on Neural Networks (IJCNN)'}",0.0,A Large-Scale Dataset for Motivational Dialogue System: An Application of Natural Language Generation to Mental Health,2021.0
270,17f5c7411eeeeedf25b0db99a9130aa353aee4ba,"
 
 We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.
 
","[{'authorId': '35224828', 'name': 'Iulian Serban'}, {'authorId': '2041695', 'name': 'Alessandro Sordoni'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '1760871', 'name': 'Aaron C. Courville'}, {'authorId': '145134886', 'name': 'Joelle Pineau'}]",1655.0,"{'bibtex': '@Inproceedings{Serban2015BuildingED,\n author = {Iulian Serban and Alessandro Sordoni and Yoshua Bengio and Aaron C. Courville and Joelle Pineau},\n pages = {3776-3784},\n title = {Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models},\n year = {2015}\n}\n'}",,{'pages': '3776-3784'},54.0,Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models,2015.0
271,1813310dfc92dcadc8bf2fadfb7408a8900169ec,"Empirical studies have frequently linked negative attentional biases with attentional dysfunction and negative moods; however, far less research has focused on how attentional deployment can be an adaptive strategy that regulates emotional experience. The authors argue that attention may be an invaluable tool for promoting emotion regulation. Accordingly, they present evidence that selective attention to positive information reflects emotion regulation and that regulating attention is a critical component of the emotion regulatory process. Furthermore, attentional regulation can be successfully trained through repeated practice. The authors ultimately propose a model of attention training methodologies integrating attention-dependent emotion regulation strategies with attention networks. Although additional interdisciplinary research is needed to bolster these nascent findings, meditative practices appear to be among the most effective training methodologies in enhancing emotional well-being. Further exploration of the positive and therapeutic qualities of attention warrants the empirical attention of social and personality psychologists.","[{'authorId': '4797931', 'name': 'H. A. Wadlinger'}, {'authorId': '1919851', 'name': 'D. Isaacowitz'}]",329.0,"{'bibtex': '@Article{Wadlinger2011FixingOF,\n author = {H. A. Wadlinger and D. Isaacowitz},\n journal = {Personality and Social Psychology Review},\n pages = {102 - 75},\n title = {Fixing Our Focus: Training Attention to Regulate Emotion},\n volume = {15},\n year = {2011}\n}\n'}",,"{'volume': '15', 'pages': '102 - 75', 'name': 'Personality and Social Psychology Review'}",220.0,Fixing Our Focus: Training Attention to Regulate Emotion,2011.0
272,1879ded191b91edbd35e40aafa7f35867d6e6dcd,"MACH--My Automated Conversation coacH--is a novel system that provides ubiquitous access to social skills training. The system includes a virtual agent that reads facial expressions, speech, and prosody and responds with verbal and nonverbal behaviors in real time. This paper presents an application of MACH in the context of training for job interviews. During the training, MACH asks interview questions, automatically mimics certain behavior issued by the user, and exhibit appropriate nonverbal behaviors. Following the interaction, MACH provides visual feedback on the user's performance. The development of this application draws on data from 28 interview sessions, involving employment-seeking students and career counselors. The effectiveness of MACH was assessed through a weeklong trial with 90 MIT undergraduates. Students who interacted with MACH were rated by human experts to have improved in overall interview performance, while the ratings of students in control groups did not improve. Post-experiment interviews indicate that participants found the interview experience informative about their behaviors and expressed interest in using MACH in the future.","[{'authorId': '144619896', 'name': 'Ehsan Hoque'}, {'authorId': '3237926', 'name': 'M. Courgeon'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",287.0,"{'bibtex': '@Article{Hoque2013MACHMA,\n author = {Ehsan Hoque and M. Courgeon and Jean-Claude Martin and Bilge Mutlu and Rosalind W. Picard},\n journal = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},\n title = {MACH: my automated conversation coach},\n year = {2013}\n}\n'}",,{'name': 'Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing'},37.0,MACH: my automated conversation coach,2013.0
273,187f5185c33e3ef34708f07c729010198fef8b58,"Several neurological patient populations, including traumatic brain injury (TBI), appear to produce an abnormally 'utilitarian' pattern of judgements to moral dilemmas; they tend to make judgements that maximize the welfare of the majority, rather than deontological judgements based on the following of moral rules (e.g., do not harm others). However, this patient research has always used extreme dilemmas with highly valued moral rules (e.g., do not kill). Data from healthy participants, however, suggest that when a wider range of dilemmas are employed, involving less valued moral rules (e.g., do not lie), moral judgements demonstrate sensitivity to the psychological intuitiveness of the judgements, rather than their deontological or utilitarian content (Kahane et al., Social Cognitive and Affective Neuroscience, 7, 2011, 393). We sought the moral judgements of 30 TBI participants and 30 controls on moral dilemmas where content (utilitarian/deontological) and intuition (intuitive/counter-intuitive) were measured concurrently. Overall TBI participants made utilitarian judgements in equal proportions to controls; disproportionately favouring utilitarian judgements only when they were counter-intuitive, and deontological judgements only when they were counter-intuitive. These results speak against the view that TBI causes a specific utilitarian bias, suggesting instead that moral intuition is broadly disrupted following TBI.","[{'authorId': '34640956', 'name': 'D. Rowley'}, {'authorId': '49798521', 'name': 'M. Rogish'}, {'authorId': '2113224907', 'name': 'Timothy Alexander'}, {'authorId': '1905575', 'name': 'Kevin J. Riggs'}]",13.0,"{'bibtex': '@Article{Rowley2018CounterintuitiveMJ,\n author = {D. Rowley and M. Rogish and Timothy Alexander and Kevin J. Riggs},\n journal = {Journal of Neuropsychology},\n pages = {200–215},\n title = {Counter‐intuitive moral judgement following traumatic brain injury},\n volume = {12},\n year = {2018}\n}\n'}",,"{'volume': '12', 'pages': '200–215', 'name': 'Journal of Neuropsychology'}",53.0,Counter‐intuitive moral judgement following traumatic brain injury,2018.0
274,1895ec065c80479c228ca0f0dbfd8c43d874e72f,"Anthropomorphic agents used in online-shopping need to be trusted by users so that users feel comfortable buying products. In this paper, we propose a model for designing trustworthy agents by assuming two factors of trust, that is, emotion and knowledgeableness perceived. Our hypothesis is that when a user feels happy and perceives an agent as being highly knowledgeable, a high level of trust results between the user and agent. We conducted four experiments with participants to verify this hypothesis by preparing transition operators utilizing emotional contagion and knowledgeable utterances. As a result, we verified that users' internal states transitioned as expected and that the two factors significantly influenced their trust states.","[{'authorId': '49201495', 'name': 'T. Matsui'}, {'authorId': '1679243', 'name': 'S. Yamada'}]",15.0,"{'bibtex': '@Article{Matsui2019DesigningTP,\n author = {T. Matsui and S. Yamada},\n booktitle = {Frontiers in Psychology},\n journal = {Frontiers in Psychology},\n title = {Designing Trustworthy Product Recommendation Virtual Agents Operating Positive Emotion and Having Copious Amount of Knowledge},\n volume = {10},\n year = {2019}\n}\n'}","[{'paperId': '6349de639bd632ed0ac1f0b1a077ab4f9e06f23e', 'title': '‘I do not know’: an examination of reactions to virtual agents that fail to answer the user’s questions'}, {'paperId': '9ff9d5903b9d738c67e5964567817438ffee8536', 'title': 'A design of trip recommendation robot agents with opinions'}, {'paperId': 'b3d94f9d1f4d74e2791468fd5de0d3fc5c636d0a', 'title': 'What Makes An Apology More Effective? Exploring Anthropomorphism, Individual Differences, And Emotion In Human-Automation Trust Repair'}, {'paperId': '6461a763e47ca79fc2498b7a61782c9062449d64', 'title': 'Investigating the Impact of Emotional Contagion on Customer Attitude, Trust and Brand Engagement: A Social Commerce Perspective'}, {'paperId': '5bafa5c6796fb5c327d9d844b389a17f55ea21a2', 'title': 'Estimating Emotion Contagion on Social Media via Localized Diffusion in Dynamic Graphs'}, {'paperId': 'ccf4ec3fac18c1fd904d1be1612964e30f16b6ed', 'title': ""Moe-Phobia: Effect of Users' Gender on Perceived Sexuality and Likability Toward Manga-Like Virtual Agents""}, {'paperId': 'b36868eda6196c6561a61d461e6f74d43a8a906c', 'title': 'How anthropomorphism affects trust in intelligent personal assistants'}, {'paperId': '4a2d38e9605cb269f088fdb91f3bb910700493e6', 'title': 'Emotional Contagion: A Brief Overview and Future Directions'}, {'paperId': 'bd4d6b5c2e3c5346c62b7dc7a1ecf5b0ccdf59f8', 'title': 'Who Is to Blame? The Appearance of Virtual Agents and the Attribution of Perceived Responsibility'}, {'paperId': '2c8c2c6a55235f248ac6d63a33b4b02f419e4f78', 'title': 'The Impact of Trust on the Willingness of Co-Tenancy Behavior: Evidence from China'}, {'paperId': 'ef0c62ff070a476f216fe478cc190c773f12a1f6', 'title': 'Human Trust in Artificial Intelligence: Review of Empirical Research'}, {'paperId': 'b6a265f8c8c84e8d8e3c5f3d93c79fee14bb1d23', 'title': 'Can We Recognize Atmosphere as an Agent? - Pilot Study'}, {'paperId': '401265af33a1e104ead4788f78af7e8bf0a22c74', 'title': 'A Discussion of Trust Abuse and Lack of Trust in Human-Computer Trust'}, {'paperId': '66cf04406c5587b6bdd3a7a820c05662aaae25a9', 'title': 'The Transition From Intelligent to Affective Tutoring System: A Review and Open Issues'}, {'paperId': None, 'title': 'PRBM_A_301393 365..383'}]","{'name': 'Frontiers in Psychology', 'volume': '10'}",40.0,Designing Trustworthy Product Recommendation Virtual Agents Operating Positive Emotion and Having Copious Amount of Knowledge,2019.0
275,18a71fcaa7c59ea9b166605558d9a963debdce21,,"[{'authorId': '30169286', 'name': 'Thurid Vogt'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '1790555', 'name': 'Nikolaus Bee'}]",185.0,"{'bibtex': '@Inproceedings{Vogt2008EmoVoiceA,\n author = {Thurid Vogt and E. André and Nikolaus Bee},\n pages = {188-199},\n title = {EmoVoice - A Framework for Online Recognition of Emotions from Voice},\n year = {2008}\n}\n'}",,{'pages': '188-199'},34.0,EmoVoice - A Framework for Online Recognition of Emotions from Voice,2008.0
276,18ab703c9959fbea7ad253a4062eb705b245552c,"We present a trajectory extraction and behavior-learning algorithm for data-driven crowd simulation. Our formulation is based on incrementally learning pedestrian motion models and behaviors from crowd videos. We combine this learned crowd-simulation model with an online tracker based on particle filtering to compute accurate, smooth pedestrian trajectories. We refine this motion model using an optimization technique to estimate the agents' simulation parameters. We highlight the benefits of our approach for improved data-driven crowd simulation, including crowd replication from videos and merging the behavior of pedestrians from multiple videos. We highlight our algorithm's performance in various test scenarios containing tens of human-like agents.","[{'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",43.0,"{'bibtex': '@Inproceedings{Bera2015EfficientTE,\n author = {Aniket Bera and Sujeong Kim and Dinesh Manocha},\n pages = {65-72},\n title = {Efficient trajectory extraction and parameter learning for data-driven crowd simulation},\n year = {2015}\n}\n'}",,{'pages': '65-72'},35.0,Efficient trajectory extraction and parameter learning for data-driven crowd simulation,2015.0
277,18ad13cd015cf6bf32e2aaea476eb030adfb1093,"The design of serious games requires developers to tackle pedagogical challenges calling for advanced solutions that the entertainment industry might deem too risky to pursue. One such challenge is the creation of autonomous socially intelligent characters with whom players can practice different social skills. Although there are several architectures in the field of virtual agents that are designed specifically to enable more human-like interactions, they are still not widely adopted by game studios that develop serious games, in particular for learning. In this paper, we present a virtual agent toolkit that was specifically developed with the intent of making agent-based solutions more accessible and reliable to game developers. To this end, a collaborative effort was established with a game studio that has used the toolkit to develop two different serious games. Among other advantages, the toolkit facilitated the inclusion of a dynamic model of emotions that affects not just how the character looks and acts but also how the player’s performance is determined.","[{'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '28004507', 'name': 'Manuel Guimarães'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145255182', 'name': 'P. A. Santos'}, {'authorId': '2065199', 'name': 'Kam Star'}, {'authorId': '80549358', 'name': 'Ben Hirsh'}, {'authorId': '81303160', 'name': 'Ellis Spice'}, {'authorId': '1854340', 'name': 'Rob Kommeren'}]",17.0,"{'bibtex': '@Article{Mascarenhas2018AVA,\n author = {S. Mascarenhas and Manuel Guimarães and R. Prada and João Dias and P. A. Santos and Kam Star and Ben Hirsh and Ellis Spice and Rob Kommeren},\n journal = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {1-7},\n title = {A Virtual Agent Toolkit for Serious Games Developers},\n year = {2018}\n}\n'}",,"{'pages': '1-7', 'name': '2018 IEEE Conference on Computational Intelligence and Games (CIG)'}",22.0,A Virtual Agent Toolkit for Serious Games Developers,2018.0
278,18b07146d8d6f631d17fb90c50f8affdf3a527f6,"The influence of culture on the social effects of emotions in negotiations has recently gained the attention of researchers, but to date this research has focused exclusively on the cultural background of the perceiver of the emotion expression. The current research offers the first investigation of how the cultural background of the expresser influences negotiation outcomes. On the basis of the stereotype that East Asians are emotionally inexpressive and European Americans are emotionally expressive, we predicted that anger will have a stronger signaling value when East Asians rather than European American negotiators express it. Specifically, we predicted that angry East Asian negotiators will be perceived as tougher and more threatening and therefore elicit great cooperation from counterparts compared with angry European American negotiators. Results from 4 negotiation studies supported our predictions. In Study 1, angry East Asian negotiators elicited greater cooperation than angry European American and Hispanic negotiators. In Study 2, angry East Asian negotiators elicited greater cooperation than angry European American ones, but emotionally neutral East Asian and European American negotiators elicited the same level of cooperation. Study 3 showed that this effect holds for both East Asian and European American perceivers and that it is mediated by angry East Asian negotiators being perceived as tougher and more threatening than angry European American negotiators. Finally, Study 4 demonstrated that the effect emerges only when negotiators hold the stereotype of East Asians being emotionally inexpressive and European Americans being emotionally expressive. We discuss implications for our understanding of culture, emotions, and negotiations.","[{'authorId': '3826921', 'name': 'Hajo Adam'}, {'authorId': '5732160', 'name': 'Aiwa Shirako'}]",61.0,"{'bibtex': ""@Article{Adam2013NotAA,\n author = {Hajo Adam and Aiwa Shirako},\n journal = {The Journal of applied psychology},\n pages = {\n          785-98\n        },\n title = {Not all anger is created equal: the impact of the expresser's culture on the social effects of anger in negotiations.},\n volume = {98 5},\n year = {2013}\n}\n""}",,"{'volume': '98 5', 'pages': '\n          785-98\n        ', 'name': 'The Journal of applied psychology'}",67.0,Not all anger is created equal: the impact of the expresser's culture on the social effects of anger in negotiations.,2013.0
279,18be87938536db985d9528c6090b39dd951d6b1e,,"[{'authorId': '2244212899', 'name': 'W. Lewis Johnson'}, {'authorId': '2244281299', 'name': 'James C. Lester'}]",151.0,"{'bibtex': '@Article{Johnson2016FacetoFaceIW,\n author = {W. Lewis Johnson and James C. Lester},\n journal = {International Journal of Artificial Intelligence in Education},\n pages = {25-36},\n title = {Face-to-Face Interaction with Pedagogical Agents, Twenty Years Later},\n volume = {26},\n year = {2016}\n}\n'}",,"{'volume': '26', 'pages': '25-36', 'name': 'International Journal of Artificial Intelligence in Education'}",42.0,"Face-to-Face Interaction with Pedagogical Agents, Twenty Years Later",2016.0
280,18cdd07ab28306da04c1b7b27c2c1a1b928b4c1d,,"[{'authorId': '6206591', 'name': 'R. McCrae'}]",92.0,"{'bibtex': '@Inproceedings{McCrae2009TheCH,\n author = {R. McCrae},\n title = {The Cambridge Handbook of Personality Psychology: The Five-Factor Model of personality traits: consensus and controversy},\n year = {2009}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Cambridge Handbook of Personality Psychology: The Five-Factor Model of personality traits: consensus and controversy,2009.0
281,18ceae10890ce0c01143fd25a0ee176a148efa23,"In this paper, we describe a robot that interacts with humans in a crowded conference environment. The robot detects faces, determines the shirt color of onlooking conference attendants, and reacts with a combination of speech, musical, and movement responses. It continuously updates an internal emotional state, modeled realistically after human psychology research. Using empirically-determined mapping functions, the robot's state in the emotion space is translated to a particular set of sound and movement responses. We successfully demonstrate this system at the AAAI '05 Open Interaction Event, showing the potential for emotional modeling to improve human-robot interaction","[{'authorId': '2585010', 'name': 'Geoffrey A. Hollinger'}, {'authorId': '2071900024', 'name': 'Yavor Georgiev'}, {'authorId': '2391049', 'name': 'A. Manfredi'}, {'authorId': '2653870', 'name': 'B. Maxwell'}, {'authorId': '2327074', 'name': 'Zachary A. Pezzementi'}, {'authorId': '1595215522', 'name': 'B. Mitchell'}]",38.0,"{'bibtex': '@Article{Hollinger2006DesignOA,\n author = {Geoffrey A. Hollinger and Yavor Georgiev and A. Manfredi and B. Maxwell and Zachary A. Pezzementi and B. Mitchell},\n journal = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n pages = {3093-3098},\n title = {Design of a Social Mobile Robot Using Emotion-Based Decision Mechanisms},\n year = {2006}\n}\n'}",,"{'pages': '3093-3098', 'name': '2006 IEEE/RSJ International Conference on Intelligent Robots and Systems'}",10.0,Design of a Social Mobile Robot Using Emotion-Based Decision Mechanisms,2006.0
282,18d079a6d72e3f0b0c9214f597b6b178265b05ee,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","[{'authorId': '1947267', 'name': 'Michel Galley'}, {'authorId': '145590324', 'name': 'K. McKeown'}, {'authorId': '144049352', 'name': 'Julia Hirschberg'}, {'authorId': '70422141', 'name': 'Elizabeth Shriberg'}]",240.0,"{'bibtex': '@Inproceedings{Galley2004IdentifyingAA,\n author = {Michel Galley and K. McKeown and Julia Hirschberg and Elizabeth Shriberg},\n pages = {669-676},\n title = {Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies},\n year = {2004}\n}\n'}",,{'pages': '669-676'},19.0,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,2004.0
283,18e34a102238311f0b95440f0aedb340cedc8c1c,"A flurry of theoretical and empirical work concerning the production of and response to facial and vocal expressions has occurred in the past decade. That emotional expressions express emotions is a tautology but may not be a fact. Debates have centered on universality, the nature of emotion, and the link between emotions and expressions. Modern evolutionary theory is informing more models, emphasizing that expressions are directed at a receiver, that the interests of sender and receiver can conflict, that there are many determinants of sending an expression in addition to emotion, that expressions influence the receiver in a variety of ways, and that the receiver's response is more than simply decoding a message.","[{'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '145525513', 'name': 'J. Bachorowski'}, {'authorId': '1412884931', 'name': 'J. Fernández-Dols'}]",763.0,"{'bibtex': '@Article{Russell2003FacialAV,\n author = {J. Russell and J. Bachorowski and J. Fernández-Dols},\n journal = {Annual review of psychology},\n pages = {\n          329-49\n        },\n title = {Facial and vocal expressions of emotion.},\n volume = {54},\n year = {2003}\n}\n'}",,"{'volume': '54', 'pages': '\n          329-49\n        ', 'name': 'Annual review of psychology'}",154.0,Facial and vocal expressions of emotion.,2003.0
284,1905a578b8f1eb78128c5e2176a78f82355b1b96,"In this paper we propose a mathematical model for the concept of Personal Space (PS) and apply it to simulate the non-verbal communication between agents in virtual worlds. The distance between two persons reflects the type of their relationship. Human-like autonomous virtual agents should be equipped with such capability to simulate natural interactions. We define three types of relationships; (1) stranger relationship, (2) business relationship, and (3) friendly relationship. First we model the space around an agent as a probability distribution function which reflects at each point in the space the importance of that point to the agent. The agent updates dynamically this function according to(1) his relation with the other agent, (2) his face orientation, and(3) the evolution of the relationship over time as a stranger agent may become a friend. We demonstrate the concept on a multi-agent platform and show that space-aware agents exhibit better natural behavior.","[{'authorId': '2965739', 'name': 'Toshitaka Amaoka'}, {'authorId': '47028380', 'name': 'Hamid Laga'}, {'authorId': '2865018', 'name': 'M. Nakajima'}]",23.0,"{'bibtex': '@Article{Amaoka2009ModelingTP,\n author = {Toshitaka Amaoka and Hamid Laga and M. Nakajima},\n journal = {2009 International Conference on CyberWorlds},\n pages = {364-370},\n title = {Modeling the Personal Space of Virtual Agents for Behavior Simulation},\n year = {2009}\n}\n'}",,"{'pages': '364-370', 'name': '2009 International Conference on CyberWorlds'}",15.0,Modeling the Personal Space of Virtual Agents for Behavior Simulation,2009.0
286,1907983ff874e2f80fd1df8247f855163dab9241,"This paper describes the implementation and evaluation of a framework for modeling emotions in complex, decision-making agents. Sponsored by U.S. Army Research Institute (ARI), the objective of this research is to make the decision-making process of complex agents less predictable and more realistic, by incorporating emotional factors that affect humans. In tune with modern theories of emotions, we regard emotions essentially as subconscious signals and evaluations that inform, modify, and receive feedback from a variety of sources including higher cognitive processes and the sensorimotor system. Thus, our work explicitly distinguishes the subconscious processes (in a connectionist implementation) and the decision making that is subject to emotional influences (in a symbolic cognitive architecture).It is our position that ""emotional states"" are emergent patterns of interaction between decision-making knowledge and these emotional signal systems. To this end, we have adopted an approach that promotes the emergence of behavior as a result of complex interactions between factors affecting emotions, integrated in the connectionist-style model, and factors affecting decision making, represented in the symbolic model.This paper presents the implementation of emotions architecture and explains how we evaluated the system. This includes a description of the behaviors we used in our prototype, the design of our experiments, a representative set of behavior patterns that emerged as a result of exercising our model over the design space, and our project's lessons learned.","[{'authorId': '8993685', 'name': 'Amy E. Henninger'}, {'authorId': '153788834', 'name': 'Randolph M. Jones'}, {'authorId': '2756158', 'name': 'E. Chown'}]",34.0,"{'bibtex': '@Inproceedings{Henninger2003BehaviorsTE,\n author = {Amy E. Henninger and Randolph M. Jones and E. Chown},\n pages = {321-328},\n title = {Behaviors that emerge from emotion and cognition: implementation and evaluation of a symbolic-connectionist architecture},\n year = {2003}\n}\n'}",,{'pages': '321-328'},14.0,Behaviors that emerge from emotion and cognition: implementation and evaluation of a symbolic-connectionist architecture,2003.0
287,190fe73e685c5d225439832f5fb0a5ccf0d4bd04,"Emotions are expressed in the voice as well as on the face. As a first step to explore the question of their integration, we used a bimodal perception situation modelled after the McGurk paradigm, in which varying degrees of discordance can be created between the affects expressed in a face and in a tone of voice. Experiment 1 showed that subjects can effectively combine information from the two sources, in that identification of the emotion in the face is biased in the direction of the simultaneously presented tone of voice. Experiment 2 showed that this effect occurs also under instructions to base the judgement exclusively on the face. Experiment 3 showed the reverse effect, a bias from the emotion in the face on judgement of the emotion in the voice. These results strongly suggest the existence of mandatory bidirectional links between affect detection structures in vision and audition.","[{'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '2621791', 'name': 'J. Vroomen'}]",615.0,"{'bibtex': '@Article{Gelder2000ThePO,\n author = {B. de Gelder and J. Vroomen},\n journal = {Cognition and Emotion},\n pages = {289 - 311},\n title = {The perception of emotions by ear and by eye},\n volume = {14},\n year = {2000}\n}\n'}",,"{'volume': '14', 'pages': '289 - 311', 'name': 'Cognition and Emotion'}",59.0,The perception of emotions by ear and by eye,2000.0
288,19137d74a765614fd4057c4473d1b9dbb3707756,"Human mimicry is a behavioural cue occurring during social interaction that can inform us about the participants' inter-personal states and attitudes. It occurs when a participant in an interaction exhibits some behaviour as a result of a co-participants prior display of that signal, and occurs on both short and long time-scales. To develop a detection method for such behaviour, we use a method based on feature prediction, where we train an ensemble of regression models from one subject's features to the co-subject's features, for each class. The ensemble of models with lowest reconstruction error is used to detect mimicry and non-mimicry, using continuous audiovisual streams. As mimicry events are dynamical phenomena, we use a temporal regression model (long short-term memory neural networks) to capture sequential dependencies in the data. On a data set of ten 12-minute dyadic interaction episodes, our method gave average positive and negative recall rates of 77.5% and 60.0% respectively, on data with significant class imbalances, due to the relative sparsity of mimicry samples when doing continuous detection.","[{'authorId': '2831374', 'name': 'Sanjay Bilakhia'}, {'authorId': '2403354', 'name': 'Stavros Petridis'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",32.0,"{'bibtex': '@Article{Bilakhia2013AudiovisualDO,\n author = {Sanjay Bilakhia and Stavros Petridis and M. Pantic},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {123-128},\n title = {Audiovisual Detection of Behavioural Mimicry},\n year = {2013}\n}\n'}",,"{'pages': '123-128', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",28.0,Audiovisual Detection of Behavioural Mimicry,2013.0
289,192340d54b6c38f4fd18ba72051f53478e205779,,"[{'authorId': '144097660', 'name': 'M. Turk'}]",207.0,"{'bibtex': '@Inproceedings{Turk2000PerceptualUI,\n author = {M. Turk},\n pages = {39-51},\n title = {Perceptual user interfaces},\n year = {2000}\n}\n'}",,"{'volume': '', 'pages': '39-51', 'name': ''}",24.0,Perceptual user interfaces,2000.0
290,193a0319afa14e6a7c3ff6e5699c979bc6286e29,,"[{'authorId': '1788662', 'name': 'P. M. Dung'}]",4621.0,"{'bibtex': '@Article{Dung1995OnTA,\n author = {P. M. Dung},\n journal = {Artif. Intell.},\n pages = {321-358},\n title = {On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games},\n volume = {77},\n year = {1995}\n}\n'}",,"{'volume': '77', 'pages': '321-358', 'name': 'Artif. Intell.'}",79.0,"On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games",1995.0
291,196ea4a1a862808828d215ca7415192de883b779,,"[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",57.0,"{'bibtex': '@Inproceedings{Melo2009ExpressionOE,\n author = {C. D. Melo and J. Gratch},\n pages = {188-200},\n title = {Expression of Emotions Using Wrinkles, Blushing, Sweating and Tears},\n year = {2009}\n}\n'}",,{'pages': '188-200'},42.0,"Expression of Emotions Using Wrinkles, Blushing, Sweating and Tears",2009.0
292,198e484e73d291994be8e0ac6d165a60dde37a5f,,"[{'authorId': '9760933', 'name': 'Evelien Heyselaar'}, {'authorId': '2608476', 'name': 'P. Hagoort'}, {'authorId': '3365604', 'name': 'K. Segaert'}]",64.0,"{'bibtex': '@Article{Heyselaar2015InDW,\n author = {Evelien Heyselaar and P. Hagoort and K. Segaert},\n journal = {Behavior Research Methods},\n pages = {46 - 60},\n title = {In dialogue with an avatar, language behavior is identical to dialogue with a human partner},\n volume = {49},\n year = {2015}\n}\n'}",,"{'volume': '49', 'pages': '46 - 60', 'name': 'Behavior Research Methods'}",50.0,"In dialogue with an avatar, language behavior is identical to dialogue with a human partner",2015.0
293,19a954e8aab0ecba00d2aad48e63738c0d7db467,"In creating an evacuation simulation for training and planning, realistic agents that reproduce known phenomenon are required. Evacuation simulation in the airport domain requires additional features beyond most simulations, including the unique behaviors of first-time visitors who have incomplete knowledge of the area and families that do not necessarily adhere to often-assumed pedestrian behaviors. Evacuation simulations not customized for the airport domain do not incorporate the factors important to it, leading to inaccuracies when applied to it. 
 
In this paper, we describe ESCAPES, a multiagent evacuation simulation tool that incorporates four key features: (i) different agent types; (ii) emotional interactions; (iii) informational interactions; (iv) behavioral interactions. Our simulator reproduces phenomena observed in existing studies on evacuation scenarios and the features we incorporate substantially impact escape time. We use ESCAPES to model the International Terminal at Los Angeles International Airport (LAX) and receive high praise from security officials.","[{'authorId': '145009779', 'name': 'J. Tsai'}, {'authorId': '144447024', 'name': 'N. Fridman'}, {'authorId': '1740910', 'name': 'E. Bowring'}, {'authorId': '2023945041', 'name': 'Matthew Brown'}, {'authorId': '48134644', 'name': 'S. Epstein'}, {'authorId': '1725049', 'name': 'G. Kaminka'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2063035943', 'name': 'A. Ogden'}, {'authorId': '1765821', 'name': 'Inbal Rika'}, {'authorId': '35307122', 'name': 'Ankur Sheel'}, {'authorId': '39286677', 'name': 'Matthew E. Taylor'}, {'authorId': '2108181066', 'name': 'Xuezhi Wang'}, {'authorId': '39336230', 'name': 'Avishay Zilka'}, {'authorId': '143736701', 'name': 'Milind Tambe'}]",178.0,"{'bibtex': '@Inproceedings{Tsai2011ESCAPESES,\n author = {J. Tsai and N. Fridman and E. Bowring and Matthew Brown and S. Epstein and G. Kaminka and S. Marsella and A. Ogden and Inbal Rika and Ankur Sheel and Matthew E. Taylor and Xuezhi Wang and Avishay Zilka and Milind Tambe},\n pages = {457-464},\n title = {ESCAPES: evacuation simulation with children, authorities, parents, emotions, and social comparison},\n year = {2011}\n}\n'}",,{'pages': '457-464'},24.0,"ESCAPES: evacuation simulation with children, authorities, parents, emotions, and social comparison",2011.0
297,19afec957119e1f726e8a8ac39003779cfbc55f0,"This paper describes a generic model for personality, mood and emotion simulation for conversational virtual humans. We present a generic model for updating the parameters related to emotional behaviour, as well as a linear implementation of the generic update mechanisms. We explore how existing theories for appraisal can be integrated into the framework. Then we describe a prototype system that uses the described models in combination with a dialogue system and a talking head with synchronized speech and facial expressions. Copyright © 2004 John Wiley & Sons, Ltd.","[{'authorId': '2479558', 'name': 'A. Egges'}, {'authorId': '144243072', 'name': 'S. Kshirsagar'}, {'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}]",205.0,"{'bibtex': '@Article{Egges2004GenericPA,\n author = {A. Egges and S. Kshirsagar and N. Magnenat-Thalmann},\n journal = {Computer Animation and Virtual Worlds},\n title = {Generic personality and emotion simulation for conversational agents},\n volume = {15},\n year = {2004}\n}\n'}",,"{'volume': '15', 'name': 'Computer Animation and Virtual Worlds'}",20.0,Generic personality and emotion simulation for conversational agents,2004.0
300,19ba0249dcada4bbdc3366d37e1f3362f324e904,"Most functional imaging studies use analyses that look for effects anywhere in the brain. The standard approach is to calculate a statistic relating the experimental effect of interest to the data for each brain voxel. This method has the advantage that it can detect strong effects without apriori constraint on the area that activation will occur. Problems arise when we wish to ask the question whether a particular brain area has been activated: if we know the shape and location of the expected activity, then voxel by voxel approaches have low power, because of the multiple comparisons across voxels. Whole brain analyses usually use image smoothing in order to increase signal to noise; however, the best smoothing filter will depend on the shape of the activation, which may well not be matched by a standard kernel such as a Gaussian. The most direct answer to the question ""has this area been activated"" is to use a region of interest analysis. Here we define a region, and perform the statistical test on the mean time course of the voxels within the region. The contribution of the voxels may be weighted using the expected shape of the activation. This approach has two advantages. First, we increase power by avoiding the multiple comparison problem. Second, if we are correct about the shape and location of the region, the process of taking the mean is equivalent to using the best smoothing kernel to recover the signal. This method has proved very powerful in analysing the activation in well defined regions. For example, Kanwisher et al have used screening tasks and voxel by voxel statistics to define regions of interest, and used these regions to investigate the nature of the original activation in further experiments of visual analysis of faces, scenes, objects and body parts. We have implemented a toolbox called ""MarsBar"" for region of interest analysis within the SPM99 software package (available for free download from http://www.mrc-cbu.cam.ac.uk/Imaging/marsbar.html). The user can define regions using activations from previous SPM analyses, binary or weighted images, or simple shapes (boxes or spheres). Regions can be combined using a full range of algebra to give new regions. Functions include overlap between regions (logical and) or combination (logical or). The software can then extract raw or filtered time courses from the region for futher analysis outside SPM. It can also use new or previous SPM analysis files to analyze the regional time course. t or F statistics for multiple regions can be computed, and the results plotted using the SPM graphical interface. We are currently working on estimation of percentage signal change for the region data. Region of interest analyses are likely to become more important as prior hypotheses about the location of activation become more specific. This may have many advantages in terms of statistical power and the ease of interpretation of neuroimaging data. We hope that this toolbox will make such analyses simpler to implement.","[{'authorId': '144082394', 'name': 'M. Brett'}, {'authorId': '31885367', 'name': 'J. Anton'}, {'authorId': '1828848', 'name': 'R. Valabrègue'}, {'authorId': '1802033', 'name': 'J B Poline'}]",2893.0,"{'bibtex': '@Inproceedings{Brett2010RegionOI,\n author = {M. Brett and J. Anton and R. Valabrègue and J B Poline},\n title = {Region of interest analysis using an SPM toolbox},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Region of interest analysis using an SPM toolbox,2010.0
301,19dcedeb1bf649cc25a9245bb9a6d2b744f2d54a,"The integration of emotion and cognition in cognitive architectures for embodied agents is a problem of increasing importance. In this paper, we describe how two separate modules for these tasks, as we employ them in our virtual human Max, can give rise to secondary emotions such as frustration and relief. The BDI-based cognitive module is responsible for appraisal as well as reappraisal of elicited emotions that our conversational agent Max becomes aware of. The emotion dynamics simulation system is driven by the valence information of every emotion and assures a general consistency of the simulated emotions over time by dynamically providing an awareness likelihood for every emotion.","[{'authorId': '2068695177', 'name': 'Christian Becker'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",22.0,"{'bibtex': '@Inproceedings{Becker2006ModelingPA,\n author = {Christian Becker and I. Wachsmuth},\n title = {Modeling primary and secondary emotions for a believable communication agent},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",14.0,Modeling primary and secondary emotions for a believable communication agent,2006.0
302,19fe091a69ca809c65221dba6fddf6cdd6082a5b,"During emergencies, emotions greatly affect human behaviour. For more realistic multi-agent systems in simulations of emergency evacuations, it is important to incorporate emotions and their effects on the agents. In few words, emotional contagion is a process in which a person or group influences the emotions or behavior of another person or group through the conscious or unconscious induction of emotion states and behavioral attitudes. In this study, we simulate an emergency situation in an open square area with three exits considering Adults and Children agents with different behavior. Also, Security agents are considered in order to guide Adults and Children for finding the exits and be calm. Six levels of emotion levels are considered for each agent in different scenarios and situations. The agent-based simulated model initialize with the random scattering of agent populations and then when an alarm occurs, each agent react to the situation based on its and neighbors current circumstances. The main goal of each agent is firstly to find the exit, and then help other agents to find their ways. Numbers of exited agents along with their emotion levels and damaged agents are compared in different scenarios with different initialization in order to evaluate the achieved results of the simulated model. NetLogo 5.2 is used as the multi-agent simulation framework with R language as the developing language.","[{'authorId': '8877601', 'name': 'H. Faroqi'}, {'authorId': '35650352', 'name': 'M. Mesgari'}]",29.0,"{'bibtex': '@Article{Faroqi2015AgentbasedCS,\n author = {H. Faroqi and M. Mesgari},\n journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},\n pages = {193-196},\n title = {Agent-based Crowd Simulation Considering Emotion Contagion for Emergency Evacuation Problem},\n year = {2015}\n}\n'}",,"{'volume': '', 'pages': '193-196', 'name': 'ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences'}",12.0,Agent-based Crowd Simulation Considering Emotion Contagion for Emergency Evacuation Problem,2015.0
303,1a0b41815766b5952dbabb04cfbde5be6d536ac0,,"[{'authorId': '31057244', 'name': 'D. Berry'}, {'authorId': '31793441', 'name': 'L. Butler'}, {'authorId': '1807752', 'name': 'F. D. Rosis'}]",124.0,"{'bibtex': '@Article{Berry2005EvaluatingAR,\n author = {D. Berry and L. Butler and F. D. Rosis},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {304-327},\n title = {Evaluating a realistic agent in an advice-giving task},\n volume = {63},\n year = {2005}\n}\n'}",,"{'volume': '63', 'pages': '304-327', 'name': 'Int. J. Hum. Comput. Stud.'}",34.0,Evaluating a realistic agent in an advice-giving task,2005.0
304,1a238b8472cf2790b380d8be1030d589412ca7ac,"Synopsis Delsarte’s influence over American oratory, theatrical training, and dance has long been established. Should cinema be added to this list of fields shaped by American Delsartism? Those who received Delsartean training, either professionally or in public school oratory classes, most certainly found their way into filmmaking, as actors and directors. An examination of the context into which Americans enthusiastically embraced Delsarte’s ideas reveals that Americans shared the following precepts regarding the experience and representation of human emotions: emotions have universal expression; the job of the artist is to study these universal expressions; hitting upon a universal emotional expression is the quickest route to exciting an audience’s emotions; and finally, the primary role of art is the stirring of emotions. As long as these ideas flourish, so do the performance practices that aim to meet these goals. A review of Griffith’s feature films demonstrates a persistence of gestures, pantomime, and postures common to acting and oratory manuals and handbooks that profess to help the student discover universal human expressions. These findings demonstrate a greater endurance of conventional acting styles than is currently represented in film scholarship and recommend further research into Delsarte’s influence upon cinematic acting practices of the silent era. DOI 10.5642/mimejournal.20052301.11 First Page 184 Last Page 199 Rights © 2005 Mime Journal, Pomona College, Claremont Colleges Terms of Use & License Information http://creativecommons.org/licenses/by-nc-nd/4.0/ Recommended Citation Hart, Hilary (2005) ""Do You See What I See? The Impact of Delsarte on Silent Film Acting,"" Mime Journal: Vol. 23, Article 11. DOI: 10.5642/mimejournal.20052301.11 Available at: https://scholarship.claremont.edu/mimejournal/vol23/iss1/11  Download","[{'authorId': '146892628', 'name': 'Hilary Hart'}]",3.0,"{'bibtex': '@Inproceedings{Hart2005DoYS,\n author = {Hilary Hart},\n pages = {184-199},\n title = {Do You See What I See? The Impact of Delsarte on Silent Film Acting},\n volume = {23},\n year = {2005}\n}\n'}",,"{'volume': '23', 'pages': '184-199', 'name': ''}",2.0,Do You See What I See? The Impact of Delsarte on Silent Film Acting,2005.0
305,1a3c8c958eb46b28c9222501295bcbe9fad9ad43,"We explore, using a variety of models grounded in computational neuroscience, the dynamics of attachment formation and change. In the first part of the thesis we consider the formation of the traditional organised forms of attachment (as defined by Mary Ainsworth) within the context of the free energy principle, showing how each type of attachment might arise in infant agents who minimise free energy over interoceptive states while interacting with caregivers with varying responsiveness. We show how exteroceptive cues (in the form of disrupted affective communication from the caregiver) can result in disorganised forms of attachment (as first uncovered by Mary Main) in infants of caregivers who consistently increase stress on approach, but can have an organising (towards ambivalence) effect in infants of inconsistent caregivers. The second part of the thesis concerns Self-Attachment: a new self-administrable attachment-based psychotherapy recently introduced by Abbas Edalat, which aims to induce neural plasticity in order to retrain an individual’s suboptimal attachment schema. We begin with a model of the hypothesised neurobiological underpinnings of the Self-Attachment bonding protocols, which are concerned with the formation of an abstract, self-directed bond. Finally, using neuroscientific findings related to empathy and the self-other distinction within the context of pain, we propose a simple spiking neural model for how empathic states might serve to motivate application of the aforementioned bonding protocols.","[{'authorId': '2585368', 'name': 'David Cittern'}]",4.0,"{'bibtex': '@Inproceedings{Cittern2016ComputationalMO,\n author = {David Cittern},\n title = {Computational models of attachment and self-attachment},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",399.0,Computational models of attachment and self-attachment,2016.0
306,1a725cdfc61d90be7a8318193cd7c571f374c3d2,"We present an information-theoretic method to measure the similarity between a given set of observed, real-world data and visual simulation technique for aggregate crowd motions of a complex system consisting of many individual agents. This metric uses a two-step process to quantify a simulator's ability to reproduce the collective behaviors of the whole system, as observed in the recorded real-world data. First, Bayesian inference is used to estimate the simulation states which best correspond to the observed data, then a maximum likelihood estimator is used to approximate the prediction errors. This process is iterated using the EM-algorithm to produce a robust, statistical estimate of the magnitude of the prediction error as measured by its entropy (smaller is better). This metric serves as a simulator-to-data similarity measurement. We evaluated the metric in terms of robustness to sensor noise, consistency across different datasets and simulation methods, and correlation to perceptual metrics.","[{'authorId': '35170565', 'name': 'S. Guy'}, {'authorId': '144721873', 'name': 'J. V. D. Berg'}, {'authorId': '143800486', 'name': 'Wenxi Liu'}, {'authorId': '1726262', 'name': 'Rynson W. H. Lau'}, {'authorId': '144247566', 'name': 'M. Lin'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",138.0,"{'bibtex': '@Article{Guy2012ASS,\n author = {S. Guy and J. V. D. Berg and Wenxi Liu and Rynson W. H. Lau and M. Lin and Dinesh Manocha},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 11},\n title = {A statistical similarity measure for aggregate crowd dynamics},\n volume = {31},\n year = {2012}\n}\n'}",,"{'volume': '31', 'pages': '1 - 11', 'name': 'ACM Transactions on Graphics (TOG)'}",42.0,A statistical similarity measure for aggregate crowd dynamics,2012.0
307,1a77e19441f3a0e030998fb2d11d9dd774582403,,"[{'authorId': '2162210436', 'name': 'Stephen M. Smith'}, {'authorId': '1733095', 'name': 'M. Jenkinson'}, {'authorId': '1813998', 'name': 'M. Woolrich'}, {'authorId': '2683150', 'name': 'C. Beckmann'}, {'authorId': '2591098', 'name': 'Timothy Edward John Behrens'}, {'authorId': '1392848204', 'name': 'H. Johansen-Berg'}, {'authorId': '2076556', 'name': 'P. Bannister'}, {'authorId': '144739224', 'name': 'M. D. Luca'}, {'authorId': '2431102', 'name': 'I. Drobnjak'}, {'authorId': '3514783', 'name': 'D. Flitney'}, {'authorId': '2146541', 'name': 'R. Niazy'}, {'authorId': '2059100876', 'name': 'James Saunders'}, {'authorId': '2060535089', 'name': 'J. Vickers'}, {'authorId': '2108002908', 'name': 'Yongyue Zhang'}, {'authorId': '8434785', 'name': 'N. Stefano'}, {'authorId': '2061193806', 'name': 'J. Brady'}, {'authorId': '1851342', 'name': 'P. Matthews'}]",12076.0,"{'bibtex': '@Article{Smith2004AdvancesIF,\n author = {Stephen M. Smith and M. Jenkinson and M. Woolrich and C. Beckmann and Timothy Edward John Behrens and H. Johansen-Berg and P. Bannister and M. D. Luca and I. Drobnjak and D. Flitney and R. Niazy and James Saunders and J. Vickers and Yongyue Zhang and N. Stefano and J. Brady and P. Matthews},\n journal = {NeuroImage},\n pages = {S208-S219},\n title = {Advances in functional and structural MR image analysis and implementation as FSL},\n volume = {23},\n year = {2004}\n}\n'}",,"{'volume': '23', 'pages': 'S208-S219', 'name': 'NeuroImage'}",51.0,Advances in functional and structural MR image analysis and implementation as FSL,2004.0
308,1a948795c60ce136e265112b6cf5594c7d29048a,"In Study 1, 24 participants generated sentences expressing ways of dealing with positively and negatively valued noun stimuli (objects and humans). They were instructed to begin each sentence with One + auxiliary verb. The auxiliary was to be selected from a set including auxiliaries expressing high (must) and low (can) necessity. As predicted on the basis of a minimal nonsocial model of behavioral adaptation, higher necessity was associated with negative stimuli than with positive stimuli. In Study 2, this effect was replicated using trait adjectives as stimuli. Consistent with the model, the effect was produced by stimulus valences belonging to an approach-avoidance related evaluative dimension ‘other-profitability’. However, additional effects, involving an alternative evaluative dimension ‘self-profitability’, were not fully accounted for by the model. They suggested that genuine social factors were involved that, however, were only required to explain some marginal effects. Copyright © 2002 John Wiley & Sons, Ltd.","[{'authorId': '115425028', 'name': 'Guido Peeters'}]",117.0,"{'bibtex': '@Article{Peeters2002FromGA,\n author = {Guido Peeters},\n journal = {European Journal of Social Psychology},\n pages = {125-136},\n title = {From good and bad to can and must: subjective necessity of acts associated with positively and negatively valued stimuli},\n volume = {32},\n year = {2002}\n}\n'}",,"{'volume': '32', 'pages': '125-136', 'name': 'European Journal of Social Psychology'}",25.0,From good and bad to can and must: subjective necessity of acts associated with positively and negatively valued stimuli,2002.0
309,1ab38e55ed557f4dd03158268235fd2050baa730,,"[{'authorId': '84527386', 'name': 'R. Plutchik'}]",735.0,"{'bibtex': '@Article{Plutchik2001TheNO,\n author = {R. Plutchik},\n journal = {American Scientist},\n title = {The Nature of Emotions},\n year = {2001}\n}\n'}",,{'name': 'American Scientist'},0.0,The Nature of Emotions,2001.0
310,1ab4a7fe5e624a8de30966aefeb7a38dee84287e,"Two studies examined whether participant attitudes would change toward positions advocated by an ingroup member even if the latter was known to be an embodied agent; that is, a human-like representation of a computer algorithm. While immersed in a virtual environment, participants listened to a persuasive communication from a digital representation of another student. The latter was actually an embodied agent (a computer-controlled digital representation of a human). Study 1 examined the extent to which gender of the virtual human, participant gender, and the agent's behavior affected attitude change. Results revealed gender-based ingroup favoritism in the form of greater attitude change for same gender virtual humans. Study 2 examined behavioral realism and agency beliefs; that is, whether participants believed the other to be an agent or an avatar (an online representation of an actual person). Results supported Blascovich and colleague's model of social influence within immersive virtual environments. Specifically, the prediction that virtual humans high in behavioral realism would be more influential than those low in behavioral realism was supported, but this effect was moderated by the gender of the virtual human and the research participant. Implications of these findings for the model are discussed.","[{'authorId': '1715525', 'name': 'R. Guadagno'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '143682925', 'name': 'C. McCall'}]",307.0,"{'bibtex': '@Article{Guadagno2007VirtualHA,\n author = {R. Guadagno and J. Blascovich and J. Bailenson and C. McCall},\n journal = {Media Psychology},\n pages = {1 - 22},\n title = {Virtual Humans and Persuasion: The Effects of Agency and Behavioral Realism},\n volume = {10},\n year = {2007}\n}\n'}",,"{'volume': '10', 'pages': '1 - 22', 'name': 'Media Psychology'}",35.0,Virtual Humans and Persuasion: The Effects of Agency and Behavioral Realism,2007.0
311,1abb2c3bf0dab6ceb720494516362133fba5fd7e,,"[{'authorId': '2605715', 'name': 'Edgar Cebolledo'}, {'authorId': '1745846', 'name': 'Olga De Troyer'}]",5.0,"{'bibtex': '@Inproceedings{Cebolledo2015iATTACAS,\n author = {Edgar Cebolledo and Olga De Troyer},\n pages = {135-146},\n title = {iATTAC: A System for Autonomous Agents and Dynamic Social Interactions - The Architecture},\n year = {2015}\n}\n'}",,{'pages': '135-146'},21.0,iATTAC: A System for Autonomous Agents and Dynamic Social Interactions - The Architecture,2015.0
312,1abcc5349eeec3608f87caa32fcdb9baa24a3265,"Abstract Objective Our objective was to review the characteristics, current applications, and evaluation measures of conversational agents with unconstrained natural language input capabilities used for health-related purposes. Methods We searched PubMed, Embase, CINAHL, PsycInfo, and ACM Digital using a predefined search strategy. Studies were included if they focused on consumers or healthcare professionals; involved a conversational agent using any unconstrained natural language input; and reported evaluation measures resulting from user interaction with the system. Studies were screened by independent reviewers and Cohen’s kappa measured inter-coder agreement. Results The database search retrieved 1513 citations; 17 articles (14 different conversational agents) met the inclusion criteria. Dialogue management strategies were mostly finite-state and frame-based (6 and 7 conversational agents, respectively); agent-based strategies were present in one type of system. Two studies were randomized controlled trials (RCTs), 1 was cross-sectional, and the remaining were quasi-experimental. Half of the conversational agents supported consumers with health tasks such as self-care. The only RCT evaluating the efficacy of a conversational agent found a significant effect in reducing depression symptoms (effect size d = 0.44, p = .04). Patient safety was rarely evaluated in the included studies. Conclusions The use of conversational agents with unconstrained natural language input capabilities for health-related purposes is an emerging field of research, where the few published studies were mainly quasi-experimental, and rarely evaluated efficacy or safety. Future studies would benefit from more robust experimental designs and standardized reporting. Protocol Registration The protocol for this systematic review is registered at PROSPERO with the number CRD42017065917.","[{'authorId': '3314963', 'name': 'L. Laranjo'}, {'authorId': '34318729', 'name': 'A. Dunn'}, {'authorId': '32260194', 'name': 'H. L. Tong'}, {'authorId': '51374066', 'name': 'A. B. Kocaballi'}, {'authorId': '2115896431', 'name': 'Jessica A. Chen'}, {'authorId': '10280969', 'name': 'R. Bashir'}, {'authorId': '2609488', 'name': 'Didi Surian'}, {'authorId': '145497572', 'name': 'B. Gallego'}, {'authorId': '2817191', 'name': 'F. Magrabi'}, {'authorId': '144417710', 'name': 'A. Lau'}, {'authorId': '145948914', 'name': 'E. Coiera'}]",584.0,"{'bibtex': '@Article{Laranjo2018ConversationalAI,\n author = {L. Laranjo and A. Dunn and H. L. Tong and A. B. Kocaballi and Jessica A. Chen and R. Bashir and Didi Surian and B. Gallego and F. Magrabi and A. Lau and E. Coiera},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n pages = {1248 - 1258},\n title = {Conversational agents in healthcare: a systematic review},\n volume = {25},\n year = {2018}\n}\n'}",,"{'volume': '25', 'pages': '1248 - 1258', 'name': 'Journal of the American Medical Informatics Association : JAMIA'}",63.0,Conversational agents in healthcare: a systematic review,2018.0
313,1ac1326157dda043d15092c1ae0d34131359169e,,"[{'authorId': '3489989', 'name': 'H. Helgadóttir'}, {'authorId': '2042414488', 'name': 'Svanhvít Jónsdóttir'}, {'authorId': '40153988', 'name': 'Andri Már Sigurdsson'}, {'authorId': '1903594', 'name': 'Stephan Schiffel'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}]",4.0,"{'bibtex': '@Inproceedings{Helgadóttir2016VirtualGG,\n author = {H. Helgadóttir and Svanhvít Jónsdóttir and Andri Már Sigurdsson and Stephan Schiffel and H. Vilhjálmsson},\n pages = {464-469},\n title = {Virtual General Game Playing Agent},\n year = {2016}\n}\n'}",,{'pages': '464-469'},14.0,Virtual General Game Playing Agent,2016.0
314,1adfc481c1019019b9d6bdb7055fcd2dba83ad6e,"SUMMARY Saltational changes in segment numbers have likely occurred in arthropod evolution, especially if mechanisms of segment formation involve a multiplicative phase, as recently suggested in the evo‐devo literature. Here we provide for the first time evidence of major phenotypic saltation in the evolution of segment number in a lineage of centipedes, with a newly discovered species of scolopender having segment numbers duplicated with respect to its closest relatives, and to all the remaining 700+ species of Scolopendromorpha known to date.","[{'authorId': '46728019', 'name': 'A. Minelli'}, {'authorId': '1405357108', 'name': 'Amazonas Chagas-Júnior'}, {'authorId': '5537475', 'name': 'G. Edgecombe'}]",48.0,"{'bibtex': '@Article{Minelli2009SaltationalEO,\n author = {A. Minelli and Amazonas Chagas-Júnior and G. Edgecombe},\n journal = {Evolution & Development},\n title = {Saltational evolution of trunk segment number in centipedes},\n volume = {11},\n year = {2009}\n}\n'}",,"{'volume': '11', 'name': 'Evolution & Development'}",29.0,Saltational evolution of trunk segment number in centipedes,2009.0
315,1b114e2086b356b9a29ec93d8f4c72a1fb5bca73,,"[{'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",288.0,"{'bibtex': '@Inproceedings{Dias2005FeelingAR,\n author = {João Dias and Ana Paiva},\n pages = {127-140},\n title = {Feeling and Reasoning: A Computational Model for Emotional Characters},\n year = {2005}\n}\n'}",,{'pages': '127-140'},25.0,Feeling and Reasoning: A Computational Model for Emotional Characters,2005.0
319,1b226019015d6c64e7e20bb280f8d2b47cc1b0c4,Nous proposons dans cet article une description de la Langue des Signes Française dans le but de traduire des énoncés courts du français et de les faire signer par un personnage de synthèse. Cette description pose en préalable la question de la transcription des éléments d’une langue dont le signal n’est pas linéaire. Il s’agit ensuite de repérer les différentes couches linguistiques et la forme de leurs unités constitutives en vue de la répartition des tâches informatiques : la synthèse de gestes nécessite un traitement des éléments constitutifs du geste et la génération syntaxique doit pouvoir manipuler des morphèmes.,"[{'authorId': '3167710', 'name': 'Loïc Kervajan'}]",7.0,"{'bibtex': '@Inproceedings{Kervajan2006ProblèmesDR,\n author = {Loïc Kervajan},\n pages = {670-679},\n title = {Problèmes de représentation de la Langue des Signes Française en vue du traitement automatique},\n year = {2006}\n}\n'}",,{'pages': '670-679'},8.0,Problèmes de représentation de la Langue des Signes Française en vue du traitement automatique,2006.0
320,1b3b0f715da1d2988d078c53636730fa6a9769ad,"The Behavior Expression Animation Toolkit (BEAT) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior. The toolkit is extensible, so that new rules can be quickly added. It is designed to plug into larger systems that may also assign personality profiles, motion characteristics, scene constraints, or the animation styles of particular animators.","[{'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}, {'authorId': '1690448', 'name': 'T. Bickmore'}]",926.0,"{'bibtex': '@Article{Cassell2001BEATTB,\n author = {Justine Cassell and H. Vilhjálmsson and T. Bickmore},\n journal = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques},\n title = {BEAT: the Behavior Expression Animation Toolkit},\n year = {2001}\n}\n'}",,{'name': 'Proceedings of the 28th annual conference on Computer graphics and interactive techniques'},40.0,BEAT: the Behavior Expression Animation Toolkit,2001.0
322,1b5b3a5e052c96f591f19569ca29b972ab1f5738,"More than 30 years after its initial publication, this new edition of The Cognitive Structure of Emotions refines and updates Ortony, Clore, and Collins's OCC model of emotions. Starting from a three-way classification of construals of the world––events, the attribution of responsibility for events, and objects––the authors propose a systematic account of emotion differentiation. Rejecting the oft-favored features of bodily feelings, emotion-related behaviors, and facial expressions as too intensity-dependent and insufficiently diagnostic, they provide a detailed analysis of emotion differentiation in terms of the cognitive underpinnings of emotion types. Using numerous examples, they explain how different variables influence emotion intensity, and show how emotions can be formalized for computational purposes. Now with a contributed chapter describing the OCC model's influence, this book will interest a wide audience in cognitive, clinical, and social psychology, as well as in artificial intelligence and affective computing, and other cognitive science disciplines.","[{'authorId': '1802934', 'name': 'A. Ortony'}, {'authorId': '31458494', 'name': 'G. Clore'}, {'authorId': '31533192', 'name': 'A. Collins'}]",6710.0,"{'bibtex': '@Inproceedings{Ortony1988TheCS,\n author = {A. Ortony and G. Clore and A. Collins},\n title = {The Cognitive Structure of Emotions},\n year = {1988}\n}\n'}",,"{'volume': '', 'name': ''}",3.0,The Cognitive Structure of Emotions,1988.0
343,1b8ffe5689cf56755011f37555a03becccbc19fb,"A number of apparently diverse personality scales—variously called trait anxiety, neuroticism, ego strength, general maladjustment, repression-sensitization, and social desirability—are reviewed and are shown to be in fact measures of the same stable and pervasive trait. An integrative interpretation of the construct as Negative Affectivity (NA) is presented. Extensive data indicate that high-NA individuals are more likely to experience discomfort at all times and across situations, even in the absence of overt stress. They are relatively more introspective and tend differentially to dwell on the negative side of themselves and the world. Further research is needed to explain the origins of NA and to elucidate the characteristics of low-NA individuals. Rorer and Widiger (1983) recently bemoaned that in the field of personality ""literature reviews appear to be disparate conglomerations rather than cumulative or conclusive integrations"" (p. 432). We intend this review to be an exception to this discouraging statement. Distinct and segregated literatures have developed around a number of specific personality measures that, despite dissimilar names, nevertheless intercorrelate so highly that they must be considered measures of the same construct. Following Tellegen (1982), we call this construct Negative Affectivity (NA) and present a comprehensive view of the trait that integrates data from a wide variety of relevant research. We are not the first to note this broad and pervasive personality trait. The Eysencks, for example, (e.g. Eysenck & Eysenck, 1968) have done extensive research in the area, traditionally calling the dimension ""Neuroticism,"" although in their most recent revision (Eysenck & Eysenck, 1975) they suggest a label, ""emotionality,"" that is similar to our own. Nonetheless, in discussing the relation between our interpretation and previous views of the domain, we argue for the preferability of our term, Negative Affectivity. We also present","[{'authorId': '145213999', 'name': 'D. Watson'}, {'authorId': '10034636', 'name': 'L. Clark'}]",4652.0,"{'bibtex': '@Article{Watson1984NegativeAT,\n author = {D. Watson and L. Clark},\n journal = {Psychological bulletin},\n pages = {\n          465-90\n        },\n title = {Negative affectivity: the disposition to experience aversive emotional states.},\n volume = {96 3},\n year = {1984}\n}\n'}",,"{'volume': '96 3', 'pages': '\n          465-90\n        ', 'name': 'Psychological bulletin'}",178.0,Negative affectivity: the disposition to experience aversive emotional states.,1984.0
344,1b9976fea3c1cf13f0a102a884f027d9d80a14b3,"This paper presents an exploratory study in which children with autism interact with ZECA (Zeno Engaging Children with Autism). ZECA is a humanoid robot with a face covered with a material allowing the display of varied facial expressions. The study investigates a novel scenario for robot-assisted play, to help promoting labelling of emotions by children with autism spectrum disorders (ASD). The study was performed during three sessions with two boys diagnosed with ASD. The results obtained from the analysis of the children's behaviours while interacting with ZECA helped us improve several aspects of our game scenario such as the technical specificities of the game and its dynamics, and the experimental setup. The software produced for this study allows the robot to autonomously identify the answers of the child during the session. This automatic identification helped the fluidity of the game and freed the experimenter to participate in triadic interactions with the child. The evaluation of the game scenario that will be used in a future study was the main goal of this pilot study, rather than to quantify and evaluate the performance of the children. Overall, this exploratory study in teaching children about labelling emotions using a humanoid robot embedded in a game scenario demonstrated the possible positive outcomes this child-robot interaction can produce and highlighted the issues regarding data collection and their analysis that will inform future studies.","[{'authorId': '145794749', 'name': 'S. Costa'}, {'authorId': '2054291674', 'name': 'F. Soares'}, {'authorId': '40417709', 'name': 'Ana Paula Pereira Vieira'}, {'authorId': '145069229', 'name': 'C. Santos'}, {'authorId': '3171783', 'name': 'Antoine Hiolle'}]",16.0,"{'bibtex': '@Article{Costa2014BuildingAG,\n author = {S. Costa and F. Soares and Ana Paula Pereira Vieira and C. Santos and Antoine Hiolle},\n journal = {The 23rd IEEE International Symposium on Robot and Human Interactive Communication},\n pages = {820-825},\n title = {Building a game scenario to encourage children with autism to recognize and label emotions using a humanoid robot},\n year = {2014}\n}\n'}",,"{'pages': '820-825', 'name': 'The 23rd IEEE International Symposium on Robot and Human Interactive Communication'}",18.0,Building a game scenario to encourage children with autism to recognize and label emotions using a humanoid robot,2014.0
345,1ba244d1e11bea20eaa15a4c90b330201fe17d42,,"[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '1947943', 'name': 'A. Okhmatovskaia'}, {'authorId': '2058678292', 'name': 'François Lamothe'}, {'authorId': '35931377', 'name': 'Mathieu Morales'}, {'authorId': '2015385', 'name': 'R. J. V. D. Werf'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",109.0,"{'bibtex': '@Inproceedings{Gratch2007CanVH,\n author = {J. Gratch and Ning Wang and A. Okhmatovskaia and François Lamothe and Mathieu Morales and R. J. V. D. Werf and Louis-Philippe Morency},\n pages = {286-297},\n title = {Can Virtual Humans Be More Engaging Than Real Ones?},\n year = {2007}\n}\n'}",,{'pages': '286-297'},51.0,Can Virtual Humans Be More Engaging Than Real Ones?,2007.0
346,1bbef6d1dec8aee918ab315d69ddfba2dd0e5a7b,"Two experiments examined the effects of pictorial realism, observer interactivity, and delay of visual feedback on the sense of presence. Subjects were presented pairs of virtual environments (a simulated driving task) that differed In one or more ways from each other. After subjects had completed the second member of each pair they reported which of the two had produced the greater amount of presence and indicated the size of this difference by means of a 1-100 scale. As predicted, realism and interactivity increased presence while delay of visual feedback diminished it. According to subjects' verbal responses to a postexperiment Interview, pictorial realism was the least influential of the three variables examined. Further, although some subjects reported an increase in the sense of presence over the course of the experiment, most said that it had remained unchanged or become weaker.","[{'authorId': '11811305', 'name': 'R. Welch'}, {'authorId': '15809657', 'name': 'T. Blackmon'}, {'authorId': '2107393586', 'name': 'Andrew Liu'}, {'authorId': '3347568', 'name': 'B. Mellers'}, {'authorId': '144145222', 'name': 'L. Stark'}]",294.0,"{'bibtex': '@Article{Welch1996TheEO,\n author = {R. Welch and T. Blackmon and Andrew Liu and B. Mellers and L. Stark},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {263-273},\n title = {The Effects of Pictorial Realism, Delay of Visual Feedback, and Observer Interactivity on the Subjective Sense of Presence},\n volume = {5},\n year = {1996}\n}\n'}",,"{'volume': '5', 'pages': '263-273', 'name': 'Presence: Teleoperators & Virtual Environments'}",19.0,"The Effects of Pictorial Realism, Delay of Visual Feedback, and Observer Interactivity on the Subjective Sense of Presence",1996.0
347,1be002eee21901925fcf4c87d5ce102cafd70998,"Background: Social rejection elicits negative mood, emotional distress, and neural activity in networks that are associated with physical pain. However, studies assessing physiological reactions to social rejection are rare and results of these studies were found to be ambiguous. Therefore, the present study aimed to examine and specify physiological effects of social rejection. Methods: Participants (n = 50) were assigned to either a social exclusion or inclusion condition of a virtual ball-tossing game (Cyberball). Immediate and delayed physiological [skin conductance level (SCL) and heart rate] reactions were recorded. In addition, subjects reported levels of affect, emotional states, and fundamental needs. Results: Subjects who were socially rejected showed increased heart rates. However, social rejection had no effect on subjects' SCLs. Both conditions showed heightened arousal on this measurement. Furthermore, psychological consequences of social rejection indicated the validity of the paradigm. Conclusions: Our results reveal that social rejection evokes an immediate physiological reaction. Accelerated heart rates indicate that behavior activation rather than inhibition is associated with socially threatening events. In addition, results revealed gender-specific response patterns suggesting that sample characteristics such as differences in gender may account for ambiguous findings of physiological reactions to social rejection.","[{'authorId': '4973279', 'name': 'Benjamin Iffland'}, {'authorId': '6999691', 'name': 'L. M. Sansen'}, {'authorId': '3781074', 'name': 'C. Catani'}, {'authorId': '5637208', 'name': 'F. Neuner'}]",49.0,"{'bibtex': '@Article{Iffland2014RapidHB,\n author = {Benjamin Iffland and L. M. Sansen and C. Catani and F. Neuner},\n journal = {Frontiers in Psychology},\n title = {Rapid heartbeat, but dry palms: reactions of heart rate and skin conductance levels to social rejection},\n volume = {5},\n year = {2014}\n}\n'}",,"{'volume': '5', 'name': 'Frontiers in Psychology'}",84.0,"Rapid heartbeat, but dry palms: reactions of heart rate and skin conductance levels to social rejection",2014.0
348,1beefd6c34e146c0dafbe7533ffd6da8dd9c8eea,,"[{'authorId': '2082714', 'name': 'Emily C. Collins'}, {'authorId': '1750570', 'name': 'T. Prescott'}, {'authorId': '7514567', 'name': 'B. Mitchinson'}]",32.0,"{'bibtex': '@Inproceedings{Collins2015SayingIW,\n author = {Emily C. Collins and T. Prescott and B. Mitchinson},\n pages = {243-255},\n title = {Saying It with Light: A Pilot Study of Affective Communication Using the MIRO Robot},\n year = {2015}\n}\n'}",,{'pages': '243-255'},38.0,Saying It with Light: A Pilot Study of Affective Communication Using the MIRO Robot,2015.0
349,1c2083514faa4991515064a41cafb99edda8fe7c,"We explore the presence of indicators of psychological distress in the linguistic behavior of subjects in a corpus of semistructured virtual human interviews. At the level of aggregate dialogue-level features, we identify several significant differences between subjects with depression and PTSD when compared to nondistressed subjects. At a more fine-grained level, we show that significant differences can also be found among features that represent subject behavior during specific moments in the dialogues. Finally, we present statistical classification results that suggest the potential for automatic assessment of psychological distress in individual interactions with a virtual human dialogue system.","[{'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '3194430', 'name': 'Kallirroi Georgila'}, {'authorId': '2038490', 'name': 'Ron Artstein'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",47.0,"{'bibtex': '@Inproceedings{DeVault2013VerbalIO,\n author = {D. DeVault and Kallirroi Georgila and Ron Artstein and Fabrizio Morbini and D. Traum and Stefan Scherer and A. Rizzo and Louis-Philippe Morency},\n pages = {193-202},\n title = {Verbal indicators of psychological distress in interactive dialogue with a virtual human},\n year = {2013}\n}\n'}",,{'pages': '193-202'},17.0,Verbal indicators of psychological distress in interactive dialogue with a virtual human,2013.0
350,1c235f07773e3f7184b02957c1b34725d0220671,,"[{'authorId': '2262168653', 'name': 'Blisabeth André'}, {'authorId': '2262189126', 'name': 'Thomas Rist'}, {'authorId': '2106252', 'name': 'Susanne van Mulken'}]",385.0,"{'bibtex': '@Inproceedings{André2006EmbodiedCA,\n author = {Blisabeth André and Thomas Rist and Susanne van Mulken},\n title = {Embodied Conversational Agents},\n year = {2006}\n}\n'}",,,0.0,Embodied Conversational Agents,2006.0
351,1c386341b349392c32a342d18457e508e91b0611,"Background The humanoid robot WE4-RII was designed to express human emotions in order to improve human-robot interaction. We can read the emotions depicted in its gestures, yet might utilize different neural processes than those used for reading the emotions in human agents. Methodology Here, fMRI was used to assess how brain areas activated by the perception of human basic emotions (facial expression of Anger, Joy, Disgust) and silent speech respond to a humanoid robot impersonating the same emotions, while participants were instructed to attend either to the emotion or to the motion depicted. Principal Findings Increased responses to robot compared to human stimuli in the occipital and posterior temporal cortices suggest additional visual processing when perceiving a mechanical anthropomorphic agent. In contrast, activity in cortical areas endowed with mirror properties, like left Broca's area for the perception of speech, and in the processing of emotions like the left anterior insula for the perception of disgust and the orbitofrontal cortex for the perception of anger, is reduced for robot stimuli, suggesting lesser resonance with the mechanical agent. Finally, instructions to explicitly attend to the emotion significantly increased response to robot, but not human facial expressions in the anterior part of the left inferior frontal gyrus, a neural marker of motor resonance. Conclusions Motor resonance towards a humanoid robot, but not a human, display of facial emotion is increased when attention is directed towards judging emotions. Significance Artificial agents can be used to assess how factors like anthropomorphism affect neural response to the perception of human actions.","[{'authorId': '1728769', 'name': 'T. Chaminade'}, {'authorId': '144810580', 'name': 'M. Zecca'}, {'authorId': '2797046', 'name': 'S. Blakemore'}, {'authorId': '1737432', 'name': 'A. Takanishi'}, {'authorId': '144155759', 'name': 'C. Frith'}, {'authorId': '1736797', 'name': 'S. Micera'}, {'authorId': '145745294', 'name': 'P. Dario'}, {'authorId': '2460061', 'name': 'G. Rizzolatti'}, {'authorId': '2914469', 'name': 'V. Gallese'}, {'authorId': '4224905', 'name': 'M. Umiltà'}]",108.0,"{'bibtex': '@Article{Chaminade2010BrainRT,\n author = {T. Chaminade and M. Zecca and S. Blakemore and A. Takanishi and C. Frith and S. Micera and P. Dario and G. Rizzolatti and V. Gallese and M. Umiltà},\n journal = {PLoS ONE},\n title = {Brain Response to a Humanoid Robot in Areas Implicated in the Perception of Human Emotional Gestures},\n volume = {5},\n year = {2010}\n}\n'}",,"{'volume': '5', 'name': 'PLoS ONE'}",96.0,Brain Response to a Humanoid Robot in Areas Implicated in the Perception of Human Emotional Gestures,2010.0
352,1c768358444c41fafc7321c6906e6e670b0dae5d,,"[{'authorId': '8594798', 'name': 'G. Schoenewolf'}]",229.0,"{'bibtex': '@Inproceedings{Schoenewolf1990EmotionalCB,\n author = {G. Schoenewolf},\n title = {Emotional contagion: Behavioral induction in individuals and groups.},\n year = {1990}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotional contagion: Behavioral induction in individuals and groups.,1990.0
353,1c77c1591bbff91eca755d873ff569126e701959,,"[{'authorId': '1765407', 'name': 'G. Konidaris'}, {'authorId': '1730590', 'name': 'A. Barto'}]",71.0,"{'bibtex': '@Inproceedings{Konidaris2006AnAR,\n author = {G. Konidaris and A. Barto},\n pages = {346-356},\n title = {An Adaptive Robot Motivational System},\n year = {2006}\n}\n'}",,{'pages': '346-356'},26.0,An Adaptive Robot Motivational System,2006.0
354,1c9e3c3e1d0f686bf46737420e4b4aa39ee4791f,,"[{'authorId': '1799528', 'name': 'D. Budakova'}, {'authorId': '1753312', 'name': 'L. Dakovski'}, {'authorId': '1581448954', 'name': 'Veselka Petrova-Dimitrova'}]",6.0,"{'bibtex': '@Article{Budakova2019SmartSC,\n author = {D. Budakova and L. Dakovski and Veselka Petrova-Dimitrova},\n journal = {IFAC-PapersOnLine},\n title = {Smart Shopping Cart Learning Agents Development},\n year = {2019}\n}\n'}",,{'name': 'IFAC-PapersOnLine'},10.0,Smart Shopping Cart Learning Agents Development,2019.0
355,1ca55691605dacec5a27ed981c0721c471222109,"A sudden comprehension that solves a problem, reinterprets a situation, explains a joke, or resolves an ambiguous percept is called an insight (i.e., the “Aha! moment”). Psychologists have studied insight using behavioral methods for nearly a century. Recently, the tools of cognitive neuroscience have been applied to this phenomenon. A series of studies have used electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) to study the neural correlates of the “Aha! moment” and its antecedents. Although the experience of insight is sudden and can seem disconnected from the immediately preceding thought, these studies show that insight is the culmination of a series of brain states and processes operating at different time scales. Elucidation of these precursors suggests interventional opportunities for the facilitation of insight.","[{'authorId': '1865460', 'name': 'J. Kounios'}, {'authorId': '144390263', 'name': 'M. Beeman'}]",240.0,"{'bibtex': '@Article{Kounios2009TheAM,\n author = {J. Kounios and M. Beeman},\n journal = {Current Directions in Psychological Science},\n pages = {210 - 216},\n title = {The Aha! Moment},\n volume = {18},\n year = {2009}\n}\n'}",,"{'volume': '18', 'pages': '210 - 216', 'name': 'Current Directions in Psychological Science'}",19.0,The Aha! Moment,2009.0
356,1cb9e2311ecf92a5e716876e914ccd36514e31c0,"Moral judgment often requires making difficult tradeoffs (e.g., is it appropriate to torture to save the lives of innocents at risk?). Previous research suggests that both emotional appraisals and more deliberative utilitarian appraisals influence such judgments and that these appraisals often conflict. However, it is unclear how these different types of appraisals are represented in the brain, or how they are integrated into an overall moral judgment. We addressed these questions using an fMRI paradigm in which human subjects provide separate emotional and utilitarian appraisals for different potential actions, and then make difficult moral judgments constructed from combinations of these actions. We found that anterior cingulate, insula, and superior temporal gyrus correlated with emotional appraisals, whereas temporoparietal junction and dorsomedial prefrontal cortex correlated with utilitarian appraisals. Overall moral value judgments were represented in an anterior portion of the ventromedial prefrontal cortex. Critically, the pattern of responses and functional interactions between these three sets of regions are consistent with a model in which emotional and utilitarian appraisals are computed independently and in parallel, and passed to the ventromedial prefrontal cortex where they are integrated into an overall moral value judgment. SIGNIFICANCE STATEMENT Popular accounts of moral judgment often describe it as a battle for control between two systems, one intuitive and emotional, the other rational and utilitarian, engaged in winner-take-all inhibitory competition. Using a novel fMRI paradigm, we identified distinct neural signatures of emotional and utilitarian appraisals and used them to test different models of how they compete for the control of moral behavior. Importantly, we find little support for competitive inhibition accounts. Instead, moral judgments resembled the architecture of simple economic choices: distinct regions represented emotional and utilitarian appraisals independently and passed this information to the ventromedial prefrontal cortex for integration into an overall moral value signal.","[{'authorId': '2950208', 'name': 'Cendri A. C. Hutcherson'}, {'authorId': '1421913481', 'name': 'Leila Montaser-Kouhsari'}, {'authorId': '144723051', 'name': 'J. Woodward'}, {'authorId': '145542456', 'name': 'A. Rangel'}]",76.0,"{'bibtex': '@Article{Hutcherson2015EmotionalAU,\n author = {Cendri A. C. Hutcherson and Leila Montaser-Kouhsari and J. Woodward and A. Rangel},\n journal = {The Journal of Neuroscience},\n pages = {12593 - 12605},\n title = {Emotional and Utilitarian Appraisals of Moral Dilemmas Are Encoded in Separate Areas and Integrated in Ventromedial Prefrontal Cortex},\n volume = {35},\n year = {2015}\n}\n'}",,"{'volume': '35', 'pages': '12593 - 12605', 'name': 'The Journal of Neuroscience'}",46.0,Emotional and Utilitarian Appraisals of Moral Dilemmas Are Encoded in Separate Areas and Integrated in Ventromedial Prefrontal Cortex,2015.0
357,1cc796ef7e47d73f94bf9df921fd24a0115a6899,"This paper describes a behavioural model used to simulate realistic eye‐gaze behaviour and body animations for avatars representing participants in a shared immersive virtual environment (IVE). The model was used in a study designed to explore the impact of avatar realism on the perceived quality of communication within a negotiation scenario. Our eye‐gaze model was based on data and studies carried out on the behaviour of eye‐gaze during face‐to‐face communication. The technical features of the model are reported here. Information about the motivation behind the study, experimental procedures and a full analysis of the results obtained are given in [ 17 ].","[{'authorId': '2020259', 'name': 'V. Vinayagamoorthy'}, {'authorId': '11554704', 'name': 'Maia Garau'}, {'authorId': '143903462', 'name': 'A. Steed'}, {'authorId': '144931212', 'name': 'M. Slater'}]",103.0,"{'bibtex': '@Article{Vinayagamoorthy2004AnEG,\n author = {V. Vinayagamoorthy and Maia Garau and A. Steed and M. Slater},\n journal = {Computer Graphics Forum},\n title = {An Eye Gaze Model for Dyadic Interaction in an Immersive Virtual Environment: Practice and Experience},\n volume = {23},\n year = {2004}\n}\n'}",,"{'volume': '23', 'name': 'Computer Graphics Forum'}",48.0,An Eye Gaze Model for Dyadic Interaction in an Immersive Virtual Environment: Practice and Experience,2004.0
358,1d1286676088884132cc41af051495f02fdb6bfd,,"[{'authorId': '2816755', 'name': 'J. Bavelas'}, {'authorId': '12566494', 'name': 'A. Black'}, {'authorId': '117243466', 'name': 'Charles R. Lemery'}, {'authorId': '49885938', 'name': 'J. Mullett'}]",451.0,"{'bibtex': '@Article{Bavelas1986ISH,\n author = {J. Bavelas and A. Black and Charles R. Lemery and J. Mullett},\n journal = {Journal of Personality and Social Psychology},\n pages = {322-329},\n title = {""I show how you feel"": Motor mimicry as a communicative act.},\n volume = {50},\n year = {1986}\n}\n'}",,"{'volume': '50', 'pages': '322-329', 'name': 'Journal of Personality and Social Psychology'}",31.0,"""I show how you feel"": Motor mimicry as a communicative act.",1986.0
360,1d4a4ded46582e59ccafb3dcd576841a819f6b1e,This study investigated how rapidly emotion specific facial muscle reactions are released when subjects are exposed to pictures of different categories of positively and negatively rated emotional stimuli. Facial electromyographic (EMG) activity was meas,"[{'authorId': '4583182', 'name': 'U. Dimberg'}]",96.0,"{'bibtex': '@Article{Dimberg1997FacialRR,\n author = {U. Dimberg},\n journal = {Journal of Psychophysiology},\n pages = {115-123},\n title = {Facial reactions: Rapidly evoked emotional responses},\n volume = {11},\n year = {1997}\n}\n'}",,"{'volume': '11', 'pages': '115-123', 'name': 'Journal of Psychophysiology'}",0.0,Facial reactions: Rapidly evoked emotional responses,1997.0
361,1d5b0ad40875c4f25bb4361a876324e434ecf2ed,"The current studies aimed to find out whether a nonintentional form of mood contagion exists and which mechanisms can account for it. In these experiments participants who expected to be tested for text comprehension listened to an affectively neutral speech that was spoken in a slightly sad or happy voice. The authors found that (a) the emotional expression induced a congruent mood state in the listeners, (b) inferential accounts to emotional sharing were not easily reconciled with the findings, (c) different affective experiences emerged from intentional and nonintentional forms of emotional sharing, and (d) findings suggest that a perception-behavior link (T. L. Chartrand & J. A. Bargh, 1999) can account for these findings, because participants who were required to repeat the philosophical speech spontaneously imitated the target person's vocal expression of emotion.","[{'authorId': '47921749', 'name': 'Roland Neumann'}, {'authorId': '4574442', 'name': 'F. Strack'}]",754.0,"{'bibtex': '@Article{Neumann2000MoodCT,\n author = {Roland Neumann and F. Strack},\n journal = {Journal of personality and social psychology},\n pages = {\n          211-23\n        },\n title = {""Mood contagion"": the automatic transfer of mood between persons.},\n volume = {79 2},\n year = {2000}\n}\n'}",,"{'volume': '79 2', 'pages': '\n          211-23\n        ', 'name': 'Journal of personality and social psychology'}",63.0,"""Mood contagion"": the automatic transfer of mood between persons.",2000.0
362,1d9c0d6f42a5b2398dbd8c7a49385afb4e465770,"What happens in our brains to make us feel fear, love, hate, anger, joy? do we control our emotions, or do they control us? Do animals have emotions? How can traumatic experiences in early childhood influence adult behavior, even though we have no conscious memory of them? In The Emotional Brain, Joseph LeDoux investigates the origins of human emotions and explains that many exist as part of complex neural systems that evolved to enable us to survive. Unlike conscious feelings, emotions originate in the brain at a much deeper level, says LeDoux, a leading authority in the field of neural science and one of the principal researchers profiled in Daniel Goleman's Emotional Intelligence. In this provocative book, LeDoux explores the underlying brain mechanisms responsible for our emotions, mechanisms that are only now being revealed. The Emotional Brain presents some fascinating findings about our familiar yet little understood emotions. For example, our brains can detect danger before we even experience the feeling of being afraid. The brain also begins to initiate physical responses (heart palpitations, sweaty palms, muscle tension) before we become aware of an associated feeling of fear. Conscious feelings, says LeDoux, are somewhat irrelevant to the way the emotional brain works. He points out that emotional responses are hard-wired into the brain's circuitry, but the things that make us emotional are learned through experience. And this may be the key to understanding, even changing, our emotional makeup. Many common psychiatric problems - such as phobias or posttraumatic stress disorder - involve malfunctions in the way emotion systems learn and remember. Understanding how thesemechanisms normally work will have important consequences for how we view ourselves and how we treat emotional disorders.","[{'authorId': '2332694', 'name': 'Joseph E LeDoux'}]",4366.0,"{'bibtex': '@Inproceedings{LeDoux1996TheEB,\n author = {Joseph E LeDoux},\n title = {The Emotional Brain: The Mysterious Underpinnings of Emotional Life},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Emotional Brain: The Mysterious Underpinnings of Emotional Life,1996.0
363,1d9f5cc0309c6994ed3bb2392b3a3b88178bcf12,,"[{'authorId': '7930852', 'name': 'S. Burke'}, {'authorId': '48552907', 'name': 'Tammy Bresnahan'}, {'authorId': '48779330', 'name': 'Tan Li'}, {'authorId': '12283319', 'name': 'K. Epnere'}, {'authorId': '32772262', 'name': 'Albert A. Rizzo'}, {'authorId': '119281738', 'name': 'Mary Partin'}, {'authorId': '29825535', 'name': 'Robert M. Ahlness'}, {'authorId': '32810856', 'name': 'Matthew Trimmer'}]",87.0,"{'bibtex': '@Article{Burke2018UsingVI,\n author = {S. Burke and Tammy Bresnahan and Tan Li and K. Epnere and Albert A. Rizzo and Mary Partin and Robert M. Ahlness and Matthew Trimmer},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {905-912},\n title = {Using Virtual Interactive Training Agents (ViTA) with Adults with Autism and Other Developmental Disabilities},\n volume = {48},\n year = {2018}\n}\n'}",,"{'volume': '48', 'pages': '905-912', 'name': 'Journal of Autism and Developmental Disorders'}",19.0,Using Virtual Interactive Training Agents (ViTA) with Adults with Autism and Other Developmental Disabilities,2018.0
365,1da64b9055430fffdb388271761718a32aedfac0,"""Automatic imitation"" is a type of stimulus-response compatibility effect in which the topographical features of task-irrelevant action stimuli facilitate similar, and interfere with dissimilar, responses. This article reviews behavioral, neurophysiological, and neuroimaging research on automatic imitation, asking in what sense it is ""automatic"" and whether it is ""imitation."" This body of research reveals that automatic imitation is a covert form of imitation, distinct from spatial compatibility. It also indicates that, although automatic imitation is subject to input modulation by attentional processes, and output modulation by inhibitory processes, it is mediated by learned, long-term sensorimotor associations that cannot be altered directly by intentional processes. Automatic imitation provides an important tool for the investigation of the mirror neuron system, motor mimicry, and complex forms of imitation. It is a new behavioral phenomenon, comparable with the Stroop and Simon effects, providing strong evidence that even healthy adult humans are prone, in an unwilled and unreasoned way, to copy the actions of others.","[{'authorId': '31433567', 'name': 'C. Heyes'}]",464.0,"{'bibtex': '@Article{Heyes2011AutomaticI,\n author = {C. Heyes},\n journal = {Psychological bulletin},\n pages = {\n          463-83\n        },\n title = {Automatic imitation.},\n volume = {137 3},\n year = {2011}\n}\n'}",,"{'volume': '137 3', 'pages': '\n          463-83\n        ', 'name': 'Psychological bulletin'}",130.0,Automatic imitation.,2011.0
366,1dc090051768f16ee3daca1e264d17c91d663f77,"Pre-Darwinian Views on Facial Expression. Darwin's Anti-Darwinism in Expression of the Emotions in Man and Animals. Facial Expression and the Methods of Contemporary Evolutionary Research. Mechanisms for the Evolution of Facial Expressions. Facial Hardware: The Nerves and Muscles of the Face. Facial Reflexes and the Ontogeny of Facial Displays. Emotions Versus Behavioural Ecology Views of Facial Expression: Theory and Concepts. Emotions Versus Behavioural Ecology Views of Facial Expreesion: The State of the Evidence. Introduction: Cross Cultural Studies of Facial Expressions of Emotion. Is There Universal Recognition of Emotion from Facial Expression? A Review of Cross-Cultural Studiew, By James A. Russell. How Do We Account for Both Universal and Regional Variations in Facial Expressions of Emotion?. Facial Paralanguage and Gesture. Conclusion: The Study of Facial Displays-Where Do We Go from Here?. References. Index.","[{'authorId': '1903481', 'name': 'A. J. Fridlund'}]",1217.0,"{'bibtex': '@Inproceedings{Fridlund1994HumanFE,\n author = {A. J. Fridlund},\n title = {Human Facial Expression: An Evolutionary View},\n year = {1994}\n}\n'}",,"{'volume': '', 'name': ''}",8.0,Human Facial Expression: An Evolutionary View,1994.0
367,1e2355cea9da54478de58efdec1e4f8e995cf231,"The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.","[{'authorId': '2942126', 'name': 'Panagiotis G. Ipeirotis'}, {'authorId': '1684012', 'name': 'L. Gravano'}, {'authorId': '1764547', 'name': 'M. Sahami'}]",186.0,"{'bibtex': '@Inproceedings{Ipeirotis2001ProbeCA,\n author = {Panagiotis G. Ipeirotis and L. Gravano and M. Sahami},\n pages = {67-78},\n title = {Probe, count, and classify: categorizing hidden web databases},\n year = {2001}\n}\n'}",,{'pages': '67-78'},30.0,"Probe, count, and classify: categorizing hidden web databases",2001.0
368,1e43c7084bdcb6b3102afaf301cce10faead2702,"Abstract Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.","[{'authorId': '46664096', 'name': 'Jinhyuk Lee'}, {'authorId': '51433082', 'name': 'Wonjin Yoon'}, {'authorId': '2829848', 'name': 'Sungdong Kim'}, {'authorId': '2145183568', 'name': 'Donghyeon Kim'}, {'authorId': '2144247125', 'name': 'Sunkyu Kim'}, {'authorId': '51435068', 'name': 'Chan Ho So'}, {'authorId': '144323862', 'name': 'Jaewoo Kang'}]",3626.0,"{'bibtex': '@Article{Lee2019BioBERTAP,\n author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},\n journal = {Bioinformatics},\n pages = {1234 - 1240},\n title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\n volume = {36},\n year = {2019}\n}\n'}",,"{'volume': '36', 'pages': '1234 - 1240', 'name': 'Bioinformatics'}",45.0,BioBERT: a pre-trained biomedical language representation model for biomedical text mining,2019.0
369,1e4db73baf40278ca334d6107102e205953b6384,"In face-to-face work, discussion and negotiation relies strongly on non-verbal feedback, which provides important clues to negotiation states such as agreement/disagreement and understanding/confusion. With the continued rise of virtual teams, collaborative work increasingly requires tools to manage the reality of distributed and remote work, which is often hampered by a lack of social cohesion and such phenomena as participant multi-tasking. This paper outlines our project which develops and experimentally assesses a proof-of-concept AI- and Image Processing- Based software agent (Emotion tracking Agent or ETA) for the monitoring of the presence and emotional states of co-workers in virtual meetings.","[{'authorId': '2118148111', 'name': 'Paul Smith'}, {'authorId': '46549484', 'name': 'S. Redfern'}]",2.0,"{'bibtex': '@Article{Smith2010FacialET,\n author = {Paul Smith and S. Redfern},\n journal = {2010 Second International Conference on Games and Virtual Worlds for Serious Applications},\n pages = {91-92},\n title = {Facial Expression Tracking for Remote Conferencing Applications: An Approach to Tackling the Disadvantages of Remote Worker Multitasking},\n year = {2010}\n}\n'}",,"{'pages': '91-92', 'name': '2010 Second International Conference on Games and Virtual Worlds for Serious Applications'}",11.0,Facial Expression Tracking for Remote Conferencing Applications: An Approach to Tackling the Disadvantages of Remote Worker Multitasking,2010.0
370,1e50396b494b2d0c615a03d07b133f3b625188df,"The idea of innate and universal facial expressions that have links with human emotions was given the status of scientific hypothesis by Darwin (1872/1965). Substantial evidence, old and new, supports his hypothesis. Much of the evidence is independent of language, but Russell's (1994) criticisms of the hypothesis focus on language-dependent data. In this article, it is argued that Russell's critique was off target in that his arguments relate only to a hypothesis of the universality of semantic attributions and overstated in that he used questionable logic in designing studies to support his claims. It is also argued that Russell misinterpreted the relation between the universality hypothesis and differential emotions theory. Finally, new evidence is presented that supports the Darwinian hypothesis of the innateness and universality of the facial expressions of a limited set of emotions and the efficacy of the most commonly used method of testing it.","[{'authorId': '38430881', 'name': 'C. Izard'}]",835.0,"{'bibtex': '@Article{Izard1994InnateAU,\n author = {C. Izard},\n journal = {Psychological bulletin},\n pages = {\n          288-99\n        },\n title = {Innate and universal facial expressions: evidence from developmental and cross-cultural research.},\n volume = {115 2},\n year = {1994}\n}\n'}",,"{'volume': '115 2', 'pages': '\n          288-99\n        ', 'name': 'Psychological bulletin'}",63.0,Innate and universal facial expressions: evidence from developmental and cross-cultural research.,1994.0
371,1e61bc7abe5fd33102c5bc4e21ab8a1627cbcdfc,"Previous studies regarding the perception of emotions for embodied virtual agents have shown the effectiveness of using virtual characters in conveying emotions through interactions with humans. However, creating an autonomous embodied conversational agent with expressive behaviors presents two major challenges. The first challenge is the difficulty of synthesizing the conversational behaviors for each modality that are as expressive as real human behaviors. The second challenge is that the affects are modeled independently, which makes it difficult to generate multimodal responses with consistent emotions across all modalities. In this work, we propose a conceptual framework, ACTOR (Affect-Consistent mulTimodal behaviOR generation), that aims to increase the perception of affects by generating multimodal behaviors conditioned on a consistent driving affect. We have conducted a user study with 199 participants to assess how the average person judges the affects perceived from multimodal behaviors that are consistent and inconsistent with respect to a driving affect. The result shows that among all model conditions, our affect-consistent framework receives the highest Likert scores for the perception of driving affects. Our statistical analysis suggests that making a modality affect-inconsistent significantly decreases the perception of driving affects. We also observe that multimodal behaviors conditioned on consistent affects are more expressive compared to behaviors with inconsistent affects. Therefore, we conclude that multimodal emotion conditioning and affect consistency are vital to enhancing the perception of affects for embodied conversational agents.","[{'authorId': '2152342254', 'name': 'Che-Jui Chang'}, {'authorId': '51118484', 'name': 'Samuel S. Sohn'}, {'authorId': '2175553614', 'name': 'Sen Zhang'}, {'authorId': '2089478166', 'name': 'R. Jayashankar'}, {'authorId': '145274939', 'name': 'Muhammad Usman'}, {'authorId': '143980997', 'name': 'M. Kapadia'}]",1.0,"{'bibtex': '@Article{Chang2023TheIO,\n author = {Che-Jui Chang and Samuel S. Sohn and Sen Zhang and R. Jayashankar and Muhammad Usman and M. Kapadia},\n booktitle = {International Conference on Intelligent User Interfaces},\n journal = {Proceedings of the 28th International Conference on Intelligent User Interfaces},\n title = {The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents},\n year = {2023}\n}\n'}","[{'paperId': 'c1fbe464d3af72527b7a56a33fd1603278d3ebc3', 'title': 'Learning from Synthetic Human Group Activities'}]",{'name': 'Proceedings of the 28th International Conference on Intelligent User Interfaces'},68.0,The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents,2023.0
372,1e7ae86a78a9b4860aa720fb0fd0bdc199b092c3,"Facial emotion recognition (FER) is an important topic in the fields of computer vision and artificial intelligence owing to its significant academic and commercial potential. Although FER can be conducted using multiple sensors, this review focuses on studies that exclusively use facial images, because visual expressions are one of the main information channels in interpersonal communication. This paper provides a brief review of researches in the field of FER conducted over the past decades. First, conventional FER approaches are described along with a summary of the representative categories of FER systems and their main algorithms. Deep-learning-based FER approaches using deep networks enabling “end-to-end” learning are then presented. This review also focuses on an up-to-date hybrid deep-learning approach combining a convolutional neural network (CNN) for the spatial features of an individual frame and long short-term memory (LSTM) for temporal features of consecutive frames. In the later part of this paper, a brief review of publicly available evaluation metrics is given, and a comparison with benchmark results, which are a standard for a quantitative comparison of FER researches, is described. This review can serve as a brief guidebook to newcomers in the field of FER, providing basic knowledge and a general understanding of the latest state-of-the-art studies, as well as to experienced researchers looking for productive directions for future work.","[{'authorId': '144893429', 'name': 'ByoungChul Ko'}]",474.0,"{'bibtex': '@Article{Ko2018ABR,\n author = {ByoungChul Ko},\n journal = {Sensors (Basel, Switzerland)},\n title = {A Brief Review of Facial Emotion Recognition Based on Visual Information},\n volume = {18},\n year = {2018}\n}\n'}",,"{'volume': '18', 'name': 'Sensors (Basel, Switzerland)'}",71.0,A Brief Review of Facial Emotion Recognition Based on Visual Information,2018.0
373,1e8020838dbb4f7ad5333ec348de7e9f745ce6de,,"[{'authorId': '2931209', 'name': 'Christopher P. Said'}, {'authorId': '2365875', 'name': 'R. Dotsch'}, {'authorId': '145441940', 'name': 'A. Todorov'}]",77.0,"{'bibtex': '@Article{Said2010TheAA,\n author = {Christopher P. Said and R. Dotsch and A. Todorov},\n journal = {Neuropsychologia},\n pages = {3596-3605},\n title = {The amygdala and FFA track both social and non-social face dimensions},\n volume = {48},\n year = {2010}\n}\n'}",,"{'volume': '48', 'pages': '3596-3605', 'name': 'Neuropsychologia'}",44.0,The amygdala and FFA track both social and non-social face dimensions,2010.0
374,1e86da7940dd2b4227ed6a76fce40a5f8604b808,"This volume brings together, for the first time, the various strands of inquiry and latest research in the scientific study of the relationship between the mechanisms of the brain and the psychology of the mind. In recent years, scientists have made considerable advances in understanding how brain processes shape emotions and are changed by human emotion. Drawing on a wide range of neuroimaging techniques, neuropsychological assessment, and clinical research, scientists are beginning to understand the biological mechanisms for emotions. The book is divided into ten sections: Neuroscience; Autonomic Psychophysiology; Genetics and Development; Expression; Components of Emotion; Personality; Emotion and Social Processes; Adaptation, Culture, and Evolution; Emotion and Psychopathology; and Emotion and Health.","[{'authorId': '118576023', 'name': 'R. Davidson'}, {'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '143757695', 'name': 'H. Goldsmith'}]",1931.0,"{'bibtex': '@Inproceedings{Davidson2003HandbookOA,\n author = {R. Davidson and K. Scherer and H. Goldsmith},\n title = {Handbook of affective sciences.},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Handbook of affective sciences.,2003.0
375,1e9ac01955e4d6eb74b9a84d6db4d34ae4189139,"Emotion-specific activity in the autonomic nervous system was generated by constructing facial prototypes of emotion muscle by muscle and by reliving past emotional experiences. The autonomic activity produced distinguished not only between positive and negative emotions, but also among negative emotions. This finding challenges emotion theories that have proposed autonomic activity to be undifferentiated or that have failed to address the implications of autonomic differentiation in emotion.","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '2001910', 'name': 'R. Levenson'}, {'authorId': '1388284460', 'name': 'W. Friesen'}]",2203.0,"{'bibtex': '@Article{Ekman1983AutonomicNS,\n author = {P. Ekman and R. Levenson and W. Friesen},\n journal = {Science},\n pages = {\n          1208-10\n        },\n title = {Autonomic nervous system activity distinguishes among emotions.},\n volume = {221 4616},\n year = {1983}\n}\n'}",,"{'volume': '221 4616', 'pages': '\n          1208-10\n        ', 'name': 'Science'}",20.0,Autonomic nervous system activity distinguishes among emotions.,1983.0
376,1ea72869254a3ea03d76cdd27cf36ca1d5698efc,,"[{'authorId': '82128751', 'name': 'M. Hoffman'}]",511.0,"{'bibtex': '@Inproceedings{Hoffman1985InteractionOA,\n author = {M. Hoffman},\n title = {Interaction of affect and cognition in empathy.},\n year = {1985}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Interaction of affect and cognition in empathy.,1985.0
377,1eb30effd2270bacad81d417293283441729056f,"Recently, several systems have been proposed which display a virtual display in a VR space and provide a working environment equivalent to everyday life. In particular, “Mikulus”[15] provides an environment where a CG agent is always sitting next to users. In this paper, we focused on the experience of evoking fear and revealed the difference in emotional evoked from experience by CG agent in the virtual space. In the experiment, we used film to evoke fear emotion, we compared conditions for viewing with one participant with the condition of CG agent. We investigated the emotion evoked from the experience by rating the valence/arousal dimension scales and 18 emotion intensity scales. Experimental results showed that there is a difference in the emotion evoked from the experience by CG agent in the virtual space.","[{'authorId': '2075020479', 'name': 'Nanako Takai'}, {'authorId': '1796760', 'name': 'Homei Miyashita'}]",0.0,"{'bibtex': '@Inproceedings{Takai2016InfluenceBC,\n author = {Nanako Takai and Homei Miyashita},\n title = {Influence by CG Agent in the Virtual Space to Emotion at the Time of Viewing Fear Video},\n year = {2016}\n}\n'}",[],,2.0,Influence by CG Agent in the Virtual Space to Emotion at the Time of Viewing Fear Video,2016.0
378,1ec1a43b39ee04e4ac9e76cf8a6ad114e11f03d8,,"[{'authorId': '4446717', 'name': 'H. Eysenck'}]",67.0,"{'bibtex': '@Inproceedings{Eysenck1991DimensionsOP,\n author = {H. Eysenck},\n title = {Dimensions of personality: The biosocial approach to personality.},\n year = {1991}\n}\n'}",,"{'volume': '', 'name': ''}",56.0,Dimensions of personality: The biosocial approach to personality.,1991.0
379,1ecf14d63f0e5f6868b111c8cc65325161b6a003,"The OCC (Ortony, Clore, & Collins, 1988) model has established itself as the standard model for emotion synthesis. A large number of studies employed the OCC model to generate emotions for their embodied characters. Many developers of such characters believe that the OCC model will be all they ever need to equip their character with emotions. This paper points out what the OCC model is able to do for an embodied emotional character and what it does not. Missing features include a history function, a personality designer and the interaction of the emotional categories.","[{'authorId': '1728894', 'name': 'C. Bartneck'}]",151.0,"{'bibtex': '@Inproceedings{Bartneck2002IntegratingTO,\n author = {C. Bartneck},\n title = {Integrating the OCC model of emotions in embodied characters},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",16.0,Integrating the OCC model of emotions in embodied characters,2002.0
380,1f2186c2a89c4b39ca370ecede10744f8b593073,,"[{'authorId': '2240187143', 'name': 'S. Messenger'}]",643.0,"{'bibtex': '@Inproceedings{Messenger2009TheEO,\n author = {S. Messenger},\n title = {The Expression of the Emotions in Man and Animals},\n year = {2009}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Expression of the Emotions in Man and Animals,2009.0
381,1f268af7aef9da4c87dbef6843d69ff7cfafdeb8,"In the social psychology literature, crowds are classified as audiences and mobs. Audiences are passive crowds, whereas mobs are active crowds with emotional, irrational and seemingly homogeneous behavior. In this study, we aim to create a system that enables the specification of different crowd types ranging from audiences to mobs. In order to achieve this goal we parametrize the common properties of mobs to create collective misbehavior. Because mobs are characterized by emotionality, we describe a framework that associates psychological components with individual agents comprising a crowd and yields emergent behaviors in the crowd as a whole. To explore the effectiveness of our framework we demonstrate two scenarios simulating the behavior of distinct mob types.","[{'authorId': '2643744', 'name': 'Funda Durupinar'}, {'authorId': '1746035', 'name': 'U. Güdükbay'}, {'authorId': '3435784', 'name': 'Aytek Aman'}, {'authorId': '1699200', 'name': 'N. Badler'}]",102.0,"{'bibtex': '@Article{Durupinar2016PsychologicalPF,\n author = {Funda Durupinar and U. Güdükbay and Aytek Aman and N. Badler},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {2145-2159},\n title = {Psychological Parameters for Crowd Simulation: From Audiences to Mobs},\n volume = {22},\n year = {2016}\n}\n'}",,"{'volume': '22', 'pages': '2145-2159', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",67.0,Psychological Parameters for Crowd Simulation: From Audiences to Mobs,2016.0
384,1f2ba45dc22678551294935b20fe3c098f85314e,"This paper presents the development of fuzzy-appearance manifold and fuzzy-nearest distance calculation in the eigenspace domain for pose estimation of degraded face images. In order to obtain a robust pose estimation system which can deal with the fuzziness of face data caused by statistical errors, we proposed the fuzzy-vector representation in eigenspace domain of the face images. Using fuzzy-vector representations, all of the crisp vectors of face data in the eigenspace domain are firstly transformed into fuzzy-vectors as fuzzy-points. Next, the fuzzy-appearance manifold is constructed from all the available fuzzy-points and the fuzzy-nearest distance calculation is proposed as the classifier of the pose estimation system. The pose estimation of an unknown face image is performed by firstly being projected onto the eigenspace domain then transformed to become an unknown fuzzy-point, and its fuzzy-distance with all of the available fuzzy-points in the fuzzy-appearance manifold will be calculated. The fuzzy-point in the manifold which has the nearest distance to that unknown fuzzy-point will be determined as the pose position of the unknown face image. In the experiment, face images with various quality degradation effects were used. The results show that the system could maintain high recognition rates for estimating the pose position of the degraded face images.","[{'authorId': '2550523', 'name': 'B. Kusumoputro'}, {'authorId': '2076423895', 'name': 'Lina'}]",224.0,"{'bibtex': '@Article{Kusumoputro2013FuzzyAppearanceMA,\n author = {B. Kusumoputro and Lina},\n journal = {International Journal of Advanced Robotic Systems},\n title = {Fuzzy-Appearance Manifold and Fuzzy-Nearest Distance Calculation for Model-Less 3D Pose Estimation of Degraded Face Images},\n volume = {10},\n year = {2013}\n}\n'}",,"{'volume': '10', 'name': 'International Journal of Advanced Robotic Systems'}",18.0,Fuzzy-Appearance Manifold and Fuzzy-Nearest Distance Calculation for Model-Less 3D Pose Estimation of Degraded Face Images,2013.0
385,1f2bc5d57ccbf5a04e7fea87f1f4db464f533ca8,"In this paper, we review recent deep learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games, and real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.","[{'authorId': '2775866', 'name': 'Niels Justesen'}, {'authorId': '14171685', 'name': 'Philip Bontrager'}, {'authorId': '1810053', 'name': 'J. Togelius'}, {'authorId': '1745664', 'name': 'S. Risi'}]",177.0,"{'bibtex': '@Article{Justesen2017DeepLF,\n author = {Niels Justesen and Philip Bontrager and J. Togelius and S. Risi},\n journal = {IEEE Transactions on Games},\n pages = {1-20},\n title = {Deep Learning for Video Game Playing},\n volume = {12},\n year = {2017}\n}\n'}",,"{'volume': '12', 'pages': '1-20', 'name': 'IEEE Transactions on Games'}",182.0,Deep Learning for Video Game Playing,2017.0
386,1f43724e9c47159c33dd3dbbb97efec885e9fab8,"Previous evidence suggests that eye-contact serves a number of different functions in two-person encounters, of which one of the most important is gathering feed-back on the other person's reactions. It is further postulated that eye-contact is linked to affiliative motivation, and that approach and avoidance forces produce an equilibrium level of physical proximity, eyecontact and other aspects of intimacy. If one of these is disturbed, compensatory changes may occur along the other dimensions. Experiments are reported which suggest that people move towards an equilibrium distance, and adopt a particular level of eye-contact. As predicted, there was less eyecontact and glances were shorter, the closer two subjects were placed together (where one member of each pair was a confederate who gazed continuously at the other). The effect was greatest for opposite-sex pairs. In another experiment it was found that subjects would stand closer to a second person when his eyes were shut, as predicted by the theory.","[{'authorId': '2409411', 'name': 'M. Argyle'}, {'authorId': '11938736', 'name': 'Janet B. Dean'}]",1738.0,"{'bibtex': '@Article{Argyle1965EYECONTACTDA,\n author = {M. Argyle and Janet B. Dean},\n journal = {Sociometry},\n pages = {\n          289-304\n        },\n title = {EYE-CONTACT, DISTANCE AND AFFILIATION.},\n volume = {28},\n year = {1965}\n}\n'}",,"{'volume': '28', 'pages': '\n          289-304\n        ', 'name': 'Sociometry'}",0.0,"EYE-CONTACT, DISTANCE AND AFFILIATION.",1965.0
387,1f5a6d5593829bf27be97db7571b58f3b627fb84,"Although research on the nonverbal expression of emotion has played a prominent role throughout psychology during the past two decades—including an instrumental role in the development of contemporary evolutionary psychology—little research has focused on the evolutionary origins and functions of the emotional expressions themselves. However, recent findings from psychophysical, comparative, social, and cross-cultural psychology are converging to produce a compelling functionalist account, suggesting that emotional expressions serve critical adaptive purposes. Most of these studies have narrowly focused on single emotions—an approach that has been very useful for providing new insights about specific expressions but not for developing a broader understanding of why humans universally display and recognize distinct emotions. Here we unify these disparate findings in order to illuminate this fundamental form of social communication.","[{'authorId': '1999088', 'name': 'A. Shariff'}, {'authorId': '37930132', 'name': 'J. Tracy'}]",270.0,"{'bibtex': '@Article{Shariff2011WhatAE,\n author = {A. Shariff and J. Tracy},\n journal = {Current Directions in Psychological Science},\n pages = {395 - 399},\n title = {What Are Emotion Expressions For?},\n volume = {20},\n year = {2011}\n}\n'}",,"{'volume': '20', 'pages': '395 - 399', 'name': 'Current Directions in Psychological Science'}",35.0,What Are Emotion Expressions For?,2011.0
388,1f92d55788cd2c190e8091b677f27a4e04091c16,"We present a new technique to generate heterogeneous crowd behaviors using personality trait theory. Our formulation is based on adopting results of a user study to derive a mapping from crowd simulation parameters to the perceived behaviors of agents in computer-generated crowd simulations. We also derive a linear mapping between simulation parameters and personality descriptors corresponding to the well-established Eysenck Three-factor personality model. Furthermore, we propose a novel two-dimensional factorization of perceived personality in crowds based on a statistical analysis of the user study results. Finally, we demonstrate that our mappings and factorizations can be used to generate heterogeneous crowd behaviors in different settings.","[{'authorId': '35170565', 'name': 'S. Guy'}, {'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '144247566', 'name': 'M. Lin'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",187.0,"{'bibtex': '@Inproceedings{Guy2011SimulatingHC,\n author = {S. Guy and Sujeong Kim and M. Lin and Dinesh Manocha},\n pages = {43-52},\n title = {Simulating heterogeneous crowd behaviors using personality trait theory},\n year = {2011}\n}\n'}",,{'pages': '43-52'},28.0,Simulating heterogeneous crowd behaviors using personality trait theory,2011.0
389,1f95ede26b61c14f685c3a65d601492b693cda5a,"From past research it is well known that social exclusion has detrimental consequences for mental health. To deal with these adverse effects, socially excluded individuals frequently turn to other humans for emotional support. While chatbots can elicit social and emotional responses on the part of the human interlocutor, their effectiveness in the context of social exclusion has not been investigated. In the present study, we examined whether an empathic chatbot can serve as a buffer against the adverse effects of social ostracism. After experiencing exclusion on social media, participants were randomly assigned to either talk with an empathetic chatbot about it (e.g., “I’m sorry that this happened to you”) or a control condition where their responses were merely acknowledged (e.g., “Thank you for your feedback”). Replicating previous research, results revealed that experiences of social exclusion dampened the mood of participants. Interacting with an empathetic chatbot, however, appeared to have a mitigating impact. In particular, participants in the chatbot intervention condition reported higher mood than those in the control condition. Theoretical, methodological, and practical implications, as well as directions for future research are discussed.","[{'authorId': '1490663549', 'name': 'Mauro de Gennaro'}, {'authorId': '50755536', 'name': 'Eva G. Krumhuber'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}]",73.0,"{'bibtex': '@Article{Gennaro2020EffectivenessOA,\n author = {Mauro de Gennaro and Eva G. Krumhuber and Gale M. Lucas},\n journal = {Frontiers in Psychology},\n title = {Effectiveness of an Empathic Chatbot in Combating Adverse Effects of Social Exclusion on Mood},\n volume = {10},\n year = {2020}\n}\n'}",,"{'volume': '10', 'name': 'Frontiers in Psychology'}",149.0,Effectiveness of an Empathic Chatbot in Combating Adverse Effects of Social Exclusion on Mood,2020.0
390,1f997cae6989be7348492677554e84276f3c602b,"OBJECTIVE
Genetic and neurofunctional research in autism has highlighted the need for improved characterization of the core social disorder defining the broad spectrum of syndrome manifestations.


METHOD
This article reviews the advantages and limitations of current methods for the refinement and quantification of this highly heterogeneous social phenotype.


RESULTS
The study of social visual pursuit by use of eye-tracking technology is offered as a paradigm for novel tools incorporating these requirements and as a research effort that builds on the emerging synergy of different branches of social neuroscience.


CONCLUSIONS
Advances in the area will require increased consideration of processes underlying experimental results and a closer approximation of experimental methods to the naturalistic demands inherent in real-life social situations.","[{'authorId': '6261339', 'name': 'A. Klin'}, {'authorId': '8137420', 'name': 'W. Jones'}, {'authorId': '145157155', 'name': 'R. Schultz'}, {'authorId': '2155644', 'name': 'F. Volkmar'}, {'authorId': '48858469', 'name': 'D. Cohen'}]",445.0,"{'bibtex': '@Article{Klin2002DefiningAQ,\n author = {A. Klin and W. Jones and R. Schultz and F. Volkmar and D. Cohen},\n journal = {The American journal of psychiatry},\n pages = {\n          895-908\n        },\n title = {Defining and quantifying the social phenotype in autism.},\n volume = {159 6},\n year = {2002}\n}\n'}",,"{'volume': '159 6', 'pages': '\n          895-908\n        ', 'name': 'The American journal of psychiatry'}",58.0,Defining and quantifying the social phenotype in autism.,2002.0
391,1fe261fb9bf35591b1d86dcd7ea4d6aa4d4ffca6,,"[{'authorId': '3471157', 'name': 'Suman Ojha'}, {'authorId': '144826976', 'name': 'Mary-Anne Williams'}]",15.0,"{'bibtex': '@Inproceedings{Ojha2016EthicallyGuidedER,\n author = {Suman Ojha and Mary-Anne Williams},\n pages = {233-242},\n title = {Ethically-Guided Emotional Responses for Social Robots: Should I Be Angry?},\n year = {2016}\n}\n'}",,{'pages': '233-242'},26.0,Ethically-Guided Emotional Responses for Social Robots: Should I Be Angry?,2016.0
392,1fe2f8fdcb94c4872f78198310ea367bcb0e7160,"The role of cognition--and to some extent motivation--in emotion, the ways meaning is generated, unconscious appraising, and the implications of this way of thinking for life-span development are addressed. It is argued that appraisal is a necessary as well as sufficient cause of emotion and that knowledge is necessary but not sufficient. This position is examined in light of what is known about emotions in infants and young children, the effects of drugs on acute emotions and moods, and recent patterns of thought about the brain in emotions. The discussion of how meaning is generated is the core of the article. Automatic processing without awareness is contrasted with deliberate and conscious processing, and the concept of resonance between an animal's needs and what is encountered in the environment is examined. The idea that there is more than one way meaning is achieved strengthens and enriches the case for the role of appraisal in emotion and allows the consideration of what is meant by unconscious and preconscious appraisal and the examination of how they might work.","[{'authorId': '5628684', 'name': 'R. Lazarus'}]",1568.0,"{'bibtex': '@Article{Lazarus1991CognitionAM,\n author = {R. Lazarus},\n journal = {The American psychologist},\n pages = {\n          352-67\n        },\n title = {Cognition and motivation in emotion.},\n volume = {46 4},\n year = {1991}\n}\n'}",,"{'volume': '46 4', 'pages': '\n          352-67\n        ', 'name': 'The American psychologist'}",103.0,Cognition and motivation in emotion.,1991.0
394,1ff3c02d1d1f9183072a08e1c428dc8ad3840f3b,"The first scientific text on the psychology of memory. Relating retention to repetition, describing the shape of the forgetting curve, and measuring strength of association, Hermann Ebbinghaus extended the province of systematic, experimental research to the higher mental processes.","[{'authorId': '70675558', 'name': 'H. Ebbinghaus'}]",687.0,"{'bibtex': '@Article{Ebbinghaus1987MemoryAC,\n author = {H. Ebbinghaus},\n journal = {Annals of neurosciences},\n pages = {\n          155-6\n        },\n title = {Memory: a contribution to experimental psychology.},\n volume = {20 4},\n year = {1987}\n}\n'}",,"{'volume': '20 4', 'pages': '\n          155-6\n        ', 'name': 'Annals of neurosciences'}",0.0,Memory: a contribution to experimental psychology.,1987.0
395,20272d748f5502f34fc4988228fb2377f7dd29f7,,[],70.0,"{'bibtex': '@Misc{None,\n title = {Auton Agent Multi-Agent Syst DOI 10.1007/s10458-009-9093-x Modeling appraisal in theory of mind reasoning}\n}\n'}",,,0.0,Auton Agent Multi-Agent Syst DOI 10.1007/s10458-009-9093-x Modeling appraisal in theory of mind reasoning,
398,20494dca1b47567e9a6d36937647187b238b9fdc,,"[{'authorId': '1802934', 'name': 'A. Ortony'}, {'authorId': '1728478', 'name': 'D. Norman'}, {'authorId': '2443754', 'name': 'W. Revelle'}]",188.0,"{'bibtex': '@Inproceedings{Ortony2005AffectAP,\n author = {A. Ortony and D. Norman and W. Revelle},\n pages = {173-202},\n title = {Affect and Proto-Affect in Effective Functioning},\n year = {2005}\n}\n'}",,{'pages': '173-202'},59.0,Affect and Proto-Affect in Effective Functioning,2005.0
399,204cbf99a359f38599ed0ebd0d84c6bc50c21c80,"We present a generative adversarial network to synthesize 3D pose sequences of co-speech upper-body gestures with appropriate affective expressions. Our network consists of two components: a generator to synthesize gestures from a joint embedding space of features encoded from the input speech and the seed poses, and a discriminator to distinguish between the synthesized pose sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral coefficients and the text transcript computed from the input speech in separate encoders in our generator to learn the desired sentiments and the associated affective cues. We design an affective encoder using multi-scale spatial-temporal graph convolutions to transform 3D pose sequences into latent, pose-based affective features. We use our affective encoder in both our generator, where it learns affective features from the seed poses to guide the gesture synthesis, and our discriminator, where it enforces the synthesized gestures to contain the appropriate affective expressions. We perform extensive evaluations on two benchmark datasets for gesture synthesis from the speech, the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the best baselines, we improve the mean absolute joint error by 10-33%, the mean acceleration difference by 8-58%, and the Fréchet Gesture Distance by 21-34%. We also conduct a user study and observe that compared to the best current baselines, around 15.28% of participants indicated our synthesized gestures appear more plausible, and around 16.32% of participants felt the gestures had more appropriate affective expressions aligned with the speech.","[{'authorId': '50227009', 'name': 'Uttaran Bhattacharya'}, {'authorId': '2122327690', 'name': 'Elizabeth Childs'}, {'authorId': '10172108', 'name': 'Nicholas Rewkowski'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",45.0,"{'bibtex': '@Article{Bhattacharya2021Speech2AffectiveGesturesSC,\n author = {Uttaran Bhattacharya and Elizabeth Childs and Nicholas Rewkowski and Dinesh Manocha},\n journal = {Proceedings of the 29th ACM International Conference on Multimedia},\n title = {Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning},\n year = {2021}\n}\n'}",,{'name': 'Proceedings of the 29th ACM International Conference on Multimedia'},63.0,Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning,2021.0
400,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","[{'authorId': '40348417', 'name': 'Ashish Vaswani'}, {'authorId': '1846258', 'name': 'Noam M. Shazeer'}, {'authorId': '3877127', 'name': 'Niki Parmar'}, {'authorId': '39328010', 'name': 'Jakob Uszkoreit'}, {'authorId': '145024664', 'name': 'Llion Jones'}, {'authorId': '19177000', 'name': 'Aidan N. Gomez'}, {'authorId': '40527594', 'name': 'Lukasz Kaiser'}, {'authorId': '3443442', 'name': 'Illia Polosukhin'}]",75517.0,"{'bibtex': '@Inproceedings{Vaswani2017AttentionIA,\n author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n pages = {5998-6008},\n title = {Attention is All you Need},\n year = {2017}\n}\n'}",,{'pages': '5998-6008'},42.0,Attention is All you Need,2017.0
402,207f821cb8374e615a5ad6cd902653411806a24b,"Abstract Over the last 15 years, a virtual revolution has taken place in the use of Virtual Reality simulation technology for clinical purposes. Shifts in the social and scientific landscape have now set the stage for the next major movement in Clinical Virtual Reality with the “birth” of intelligent virtual humans. Seminal research and development has appeared in the creation of highly interactive, artificially intelligent and natural language capable virtual human agents that can engage real human users in a credible fashion. No longer at the level of a prop to add context or minimal faux interaction in a virtual world, virtual humans can be designed to perceive and act in a 3D virtual world, engage in spoken dialogs with real users and can be capable of exhibiting human-like emotional reactions. This paper will present an overview of the SimCoach project that aims to develop virtual human support agents to serve as online guides for promoting access to psychological healthcare information and for assisting military personnel and family members in breaking down barriers to initiating care. The SimCoach experience is being designed to attract and engage military Service Members, Veterans and their significant others who might not otherwise seek help with a live healthcare provider. It is expected that this experience will motivate users to take the first step – to empower themselves to seek advice and information regarding their healthcare and general personal welfare and encourage them to take the next step towards seeking other, more formal resources if needed.","[{'authorId': '2255096086', 'name': 'Albert Rizzo'}, {'authorId': '2255149426', 'name': 'Belinda Lange'}, {'authorId': '145024149', 'name': 'J. Galen Buckwalter'}, {'authorId': '1757166', 'name': 'Kenji Sagae'}, {'authorId': '2257313920', 'name': 'Julia Kim'}, {'authorId': '2257213198', 'name': 'Kenji Sagae'}, {'authorId': '2257375476', 'name': 'Josh Williams'}, {'authorId': '2572088', 'name': 'J. Difede'}, {'authorId': '1831766', 'name': 'B. Rothbaum'}, {'authorId': '2244890836', 'name': 'Greg Reger'}, {'authorId': '2244890070', 'name': 'Thomas Parsons'}, {'authorId': '2264170075', 'name': 'Patrick Kenny'}]",85.0,"{'bibtex': '@Inproceedings{Rizzo2011SimCoachAI,\n author = {Albert Rizzo and Belinda Lange and J. Galen Buckwalter and Kenji Sagae and Julia Kim and Kenji Sagae and Josh Williams and J. Difede and B. Rothbaum and Greg Reger and Thomas Parsons and Patrick Kenny},\n pages = {277 - 281},\n title = {SimCoach: an intelligent virtual human system for providing healthcare information and support},\n volume = {10},\n year = {2011}\n}\n'}",,"{'volume': '10', 'pages': '277 - 281'}",43.0,SimCoach: an intelligent virtual human system for providing healthcare information and support,2011.0
403,2092d7d2154d6e28c4a27607d9702c296da35b3c,"Learning and work-oriented assistive systems are often designed to fit the workflow of neurotypical workers. Neurodivergent workers and individuals with learning disabilities often present cognitive and sensorimotor characteristics that are better accommodated with personalized learning and working processes. Therefore, we designed an adaptive learning system that combines an augmented interaction space with user-sensitive virtual assistance to support step-by-step guidance for neurodivergent workers in electronic assembling tasks. Gamified learning elements were also included in the interface to provide self-motivation and praise whenever users progress in their learning and work achievements.","[{'authorId': '1932186671', 'name': 'Jonas Grund'}, {'authorId': '1780741096', 'name': 'Moritz Umfahrer'}, {'authorId': '71985197', 'name': 'Lea Buchweitz'}, {'authorId': '1780755254', 'name': 'James Gay'}, {'authorId': '1791011689', 'name': 'A. Theil'}, {'authorId': '50069571', 'name': 'Oliver Korn'}]",7.0,"{'bibtex': '@Article{Grund2020AGA,\n author = {Jonas Grund and Moritz Umfahrer and Lea Buchweitz and James Gay and A. Theil and Oliver Korn},\n journal = {Proceedings of Mensch und Computer 2020},\n title = {A gamified and adaptive learning system for neurodivergent workers in electronic assembling tasks},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of Mensch und Computer 2020'},18.0,A gamified and adaptive learning system for neurodivergent workers in electronic assembling tasks,2020.0
404,20ae1a58249bb2fd4bc7f0b72f47f0e50cd6d26f,"Herman, Foreword. Courtois, Ford, Introduction. Part I: Overview. Ford, Courtois, Defining and Understanding Complex Trauma and Complex Traumatic Stress Disorders. Ford, Neurobiological and Developmental Research: Clinical Implications. Ford, Cloitre, Best Practices in Psychotherapy for Children and Adolescents. Courtois, Ford, Cloitre, Best Practices in Psychotherapy for Adults. Briere, Spinazzola, Assessment of the Sequelae of Complex Trauma: Evidence-based Measures. Brown, Attachment and Abuse History, and Adult Attachment Style. Steele, van der Hart, Treating Dissociation. Brown, Cultural Competence. Kinsler, Courtois, Frankel, Therapeutic Alliance and Risk Management. Pearlman, Caringi, Living and Working Self-reflectively to Address Vicarious Trauma. Part II: Individual Treatment Approaches and Strategies. Gold, Contextual Therapy. Jackson, Nissenson, Cloitre, Cognitive-Behavioral Therapy. Follette, Iverson, Ford, Contextual Behavior Trauma Therapy. Fosha, Paivio, Gleiser, Ford, Experiential and Emotion-focused Therapy. Fisher, Ogden, Sensorimotor Psychotherapy. Opler, Grennan, Ford, Pharmacotherapy. Part III: Systemic Treatment Approaches and Strategies. R. C. Schwartz, M. F. Schwartz, Galperin, Internal Family Systems Therapy. Johnson, Courtois, Couple Therapy. Ford, Saltzman, Family Systems Therapy. Ford, Fallot, Harris, Group Therapy. Ford, Courtois, Conclusion: The Clinical Utility of a Complex Traumatic Stress Disorders Framework. van der Kolk, Afterword.","[{'authorId': '1690921', 'name': 'C. Courtois'}, {'authorId': '1411956410', 'name': 'Julian D. Norton-Ford'}, {'authorId': '84268272', 'name': 'J. Herman'}, {'authorId': '5223699', 'name': 'B. Kolk'}]",640.0,"{'bibtex': '@Inproceedings{Courtois2009TreatingCT,\n author = {C. Courtois and Julian D. Norton-Ford and J. Herman and B. Kolk},\n title = {Treating complex traumatic stress disorders: An evidence-based guide.},\n year = {2009}\n}\n'}",,"{'volume': '', 'name': ''}",6.0,Treating complex traumatic stress disorders: An evidence-based guide.,2009.0
405,20e46645368a96070d6ceb5198fe73ba170843f9,"Hayo Reinders, Unitec Institute of Technology Sorada Wattana, Dhurakij Pundit University This paper reports on a study into the effects of digital game play on learners’ Willingness to Communicate (WTC), or individuals’ “readiness to enter into discourse at a particular time with a specific person or persons, using a L2” (MacIntyre, Dörnyei, Clément, & Noels, 1998, p. 547). Thirty Thai learners of English as a foreign language enrolled in a University language course completed six 90–minute lessons playing Ragnarok Online, a popular online role–playing game. The game had been installed on a private server and was thus only available to participants in the study. We modified the game to include special instructions, or quests (missions that players are assigned to accomplish in order to get items and progress throughout the game), designed to encourage collaboration and communication. To gauge participants’ WTC, a series of questionnaires was designed, adapted from MacIntyre et al’s (2001) WTC scale and previous studies on language and communication anxiety (Horwitz, Horwitz, & Cope, 1986; McCroskey & Richmond, 1982) and perceived competence (Compton, 2004; MacIntyre & Charos, 1996). These asked respondents about their (own perceptions of their) willingness to use English, as well as their confidence, anxiety, and perceived communicative competence in communicating in English. The questionnaires were administered at the start of the course, and again after six gaming sessions. Results on the first set of questionnaires showed that students had low confidence, high anxiety, low perceived competence, and low WTC. The second set of results showed a marked and significant improvement, with participants feeling more confident, less anxious, more competent, and more willing to communicate. We argue that the careful construction of tasks that draw on the affordances of games can have a positive effect on the language learning process.","[{'authorId': '145665734', 'name': 'H. Reinders'}, {'authorId': '2466081', 'name': 'S. Wattana'}]",158.0,"{'bibtex': '@Article{Reinders2014CanIS,\n author = {H. Reinders and S. Wattana},\n journal = {Language Learning & Technology},\n pages = {101-123},\n title = {Can I Say Something? The Effects of Digital Game Play on Willingness to Communicate.},\n volume = {18},\n year = {2014}\n}\n'}",,"{'volume': '18', 'pages': '101-123', 'name': 'Language Learning & Technology'}",62.0,Can I Say Something? The Effects of Digital Game Play on Willingness to Communicate.,2014.0
406,20ec8a7397ad6c1b37b408037d9186145eab583e,,"[{'authorId': '48687370', 'name': 'V. Patel'}, {'authorId': '46708406', 'name': 'S. Saxena'}, {'authorId': '144573921', 'name': 'C. Lund'}, {'authorId': '5984519', 'name': 'G. Thornicroft'}, {'authorId': '2487998', 'name': 'F. Baingana'}, {'authorId': '145938826', 'name': 'P. Bolton'}, {'authorId': '120889812', 'name': 'D. Chisholm'}, {'authorId': '3602165', 'name': 'P. Collins'}, {'authorId': '144607720', 'name': 'J. Cooper'}, {'authorId': '39875915', 'name': 'J. Eaton'}, {'authorId': '4270012', 'name': 'H. Herrman'}, {'authorId': '3910191', 'name': 'Mohammad M. Herzallah'}, {'authorId': '2108715340', 'name': 'Yueqin Huang'}, {'authorId': '40235023', 'name': 'M. Jordans'}, {'authorId': '145566744', 'name': 'A. Kleinman'}, {'authorId': '102166921', 'name': 'M. Medina-Mora'}, {'authorId': '48281154', 'name': 'E. Morgan'}, {'authorId': '40504038', 'name': 'Unaiza Niaz'}, {'authorId': '153651466', 'name': 'O. Omigbodun'}, {'authorId': '3765838', 'name': 'M. Prince'}, {'authorId': '2148758359', 'name': 'A. Rahman'}, {'authorId': '6742564', 'name': 'B. Saraceno'}, {'authorId': '153676440', 'name': 'B. Sarkar'}, {'authorId': '144901343', 'name': 'Mary J De Silva'}, {'authorId': '46472839', 'name': 'I. Singh'}, {'authorId': '50497484', 'name': 'Dan J Stein'}, {'authorId': '4498538', 'name': 'C. Sunkel'}, {'authorId': '5105053', 'name': 'J. Unützer'}]",1353.0,"{'bibtex': '@Article{Patel2018TheLC,\n author = {V. Patel and S. Saxena and C. Lund and G. Thornicroft and F. Baingana and P. Bolton and D. Chisholm and P. Collins and J. Cooper and J. Eaton and H. Herrman and Mohammad M. Herzallah and Yueqin Huang and M. Jordans and A. Kleinman and M. Medina-Mora and E. Morgan and Unaiza Niaz and O. Omigbodun and M. Prince and A. Rahman and B. Saraceno and B. Sarkar and Mary J De Silva and I. Singh and Dan J Stein and C. Sunkel and J. Unützer},\n journal = {The Lancet},\n pages = {1553-1598},\n title = {The Lancet Commission on global mental health and sustainable development},\n volume = {392},\n year = {2018}\n}\n'}",,"{'volume': '392', 'pages': '1553-1598', 'name': 'The Lancet'}",272.0,The Lancet Commission on global mental health and sustainable development,2018.0
407,210dfaa504414e5a98c1b013f86c66204dc614a7,"End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.","[{'authorId': '8299781', 'name': 'Weiyan Shi'}, {'authorId': '144007938', 'name': 'Zhou Yu'}]",73.0,"{'bibtex': '@Inproceedings{Shi2018SentimentAE,\n author = {Weiyan Shi and Zhou Yu},\n pages = {1509-1519},\n title = {Sentiment Adaptive End-to-End Dialog Systems},\n year = {2018}\n}\n'}",,{'pages': '1509-1519'},47.0,Sentiment Adaptive End-to-End Dialog Systems,2018.0
408,21197316a8b1cfc3ecd1320c807d7d9299cb2fc7,"Theories of episodic memory need to specify the encoding (representing), storage, and retrieval processes that underlie this form of memory and indicate the brain regions that mediate these processes and how they do so. Representation and re-representation (retrieval) of the spatiotemporally linked series of scenes, which constitute an episode, are probably mediated primarily by those parts of the posterior neocortex that process perceptual and semantic information. However, some role of the frontal neocortex and medial temporal lobes in representing aspects of context and high-level visual object information at encoding and retrieval cannot currently be excluded. Nevertheless, it is widely believed that the frontal neocortex is mainly involved in coordinating episodic encoding and retrieval and that the medial temporal lobes store aspects of episodic information. Establishing where storage is located is very difficult and disagreement remains about the role of the posterior neocortex in episodic memory storage. One view is that this region stores all aspects of episodic memory ab initio for as long as memory lasts. This is compatible with evidence that the amygdala, basal forebrain, and midbrain modulate neocortical storage. Another view is that the posterior neocortex only gradually develops the ability to store some aspects of episodic information as a function of rehearsal over time and that this information is initially stored by the medial temporal lobes. A third view is that the posterior neocortex never stores these aspects of episodic information because the medial temporal lobes store them for as long as memory lasts in an increasingly redundant fashion. The last two views both postulate that the medial temporal lobes initially store contextual markers that serve to cohere featural information stored in the neocortex. Lesion and functional neuroimaging evidence still does not clearly distinguish between these views. Whether the feeling that an episodic memory is familiar depends on retrieving an association between a retrieved episode and this feeling, or by an attribution triggered by a priming process, is unclear. Evidence about whether the hippocampus and medial temporal lobe cortices play different roles in episodic memory is conflicting. Identifying similarities and differences between episodic memory and both semantic memory and priming will require careful componential analysis of episodic memory.","[{'authorId': '2245123725', 'name': 'Andrew R. Mayes'}, {'authorId': '2245125165', 'name': 'Neil Roberts'}]",97.0,"{'bibtex': '@Article{Mayes2001TheoriesOE,\n author = {Andrew R. Mayes and Neil Roberts},\n journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},\n pages = {\n          1395-408\n        },\n title = {Theories of episodic memory.},\n volume = {356 1413},\n year = {2001}\n}\n'}",,"{'volume': '356 1413', 'pages': '\n          1395-408\n        ', 'name': 'Philosophical transactions of the Royal Society of London. Series B, Biological sciences'}",70.0,Theories of episodic memory.,2001.0
409,2154255e3f7d42687c71fcb94b66c5efa5516483,"This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user’s or another agent’s emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent’s situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future.","[{'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '3262504', 'name': 'Hana Boukricha'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",215.0,"{'bibtex': '@Article{Paiva2017EmpathyIV,\n author = {Ana Paiva and Iolanda Leite and Hana Boukricha and I. Wachsmuth},\n journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},\n pages = {1 - 40},\n title = {Empathy in Virtual Agents and Robots},\n volume = {7},\n year = {2017}\n}\n'}",,"{'volume': '7', 'pages': '1 - 40', 'name': 'ACM Transactions on Interactive Intelligent Systems (TiiS)'}",118.0,Empathy in Virtual Agents and Robots,2017.0
411,21676cc3fbaea3e6044b2eef259c94b7515fe7b3,,"[{'authorId': '2100046', 'name': 'B. Robins'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}, {'authorId': '122175509', 'name': 'Te Boekhorst'}, {'authorId': '1807928', 'name': 'A. Billard'}]",702.0,"{'bibtex': '@Article{Robins2005RoboticAI,\n author = {B. Robins and K. Dautenhahn and Te Boekhorst and A. Billard},\n journal = {Universal Access in the Information Society},\n pages = {105-120},\n title = {Robotic assistants in therapy and education of children with autism: can a small humanoid robot help encourage social interaction skills?},\n volume = {4},\n year = {2005}\n}\n'}",,"{'volume': '4', 'pages': '105-120', 'name': 'Universal Access in the Information Society'}",56.0,Robotic assistants in therapy and education of children with autism: can a small humanoid robot help encourage social interaction skills?,2005.0
412,218bc4b63c57b7e1d12c6aa0b11de44a2905b4a8,,"[{'authorId': '145394858', 'name': 'R. Azevedo'}, {'authorId': '50383455', 'name': 'S. A. Martin'}, {'authorId': '37057683', 'name': 'M. Taub'}, {'authorId': '3408438', 'name': 'Nicholas V. Mudrick'}, {'authorId': '3408655', 'name': 'Garrett C. Millar'}, {'authorId': '1931563', 'name': 'Joseph F. Grafsgaard'}]",38.0,"{'bibtex': ""@Inproceedings{Azevedo2016ArePA,\n author = {R. Azevedo and S. A. Martin and M. Taub and Nicholas V. Mudrick and Garrett C. Millar and Joseph F. Grafsgaard},\n pages = {197-207},\n title = {Are Pedagogical Agents' External Regulation Effective in Fostering Learning with Intelligent Tutoring Systems?},\n year = {2016}\n}\n""}",,{'pages': '197-207'},12.0,Are Pedagogical Agents' External Regulation Effective in Fostering Learning with Intelligent Tutoring Systems?,2016.0
413,219f18a20f839c4bb6cec6f5cd81cdec86654e8e,"While research in metacognition has grown significantly in the past 10 years, there has been a relative lack of research devoted to the focused study of the interactions between metacognition and affective processes. Computational models represent a useful tool which can help remedy this situation by constructing causal models of demonstrated correlational relationships, and by generating empirical hypotheses which can be verified experimentally. In this paper we describe enhancements to an existing cognitive‐affective architecture that will enable it to perform a subset of metacognitive functions. We focus on modeling the role of a specific metacognitive factor, the feeling of confidence (FOC), and the anxiety-linked metacognitive strategy of emotionfocused coping.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",27.0,"{'bibtex': '@Inproceedings{Hudlicka2005ModelingIB,\n author = {E. Hudlicka},\n pages = {55-61},\n title = {Modeling interactions between metacognition and emotion in a cognitive architecture},\n year = {2005}\n}\n'}",,{'pages': '55-61'},32.0,Modeling interactions between metacognition and emotion in a cognitive architecture,2005.0
414,219fb62624dd785db5774c46326d3d2b9274e420,,"[{'authorId': '2910342', 'name': 'A. Schotter'}]",2.0,"{'bibtex': '@Inproceedings{Schotter1985ReasonIH,\n author = {A. Schotter},\n title = {Reason in human affairs: Herbert A. Simon, (Standford University Press, Standford, CA, 1983) viii + 107 pp., $10.00},\n year = {1985}\n}\n'}",,,0.0,"Reason in human affairs: Herbert A. Simon, (Standford University Press, Standford, CA, 1983) viii + 107 pp., $10.00",1985.0
415,21c818c688d2106d03a8024ceba95981ba14581a,"23rd European Conference on Modelling and Simulation (ECMS'09), Madrid, Spain, June 9th-12th, 2009","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '2790338', 'name': 'R. Duell'}, {'authorId': '2468373', 'name': 'Z. A. Memon'}, {'authorId': '1726343', 'name': 'Jan Treur'}, {'authorId': '1881843', 'name': 'C. N. V. D. Wal'}]",65.0,"{'bibtex': '@Inproceedings{Bosse2009MultiAgentMF,\n author = {T. Bosse and R. Duell and Z. A. Memon and Jan Treur and C. N. V. D. Wal},\n pages = {212-218},\n title = {Multi-Agent Model For Mutual Absorption Of Emotions},\n year = {2009}\n}\n'}",,{'pages': '212-218'},19.0,Multi-Agent Model For Mutual Absorption Of Emotions,2009.0
417,21cad882fea264facb716abf561b694e03b9c1f3,,"[{'authorId': '3019359', 'name': 'N. Marwan'}, {'authorId': '143995301', 'name': 'M. Romano'}, {'authorId': '33599229', 'name': 'M. Thiel'}, {'authorId': '143842718', 'name': 'J. Kurths'}]",3031.0,"{'bibtex': '@Inproceedings{Marwan2009RecurrencePF,\n author = {N. Marwan and M. Romano and M. Thiel and J. Kurths},\n pages = {8792},\n title = {Recurrence plots for the analysis of complex systems},\n year = {2009}\n}\n'}",,"{'volume': '', 'pages': '8792', 'name': ''}",223.0,Recurrence plots for the analysis of complex systems,2009.0
418,21de89f776d17ae0c39d0ab0f37dd34fcdf61906,,"[{'authorId': '2145232448', 'name': 'K. M. Lee'}, {'authorId': '3669749', 'name': 'Younbo Jung'}, {'authorId': '2109187332', 'name': 'Jaywoo Kim'}, {'authorId': '2109830893', 'name': 'Sang Ryong Kim'}]",405.0,"{'bibtex': ""@Article{Lee2006ArePE,\n author = {K. M. Lee and Younbo Jung and Jaywoo Kim and Sang Ryong Kim},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {962-973},\n title = {Are physically embodied social agents better than disembodied social agents?: The effects of physical embodiment, tactile interaction, and people's loneliness in human-robot interaction},\n volume = {64},\n year = {2006}\n}\n""}",,"{'volume': '64', 'pages': '962-973', 'name': 'Int. J. Hum. Comput. Stud.'}",29.0,"Are physically embodied social agents better than disembodied social agents?: The effects of physical embodiment, tactile interaction, and people's loneliness in human-robot interaction",2006.0
419,220a98e19e0da729476175f285360545541d2f9d,"Ever since the publication of Darwin's The Expression of the Emotions in Man and Animals in 1872, questions about the nature and purpose of emotional expressions have represented some of the most i...","[{'authorId': '2238998768', 'name': 'Gerben A. Van Kleef'}]",31.0,"{'bibtex': '@Article{Kleef2017TheSE,\n author = {Gerben A. Van Kleef},\n journal = {Psychological Inquiry},\n pages = {211 - 216},\n title = {The Social Effects of Emotions are Functionally Equivalent Across Expressive Modalities},\n volume = {28},\n year = {2017}\n}\n'}",,"{'volume': '28', 'pages': '211 - 216', 'name': 'Psychological Inquiry'}",66.0,The Social Effects of Emotions are Functionally Equivalent Across Expressive Modalities,2017.0
420,2212fcf7a223480c056c15100ada068cd0532d07,"A human-computer interaction (HCI) agent was designed and built to support users in their ability to recover from negative emotional states, particularly frustration. The agent uses social-affective feedback strategies delivered to the user with text-only interaction. The agent's effectiveness was evaluated against two control conditions: (1) user's emotions were ignored, and (2) users were able to report problems and ""vent"" their feelings and thoughts to the computer. Behavioral results showed that the agent was significantly more effective than the control conditions in helping relieve frustration levels.","[{'authorId': '2114063815', 'name': 'Jonathan Klein'}, {'authorId': '33875827', 'name': 'Youngme Moon'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",129.0,"{'bibtex': ""@Article{Klein1999ThisCR,\n author = {Jonathan Klein and Youngme Moon and Rosalind W. Picard},\n journal = {CHI '99 Extended Abstracts on Human Factors in Computing Systems},\n title = {This computer responds to user frustration},\n year = {1999}\n}\n""}",,"{'name': ""CHI '99 Extended Abstracts on Human Factors in Computing Systems""}",5.0,This computer responds to user frustration,1999.0
421,2224c4350bf6c9bcb20607102442a1936160d592,"The ability to display emotions is a key feature in human communication and also for robots that are expected to interact with humans in social environments. For expressions based on Body Movement and other signals than facial expressions, like Sound, no common grounds have been established so far. Based on psychological research on human expression of emotions and perception of emotional stimuli we created eight different expressional designs for the emotions Anger, Sadness, Fear and Joy, consisting of Body Movements, Sounds and Eye Colors. In a large pre-test we evaluated the recognition ratios for the different expressional designs. In our main experiment we separated the expressional designs into their single cues (Body Movement, Sound, Eye Color) and evaluated their expressivity. The detailed view at the perception of our expressional cues, allowed us to evaluate the appropriateness of the stimuli, check our implementations for flaws and build a basis for systematical revision. Our analysis revealed that almost all Body Movements were appropriate for their target emotion and that some of our Sounds need a revision. Eye Colors could be identified as an unreliable component for emotional expression.","[{'authorId': '2056373140', 'name': 'M. Häring'}, {'authorId': '1790555', 'name': 'Nikolaus Bee'}, {'authorId': '1742930', 'name': 'E. André'}]",128.0,"{'bibtex': '@Article{Häring2011CreationAE,\n author = {M. Häring and Nikolaus Bee and E. André},\n journal = {2011 RO-MAN},\n pages = {204-209},\n title = {Creation and Evaluation of emotion expression with body movement, sound and eye color for humanoid robots},\n year = {2011}\n}\n'}",,"{'pages': '204-209', 'name': '2011 RO-MAN'}",17.0,"Creation and Evaluation of emotion expression with body movement, sound and eye color for humanoid robots",2011.0
422,2237e945ad8e4589f96b7aa1fa5708d01c5b1590,,"[{'authorId': '48235291', 'name': 'E. Aronson'}, {'authorId': '39850782', 'name': 'D. Linder'}]",346.0,"{'bibtex': '@Article{Aronson1965GainAL,\n author = {E. Aronson and D. Linder},\n journal = {Journal of Experimental Social Psychology},\n pages = {156-171},\n title = {Gain and loss of esteem as determinants of interpersonal attractiveness},\n volume = {1},\n year = {1965}\n}\n'}",,"{'volume': '1', 'pages': '156-171', 'name': 'Journal of Experimental Social Psychology'}",16.0,Gain and loss of esteem as determinants of interpersonal attractiveness,1965.0
423,225ab689f41cef1dc18237ef5dab059a49950abf,"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward; 2) exploration with no extrinsic reward; and 3) generalization to unseen scenarios (e.g. new levels of the same game).","[{'authorId': '38236002', 'name': 'Deepak Pathak'}, {'authorId': '33932184', 'name': 'Pulkit Agrawal'}, {'authorId': '1763086', 'name': 'Alexei A. Efros'}, {'authorId': '1753210', 'name': 'Trevor Darrell'}]",1952.0,"{'bibtex': '@Article{Pathak2017CuriosityDrivenEB,\n author = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {488-489},\n title = {Curiosity-Driven Exploration by Self-Supervised Prediction},\n year = {2017}\n}\n'}",,"{'pages': '488-489', 'name': '2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)'}",49.0,Curiosity-Driven Exploration by Self-Supervised Prediction,2017.0
424,225cfb48f28d74b40ac382d2c1120f6e0e4e372c,"The ability of artificial characters to express emotions is essential for the natural interaction with humans. Their absence could be interpreted as coldness towards the user. Artificial characters can have different embodiments. Screen characters and robotic characters are currently among the most widely used. This study investigates the influence of the character’s embodiment on how users perceive the character’s emotional expressions. The results show that there is no significant difference in the perceived intensity and recognition accuracy between a robotic character and a screen character. Another important aspect of the character is its ability to express different emotional intensity levels. Developers create different geometrical intensity levels of emotional expressions by equally dividing the spatial difference of each facial component between the neutral and maximum expression. However, the relationship between this geometrical intensity and the intensity perceived by the user might not be strictly linear. This study shows that also a quadratic trend is present in this relationship and that10% steps increase of geometrical intensity can often be distinguished whereas 20% steps can be distinguished almost all the time.","[{'authorId': '1728894', 'name': 'C. Bartneck'}, {'authorId': '153346247', 'name': 'Juliane Reichenbach'}, {'authorId': '101207609', 'name': 'Ajn Albert van Breemen'}]",148.0,"{'bibtex': ""@Inproceedings{Bartneck2004InYF,\n author = {C. Bartneck and Juliane Reichenbach and Ajn Albert van Breemen},\n title = {In your face, robot! The influence of a character's embodiment on how users perceive its emotional expressions},\n year = {2004}\n}\n""}",,"{'volume': '', 'name': ''}",36.0,"In your face, robot! The influence of a character's embodiment on how users perceive its emotional expressions",2004.0
426,2260975bc3a328911958d14a9a59caec270c7b8b,,"[{'authorId': '3119182', 'name': 'Andrea Omicini'}, {'authorId': '1880804', 'name': 'Sascha Ossowski'}]",87.0,"{'bibtex': '@Inproceedings{Omicini2003ObjectiveVS,\n author = {Andrea Omicini and Sascha Ossowski},\n pages = {179-202},\n title = {Objective versus Subjective Coordination in the Engineering of Agent Systems},\n year = {2003}\n}\n'}",,{'pages': '179-202'},43.0,Objective versus Subjective Coordination in the Engineering of Agent Systems,2003.0
427,2261a7515266befb023c7a6cffe991d329a5534c,,"[{'authorId': '6797545', 'name': 'A. Schore'}]",311.0,"{'bibtex': '@Inproceedings{Schore2003AffectDA,\n author = {A. Schore},\n title = {Affect Dysregulation and Disorders of the Self},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",2.0,Affect Dysregulation and Disorders of the Self,2003.0
428,227901fca5cfb9aad3832b8cea56a96fcde30cfd,,"[{'authorId': '2581311', 'name': 'M. Berking'}, {'authorId': '2058160542', 'name': 'C. Meier'}, {'authorId': '6883965', 'name': 'Peggilee Wupperman'}]",166.0,"{'bibtex': '@Article{Berking2010EnhancingES,\n author = {M. Berking and C. Meier and Peggilee Wupperman},\n journal = {Behavior therapy},\n pages = {\n          329-39\n        },\n title = {Enhancing emotion-regulation skills in police officers: results of a pilot controlled study.},\n volume = {41 3},\n year = {2010}\n}\n'}",,"{'volume': '41 3', 'pages': '\n          329-39\n        ', 'name': 'Behavior therapy'}",56.0,Enhancing emotion-regulation skills in police officers: results of a pilot controlled study.,2010.0
429,22870d00a8664a4dd55a2700b87dc0824cbc4922,"It is now easy to nd examples of interactive software agents and animated creatures that have the ability to express emotion; this paper describes research for giving them the ability to recognize emotion. The ability to recognize a per-son's emotions is a key aspect of human \emo-tional intelligence,"" which has been described by a number of scientists as being more important to success in life than are the traditional forms of mathematical and verbal intelligence. This paper describes research underway in emotion recognition at the MIT Media Lab, especially research involving new wearable interfaces.","[{'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",69.0,"{'bibtex': '@Inproceedings{Picard1998TowardAT,\n author = {Rosalind W. Picard},\n title = {Toward Agents that Recognize Emotion},\n year = {1998}\n}\n'}",,"{'volume': '', 'name': ''}",12.0,Toward Agents that Recognize Emotion,1998.0
430,228d5c053f71c6da22e56e2a5ba93f4bdb5a787d,,"[{'authorId': '46450294', 'name': 'Mark Coeckelbergh'}, {'authorId': '1718066', 'name': 'C. Pop'}, {'authorId': '1753188', 'name': 'Ramona Simut'}, {'authorId': '2333169', 'name': 'Andreea Peca'}, {'authorId': '2413277', 'name': 'S. Pintea'}, {'authorId': '48356644', 'name': 'Daniel O. David'}, {'authorId': '1687831', 'name': 'B. Vanderborght'}]",172.0,"{'bibtex': '@Article{Coeckelbergh2015ASO,\n author = {Mark Coeckelbergh and C. Pop and Ramona Simut and Andreea Peca and S. Pintea and Daniel O. David and B. Vanderborght},\n journal = {Science and Engineering Ethics},\n pages = {47 - 65},\n title = {A Survey of Expectations About the Role of Robots in Robot-Assisted Therapy for Children with ASD: Ethical Acceptability, Trust, Sociability, Appearance, and Attachment},\n volume = {22},\n year = {2015}\n}\n'}",,"{'volume': '22', 'pages': '47 - 65', 'name': 'Science and Engineering Ethics'}",53.0,"A Survey of Expectations About the Role of Robots in Robot-Assisted Therapy for Children with ASD: Ethical Acceptability, Trust, Sociability, Appearance, and Attachment",2015.0
431,22b63ae6444b014fadeea9e18eae595820c099d5,,"[{'authorId': '1403309968', 'name': 'N. Adamo-Villani'}, {'authorId': '2734346', 'name': 'Jason Lestina'}, {'authorId': '2095368', 'name': 'Saikiran Anasingaraju'}]",11.0,"{'bibtex': ""@Inproceedings{Adamo-Villani2015DoesCV,\n author = {N. Adamo-Villani and Jason Lestina and Saikiran Anasingaraju},\n pages = {1-8},\n title = {Does Character's Visual Style Affect Viewer's Perception of Signing Avatars?},\n year = {2015}\n}\n""}",,{'pages': '1-8'},19.0,Does Character's Visual Style Affect Viewer's Perception of Signing Avatars?,2015.0
432,22ba8e8442c9a3ba209368b9dd480ccae46ebd18,"Abstract A theory is proposed that emotions are cognitively based states which co-ordinate quasi-autonomous processes in the nervous system. Emotions provide a biological solution to certain problems of transition between plans, in systems with multiple goals. Their function is to accomplish and maintain these transitions, and to communicate them to ourselves and others. Transitions occur at significant junctures of plans when the evaluation of success in a plan changes. Complex emotions are derived from a small number of basic emotions and arise at junctures of social plans.","[{'authorId': '2297721', 'name': 'K. Oatley'}, {'authorId': '1384194899', 'name': 'P. Johnson-Laird'}]",1796.0,"{'bibtex': '@Article{Oatley1987TowardsAC,\n author = {K. Oatley and P. Johnson-Laird},\n journal = {Cognition & Emotion},\n pages = {29-50},\n title = {Towards a Cognitive Theory of Emotions},\n volume = {1},\n year = {1987}\n}\n'}",,"{'volume': '1', 'pages': '29-50', 'name': 'Cognition & Emotion'}",45.0,Towards a Cognitive Theory of Emotions,1987.0
433,22d03e2d9ea2bdc9dc74d63c281959e0fc4be84c,,"[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}]",1023.0,"{'bibtex': '@Article{Ekman1976MeasuringFM,\n author = {P. Ekman and Wallace V. Friesen},\n journal = {Environmental psychology and nonverbal behavior},\n pages = {56-75},\n title = {Measuring facial movement},\n volume = {1},\n year = {1976}\n}\n'}",,"{'volume': '1', 'pages': '56-75', 'name': 'Environmental psychology and nonverbal behavior'}",23.0,Measuring facial movement,1976.0
434,22dbceee742c019455dfc1a65397fe1b8dee1f3a,"In this paper we computationally study the relation between adaptive behaviour and emotion. Using the reinforcement learning framework, we propose that learned state utility, , models fear (negative) and hope (positive) based on the fact that both signals are about anticipation of loss or gain. Further, we propose that joy/distress is a signal similar to the error signal. We present agent-based simulation experiments that show that this model replicates psychological and behavioural dynamics of emotion. This work distinguishes itself by assessing the dynamics of emotion in an adaptive agent framework – coupling it to the literature on habituation, development, extinction and hope theory. Our results support the idea that the function of emotion is to provide a complex feedback signal for an organism to adapt its behaviour. Our work is relevant for understanding the relation between emotion and adaptation in animals, as well as for human–robot interaction, in particular how emotional signals can be used to communicate between adaptive agents and humans.","[{'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '40522369', 'name': 'E. Jacobs'}, {'authorId': '1689001', 'name': 'C. Jonker'}]",32.0,"{'bibtex': '@Article{Broekens2015ARL,\n author = {J. Broekens and E. Jacobs and C. Jonker},\n journal = {Connection Science},\n pages = {215 - 233},\n title = {A reinforcement learning model of joy, distress, hope and fear},\n volume = {27},\n year = {2015}\n}\n'}",,"{'volume': '27', 'pages': '215 - 233', 'name': 'Connection Science'}",75.0,"A reinforcement learning model of joy, distress, hope and fear",2015.0
435,22e2077034a4dc31a0f2cd125f604c3a4d656f59,"Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI). Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice. Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes. Results: The overall correlational effect size for trust was r̄ = +0.26, with an experimental effect size of d̄ = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role. Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors. Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified.","[{'authorId': '143605034', 'name': 'P. Hancock'}, {'authorId': '34893594', 'name': 'D. Billings'}, {'authorId': '2907955', 'name': 'Kristin E. Schaefer'}, {'authorId': '1403068272', 'name': 'Jessie Y.C. Chen'}, {'authorId': '7848470', 'name': 'E. D. Visser'}, {'authorId': '3264674', 'name': 'R. Parasuraman'}]",1282.0,"{'bibtex': '@Article{Hancock2011AMO,\n author = {P. Hancock and D. Billings and Kristin E. Schaefer and Jessie Y.C. Chen and E. D. Visser and R. Parasuraman},\n journal = {Human Factors: The Journal of Human Factors and Ergonomics Society},\n pages = {517 - 527},\n title = {A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction},\n volume = {53},\n year = {2011}\n}\n'}",,"{'volume': '53', 'pages': '517 - 527', 'name': 'Human Factors: The Journal of Human Factors and Ergonomics Society'}",67.0,A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction,2011.0
436,23175b146edaf762c446b245f4b1d82af41ade13,"Gesture and speech combine to form a rich basis for human conversational interaction. To exploit these modalities in HCI, we need to understand the interplay between them and the way in which they support communication. We propose a framework for the gesture research done to date, and present our work on the cross-modal cues for discourse segmentation in free-form gesticulation accompanying speech in natural conversation as a new paradigm for such multimodal interaction. The basis for this integration is the psycholinguistic concept of the coequal generation of gesture and speech from the same semantic intent. We present a detailed case study of a gesture and speech elicitation experiment in which a subject describes her living space to an interlocutor. We perform two independent sets of analyses on the video and audio data: video and audio analysis to extract segmentation cues, and expert transcription of the speech and gesture data by microanalyzing the videotape using a frame-accurate videoplayer to correlate the speech with the gestural entities. We compare the results of both analyses to identify the cues accessible in the gestural and audio data that correlate well with the expert psycholinguistic analysis. We show that ""handedness"" and the kind of symmetry in two-handed gestures provide effective supersegmental discourse cues.","[{'authorId': '1740663', 'name': 'Francis K. H. Quek'}, {'authorId': '145493778', 'name': 'D. McNeill'}, {'authorId': '2387793', 'name': 'R. Bryll'}, {'authorId': '144346436', 'name': 'S. Duncan'}, {'authorId': '3073990', 'name': 'Xin-Feng Ma'}, {'authorId': '2156795', 'name': 'C. Kirbas'}, {'authorId': '2660294', 'name': 'K. McCullough'}, {'authorId': '143806517', 'name': 'R. Ansari'}]",322.0,"{'bibtex': '@Article{Quek2002MultimodalHD,\n author = {Francis K. H. Quek and D. McNeill and R. Bryll and S. Duncan and Xin-Feng Ma and C. Kirbas and K. McCullough and R. Ansari},\n journal = {ACM Trans. Comput. Hum. Interact.},\n pages = {171-193},\n title = {Multimodal human discourse: gesture and speech},\n volume = {9},\n year = {2002}\n}\n'}",,"{'volume': '9', 'pages': '171-193', 'name': 'ACM Trans. Comput. Hum. Interact.'}",45.0,Multimodal human discourse: gesture and speech,2002.0
437,23383d3c9568d9b64334629befdbb53b5696dbc6,"This target article is concerned with the implications of the surprisingly different experimental practices in economics and in areas of psychology relevant to both economists and psychologists, such as behavioral decision making. We consider four features of experimentation in economics, namely, script enactment, repeated trials, performance-based monetary payments, and the proscription against deception, and compare them to experimental practices in psychology, primarily in the area of behavioral decision making. Whereas economists bring a precisely defined “script” to experiments for participants to enact, psychologists often do not provide such a script, leaving participants to infer what choices the situation affords. By often using repeated experimental trials, economists allow participants to learn about the task and the environment; psychologists typically do not. Economists generally pay participants on the basis of clearly defined performance criteria; psychologists usually pay a flat fee or grant a fixed amount of course credit. Economists virtually never deceive participants; psychologists, especially in some areas of inquiry, often do. We argue that experimental standards in economics are regulatory in that they allow for little variation between the experimental practices of individual researchers. The experimental standards in psychology, by contrast, are comparatively laissez-faire. We believe that the wider range of experimental practices in psychology reflects a lack of procedural regularity that may contribute to the variability of empirical findings in the research fields under consideration. We conclude with a call for more research on the consequences of methodological preferences, such as the use on monetary payments, and propose a “do-it-both-ways” rule regarding the enactment of scripts, repetition of trials, and performance-based monetary payments. We also argue, on pragmatic grounds, that the default practice should be not to deceive participants.","[{'authorId': '3133271', 'name': 'R. Hertwig'}, {'authorId': '1812554', 'name': 'A. Ortmann'}]",808.0,"{'bibtex': '@Article{Hertwig2001ExperimentalPI,\n author = {R. Hertwig and A. Ortmann},\n journal = {Behavioral and Brain Sciences},\n pages = {383 - 403},\n title = {Experimental practices in economics: A methodological challenge for psychologists?},\n volume = {24},\n year = {2001}\n}\n'}",,"{'volume': '24', 'pages': '383 - 403', 'name': 'Behavioral and Brain Sciences'}",351.0,Experimental practices in economics: A methodological challenge for psychologists?,2001.0
438,234745b20e7606bb8b1d741bc793bd6a77cbe335,"EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.","[{'authorId': '3141704', 'name': 'A. Camurri'}, {'authorId': '46319813', 'name': 'P. Coletta'}, {'authorId': '1958033', 'name': 'G. Varni'}, {'authorId': '2126253', 'name': 'Simone Ghisio'}]",65.0,"{'bibtex': '@Inproceedings{Camurri2007DevelopingMI,\n author = {A. Camurri and P. Coletta and G. Varni and Simone Ghisio},\n pages = {305-308},\n title = {Developing multimodal interactive systems with EyesWeb XMI},\n year = {2007}\n}\n'}",,{'pages': '305-308'},7.0,Developing multimodal interactive systems with EyesWeb XMI,2007.0
439,234a741a8bcdf25f5924101edc1f67f164f5277b,"Do people behave differently when they are lying compared with when they are telling the truth? The combined results of 1,338 estimates of 158 cues to deception are reported. Results show that in some ways, liars are less forthcoming than truth tellers, and they tell less compelling tales. They also make a more negative impression and are more tense. Their stories include fewer ordinary imperfections and unusual contents. However, many behaviors showed no discernible links, or only weak links, to deceit. Cues to deception were more pronounced when people were motivated to succeed, especially when the motivations were identity relevant rather than monetary or material. Cues to deception were also stronger when lies were about transgressions.","[{'authorId': '5323704', 'name': 'B. DePaulo'}, {'authorId': '32593993', 'name': 'James J. Lindsay'}, {'authorId': '2054366525', 'name': 'Brian E Malone'}, {'authorId': '5488931', 'name': 'Laura Muhlenbruck'}, {'authorId': '145086047', 'name': 'K. Charlton'}, {'authorId': '2056025803', 'name': 'Harris Cooper'}]",2282.0,"{'bibtex': '@Article{DePaulo2003CuesTD,\n author = {B. DePaulo and James J. Lindsay and Brian E Malone and Laura Muhlenbruck and K. Charlton and Harris Cooper},\n journal = {Psychological bulletin},\n pages = {\n          74-118\n        },\n title = {Cues to deception.},\n volume = {129 1},\n year = {2003}\n}\n'}",,"{'volume': '129 1', 'pages': '\n          74-118\n        ', 'name': 'Psychological bulletin'}",321.0,Cues to deception.,2003.0
440,237218b1479984fcf1e88b31c9215e02609fd975,"A comparative view of the brain, comparing related functions across species and sensory systems, offers a number of advantages. In particular, it allows separation of the formal purpose of a model structure from its implementation in specific brains. Models of auditory cortical processing can be conceived by analogy to the visual cortex, incorporating neural mechanisms that are found in both the visual and auditory systems. Examples of such canonical features at the columnar level are direction selectivity, size/bandwidth selectivity, and receptive fields with segregated vs. overlapping ON and OFF subregions. On a larger scale, parallel processing pathways have been envisioned that represent the two main facets of sensory perception: (i) identification of objects; and (ii) processing of space. Expanding this model in terms of sensorimotor integration and control offers an overarching view of cortical function independently of sensory modality.","[{'authorId': '1856522', 'name': 'J. Rauschecker'}]",38.0,"{'bibtex': '@Article{Rauschecker2015AuditoryAV,\n author = {J. Rauschecker},\n journal = {European Journal of Neuroscience},\n title = {Auditory and visual cortex of primates: a comparison of two sensory systems},\n volume = {41},\n year = {2015}\n}\n'}",,"{'volume': '41', 'name': 'European Journal of Neuroscience'}",54.0,Auditory and visual cortex of primates: a comparison of two sensory systems,2015.0
441,237525ba89488eace7698459a2b45e3343ed823d,,"[{'authorId': '3292250', 'name': 'M. Curumsing'}, {'authorId': '2131391', 'name': 'Niroshinie Fernando'}, {'authorId': '47505933', 'name': 'Mohamed Abdelrazek'}, {'authorId': '33402434', 'name': 'Rajesh Vasa'}, {'authorId': '1845229', 'name': 'K. Mouzakis'}, {'authorId': '1687239', 'name': 'J. Grundy'}]",64.0,"{'bibtex': '@Article{Curumsing2019EmotionorientedRE,\n author = {M. Curumsing and Niroshinie Fernando and Mohamed Abdelrazek and Rajesh Vasa and K. Mouzakis and J. Grundy},\n journal = {J. Syst. Softw.},\n pages = {215-229},\n title = {Emotion-oriented requirements engineering: A case study in developing a smart home system for the elderly},\n volume = {147},\n year = {2019}\n}\n'}",,"{'volume': '147', 'pages': '215-229', 'name': 'J. Syst. Softw.'}",64.0,Emotion-oriented requirements engineering: A case study in developing a smart home system for the elderly,2019.0
442,237831f1ec93e537ca769091ba40df1c945281f8,"K e y w o r d s computer forms, computer interviews, electronic surveys, measurement, disclosure, response bias, electronic","[{'authorId': '2453684', 'name': 'S. Weisband'}, {'authorId': '47198673', 'name': 'S. Kiesler'}]",222.0,"{'bibtex': '@Article{Weisband1996SelfDO,\n author = {S. Weisband and S. Kiesler},\n journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n title = {Self disclosure on computer forms: meta-analysis and implications},\n year = {1996}\n}\n'}",,{'name': 'Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'},31.0,Self disclosure on computer forms: meta-analysis and implications,1996.0
443,23ae8728e2af661d2102a548d5c3dfb9a38cdaa1,"In order for robots to be socially accepted and generate empathy they must display emotions. For robots such as Nao, body language is the best medium available, as they do not have the ability to display facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should greatly improve its acceptance.
 This research investigates the creation of an ""Affect Space"" [1] for the generation of emotional body language that could be displayed by robots. An Affect Space is generated by ""blending"" (i.e. interpolating between) different emotional expressions to create new ones. An Affect Space for body language based on the Circumplex Model of emotions [2] has been created.
 The experiment reported in this paper investigated the perception of specific key poses from the Affect Space. The results suggest that this Affect Space for body expressions can be used to improve the expressiveness of humanoid robots.
 In addition, early results of a pilot study are described. It revealed that the context helps human subjects improve their recognition rate during a human-robot imitation game, and in turn this recognition leads to better outcome of the interactions.","[{'authorId': '2609243', 'name': 'Aryel Beck'}, {'authorId': '3171783', 'name': 'Antoine Hiolle'}, {'authorId': '145489302', 'name': 'A. Mazel'}, {'authorId': '1713009', 'name': 'L. Cañamero'}]",70.0,"{'bibtex': '@Inproceedings{Beck2010InterpretationOE,\n author = {Aryel Beck and Antoine Hiolle and A. Mazel and L. Cañamero},\n pages = {37-42},\n title = {Interpretation of emotional body language displayed by robots},\n year = {2010}\n}\n'}",,{'pages': '37-42'},7.0,Interpretation of emotional body language displayed by robots,2010.0
444,23c9d63b68d71134def98c955663d3e67b8cac56,,"[{'authorId': '47824687', 'name': 'N. Savikko'}, {'authorId': '3852893', 'name': 'P. Routasalo'}, {'authorId': '3976055', 'name': 'R. Tilvis'}, {'authorId': '6200225', 'name': 'T. Strandberg'}, {'authorId': '2991669', 'name': 'K. Pitkälä'}]",585.0,"{'bibtex': '@Article{Savikko2005PredictorsAS,\n author = {N. Savikko and P. Routasalo and R. Tilvis and T. Strandberg and K. Pitkälä},\n journal = {Archives of gerontology and geriatrics},\n pages = {\n          223-33\n        },\n title = {Predictors and subjective causes of loneliness in an aged population.},\n volume = {41 3},\n year = {2005}\n}\n'}",,"{'volume': '41 3', 'pages': '\n          223-33\n        ', 'name': 'Archives of gerontology and geriatrics'}",29.0,Predictors and subjective causes of loneliness in an aged population.,2005.0
445,23d3170f4ec585795061bc8e0f2e0c406e5ae3ac,"Emotional expressions play a very important role in the interaction between virtual agents and human users. In this paper, we present a new constraint-based approach to the generation of multimodal emotional displays. The displays generated with our method are not limited to the face, but are composed of different signals partially ordered in time and belonging to different modalities. We also describe the evaluation of the main features of our approach. We examine the role of multimodality, sequentiality, and constraints in the perception of synthesized emotional states. The results of our evaluation show that applying our algorithm improves the communication of a large spectrum of emotional states, while the believability of the agent animations increases with the use of constraints over the multimodal signals.","[{'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '1783043', 'name': 'S. Hyniewska'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",52.0,"{'bibtex': '@Article{Niewiadomski2011ConstraintBasedMF,\n author = {Radoslaw Niewiadomski and S. Hyniewska and C. Pelachaud},\n journal = {IEEE Transactions on Affective Computing},\n pages = {134-146},\n title = {Constraint-Based Model for Synthesis of Multimodal Sequential Expressions of Emotions},\n volume = {2},\n year = {2011}\n}\n'}",,"{'volume': '2', 'pages': '134-146', 'name': 'IEEE Transactions on Affective Computing'}",36.0,Constraint-Based Model for Synthesis of Multimodal Sequential Expressions of Emotions,2011.0
446,23e498e565da52641126815a6d2c6ac4509d5c74,"Many producers and consumers of legacy training simulator and game environments are beginning to envision a new era where psycho-socio-physiologic models could be interoperated to enhance their environments' simulation of human agents. This paper explores whether we could embed our behavior modeling framework (described in the companion paper, Part 1) behind a legacy first person shooter 3D game environment to recreate portions of the Black Hawk Down scenario. Section 1 amplifies the interoperability needs and challenges confronting the field, presents the questions that are examined, and describes the test scenario. Sections 2 and 3 review the software and knowledge engineering methodology, respectively, needed to create the system and populate it with bots. Results (Section 4) and discussion (Section 5) reveal that we were able to generate plausible and adaptive recreations of Somalian crowds, militia, women acting as shields, suicide bombers, and more. Also, there are specific lessons learned about ways to advance the field so that such interoperabilities will become more affordable and widespread.","[{'authorId': '1714492', 'name': 'B. Silverman'}, {'authorId': '3201199', 'name': 'Gnana Bharathy'}, {'authorId': '2059261082', 'name': ""Kevin O'Brien""}, {'authorId': '2668493', 'name': 'Jason Cornwell'}]",111.0,"{'bibtex': ""@Article{Silverman2006HumanBM,\n author = {B. Silverman and Gnana Bharathy and Kevin O'Brien and Jason Cornwell},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {163-185},\n title = {Human Behavior Models for Agents in Simulators and Games: Part II: Gamebot Engineering with PMFserv},\n volume = {15},\n year = {2006}\n}\n""}",,"{'volume': '15', 'pages': '163-185', 'name': 'Presence: Teleoperators & Virtual Environments'}",42.0,Human Behavior Models for Agents in Simulators and Games: Part II: Gamebot Engineering with PMFserv,2006.0
449,23fc44f1f5e62e99d72a13c78f44a5c7025b3b18,"Although current theories suggest that affective empathy (perceivers' experience of social targets' emotions) should contribute to empathic accuracy (perceivers' ability to accurately assess targets' emotions), extant research has failed to consistently demonstrate a correspondence between them. We reasoned that prior null findings may be attributable to a failure to account for the fundamentally interpersonal nature of empathy, and tested the prediction that empathic accuracy may depend on both targets' tendency to express emotion and perceivers' tendency to empathically share that emotion. Using a continuous affect-rating paradigm, we found that perceivers' trait affective empathy was unrelated to empathic accuracy unless targets' trait expressivity was taken into account: Perceivers' trait affective empathy predicted accuracy only for expressive targets. These data suggest that perceivers' self-reported affective empathy can indeed predict their empathic accuracy, but only when targets' expressivity allows their thoughts and feelings to be read.","[{'authorId': '2268731', 'name': 'Jamil Zaki'}, {'authorId': '5539614', 'name': 'N. Bolger'}, {'authorId': '2669604', 'name': 'K. Ochsner'}]",102.0,"{'bibtex': '@Article{Zaki2008ItTT,\n author = {Jamil Zaki and N. Bolger and K. Ochsner},\n journal = {Psychological Science},\n pages = {399-404},\n title = {It Takes Two The Interpersonal Nature of Empathic Accuracy},\n volume = {19},\n year = {2008}\n}\n'}",,"{'volume': '19', 'pages': '399-404', 'name': 'Psychological Science'}",30.0,It Takes Two The Interpersonal Nature of Empathic Accuracy,2008.0
450,2418dc661f467ce41fa8ac330232303c520422cd,,"[{'authorId': '145309281', 'name': 'Michael Nixon'}, {'authorId': '144380634', 'name': 'Philippe Pasquier'}, {'authorId': '1381933697', 'name': 'M. S. El-Nasr'}]",8.0,"{'bibtex': ""@Inproceedings{Nixon2010DelsArtMapAD,\n author = {Michael Nixon and Philippe Pasquier and M. S. El-Nasr},\n pages = {139-145},\n title = {DelsArtMap: Applying Delsarte's Aesthetic System to Virtual Agents},\n year = {2010}\n}\n""}",,{'pages': '139-145'},20.0,DelsArtMap: Applying Delsarte's Aesthetic System to Virtual Agents,2010.0
451,2452a20398502411ff37384a629e0aefd5754b75,,"[{'authorId': '4797664', 'name': 'K. Lanctôt'}, {'authorId': '3496547', 'name': 'J. Amatniek'}, {'authorId': '1396236343', 'name': 'S. Ancoli-Israel'}, {'authorId': '145601268', 'name': 'S. Arnold'}, {'authorId': '144956527', 'name': 'C. Ballard'}, {'authorId': '1397811791', 'name': 'J. Cohen-Mansfield'}, {'authorId': '2005479', 'name': 'Z. Ismail'}, {'authorId': '1901398', 'name': 'C. Lyketsos'}, {'authorId': '49971136', 'name': 'David S. Miller'}, {'authorId': '4843395', 'name': 'E. Musiek'}, {'authorId': '145734521', 'name': 'R. Osorio'}, {'authorId': '144889112', 'name': 'P. Rosenberg'}, {'authorId': '4250338', 'name': 'A. Satlin'}, {'authorId': '2138706', 'name': 'D. Steffens'}, {'authorId': '6323147', 'name': 'P. Tariot'}, {'authorId': '7535238', 'name': 'L. Bain'}, {'authorId': '2077654', 'name': 'M. Carrillo'}, {'authorId': '1992761937', 'name': 'James A. Hendrix'}, {'authorId': '1401877141', 'name': 'Heidi Jurgens'}, {'authorId': '4642584', 'name': 'B. Boot'}]",229.0,"{'bibtex': ""@Article{Lanctôt2017NeuropsychiatricSA,\n author = {K. Lanctôt and J. Amatniek and S. Ancoli-Israel and S. Arnold and C. Ballard and J. Cohen-Mansfield and Z. Ismail and C. Lyketsos and David S. Miller and E. Musiek and R. Osorio and P. Rosenberg and A. Satlin and D. Steffens and P. Tariot and L. Bain and M. Carrillo and James A. Hendrix and Heidi Jurgens and B. Boot},\n journal = {Alzheimer's & Dementia : Translational Research & Clinical Interventions},\n pages = {440 - 449},\n title = {Neuropsychiatric signs and symptoms of Alzheimer's disease: New treatment paradigms},\n volume = {3},\n year = {2017}\n}\n""}",,"{'volume': '3', 'pages': '440 - 449', 'name': ""Alzheimer's & Dementia : Translational Research & Clinical Interventions""}",122.0,Neuropsychiatric signs and symptoms of Alzheimer's disease: New treatment paradigms,2017.0
452,2465313f00a4b4b426757f06e31f3a637799bda4,,"[{'authorId': '2287854', 'name': 'Alexandru Traista'}, {'authorId': '2999481', 'name': 'M. Elshaw'}]",2.0,"{'bibtex': '@Inproceedings{Traista2012AHN,\n author = {Alexandru Traista and M. Elshaw},\n pages = {353-362},\n title = {A Hybrid Neural Emotion Recogniser for Human-Robotic Agent Interaction},\n year = {2012}\n}\n'}",,{'pages': '353-362'},24.0,A Hybrid Neural Emotion Recogniser for Human-Robotic Agent Interaction,2012.0
453,247f9147f0fa07896e677549495fb6cd4a789cb4,"Research on creation of virtual humans enables increasing automatization of their behavior, including synthesis of verbal and nonverbal behavior. As the achievable realism of different aspects of agent design evolves asynchronously, it is important to understand if and how divergence in realism between behavioral channels can elicit negative user responses. Specifically, in this work, we investigate the question of whether autonomous virtual agents relying on synthetic text-to-speech voices should portray a corresponding level of realism in the non-verbal channels of motion and visual appearance, or if, alternatively, the best available realism of each channel should be used. In two perceptual studies, we assess how realism of voice, motion, and appearance influence the perceived match of speech and gesture motion, as well as the agent's likability and human-likeness. Our results suggest that maximizing realism of voice and motion is preferable even when this leads to realism mismatches, but for visual appearance, lower realism may be preferable. (A video abstract can be found at https://youtu.be/arfZZ-hxD1Y.)","[{'authorId': '3430725', 'name': 'Ylva Ferstl'}, {'authorId': '145947976', 'name': 'S. Thomas'}, {'authorId': '2080984307', 'name': 'Cédric Guiard'}, {'authorId': '31894925', 'name': 'Cathy Ennis'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",12.0,"{'bibtex': '@Article{Ferstl2021HumanOR,\n author = {Ylva Ferstl and S. Thomas and Cédric Guiard and Cathy Ennis and R. Mcdonnell},\n journal = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},\n title = {Human or Robot?: Investigating voice, appearance and gesture motion realism of conversational social agents},\n year = {2021}\n}\n'}",,{'name': 'Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents'},61.0,"Human or Robot?: Investigating voice, appearance and gesture motion realism of conversational social agents",2021.0
454,2485744f8784d478476c54f3eb4c6c2c0d96c215,"The studies of emotion function and emotional disorders complement one another. In this article, the authors outline relations between the social functions of emotion and four psychological disorders. The authors first present a social-functional account of emotion and argue that emotions help coordinate social interactions through their informative, evocative, and incentive functions. They then review evidence concerning the emotional and social problems related to depression, schizophrenia, social anxiety, and borderline personality disorder and consider how the emotional disturbances related to these disorders disrupt interactions and relationships, thus contributing further to the maintenance of the disorder. They conclude by discussing research strategies relevant to the study of emotion, social interaction, and psychopathology.","[{'authorId': '3990536', 'name': 'D. Keltner'}, {'authorId': '4234564', 'name': 'A. Kring'}]",729.0,"{'bibtex': '@Article{Keltner1998EmotionSF,\n author = {D. Keltner and A. Kring},\n journal = {Review of General Psychology},\n pages = {320 - 342},\n title = {Emotion, Social Function, and Psychopathology},\n volume = {2},\n year = {1998}\n}\n'}",,"{'volume': '2', 'pages': '320 - 342', 'name': 'Review of General Psychology'}",242.0,"Emotion, Social Function, and Psychopathology",1998.0
455,2494294303492cc662a2c30def642c4826a98107,,"[{'authorId': '3315073', 'name': 'V. Mittal'}, {'authorId': '3472842', 'name': 'E. Walker'}]",90841.0,"{'bibtex': '@Article{Mittal2011DiagnosticAS,\n author = {V. Mittal and E. Walker},\n journal = {Psychiatry Research},\n pages = {158-159},\n title = {Diagnostic and Statistical Manual of Mental Disorders},\n volume = {189},\n year = {2011}\n}\n'}",,"{'volume': '189', 'pages': '158-159', 'name': 'Psychiatry Research'}",14.0,Diagnostic and Statistical Manual of Mental Disorders,2011.0
456,249ea01aa8b33ecb251c5b41f327a09eb1d74928,"OF THESIS A COMPARISON OF THE REISS PROFILE WITH THE NEO PI-R ASSESSMENT OF PERSONALITY The purpose of this thesis was to determine whether the NEO Personality InventoryRevised (NEO PI-R) could account for significant variance within a measure of personality developed for the intellectually disabled (i.e., the Reiss Profile of Fundamental Motives), as well as to consider their comparative validity. The NEO PI-R and the Reiss Profile of Fundamental Motives were administered to 127 undergraduate students in conjunction with the Personality Research Form (PRF) and the Behavior Report Form (BRF). The NEO PI-R was able to account for a substantial amount of variance in the Reiss Profile scales, and the Reiss and the NEO accounted for approximately equivalent amounts of variance in the PRF and BRF. Implications for general personality research as well as additional research with a sample of adults with intellectual disability are discussed.","[{'authorId': '144477794', 'name': 'Sara E. Boyd'}]",2.0,"{'bibtex': '@Inproceedings{Boyd2010ACO,\n author = {Sara E. Boyd},\n title = {A COMPARISON OF THE REISS PROFILE WITH THE NEO PI-R ASSESSMENT OF PERSONALITY},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",70.0,A COMPARISON OF THE REISS PROFILE WITH THE NEO PI-R ASSESSMENT OF PERSONALITY,2010.0
457,24a8d5f99727f9a0d0a01673970f785442da7d82,"The ability to learn is a potentially compelling and important quality for interactive synthetic characters. To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation is grounded in the techniques of reinforcement learning and informed by insights from animal training. It simplifies the learning task for characters by (a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to make maximal use of any supervisory signals, and (c) making them easy to train by humans.We built an autonomous animated dog that can be trained with a technique used to train real dogs called ""clicker training"". Capabilities demonstrated include being trained to recognize and use acoustic patterns as cues for actions, as well as to synthesize new actions from novel paths through its motion space.A key contribution of this paper is to demonstrate that by addressing the three problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters.","[{'authorId': '1978994492', 'name': 'B. Blumberg'}, {'authorId': '2252543', 'name': 'Marc Downie'}, {'authorId': '32511979', 'name': 'Y. Ivanov'}, {'authorId': '40090748', 'name': 'Matt Berlin'}, {'authorId': '2109890862', 'name': 'M. P. Johnson'}, {'authorId': '34436552', 'name': 'Bill Tomlinson'}]",237.0,"{'bibtex': '@Article{Blumberg2002IntegratedLF,\n author = {B. Blumberg and Marc Downie and Y. Ivanov and Matt Berlin and M. P. Johnson and Bill Tomlinson},\n journal = {Proceedings of the 29th annual conference on Computer graphics and interactive techniques},\n title = {Integrated learning for interactive synthetic characters},\n year = {2002}\n}\n'}",,{'name': 'Proceedings of the 29th annual conference on Computer graphics and interactive techniques'},44.0,Integrated learning for interactive synthetic characters,2002.0
458,24ac243158bd65aaf48953b82b1c3c2cfde022e0,"In designing and evaluating human-robot interactions and interfaces, researchers often use a simulated robot due to the high cost of robots and time required to program them. However, it is important to consider how interaction with a simulated robot differs from a real robot; that is, do simulated robots provide authentic interaction? We contribute to a growing body of work that explores this question and maps out simulated-versus-real differences, by explicitly investigating empathy: how people empathize with a physical or simulated robot when something bad happens to it. Our results suggest that people may empathize more with a physical robot than a simulated one, a finding that has important implications on the generalizability and applicability of simulated HRI work. Empathy is particularly relevant to social HRI and is integral to, for example, companion and care robots. Our contribution additionally includes an original and reproducible HRI experimental design to induce empathy toward robots in laboratory settings, and an experimentally validated empathy-measuring instrument from psychology for use with HRI. Categories and Subject Descriptors H.5.2 [User Interfaces]: evaluation/methodology General Terms Experimentation and Human Factors.","[{'authorId': '7274678', 'name': 'S. Seo'}, {'authorId': '2402812', 'name': 'Denise Y. Geiskkovitch'}, {'authorId': '2438959', 'name': 'M. Nakane'}, {'authorId': '2064653159', 'name': 'Corey King'}, {'authorId': '144850327', 'name': 'J. Young'}]",109.0,"{'bibtex': '@Article{Seo2015PoorTW,\n author = {S. Seo and Denise Y. Geiskkovitch and M. Nakane and Corey King and J. Young},\n journal = {2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {125-132},\n title = {Poor Thing! Would You Feel Sorry for a Simulated Robot? A comparison of empathy toward a physical and a simulated robot},\n year = {2015}\n}\n'}",,"{'pages': '125-132', 'name': '2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",42.0,Poor Thing! Would You Feel Sorry for a Simulated Robot? A comparison of empathy toward a physical and a simulated robot,2015.0
459,24b1a870298497f963336372bea20537cffd14ab,,"[{'authorId': '1779275', 'name': 'H. Hoffman'}, {'authorId': '2054703703', 'name': 'Gloria T Chambers'}, {'authorId': '143931832', 'name': 'W. Meyer'}, {'authorId': '5329178', 'name': 'Lisa L. Arceneaux'}, {'authorId': '71058701', 'name': 'W. J. Russell'}, {'authorId': '3153418', 'name': 'E. Seibel'}, {'authorId': '3152109', 'name': 'T. Richards'}, {'authorId': '3348169', 'name': 'S. Sharar'}, {'authorId': '145902632', 'name': 'D. Patterson'}]",473.0,"{'bibtex': '@Article{Hoffman2011VirtualRA,\n author = {H. Hoffman and Gloria T Chambers and W. Meyer and Lisa L. Arceneaux and W. J. Russell and E. Seibel and T. Richards and S. Sharar and D. Patterson},\n journal = {Annals of Behavioral Medicine},\n pages = {183-191},\n title = {Virtual Reality as an Adjunctive Non-pharmacologic Analgesic for Acute Burn Pain During Medical Procedures},\n volume = {41},\n year = {2011}\n}\n'}",,"{'volume': '41', 'pages': '183-191', 'name': 'Annals of Behavioral Medicine'}",67.0,Virtual Reality as an Adjunctive Non-pharmacologic Analgesic for Acute Burn Pain During Medical Procedures,2011.0
460,24b1f433fba56ab7469ecc3835280eff7434bf15,"Data-driven techniques for interactive narrative generation are the subject of growing interest. Reinforcement learning (RL) offers significant potential for devising data-driven interactive narrative generators that tailor players’ story experiences by inducing policies from player interaction logs. A key open question in RL-based interactive narrative generation is how to model complex player interaction patterns to learn effective policies. In this paper we present a deep RL-based interactive narrative generation framework that leverages synthetic data produced by a bipartite simulated player model. Specifically, the framework involves training a set of Q-networks to control adaptable narrative event sequences with long short-term memory network-based simulated players. We investigate the deep RL framework’s performance with an educational interactive narrative, C RYSTAL I SLAND . Results suggest that the deep RL-based narrative generation framework yields effective personalized interactive narratives.","[{'authorId': '2108818241', 'name': 'Pengcheng Wang'}, {'authorId': '34971293', 'name': 'Jonathan P. Rowe'}, {'authorId': '1745330', 'name': 'Wookhee Min'}, {'authorId': '9808011', 'name': 'Bradford W. Mott'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",40.0,"{'bibtex': '@Inproceedings{Wang2017InteractiveNP,\n author = {Pengcheng Wang and Jonathan P. Rowe and Wookhee Min and Bradford W. Mott and James C. Lester},\n pages = {3852-3858},\n title = {Interactive Narrative Personalization with Deep Reinforcement Learning},\n year = {2017}\n}\n'}",,{'pages': '3852-3858'},23.0,Interactive Narrative Personalization with Deep Reinforcement Learning,2017.0
461,24bbf001ea545b5cdbe55408fd25a75049380bdf,"People make social inferences without intentions, awareness, or effort, i.e., spontaneously. We review recent findings on spontaneous social inferences (especially traits, goals, and causes) and closely related phenomena. We then describe current thinking on some of the most relevant processes, implicit knowledge, and theories. These include automatic and controlled processes and their interplay; embodied cognition, including mimicry; and associative versus rule-based processes. Implicit knowledge includes adult folk theories, conditions of personhood, self-knowledge to simulate others, and cultural and social class differences. Implicit theories concern Bayesian networks, recent attribution research, and questions about the utility of the disposition-situation dichotomy. Developmental research provides new insights. Spontaneous social inferences include a growing array of phenomena, but they have been insufficiently linked to other phenomena and theories. We hope the links suggested in this review begin to remedy this.","[{'authorId': '4405440', 'name': 'J. Uleman'}, {'authorId': '6292637', 'name': 'S. Adil Saribay'}, {'authorId': '2148877392', 'name': 'C. M. Gonzalez'}]",418.0,"{'bibtex': '@Article{Uleman2008SpontaneousII,\n author = {J. Uleman and S. Adil Saribay and C. M. Gonzalez},\n journal = {Annual review of psychology},\n pages = {\n          329-60\n        },\n title = {Spontaneous inferences, implicit impressions, and implicit theories.},\n volume = {59},\n year = {2008}\n}\n'}",,"{'volume': '59', 'pages': '\n          329-60\n        ', 'name': 'Annual review of psychology'}",206.0,"Spontaneous inferences, implicit impressions, and implicit theories.",2008.0
462,24c5de3c93aa8cdd779641c3172399ef5c092d1c,"This is a set of notes relating to an invited talk at the cross-disciplinary workshop on Architectures for Modeling Emotion at the AAAI Spring Symposium at Stanford University in March 2004. The organisers of the workshop note that work on emotions “is often carried out in anad hocmanner”, and hope to remedy this by focusing on two themes (a) validation of emotion models and architectures, and (b) relevance of recent findings from affective neuroscience research. I shall focus mainly on (a), but in a manner which, I hope is relevant to (b), by addressing the need for conceptual clarification to remove, or at least reduce, the adhocery, both in modelling and in empirical research. In particular I try to show how a design-based approach can provide an improved conceptual framework and sharpen empirical questions relating to the study of mind and brain. From this standpoint it turns out that what are normally called emotions are a somewhat fuzzy subset of a larger class of states and processes that can arise out of interactions between different mechanisms in an architecture. What exactly the architecture is will determine both the larger class and the subset, since different architectures support different classes of states and processes. In order to develop the design-based approach we need a good ontology for characterising varieties of architectures and the states and processes that can occur in them. At present this too is often a matter of much ad-hocery. We propose steps toward a remedy. Validation vstesting It is good to ask how theories can be validated, though I would rather ask how they can be t sted, and how they can becompared, in various dimensions, such as depth, clarity, generality, precision, explanatory power, etc., since most theories are incomplete, provisional, premature, vague, or just false. So validation is rarely to be expected, even when a theory is the best one available and provides a good basis for further research, a point that is familiar from the writings of Karl Popper (Popper 1934; Magee 1985), and work of Lakatos cited by Dean Petters in this symposium. Copyright c © 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. There is, however, a kind of validation of theories of a type Popper did not (as far as I know) admit as science, and many scientists do not acknowledge, partly because of Popper’s influence, namely theories about what is possible (what can occur). Simple logic shows that even a single example validates such a theory. The importance of theories of what is possible and how they are presupposed by the search for laws and regularities which constrain what is possible, was discussed in Sloman (1978, Ch 2). See also (Boden 1990). In particular, contrary to wide-spread views on scientific method, the truth of a statement that something can occur is established by a singleexample – which could be world-shaking (e.g. the discovery that light of low intensity and small wavelength can trigger an electric current when high intensity large wavelength light fails (the photoelectric effect), the discovery in 1919 that light from a star can be deflected by passing near the sun, or Newton’s earlier discovery that white light passing through a prism can be split into beams different colours). However, checking that the description of the example is correct may be non-trivial, especially if it requires the use of concepts that are not yet generally understood or theories that are not yet accepted. For present purposes the study of architectures and the phenomena they support is part of the study of what can exist or occur (deep science) and why, rather than an attempt to discover any new laws about what happens when or what correlates with what (shallow science). But we need to know what we are talking about. What are we talking about? It is sometimes forgotten that the question whether a theory is true or false presupposes an answer to whether it makes sense at all. All theories use concepts , for these are the building blocks of propositions, but insofar as the concepts are obscure, confused, or vague, the theories, and even the questions to which the theories are answers, will be flawed. For instance, if our concept of ‘emotion’ is ambiguous then so also will many questions about emotions be ambiguous, e.g. how emotions evolved, what their functions are, which animals have them, which brain mechanisms produce them, what types there are, whether they are needed for intelligence, whether a foetus has them, etc. Alas, our everyday concept of ‘emotion’ mixes up motivations, attitudes, moods, and other affective states and processes, and is therefore is too deeply flawed to be a useful component of scientific questions and theories for reasons recounted elsewhere. 1 But that does not prevent scientists assuming that these concepts can be used to formulate clear, unambiguous scientific questions or theories. For instance, sometimes people who argue that emotions are needed for intelligence are merely defending the truism that motivation is needed for action, and preferences are needed for selecting between options. However, not everyone would call a motive or preference, however important, an emotion. Wishful thinking isn’t science Sometimes over-generalising the notion of ‘emotion’ is related to a desire to argue that emotions are important in ways not previously acknowledged, e.g. that they are a prerequisite for intelligence. This can be wishful thinking or a trivial truism. If ‘emotion’ is construed so broadly that it covers all goals and preferences the claim that emotions are needed for intelligence is vacuous. On the other hand if it refers more narrowly to the sorts of processes in which one subsystem interferes with or disrupts the normal functioning of another, as happens in many of the states in which people are described as being ‘emotional’ then it is false that emotions arerequiredfor intelligence: on the contrary, emotions of that sort can get in the way of sensible decisions and actions. Monica Croucher and I once argued in 1981 that mechanisms required for intelligence in resource-limited robots in fast-changing environments would inevitably produce thepossibility of emotional states, involving interruption or modulation of one process by another (where the latter is often a fast and therefore relatively stupid process) that detects an urgent need for some change, e.g. using reactive mechanisms labelled ‘alarms’ in Fig. 1. But saying that states of type X can occur as a side-effect of the operation of some mechanism M that is required for intelligence does not imply that states of type X are themselves required for intelligence. Many wishful thinkers misinterpreted that paper as claiming that emotions are requiredfor intelligence, just as they fail to see the obvious flaw in Damasio’s widely quoted reasoning (1994) from the premiss:Damage to frontal lobes impairs both intelligence and emotional capabilitiesto the conclusionEmotions are required for intelligence . A moment’s thought should show that two capabilities could presuppose some common mechanisms without either capability being required for the other. A research community with too much wishful thinking does not advance science. Of course, if I have misread such people and they are merely proposing the truism (noted by Hume, which needs no empirical evidence) that motivation and preferences are required for intelligent thought and actions then that is another manifestation of the ambiguity of the word ‘emotion’. E.g. in (Sloman 2002b; 2002a; 2001; Sloman, Chrisley, & Scheutz To Appear). Figure 1: The CogAff schema defines a crude first-draft division of mechanisms into 9 categories. A particular type of fast, patterndriven, reactive central mechanism, with inputs from and outputs to many other components of the architecture could function as an alarm mechanism able to produce global reorganisation very quickly. Different sorts of alarm systems can produce different sorts of emotions, depending on where the inputs come from where the outputs go, what kinds of decisions are taken and what kinds of output signals are sent. Slower, longer lasting, more easily suppressedispositionalmechanisms can produce long term emotions, such as grief or jealousy. We need finer-grained ontologies We should not put both (i) a general preference for saving effort, and (ii) fear produced by a stampeding herd, in the same conceptual basket when they have so very many differences, including the (relative) permanence of the first and the transience of the second. Or rather, we can put them in a more general basket labelled ‘affect’ which includes sub-categories which might be aptly labelled ‘emotion’, ‘desire’, ‘preference’, ‘attitude’, ‘value’, ‘mood’, etc. I am not claiming that all emotions are short-term (though many emotion theories have that consequence, often not noticed by their proponents). In Wright et al. (1996) we tried to show, at least in outline, how long-term emotions such as grief, could exist in the form of dispositions which only rarely manifest themselves, either because of external triggers (reminders) or because other competing attention-grabbers subside. Many states generally regarded as important human emotions that form the stuff of plays, novels and garden-gate gossip are long term largely dispositional states, including jealousy, vengefulness, family love, obsessive ambition, infatuation, fascination with a mathematical problem, etc. There are other long term affective states such as preferences, and attitudes that are not normally called emotions. Of course, someone who defines an emotion as an episodic state in which there are particular sorts of bodily changes or sensed changes in a body state map will not include some of these long term states as emotions. But that’s just another example of the terminological disarray. Non-vicious cycles of defining relationships We","[{'authorId': '145788442', 'name': 'A. Sloman'}]",25.0,"{'bibtex': '@Inproceedings{Sloman2004WhatAE,\n author = {A. Sloman},\n title = {What Are Emotion Theories About},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",25.0,What Are Emotion Theories About,2004.0
463,24d257fcc11a751d53188313ca7921cbddb3175e,"Non-verbal behaviour, particularly gaze direction, plays a crucial function in regulating conversations and providing critical social information. In the current set of studies, we represented interactants in a shared immersive virtual environment. Interactants sat in physically remote rooms, entered a common virtual room and played games of 20 questions. The interactants were represented by one of three types of avatars: (1) human forms with head movements rendered in real time; (2) human forms without head movements rendered; or (3) human voice only (i.e., a conference call). The data demonstrated that interactants in the rendered head movement condition rated a higher level of co-presence, liked each other more, looked at each other's heads more, and spoke for a lower percentage of time during the game, compared to the other two conditions. We discuss implications for the design of shared virtual environments, the study of non-verbal behaviour and the goal of facilitating efficient task performance. Copyright # 2002 John Wiley & Sons, Ltd.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2307657', 'name': 'J. Blascovich'}]",140.0,"{'bibtex': '@Article{Bailenson2002GazeAT,\n author = {J. Bailenson and A. Beall and J. Blascovich},\n journal = {Comput. Animat. Virtual Worlds},\n pages = {313-320},\n title = {Gaze and task performance in shared virtual environments},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '313-320', 'name': 'Comput. Animat. Virtual Worlds'}",32.0,Gaze and task performance in shared virtual environments,2002.0
464,250576f2ec19924718bf621d34b75493e0d68618,,"[{'authorId': '1405511901', 'name': 'L. F. D’Haro'}, {'authorId': '2047181', 'name': 'Seokhwan Kim'}, {'authorId': '3174155', 'name': 'Kheng Hui Yeo'}, {'authorId': '2847726', 'name': 'Ridong Jiang'}, {'authorId': '34573320', 'name': 'Andreea Niculescu'}, {'authorId': '1694652', 'name': 'Rafael E. Banchs'}, {'authorId': '2119251083', 'name': 'Haizhou Li'}]",17.0,"{'bibtex': '@Inproceedings{D’Haro2015CLARAAM,\n author = {L. F. D’Haro and Seokhwan Kim and Kheng Hui Yeo and Ridong Jiang and Andreea Niculescu and Rafael E. Banchs and Haizhou Li},\n pages = {233-239},\n title = {CLARA: A Multifunctional Virtual Agent for Conference Support and Touristic Information},\n year = {2015}\n}\n'}",,{'pages': '233-239'},8.0,CLARA: A Multifunctional Virtual Agent for Conference Support and Touristic Information,2015.0
465,251c4e20bea350ba1c00f089809e27266f0ca80e,"Emotion‐ and attention‐related subjective and physiological responses to virtual proximity and facial expressions of embodied computer agents (ECA) were studied. Thirty participants viewed female and male characters with a neutral, unpleasant, or pleasant facial expression. Agents' size was used to simulate three levels of proximity. Participants' electrical facial muscle and heart activity were registered, and subjective ratings of emotional and attentional experiences collected. Unpleasant and large (i.e., closer) agents were more alerting (i.e., unpleasant, arousing, and dominating) and attracted more stimulus‐driven attention than neutral, pleasant, and smaller (i.e., further away) agents. Pleasant agents attracted more voluntary attention than neutral and unpleasant agents. Heart rate (HR) responded to agent proximity, while the valence of the agent affected electrical facial muscle activity. Thus, the imitation of human social emotional cues in embodied computer agents (ECAs) could be used to regulate human–computer interaction. Copyright © 2010 John Wiley & Sons, Ltd.","[{'authorId': '2011094', 'name': 'T. Vanhala'}, {'authorId': '1718377', 'name': 'Veikko Surakka'}, {'authorId': '1934593', 'name': 'H. Siirtola'}, {'authorId': '1724448', 'name': 'Kari-Jouko Räihä'}, {'authorId': '32214926', 'name': 'Benoît Morel'}, {'authorId': '2305305', 'name': 'Laurent Ach'}]",12.0,"{'bibtex': '@Article{Vanhala2010VirtualPA,\n author = {T. Vanhala and Veikko Surakka and H. Siirtola and Kari-Jouko Räihä and Benoît Morel and Laurent Ach},\n journal = {Computer Animation and Virtual Worlds},\n title = {Virtual proximity and facial expressions of computer agents regulate human emotions and attention},\n volume = {21},\n year = {2010}\n}\n'}",,"{'volume': '21', 'name': 'Computer Animation and Virtual Worlds'}",33.0,Virtual proximity and facial expressions of computer agents regulate human emotions and attention,2010.0
466,25228b341fc7bbc2eee7d8f42e0bd38f947c3046,"Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth information meaning. Our goals are: to translate written text from any language to ASL animation; to model maximum raw information using machine learning and computational techniques; and to produce a more adapted and expressive form to natural looking and understandable ASL animations. Our methods include linguistic annotation of initial text and semantic orientation to generate the facial expression. We use the genetic algorithms coupled to learning/recognized systems to produce the most natural form. To detect emotion we are based on fuzzy logic to produce the degree of interpolation between facial expressions. Roughly, we present a new expressive language Text Adapted Sign Modeling Language TASML that describes all maximum aspects related to a natural sign language interpretation. This paper is organized as follow: the next section is devoted to present the comprehension effect of using Space/Time/SVO form in ASL animation based on experimentation. In section 3, we describe our technical considerations. We present the general approach we adopted to develop our tool in section 4. Finally, we give some perspectives and future works.","[{'authorId': '2950670', 'name': 'Mehrez Boulares'}, {'authorId': '1696756', 'name': 'M. Jemni'}]",7.0,"{'bibtex': '@Article{Boulares2012TowardAE,\n author = {Mehrez Boulares and M. Jemni},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Toward an example-based machine translation from written text to ASL using virtual agent animation},\n volume = {abs/1203.3023},\n year = {2012}\n}\n'}","[{'paperId': '4ea0aa6838fd794413e1a92a3c0865a1bc0c1059', 'title': 'Machine translation from text to sign language: a systematic review'}, {'paperId': '2da192e6e9c29ed1adc62b96b8cd87c89e5489ce', 'title': 'Enhancing Bi-Lingual Example Based Machine Translation Approach'}, {'paperId': '0bdb55cf60f4735ce5349b8431cc98724640be5c', 'title': 'Evaluating Facial Expressions in American Sign Language Animations for Accessible Online Information'}, {'paperId': '9569165acb95582ecfe6595eb5f374eada0155a6', 'title': 'EB A CCESSIBILITY FOR D EAF'}, {'paperId': '5f20562783048054ab347457c8dbc93e65b2c4fd', 'title': 'PROPOSED IF_THEN GRAMMAR TO TRANSLATE ENGLISH LANGUAGE SENTENCE TO AMERICAN SIGN LANGUAGE SENTENCE'}, {'paperId': '33c9b01e2c23b790c17da1b06c028546d0ea265b', 'title': 'Data-Driven Synthesis and Evaluation of Syntactic Facial Expressions in American Sign Language Animation'}, {'paperId': '3ed46dea0faac71844fe7b415b2b81cfb94ace20', 'title': '2 IJAERS-SEP-2016-44'}]","{'name': 'ArXiv', 'volume': 'abs/1203.3023'}",24.0,Toward an example-based machine translation from written text to ASL using virtual agent animation,2012.0
467,2524e9fb27559cb505ceed996d377169d9430b57,"Rational criminals choose crime over lawfulness because it pays better; hence poverty correlates to criminal behavior. This correlation is an insufficient historical explanation. An agent-based model of urban crime, mortality, and exogenous population shocks supplements the standard economic story, closing the gap with an empirical reality that often breaks from trend. Agent decision making within the model is built around a career maximization function, with life expectancy as the key independent variable. Rational choice takes the form of a local information heuristic, resulting in subjectively rational suboptimal decision making. The effects of population shocks are explored using the Crime and Mortality Simulation (CAMSIM), with effects demonstrated to persist across generations. Past social trauma are found to lead to higher crime rates which subsequently decline as the effect degrades, though 'aftershocks' are often experienced.","[{'authorId': '37484915', 'name': 'M. Makowsky'}]",27.0,"{'bibtex': '@Article{Makowsky2006AnAM,\n author = {M. Makowsky},\n journal = {J. Artif. Soc. Soc. Simul.},\n title = {An Agent-Based Model of Mortality Shocks, Intergenerational Effects, and Urban Crime},\n volume = {9},\n year = {2006}\n}\n'}",,"{'volume': '9', 'name': 'J. Artif. Soc. Soc. Simul.'}",52.0,"An Agent-Based Model of Mortality Shocks, Intergenerational Effects, and Urban Crime",2006.0
468,2567a27bca06d4c409a1a603281cb78e68f233b1,"Personalized machine learning enables robot perception of children’s affective states and engagement during robot-assisted autism therapy. Robots have the potential to facilitate future therapies for children on the autism spectrum. However, existing robots are limited in their ability to automatically perceive and respond to human affect, which is necessary for establishing and maintaining engaging interactions. Their inference challenge is made even harder by the fact that many individuals with autism have atypical and unusually diverse styles of expressing their affective-cognitive states. To tackle the heterogeneity in children with autism, we used the latest advances in deep learning to formulate a personalized machine learning (ML) framework for automatic perception of the children’s affective states and engagement during robot-assisted autism therapy. Instead of using the traditional one-size-fits-all ML approach, we personalized our framework to each child using their contextual information (demographics and behavioral assessment scores) and individual characteristics. We evaluated this framework on a multimodal (audio, video, and autonomic physiology) data set of 35 children (ages 3 to 13) with autism, from two cultures (Asia and Europe), and achieved an average agreement (intraclass correlation) of ~60% with human experts in the estimation of affect and engagement, also outperforming nonpersonalized ML solutions. These results demonstrate the feasibility of robot perception of affect and engagement in children with autism and have implications for the design of future autism therapies.","[{'authorId': '143776835', 'name': 'Ognjen Rudovic'}, {'authorId': '77537854', 'name': 'Jaeryoung Lee'}, {'authorId': '1392261251', 'name': 'Miles Dai'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",227.0,"{'bibtex': '@Article{Rudovic2018PersonalizedML,\n author = {Ognjen Rudovic and Jaeryoung Lee and Miles Dai and Björn Schuller and Rosalind W. Picard},\n journal = {Science Robotics},\n title = {Personalized machine learning for robot perception of affect and engagement in autism therapy},\n volume = {3},\n year = {2018}\n}\n'}",,"{'volume': '3', 'name': 'Science Robotics'}",107.0,Personalized machine learning for robot perception of affect and engagement in autism therapy,2018.0
469,257843bde91001dc94fffdba287879fcb3b2f89a,An improved version of Kolmogorov's powerful 1957 theorem concerning the representation of arbitrary continuous functions from the n-dimensional cube to the real numbers in terms of one dimensional continuous functions is reinterpreted to yield an existence theorem for mapping neural networks.,"[{'authorId': '1398229863', 'name': 'R. Hecht-Nielsen'}]",913.0,"{'bibtex': ""@Inproceedings{Hecht-Nielsen1987KolmogorovsMN,\n author = {R. Hecht-Nielsen},\n title = {Kolmogorov''s Mapping Neural Network Existence Theorem},\n year = {1987}\n}\n""}",,,2.0,Kolmogorov''s Mapping Neural Network Existence Theorem,1987.0
470,258111f84a0bc25de53d82c389218541fa93bef3,"Social agents and robots will require both learning and emotional capabilities to successfully enter society. This paper connects both challenges, by studying models of emotion generation in sequential decision-making agents. Previous work in this field has focussed on model-free reinforcement learning (RL). However, important emotions like hope and fear need anticipation, which requires a model and forward simulation. Taking inspiration from the psychological Belief-Desire Theory of Emotions (BDTE), our work specifies models of hope and fear based on best and worst forward traces. To efficiently estimate these traces, we integrate a well-known Monte Carlo Tree Search procedure (UCT) into a model based RL architecture. Test results in three known RL domains illustrate emotion dynamics, dependencies on policy and environmental stochasticity, and plausibility in individual Pacman game settings. Our models enable agents to naturally elicit hope and fear during learning, and moreover, explain what anticipated event caused this.","[{'authorId': '13477045', 'name': 'T. Moerland'}, {'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '1689001', 'name': 'C. Jonker'}]",19.0,"{'bibtex': '@Inproceedings{Moerland2016FearAH,\n author = {T. Moerland and J. Broekens and C. Jonker},\n pages = {848-854},\n title = {Fear and Hope Emerge from Anticipation in Model-Based Reinforcement Learning},\n year = {2016}\n}\n'}",,{'pages': '848-854'},27.0,Fear and Hope Emerge from Anticipation in Model-Based Reinforcement Learning,2016.0
471,25830d039ed691cd1a65e617d69db20a9eb4af1c,"We have created a virtual human presenter who accepts speech texts with embedded commands as inputs. The presenter acts in real-time 3D animation synchronized with speech. The system was developed on the Jack animated-agent system. Jack provides a 3D graphical environment for controlling articulated figures, including detailed human models.","[{'authorId': '34664967', 'name': 'T. Noma'}, {'authorId': '2427954', 'name': 'Liwei Zhao'}, {'authorId': '1699200', 'name': 'N. Badler'}]",123.0,"{'bibtex': '@Article{Noma2000DesignOA,\n author = {T. Noma and Liwei Zhao and N. Badler},\n journal = {IEEE Computer Graphics and Applications},\n pages = {79-85},\n title = {Design of a Virtual Human Presenter},\n volume = {20},\n year = {2000}\n}\n'}",,"{'volume': '20', 'pages': '79-85', 'name': 'IEEE Computer Graphics and Applications'}",34.0,Design of a Virtual Human Presenter,2000.0
472,2591c59394e15a445abcaf0467c82ecc6f7dadc4,,"[{'authorId': '48790377', 'name': 'Aleksandra Kupferberg'}, {'authorId': '1829139', 'name': 'S. Glasauer'}, {'authorId': '50647134', 'name': 'Markus Huber'}, {'authorId': '1769232', 'name': 'Markus Rickert'}, {'authorId': '143873832', 'name': 'A. Knoll'}, {'authorId': '144501754', 'name': 'T. Brandt'}]",63.0,"{'bibtex': '@Article{Kupferberg2011BiologicalMI,\n author = {Aleksandra Kupferberg and S. Glasauer and Markus Huber and Markus Rickert and A. Knoll and T. Brandt},\n journal = {AI & SOCIETY},\n pages = {339-345},\n title = {Biological movement increases acceptance of humanoid robots as human partners in motor interaction},\n volume = {26},\n year = {2011}\n}\n'}",,"{'volume': '26', 'pages': '339-345', 'name': 'AI & SOCIETY'}",46.0,Biological movement increases acceptance of humanoid robots as human partners in motor interaction,2011.0
473,25932c162ac3e3cbf14d333277dfcc2b53b9bd0c,"Dynamic Bayesian Networks: Representation, Inference and Learning by Kevin Patrick Murphy Doctor of Philosophy in Computer Science University of California, Berkeley Professor Stuart Russell, Chair Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from sequential data. In particular, the main novel technical contributions of this thesis are as follows: a way of representing Hierarchical HMMs as DBNs, which enables inference to be done in O(T ) time instead of O(T ), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T ) space instead of O(T ); a simple way of using the junction tree algorithm for online inference in DBNs; new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the BK algorithm and loopy belief propagation; a way of applying Rao-Blackwellised particle filtering to DBNs in general, and the SLAM (simultaneous localization and mapping) problem in particular; a way of extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling.","[{'authorId': '2056417995', 'name': 'Kevin P. Murphy'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2893.0,"{'bibtex': '@Inproceedings{Murphy2002DynamicBN,\n author = {Kevin P. Murphy and Stuart J. Russell},\n title = {Dynamic bayesian networks: representation, inference and learning},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",425.0,"Dynamic bayesian networks: representation, inference and learning",2002.0
474,25ba57fba0eb05b08273f6e448911842ca7041cc,Background: Heart rate variability (HRV) is an accepted and reliable means for assessing autonomic nervous system dysfunction. A 5‐minute measurement of HRV is considered methodologically adequate. Several studies have attempted to use shorter recordings of 1–2 minutes or 10 seconds. The aim of this study was to determine the reliability of HRV parameters calculated from ultra‐short electrocardiogram recordings.,"[{'authorId': '6623541', 'name': 'U. Nussinovitch'}, {'authorId': '7996656', 'name': 'K. Elishkevitz'}, {'authorId': '40248580', 'name': 'Keren Katz'}, {'authorId': '7322414', 'name': 'M. Nussinovitch'}, {'authorId': '3823238', 'name': 'S. Segev'}, {'authorId': '8592109', 'name': 'B. Volovitz'}, {'authorId': '7482777', 'name': 'N. Nussinovitch'}]",168.0,"{'bibtex': '@Article{Nussinovitch2011ReliabilityOU,\n author = {U. Nussinovitch and K. Elishkevitz and Keren Katz and M. Nussinovitch and S. Segev and B. Volovitz and N. Nussinovitch},\n journal = {Annals of Noninvasive Electrocardiology},\n title = {Reliability of Ultra‐Short ECG Indices for Heart Rate Variability},\n volume = {16},\n year = {2011}\n}\n'}",,"{'volume': '16', 'name': 'Annals of Noninvasive Electrocardiology'}",17.0,Reliability of Ultra‐Short ECG Indices for Heart Rate Variability,2011.0
475,25ff416daef2991a1201af46f20f99d7490c037d,"We have developed an emotional agent based on reinforcement learning. The agent is a virtual character who lives in a three-dimensional maze world. Thus, we found that an emotional engine can induce the behavior of an agent after training. The training algorithm was the policy based reinforcement learning. Experiments showed that the emotional engine and reinforcement learning produced coherent behaviors. This is an important step towards believable autonomous virtual characters.","[{'authorId': '153468821', 'name': 'Gilzamir Gomes'}, {'authorId': '1872209', 'name': 'C. Vidal'}, {'authorId': '1870162', 'name': 'J. B. C. Neto'}, {'authorId': '2889936', 'name': 'Y. L. Nogueira'}]",5.0,"{'bibtex': '@Article{Gomes2019AnEV,\n author = {Gilzamir Gomes and C. Vidal and J. B. C. Neto and Y. L. Nogueira},\n journal = {2019 21st Symposium on Virtual and Augmented Reality (SVR)},\n pages = {223-231},\n title = {An Emotional Virtual Character: A Deep Learning Approach with Reinforcement Learning},\n year = {2019}\n}\n'}",,"{'pages': '223-231', 'name': '2019 21st Symposium on Virtual and Augmented Reality (SVR)'}",20.0,An Emotional Virtual Character: A Deep Learning Approach with Reinforcement Learning,2019.0
476,2606ce6e184d1a700974c3fd23d02c4ecd2010b9,,"[{'authorId': '3188061', 'name': 'Angela Tinwell'}, {'authorId': '2875451', 'name': 'D. Nabi'}, {'authorId': '20513080', 'name': 'John P. Charlton'}]",64.0,"{'bibtex': '@Article{Tinwell2013PerceptionOP,\n author = {Angela Tinwell and D. Nabi and John P. Charlton},\n journal = {Comput. Hum. Behav.},\n pages = {1617-1625},\n title = {Perception of psychopathy and the Uncanny Valley in virtual characters},\n volume = {29},\n year = {2013}\n}\n'}",,"{'volume': '29', 'pages': '1617-1625', 'name': 'Comput. Hum. Behav.'}",55.0,Perception of psychopathy and the Uncanny Valley in virtual characters,2013.0
477,260ce65b53822737bf29f571e81aaaba4da24f9d,,"[{'authorId': '32716879', 'name': 'Maya B. Mathur'}, {'authorId': '1829092', 'name': 'D. Reichling'}]",304.0,"{'bibtex': '@Article{Mathur2016NavigatingAS,\n author = {Maya B. Mathur and D. Reichling},\n journal = {Cognition},\n pages = {22-32},\n title = {Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley},\n volume = {146},\n year = {2016}\n}\n'}",,"{'volume': '146', 'pages': '22-32', 'name': 'Cognition'}",43.0,Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley,2016.0
478,2614277d969d2330935ba00920a3bbbaad40d4d7,"Affective signal processing algorithms were developed to allow a digital computer to recognize the affective state of a user who is intentionally expressing that state. This paper describes the method used for collecting the training data, the feature extraction algorithms used and the results of pattern recognition using a Fisher linear discriminant and the leave one out test method. Four physiological signals, skin conductivity, blood volume pressure, respiration and an electromyogram (EMG) on the masseter muscle were analyzed. It was found that anger was well differentiated from peaceful emotions (90%-100%), that high and low arousal states were distinguished (80%-88%), but positive and negative valence states were difficult to distinguish (50%-82%). Subsets of three emotion states could be well separated (75%-87%) and characteristic patterns for single emotions were found.","[{'authorId': '144139962', 'name': 'Jennifer Healey'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",138.0,"{'bibtex': ""@Article{Healey1998DigitalPO,\n author = {Jennifer Healey and Rosalind W. Picard},\n journal = {Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)},\n pages = {3749-3752 vol.6},\n title = {Digital processing of affective signals},\n volume = {6},\n year = {1998}\n}\n""}",,"{'volume': '6', 'pages': '3749-3752 vol.6', 'name': ""Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)""}",13.0,Digital processing of affective signals,1998.0
479,261f46fb5a1e58fbbb0089ec3ac21bb87bdb11b3,,"[{'authorId': '3114187', 'name': 'Johnathan Mell'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",34.0,"{'bibtex': '@Inproceedings{Mell2016IAGOIA,\n author = {Johnathan Mell and J. Gratch},\n title = {IAGO: Interactive Arbitration Guide Online},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,IAGO: Interactive Arbitration Guide Online,2016.0
480,261fa69e3260296652e02d8757f9af16f7dd6ac9,,"[{'authorId': '4529011', 'name': 'Elgiz Bal'}, {'authorId': '39397887', 'name': 'E. Harden'}, {'authorId': '145018594', 'name': 'Damon G. Lamb'}, {'authorId': '40169180', 'name': 'A. V. Van Hecke'}, {'authorId': '2086256166', 'name': 'John W. Denver'}, {'authorId': '4226466', 'name': 'S. Porges'}]",555.0,"{'bibtex': '@Article{Bal2010EmotionRI,\n author = {Elgiz Bal and E. Harden and Damon G. Lamb and A. V. Van Hecke and John W. Denver and S. Porges},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {358-370},\n title = {Emotion Recognition in Children with Autism Spectrum Disorders: Relations to Eye Gaze and Autonomic State},\n volume = {40},\n year = {2010}\n}\n'}",,"{'volume': '40', 'pages': '358-370', 'name': 'Journal of Autism and Developmental Disorders'}",70.0,Emotion Recognition in Children with Autism Spectrum Disorders: Relations to Eye Gaze and Autonomic State,2010.0
481,2628ef74935d0c2d54319bcaf43e734a456753bf,"In this paper we investigate how agents can facilitate and mediate interaction, communication and cooperation among participants of spatially distributed teams. We illustrate the architecture of an agent mediated-collaborative system that can serve the role of a tutor within a virtual group. In the virtual group the software agent should play besides the tutoring role also the group management role. The group manager has the responsibility to control the coherence of the actual group in regard to the actual group structure definition. This research will highlight also the incorporation of social-filter algorithms to mental models of software animated agents. Those algorithms may qualify an agent's expression of its emotional state by the social context, thereby enhancing the agent's believability not only as a tutor but also as a conversational partner or virtual teammate.","[{'authorId': '1824479', 'name': 'B. Marin'}, {'authorId': '1769376', 'name': 'A. Hunger'}, {'authorId': '48801263', 'name': 'S. Werner'}, {'authorId': '2495654', 'name': 'S. Meila'}, {'authorId': '2069483319', 'name': 'Christian Schuetz'}]",9.0,"{'bibtex': '@Article{Marin2005RolesOA,\n author = {B. Marin and A. Hunger and S. Werner and S. Meila and Christian Schuetz},\n journal = {The 2005 Symposium on Applications and the Internet},\n pages = {237-244},\n title = {Roles of an intelligent tutor agent in a virtual society},\n year = {2005}\n}\n'}",,"{'pages': '237-244', 'name': 'The 2005 Symposium on Applications and the Internet'}",24.0,Roles of an intelligent tutor agent in a virtual society,2005.0
482,263aa65ec7fc8631fb058420e4a4e9f95916d5b7,"Controlled interpersonal affect regulation refers to the deliberate regulation of someone else's affect. Building on existing research concerning this everyday process, the authors describe the development of a theoretical classification scheme that distinguishes between the types of strategy used to achieve interpersonal affect regulation. To test the theoretical classification, the authors generated a corpus of 378 distinct strategies using self-report questionnaires and diaries completed by student and working samples. Twenty participants then performed a card-sort of the strategies. Hierarchical cluster analysis was used to determine how well the theoretical classification represented spontaneous understandings of controlled interpersonal affect regulation. The final classification primarily distinguished between strategies used to improve versus those used to worsen others' affect, and between strategies that engaged the target in a situation or affective state versus relationship-oriented strategies. The classification provides a meaningful basis for organizing existing research and making future conceptual and empirical distinctions.","[{'authorId': '145276844', 'name': 'Karen Niven'}, {'authorId': '1993236', 'name': 'P. Totterdell'}, {'authorId': '78063653', 'name': 'D. Holman'}]",275.0,"{'bibtex': '@Article{Niven2009ACO,\n author = {Karen Niven and P. Totterdell and D. Holman},\n journal = {Emotion},\n pages = {\n          498-509\n        },\n title = {A classification of controlled interpersonal affect regulation strategies.},\n volume = {9 4},\n year = {2009}\n}\n'}",,"{'volume': '9 4', 'pages': '\n          498-509\n        ', 'name': 'Emotion'}",57.0,A classification of controlled interpersonal affect regulation strategies.,2009.0
483,2651821004f00566a0ec77d5ca434b64b49a9776,"This approach extends the HiDAC (High-Density Autonomous Crowds) system by providing each agent with a personality model based on the Ocean (openness, conscientiousness, extroversion, agreeableness, and neuroticism) personality model. Each personality trait has an associated nominal behavior. Specifying an agent's personality leads to an automation of low-level parameter tuning.","[{'authorId': '2643744', 'name': 'Funda Durupinar'}, {'authorId': '1746484', 'name': 'N. Pelechano'}, {'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '1746035', 'name': 'U. Güdükbay'}, {'authorId': '1699200', 'name': 'N. Badler'}]",146.0,"{'bibtex': '@Article{Durupinar2011HowTO,\n author = {Funda Durupinar and N. Pelechano and J. Allbeck and U. Güdükbay and N. Badler},\n journal = {IEEE Computer Graphics and Applications},\n pages = {22-31},\n title = {How the Ocean Personality Model Affects the Perception of Crowds},\n volume = {31},\n year = {2011}\n}\n'}",,"{'volume': '31', 'pages': '22-31', 'name': 'IEEE Computer Graphics and Applications'}",16.0,How the Ocean Personality Model Affects the Perception of Crowds,2011.0
486,26d8011bd3acea073ae6d3014f295928a0585f14,"Research suggests that a person's emotion recognition declines with advancing years. We examined whether or not this age-related decline was attributable to a tendency to overlook emotion information in the eyes. In Experiment 1, younger adults were significantly better than older adults at inferring emotions from full faces and eyes, though not from mouths. Using an eye tracker in Experiment 2, we found young adults, in comparison with older adults, to have superior emotion recognition performance and to look proportionately more to eyes than mouths. However, although better emotion recognition performance was significantly correlated with more eye looking in younger adults, the same was not true in older adults. We discuss these results in terms of brain changes with age.","[{'authorId': '116432126', 'name': 'Susan Sullivan'}, {'authorId': '3890791', 'name': 'T. Ruffman'}, {'authorId': '2570066', 'name': 'S. Hutton'}]",207.0,"{'bibtex': '@Article{Sullivan2007AgeDI,\n author = {Susan Sullivan and T. Ruffman and S. Hutton},\n journal = {The journals of gerontology. Series B, Psychological sciences and social sciences},\n pages = {\n          P53-60\n        },\n title = {Age differences in emotion recognition skills and the visual scanning of emotion faces.},\n volume = {62 1},\n year = {2007}\n}\n'}",,"{'volume': '62 1', 'pages': '\n          P53-60\n        ', 'name': 'The journals of gerontology. Series B, Psychological sciences and social sciences'}",59.0,Age differences in emotion recognition skills and the visual scanning of emotion faces.,2007.0
487,27130991e1649398d98a00746bdbd1616ca915ea,,"[{'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '1742930', 'name': 'E. André'}]",37.0,"{'bibtex': '@Inproceedings{Rehm2005WhereDT,\n author = {M. Rehm and E. André},\n pages = {241-252},\n title = {Where Do They Look? Gaze Behaviors of Multiple Users Interacting with an Embodied Conversational Agent},\n year = {2005}\n}\n'}",,{'pages': '241-252'},18.0,Where Do They Look? Gaze Behaviors of Multiple Users Interacting with an Embodied Conversational Agent,2005.0
488,271af3b3a3d054c7d6eb2da78635710445dcf0db,"In this study, a collision avoidance system is presented, based on the information provided by a laser-scanner sensor, in which two actions could be taken in case of danger. Firstly, the system tries to stop the vehicle in order to avoid the accident. If a reduction in speed is not sufficiently effective, the control system takes control of the steering and deviates the vehicle's trajectory in order to escape from the hazardous situation. The control system evaluates the situation and decides the most appropriate action in each case considering free areas on the surroundings using the information of a detailed digital map. This system has been implemented in a vehicle and has been tested with pedestrians and vehicles circulating along the private test track with satisfactory results.","[{'authorId': '143787360', 'name': 'F. Jiménez'}, {'authorId': '144721820', 'name': 'J. Naranjo'}, {'authorId': '144763950', 'name': 'Óscar Gómez'}]",37.0,"{'bibtex': '@Article{Jiménez2015AutonomousCA,\n author = {F. Jiménez and J. Naranjo and Óscar Gómez},\n journal = {Iet Intelligent Transport Systems},\n pages = {105-117},\n title = {Autonomous collision avoidance system based on accurate knowledge of the vehicle surroundings},\n volume = {9},\n year = {2015}\n}\n'}",,"{'volume': '9', 'pages': '105-117', 'name': 'Iet Intelligent Transport Systems'}",31.0,Autonomous collision avoidance system based on accurate knowledge of the vehicle surroundings,2015.0
489,27276f51b124dcaf11d382ee9c193dda087eb237,"This paper evaluates the use of Behavior Trees (BT) for authoring compelling narrative experiences with free-form user interaction. We systematically study extensions to traditionally BT representations, which decouple the monitoring of user input, the narrative, and how the user may influence the story outcome – referred to as Interactive Behavior Trees (IBT’s). By quantitatively evaluating the authoring complexity of BT formalisms with traditional story graph representations, we show that IBT’s better scale with the number of story arcs, and the degree and granularity of user input. Our theoretical estimate of authoring complexity is corroborated with a qualitative user study, which confirms that subjects take lesser time with reduced effort to author narratives using IBT’s. The subjective difficulty of IBT’s is also lower than traditional story graphs.","[{'authorId': '143980997', 'name': 'M. Kapadia'}, {'authorId': '145658471', 'name': 'Marcel Marti'}, {'authorId': '2257153235', 'name': 'M. Gross'}]",14.0,"{'bibtex': '@Inproceedings{Kapadia2015EvaluatingTA,\n author = {M. Kapadia and Marcel Marti and M. Gross},\n title = {Evaluating the Authoring Complexity of Interactive Narratives with Interactive Behaviour Trees},\n year = {2015}\n}\n'}",,,34.0,Evaluating the Authoring Complexity of Interactive Narratives with Interactive Behaviour Trees,2015.0
490,2728598e875568640698ee379148daa8c10a406b,"Reason and emotion have long been considered opposing forces. However, recent psychological and neuroscientific research has revealed that emotion and cognition are closely intertwined. Cognitive processing is needed to elicit emotional responses. At the same time, emotional responses modulate and guide cognition to enable adaptive responses to the environment. Emotion determines how we perceive our world, organise our memory, and make important decisions. In this review, we provide an overview of current theorising and research in the Affective Sciences. We describe how psychological theories of emotion conceptualise the interactions of cognitive and emotional processes. We then review recent research investigating how emotion impacts our perception, attention, memory, and decision-making. Drawing on studies with both healthy participants and clinical populations, we illustrate the mechanisms and neural substrates underlying the interactions of cognition and emotion.","[{'authorId': '2256291', 'name': 'T. Brosch'}, {'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '1797080', 'name': 'D. Grandjean'}, {'authorId': '143868107', 'name': 'D. Sander'}]",224.0,"{'bibtex': '@Article{Brosch2013TheIO,\n author = {T. Brosch and K. Scherer and D. Grandjean and D. Sander},\n journal = {Swiss medical weekly},\n pages = {\n          w13786\n        },\n title = {The impact of emotion on perception, attention, memory, and decision-making.},\n volume = {143},\n year = {2013}\n}\n'}",,"{'volume': '143', 'pages': '\n          w13786\n        ', 'name': 'Swiss medical weekly'}",58.0,"The impact of emotion on perception, attention, memory, and decision-making.",2013.0
491,2729b8906721af630ef346588f3e5b8fa87a12ea,,"[{'authorId': '1935867', 'name': 'Anastasis D. Petrou'}]",397.0,"{'bibtex': '@Article{Petrou2003ReviewO,\n author = {Anastasis D. Petrou},\n journal = {Journal of the Association for Information Science and Technology},\n pages = {1168-1170},\n title = {Review of “Persuasive technology: Using computers to change what we think and do by B. J. Fogg” Morgan Kaufmann, 2003},\n volume = {54},\n year = {2003}\n}\n'}",,"{'volume': '54', 'pages': '1168-1170', 'name': 'Journal of the Association for Information Science and Technology'}",0.0,"Review of “Persuasive technology: Using computers to change what we think and do by B. J. Fogg” Morgan Kaufmann, 2003",2003.0
492,279fc4fe94e5914b5570e5eb38f2785f2928d32a,"We live our lives in digital networks. We wake up in the morning, check our e-mail, make a quick phone call, commute to work, buy lunch. Many of these transactions leave digital breadcrumbs--tiny records of our daily experiences. Reality mining, which pulls together these crumbs using statistical analysis and machine learning methods, offers an increasingly comprehensive picture of our lives, both individually and collectively, with the potential of transforming our understanding of ourselves, our organizations, and our society in a fashion that was barely conceivable just a few years ago. It is for this reason that reality mining was recently identified by Technology Review as one of ""10 emerging technologies that could change the world"". Many everyday devices provide the raw database upon which reality mining builds; sensors in mobile phones, cars, security cameras, RFID ('smart card') readers, and others, all allow for the measurement of human physical and social activity. Computational models based on such data have the potential to dramatically transform the arenas of both individual and community health. Reality mining can provide new opportunities with respect to diagnosis, patient and treatment monitoring, health services planning, surveillance of disease and risk factors, and public health investigation and disease control. Currently, the single most important source of reality mining data is the ubiquitous mobile phone. Every time a person uses a mobile phone, a few bits of information are left behind. The phone pings the nearest mobile-phone towers, revealing its location. The mobile phone service provider records the duration of the call and the number dialed. In the near future, mobile phones and other technologies will collect even more information about their users, recording everything from their physical activity to their conversational cadences. While such data pose a potential threat to individual privacy, they also offer great potential value both to individuals and communities. With the aid of data-mining algorithms, these data could shed light on individual patterns of behavior and even on the well-being of communities, creating new ways to improve public health and medicine. To illustrate, consider two examples of how reality mining may benefit individual health care. By taking advantage of special sensors in mobile phones, such as the microphone or the accelerometers built into newer devices such as Apple's iPhone, important diagnostic data can be captured. Clinical pilot data demonstrate that it may be possible to diagnose depression from the way a person talks--a depressed person tends to speak more slowly, a change that speech analysis software on a phone might recognize more readily than friends or family do. Similarly, monitoring a phone's motion sensors can also reveal small changes in gait, which could be an early indicator of ailments such as Parkinson's disease. Within the next few years reality mining will become more common, thanks in part to the proliferation and increasing sophistication of mobile phones. Many handheld devices now have the processing power of low-end desktop computers, and they can also collect more varied data, due to components such as GPS chips that track location. The Chief Technology Officer of EMC, a large digital storage company, estimates that this sort of personal sensor data will balloon from 10% of all stored information to 90% within the next decade. While the promise of reality mining is great, the idea of collecting so much personal information naturally raises many questions about privacy. It is crucial that behavior-logging technology not be forced on anyone. But legal statutes are lagging behind data collection capabilities, making it particularly important to begin discussing how the technology will and should be used. Therefore, an additional focus of this chapter will be the development of a legal and ethical framework concerning the data used by reality mining techniques.","[{'authorId': '1682773', 'name': 'A. Pentland'}, {'authorId': '3185333', 'name': 'D. Lazer'}, {'authorId': '2056661864', 'name': 'Devon Brewer'}, {'authorId': '3204796', 'name': 'T. Heibeck'}]",87.0,"{'bibtex': '@Article{Pentland2009UsingRM,\n author = {A. Pentland and D. Lazer and Devon Brewer and T. Heibeck},\n journal = {Studies in health technology and informatics},\n pages = {\n          93-102\n        },\n title = {Using reality mining to improve public health and medicine.},\n volume = {149},\n year = {2009}\n}\n'}",,"{'volume': '149', 'pages': '\n          93-102\n        ', 'name': 'Studies in health technology and informatics'}",22.0,Using reality mining to improve public health and medicine.,2009.0
493,27b99851aba18216d6a498c6f959d293dda78c88,"In this article, we present ongoing research, EMO, an affective embodied conversational agent platform, aimed at depicting multi-ethnic, multi-modal communication patterns in a credible manner. We employ the methodology of integrating counseling concepts early in the design to effectively target a specific domain. The system is geared to augment solution focused therapy. We present a prototype of the architecture as proof of concept and evaluate the platform for affect portrayal.","[{'authorId': '145743883', 'name': 'Mark Allison'}, {'authorId': '143773676', 'name': 'Lynne Kendrick'}]",9.0,"{'bibtex': '@Inproceedings{Allison2013TowardsAE,\n author = {Mark Allison and Lynne Kendrick},\n title = {Towards an Expressive Embodied Conversational Agent Utilizing Multi-Ethnicity to Augment Solution Focused Therapy},\n year = {2013}\n}\n'}",,"{'volume': '', 'name': ''}",26.0,Towards an Expressive Embodied Conversational Agent Utilizing Multi-Ethnicity to Augment Solution Focused Therapy,2013.0
494,27caf712eb6f7eb4525e5c0759c4f989f54e706b,,"[{'authorId': '3188061', 'name': 'Angela Tinwell'}, {'authorId': '1778324', 'name': 'M. Grimshaw-Aagaard'}, {'authorId': '2875451', 'name': 'D. Nabi'}, {'authorId': '2110116607', 'name': 'Andrew Williams'}]",222.0,"{'bibtex': '@Article{Tinwell2011FacialEO,\n author = {Angela Tinwell and M. Grimshaw-Aagaard and D. Nabi and Andrew Williams},\n journal = {Comput. Hum. Behav.},\n pages = {741-749},\n title = {Facial expression of emotion and perception of the Uncanny Valley in virtual characters},\n volume = {27},\n year = {2011}\n}\n'}",,"{'volume': '27', 'pages': '741-749', 'name': 'Comput. Hum. Behav.'}",52.0,Facial expression of emotion and perception of the Uncanny Valley in virtual characters,2011.0
495,27daf5bb692f8fba491f349ab11821e59f974c72,,"[{'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '1751126', 'name': 'F. Eyben'}, {'authorId': '2228246', 'name': 'G. McKeown'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",297.0,"{'bibtex': '@Inproceedings{Schuller2011AVEC2F,\n author = {Björn Schuller and M. Valstar and F. Eyben and G. McKeown and R. Cowie and M. Pantic},\n pages = {415-424},\n title = {AVEC 2011-The First International Audio/Visual Emotion Challenge},\n year = {2011}\n}\n'}",,{'pages': '415-424'},20.0,AVEC 2011-The First International Audio/Visual Emotion Challenge,2011.0
496,28059b5dad91567ae0dde5827485803441e7dbbc,"There is now considerable evidence in social psychology, economics, and related disciplines that emotion plays an important role in negotiation. For example, humans make greater concessions in negotiation to an opposing human who expresses anger, and they make fewer concessions to an opponent who expresses happiness, compared to a no-emotion-expression control. However, in AI, despite the wide interest in negotiation as a means to resolve differences between agents and humans, emotion has been largely ignored. This paper explores whether expression of anger or happiness by computer agents, in a multi-issue negotiation task, can produce effects that resemble effects seen in human-human negotiation. The paper presents an experiment where participants play with agents that express emotions (anger vs. happiness vs. control) through different modalities (text vs. facial displays). An important distinction in our experiment is that participants are aware that they negotiate with computer agents. The data indicate that the emotion effects observed in past work with humans also occur in agent-human negotiation, and occur independently of modality of expression. The implications of these results are discussed for the fields of automated negotiation, intelligent virtual agents and artificial intelligence.","[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",125.0,"{'bibtex': '@Inproceedings{Melo2011TheEO,\n author = {C. D. Melo and P. Carnevale and J. Gratch},\n pages = {937-944},\n title = {The effect of expression of anger and happiness in computer agents on negotiations with humans},\n year = {2011}\n}\n'}",,{'pages': '937-944'},77.0,The effect of expression of anger and happiness in computer agents on negotiations with humans,2011.0
499,280c65b4d1b448a3e8456076e0b1e3df70624d4b,,"[{'authorId': '2216252', 'name': 'Dane Kuiper'}, {'authorId': '1397963104', 'name': 'Rym Zalila-Wenkstern'}]",19.0,"{'bibtex': '@Article{Kuiper2015AgentVI,\n author = {Dane Kuiper and Rym Zalila-Wenkstern},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {161-191},\n title = {Agent vision in multi-agent based simulation systems},\n volume = {29},\n year = {2015}\n}\n'}",,"{'volume': '29', 'pages': '161-191', 'name': 'Autonomous Agents and Multi-Agent Systems'}",39.0,Agent vision in multi-agent based simulation systems,2015.0
500,2823a1acbb4ca3fbb74608f6879dd2047fb5af6b,"A recent Nature article provided preliminary evidence that infants age 2–6 months old, who were later diagnosed with Autism Spectrum Disorder (ASD), fixated more on the mouth than eyes and more at objects than people when viewing videos of typical childhood social scenes (1). While the sample was small, a reliable pattern of decline in eye fixation accurately predicted their level and classification of symptoms at age three suggesting that – for the first time – an infant could be assessed within the first 6 months of life for their potential of developing ASD (see Table ​Table11 for studies that used eye-tracking with infants 12 months and younger). These eye-tracking devices, which are currently in clinical trials, could provide access to an affordable and objective tool with the potential for extremely early intervention. Detecting ASD risk during the first 6 months of life presents unprecedented opportunities to intervene, providing children opportunities to build critical skills before autistic characteristics fully emerge. Because the eye-tracking device allows for a non-invasive, portable assessment, the device could also enable pediatricians to provide comparable screening services globally. With such promise, a near future where infants are placed into an eye-tracking device at routine pediatric visits is compelling, if not guaranteed.","[{'authorId': '4606462', 'name': 'Jennifer C. Sarrett'}, {'authorId': '4833221', 'name': 'K. Rommelfanger'}]",440.0,"{'bibtex': '@Article{Sarrett2015CommentaryAT,\n author = {Jennifer C. Sarrett and K. Rommelfanger},\n journal = {Frontiers in Public Health},\n title = {Commentary: Attention to Eyes Is Present but in Decline in 2–6-Month-Old Infants Later Diagnosed with Autism},\n volume = {3},\n year = {2015}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Public Health'}",40.0,Commentary: Attention to Eyes Is Present but in Decline in 2–6-Month-Old Infants Later Diagnosed with Autism,2015.0
501,282ba3822f8032614ab269b0b5c66b10ac4cc02e,"Any autonomous agent behaviour generation mechanism should incorporate as a core module, a source of internal motivation that functions as a start point for agent behaviour to commence. Intelligent virtual agents are typically respondent to external stimuli, however, their behaviour becomes repetitive and trivial when these stimuli are missing. We argue that it is necessary for virtual agents to be equipped with intrinsic motivations that energise and direct their behaviour, in order to function in a coherent and believable way. Adopting the general principles of hierarchical motivation theories, in the current work, we attempt to model physiological needs as the lowest and basic level of motivations, in a layered motivational architecture. Based on readings from physiology, we present the mechanisms underlying the function of four basic needs and propose a model that allows the incorporation of plausible human-like needs in an intelligent virtual agent.","[{'authorId': '1943153', 'name': 'N. Avradinis'}, {'authorId': '1743830', 'name': 'T. Panayiotopoulos'}, {'authorId': '2619408', 'name': 'George Anastassakis'}]",1.0,"{'bibtex': '@Article{Avradinis2013ModellingBN,\n author = {N. Avradinis and T. Panayiotopoulos and George Anastassakis},\n journal = {Int. J. Comput. Intell. Stud.},\n pages = {52-75},\n title = {Modelling basic needs as agent motivations},\n volume = {2},\n year = {2013}\n}\n'}",,"{'volume': '2', 'pages': '52-75', 'name': 'Int. J. Comput. Intell. Stud.'}",46.0,Modelling basic needs as agent motivations,2013.0
502,2835fb7ffe5c6d5fad7b22a6e8eb553f20072b9a,"The number of young people not in employment, education or training is increasing across Europe. These youngsters often lack self-confidence and the essential social skills needed to seek and secure employment. The TARDIS project aims to build a scenario-based serious-game simulation platform for young people at risk of exclusion to improve their social skills. This paper intends to propose a model for a socio-emotionally realistic virtual agent in the context of job interview simulations. Our model of affect is composed of emotions, moods, social attitudes and personality that intends to create a realistic virtual recruiter.","[{'authorId': '2075858060', 'name': 'Hazaël Jones Lip'}]",20.0,"{'bibtex': '@Inproceedings{Lip2013TARDISASP,\n author = {Hazaël Jones Lip},\n title = {TARDIS-A simulation platform with an affective virtual recruiter for job interviews},\n year = {2013}\n}\n'}",,,33.0,TARDIS-A simulation platform with an affective virtual recruiter for job interviews,2013.0
503,28738fd2b43d1463714dda4ff9face9988d37596,,"[{'authorId': '50725217', 'name': 'L. Mallett'}, {'authorId': '152978807', 'name': 'R. Unger'}]",30.0,"{'bibtex': '@Misc{None,\n author = {L. Mallett and R. Unger},\n title = {Virtual Reality in Mine Training}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Virtual Reality in Mine Training,
506,287dae5b1fc7a9ea70597ce393841d86a5916285,"The Chatbots are the computer programs that interact with the users using natural language. The chatbot stores the information in the database to identify the keywords from the sentences and make a decision for the query and answers the question. In this paper keyword, ranking and sentence similarity calculation is done using n-gram, TF-IDF and cosine similarity. From the given input sentence, the score will be obtained for each sentence and more similar sentences are obtained for the given query. The query posed to the bot which isn't comprehended or not present in the database is further processed by the third party, expert system.","[{'authorId': '144158655', 'name': 'B. Kavitha'}, {'authorId': '145379162', 'name': 'C. R. Murthy'}]",49.0,"{'bibtex': '@Article{Kavitha2019ChatbotFH,\n author = {B. Kavitha and C. R. Murthy},\n journal = {International Research Journal of Modernization in Engineering Technology and Science},\n title = {Chatbot for Healthcare System Using Artificial Intelligence},\n year = {2019}\n}\n'}",,{'name': 'International Research Journal of Modernization in Engineering Technology and Science'},10.0,Chatbot for Healthcare System Using Artificial Intelligence,2019.0
507,288d7952b6648749fcbdcedabedf8f43cf7fda52,,"[{'authorId': '145796793', 'name': 'S. Franklin'}, {'authorId': '1769251', 'name': 'A. Graesser'}]",2832.0,"{'bibtex': '@Inproceedings{Franklin1996IsIA,\n author = {S. Franklin and A. Graesser},\n pages = {21-35},\n title = {Is it an Agent, or Just a Program?: A Taxonomy for Autonomous Agents},\n year = {1996}\n}\n'}",,{'pages': '21-35'},27.0,"Is it an Agent, or Just a Program?: A Taxonomy for Autonomous Agents",1996.0
508,288fcf20f4057c6eb7f244d1ea31879cc7767a8c,,"[{'authorId': '1944058', 'name': 'Chong-woo Woo'}, {'authorId': '2186744', 'name': 'M. Evens'}, {'authorId': '21285711', 'name': 'Reva Freedman'}, {'authorId': '143742138', 'name': 'Michael Glass'}, {'authorId': '3121624', 'name': 'L. Shim'}, {'authorId': '2145914494', 'name': 'Yuemei Zhang'}, {'authorId': '11362180', 'name': 'Yujian Zhou'}, {'authorId': '144125007', 'name': 'J. Michael'}]",55.0,"{'bibtex': '@Article{Woo2006AnIT,\n author = {Chong-woo Woo and M. Evens and Reva Freedman and Michael Glass and L. Shim and Yuemei Zhang and Yujian Zhou and J. Michael},\n journal = {Artificial intelligence in medicine},\n pages = {\n          25-46\n        },\n title = {An intelligent tutoring system that generates a natural language dialogue using dynamic multi-level planning},\n volume = {38 1},\n year = {2006}\n}\n'}",,"{'volume': '38 1', 'pages': '\n          25-46\n        ', 'name': 'Artificial intelligence in medicine'}",102.0,An intelligent tutoring system that generates a natural language dialogue using dynamic multi-level planning,2006.0
509,28c340988929b61e2b2ccd403b5b00256e1ed0e1,,"[{'authorId': '5672179', 'name': 'C. Judd'}, {'authorId': '144822864', 'name': 'G. McClelland'}, {'authorId': '31642890', 'name': 'C. Ryan'}]",122.0,"{'bibtex': '@Inproceedings{Judd2017DataAA,\n author = {C. Judd and G. McClelland and C. Ryan},\n title = {Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, Third Edition},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, Third Edition",2017.0
510,290890ccb5465fb31fe7111fb80e69fb2e4a2361,"Abstract Objective The concepts of mild cognitive impairment (MCI) and subjective cognitive decline (SCD) have been proposed to identify individuals in the early stages of Alzheimer’s disease (AD), or other neurodegenerative diseases. One approach to validate these concepts is to investigate the relationship between pathological brain markers and cognition in those individuals. Method We included 126 participants from the Consortium for the Early Identification of Alzheimer’s disease-Quebec (CIMA-Q) cohort (67 SCD, 29 MCI, and 30 cognitively healthy controls [CH]). All participants underwent a complete cognitive assessment and structural magnetic resonance imaging. Group comparisons were done using cognitive data, and then correlated with hippocampal volumes and white matter hyperintensities (WMHs). Results Significant differences were found between participants with MCI and CH on episodic and executive tasks, but no differences were found when comparing SCD and CH. Scores on episodic memory tests correlated with hippocampal volumes in both MCI and SCD, whereas performance on executive tests correlated with WMH in all of our groups. Discussion As expected, the SCD group was shown to be cognitively healthy on tasks where MCI participants showed impairment. However, SCD’s hippocampal volume related to episodic memory performances, and WMH to executive functions. Thus, SCD represents a valid research concept and should be used, alongside MCI, to better understand the preclinical/prodromal phase of AD.","[{'authorId': '6730429', 'name': 'Marie Caillaud'}, {'authorId': '9164412', 'name': 'C. Hudon'}, {'authorId': '40589979', 'name': 'Benjamin Boller'}, {'authorId': '145348826', 'name': 'S. Brambati'}, {'authorId': '2873793', 'name': 'S. Duchesne'}, {'authorId': '35117286', 'name': 'D. Lorrain'}, {'authorId': '113135777', 'name': 'J. Gagnon'}, {'authorId': '2258422767', 'name': 'Samantha Maltezos'}, {'authorId': '2550138', 'name': 'S. Mellah'}, {'authorId': '2227151', 'name': 'N. Phillips'}, {'authorId': '145580293', 'name': 'S. Belleville'}]",34.0,"{'bibtex': '@Article{Caillaud2019EvidenceOA,\n author = {Marie Caillaud and C. Hudon and Benjamin Boller and S. Brambati and S. Duchesne and D. Lorrain and J. Gagnon and Samantha Maltezos and S. Mellah and N. Phillips and S. Belleville},\n journal = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},\n pages = {1382 - 1392},\n title = {Evidence of a Relation Between Hippocampal Volume, White Matter Hyperintensities, and Cognition in Subjective Cognitive Decline and Mild Cognitive Impairment},\n volume = {75},\n year = {2019}\n}\n'}",,"{'volume': '75', 'pages': '1382 - 1392', 'name': 'The Journals of Gerontology Series B: Psychological Sciences and Social Sciences'}",74.0,"Evidence of a Relation Between Hippocampal Volume, White Matter Hyperintensities, and Cognition in Subjective Cognitive Decline and Mild Cognitive Impairment",2019.0
511,291292deecffb16a993689a8f5f7633795b83d3a,"As humans, we gather a wide range of information about other people from watching them move. A network of parietal, premotor, and occipitotemporal regions within the human brain, termed the action observation network (AON), has been implicated in understanding others' actions by means of an automatic matching process that links observed and performed actions. Current views of the AON assume a matching process biased towards familiar actions; specifically, those performed by conspecifics and present in the observer's motor repertoire. In this study, we test how this network responds to form and motion cues when observing natural human motion compared to rigid robotic‐like motion across two independent functional neuroimaging experiments. In Experiment 1, we report the surprising finding that premotor, parietal, occipitotemporal regions respond more robustly to rigid, robot‐like motion than natural human motion. In Experiment 2, we replicate and extend this finding by demonstrating that the same pattern of results emerges whether the agent is a human or a robot, which suggests the preferential response to robot‐like motion is independent of the agent's form. These data challenge previous ideas about AON function by demonstrating that the core nodes of this network can be flexibly engaged by novel, unfamiliar actions performed by both human and non‐human agents. As such, these findings suggest that the AON is sensitive to a broader range of action features beyond those that are simply familiar. Hum Brain Mapp 33:2238–2254, 2012. © 2011 Wiley Periodicals, Inc.","[{'authorId': '1850742', 'name': 'Emily S. Cross'}, {'authorId': '1721092', 'name': 'Roman Liepelt'}, {'authorId': '8429108', 'name': 'Antonia F. de C. Hamilton'}, {'authorId': '22803229', 'name': 'Jim Parkinson'}, {'authorId': '144772502', 'name': 'Richard Ramsey'}, {'authorId': '3060106', 'name': 'W. Stadler'}, {'authorId': '144374107', 'name': 'W. Prinz'}]",140.0,"{'bibtex': '@Article{Cross2012RoboticMP,\n author = {Emily S. Cross and Roman Liepelt and Antonia F. de C. Hamilton and Jim Parkinson and Richard Ramsey and W. Stadler and W. Prinz},\n journal = {Human Brain Mapping},\n title = {Robotic movement preferentially engages the action observation network},\n volume = {33},\n year = {2012}\n}\n'}",,"{'volume': '33', 'name': 'Human Brain Mapping'}",69.0,Robotic movement preferentially engages the action observation network,2012.0
512,293a0c383a42550d640b6af141c5c10822900606,"Automatic detection and interpretation of social signals carried by voice, gestures, mimics, etc. will play a key-role for next-generation interfaces as it paves the way towards a more intuitive and natural human-computer interaction. The paper at hand introduces Social Signal Interpretation (SSI), a framework for real-time recognition of social signals. SSI supports a large range of sensor devices, filter and feature algorithms, as well as, machine learning and pattern recognition tools. It encourages developers to add new components using SSI's C++ API, but also addresses front end users by offering an XML interface to build pipelines with a text editor. SSI is freely available under GPL at http://openssi.net.","[{'authorId': '6164138', 'name': 'J. Wagner'}, {'authorId': '2565410', 'name': 'Florian Lingenfelser'}, {'authorId': '2230836', 'name': 'Tobias Baur'}, {'authorId': '3048626', 'name': 'Ionut Damian'}, {'authorId': '2844803', 'name': 'Felix Kistler'}, {'authorId': '1742930', 'name': 'E. André'}]",182.0,"{'bibtex': '@Article{Wagner2013TheSS,\n author = {J. Wagner and Florian Lingenfelser and Tobias Baur and Ionut Damian and Felix Kistler and E. André},\n journal = {Proceedings of the 21st ACM international conference on Multimedia},\n title = {The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time},\n year = {2013}\n}\n'}",,{'name': 'Proceedings of the 21st ACM international conference on Multimedia'},16.0,The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time,2013.0
513,293fc9674b2404a1ae9b0f16778610c7849ea2b8,"Socially assistive robotics (SAR) is a growing area of research. Evaluating SAR systems presents novel challenges. Using a robot for a socially assistive task can have various benefits and ethical implications. Many questions are important to understanding whether a robot is effective for a given application domain. This paper describes several benchmarks for evaluating SAR systems. There exist numerous methods for evaluating the many factors involved in a robot’s design. Benchmarks from psychology, anthropology, medicine, and human–robot interaction are proposed as measures of success in evaluating a given SAR system and its impact on the user and broader population.","[{'authorId': '1390033049', 'name': 'David Feil-Seifer'}, {'authorId': '49205966', 'name': 'K. Skinner'}, {'authorId': '1742183', 'name': 'M. Matarić'}]",132.0,"{'bibtex': '@Inproceedings{Feil-Seifer2007BenchmarksFE,\n author = {David Feil-Seifer and K. Skinner and M. Matarić},\n pages = {423-439},\n title = {Benchmarks for evaluating socially assistive robotics},\n volume = {8},\n year = {2007}\n}\n'}",,"{'volume': '8', 'pages': '423-439', 'name': ''}",52.0,Benchmarks for evaluating socially assistive robotics,2007.0
514,294219fdce685a3245ead1098bc5d4d007924cc5,,"[{'authorId': '145471591', 'name': 'W. James'}]",1473.0,"{'bibtex': '@Inproceedings{James1977WhatIA,\n author = {W. James},\n title = {What Is an Emotion},\n year = {1977}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,What Is an Emotion,1977.0
515,294968562de6abb9c2705ffeac59eef1cdb10e1b,"As empathy is important for the emotional interaction between a human and a robot, the design factors which induce human empathy toward robots need to be explored. Human empathy toward a robot can be affected by the presence of a robot. Thus, we focused on the levels of agency and the physical embodiment of a robot, which are influential factors pertaining to social presence, by executing two experiments. In the first experiment, in a 2 (levels of agency: mediated vs. simulated) between-participants experiment, participants interacted with either a mediated robot which delivers the emotional state of a remote user or a simulated robot which expresses its own emotion. Participants empathized more with the mediated robot than with the simulated robot, demonstrating that the proper form of an emotional robot is as a mediator during emotional interaction between people. In the second study, in a 2 (physical embodiment: physically embodied vs. physically disembodied) between-participants experiment design, participants interacted with either a physically embodied robot or a physically disembodied robot. The results showed that participants empathized more with a physically embodied robot than with a physically disembodied robot, indicating the impact of physical embodiment on human empathy. Implications for the design of human-robot interactions are discussed.","[{'authorId': '1743779', 'name': 'Sonya S. Kwak'}, {'authorId': '1717667', 'name': 'Yunkyung Kim'}, {'authorId': '145391043', 'name': 'E. Kim'}, {'authorId': '2054260223', 'name': 'Christine Shin'}, {'authorId': '2111007630', 'name': 'Kwangsu Cho'}]",94.0,"{'bibtex': '@Article{Kwak2013WhatMP,\n author = {Sonya S. Kwak and Yunkyung Kim and E. Kim and Christine Shin and Kwangsu Cho},\n journal = {2013 IEEE RO-MAN},\n pages = {180-185},\n title = {What makes people empathize with an emotional robot?: The impact of agency and physical embodiment on human empathy for a robot},\n year = {2013}\n}\n'}",,"{'pages': '180-185', 'name': '2013 IEEE RO-MAN'}",37.0,What makes people empathize with an emotional robot?: The impact of agency and physical embodiment on human empathy for a robot,2013.0
516,2953f89f0fd2b59262c8f8373b9d077fd9787f63,,"[{'authorId': '2493710', 'name': 'Frank J. Bernieri'}]",521.0,"{'bibtex': '@Article{Bernieri1988CoordinatedMA,\n author = {Frank J. Bernieri},\n journal = {Journal of Nonverbal Behavior},\n pages = {120-138},\n title = {Coordinated movement and rapport in teacher-student interactions},\n volume = {12},\n year = {1988}\n}\n'}",,"{'volume': '12', 'pages': '120-138', 'name': 'Journal of Nonverbal Behavior'}",23.0,Coordinated movement and rapport in teacher-student interactions,1988.0
517,297e15edcfe342e3140644728e77ba2033eac81e,"This paper investigates the development of an urban crowd simulation for the purposes of psychophysical experimentation. Whilst artificial intelligence (AI) is advancing to produce more concise and interesting crowd behaviours, the number or sophistication of the algorithms implemented within a system does not necessarily guarantee its perceptual realism. Human perception is highly subjective and does not always conform to the reality of the situation. Therefore it is important to consider this aspect when dealing with A implementations within a crowd system aimed at humans. In this research an initial two-alternative forced choice (2AFC) with constant stimuli psychophysical experiment is presented. The purpose of the experiment is to assess whether human participants perceive crowd behaviour with a social forces model to be more realistic. Results from the experiment suggest that participants do consider crowd behaviour with social forces to be more realistic. This research could inform the development of crowd-based systems, especially those that consider viewer perception to be important, such as for example video games and other media.","[{'authorId': '1402981429', 'name': 'S. O’Connor'}, {'authorId': '1695331', 'name': 'F. Liarokapis'}, {'authorId': '8683625', 'name': 'Chrisina Jayne'}]",15.0,"{'bibtex': '@Article{O’Connor2015PerceivedRO,\n author = {S. O’Connor and F. Liarokapis and Chrisina Jayne},\n journal = {2015 19th International Conference on Information Visualisation},\n pages = {494-499},\n title = {Perceived Realism of Crowd Behaviour with Social Forces},\n year = {2015}\n}\n'}",,"{'pages': '494-499', 'name': '2015 19th International Conference on Information Visualisation'}",28.0,Perceived Realism of Crowd Behaviour with Social Forces,2015.0
518,299b18527711595652a8775d7450a5bc16222502,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '2790338', 'name': 'R. Duell'}, {'authorId': '2468373', 'name': 'Z. A. Memon'}, {'authorId': '1726343', 'name': 'Jan Treur'}, {'authorId': '1881843', 'name': 'C. N. V. D. Wal'}]",77.0,"{'bibtex': '@Inproceedings{Bosse2009AMM,\n author = {T. Bosse and R. Duell and Z. A. Memon and Jan Treur and C. N. V. D. Wal},\n pages = {48-67},\n title = {A Multi-agent Model for Emotion Contagion Spirals Integrated within a Supporting Ambient Agent Model},\n year = {2009}\n}\n'}",,{'pages': '48-67'},25.0,A Multi-agent Model for Emotion Contagion Spirals Integrated within a Supporting Ambient Agent Model,2009.0
519,29ddc1f43f28af7c846515e32cc167bc66886d0c,"Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.","[{'authorId': '2815290', 'name': 'N. Houlsby'}, {'authorId': '1911881', 'name': 'A. Giurgiu'}, {'authorId': '40569328', 'name': 'Stanislaw Jastrzebski'}, {'authorId': '68973833', 'name': 'Bruna Morrone'}, {'authorId': '51985388', 'name': 'Quentin de Laroussilhe'}, {'authorId': '2813347', 'name': 'Andrea Gesmundo'}, {'authorId': '2809991', 'name': 'Mona Attariyan'}, {'authorId': '1802148', 'name': 'S. Gelly'}]",1866.0,"{'bibtex': '@Article{Houlsby2019ParameterEfficientTL,\n author = {N. Houlsby and A. Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and S. Gelly},\n journal = {ArXiv},\n title = {Parameter-Efficient Transfer Learning for NLP},\n volume = {abs/1902.00751},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1902.00751', 'name': 'ArXiv'}",56.0,Parameter-Efficient Transfer Learning for NLP,2019.0
520,29e1a5ad5235d3500cd88cf174faa0edaa8722a5,,"[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",38.0,"{'bibtex': ""@Article{Melo2012TheEO,\n author = {C. D. Melo and P. Carnevale and J. Gratch},\n booktitle = {International Conference on Intelligent Virtual Agents},\n pages = {53-66},\n title = {The Effect of Virtual Agents' Emotion Displays and Appraisals on People's Decision Making in Negotiation},\n year = {2012}\n}\n""}","[{'paperId': '62d2c69930eeffc4dbd4562aa387bb5a9526d641', 'title': 'The Effects of Engaging and Affective Behaviors of Virtual Agents in Group Decision-Making'}, {'paperId': 'f2633b56b97482250a816ddb9b80326f98c64617', 'title': 'Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales and Perceptual Thresholds for Comfort and Facial Cues'}, {'paperId': '8de7a559d3d2e91e6311a1cdf448bec9fb52aacd', 'title': 'A Behavioral Assessment Model for Emotional Persuasion Driven by Agent-Based Decision-Making'}, {'paperId': '14a3e6bad8744b9e7790659c4f8510102bfdbd25', 'title': ""Towards Understanding How Virtual Human's Verbal Persuasion Strategies Influence User Intentions To Perform Health Behavior""}, {'paperId': 'a407b48a77b20e0cf24933c0912976bc16f8dc18', 'title': 'An Architecture for Emotional Facial Expressions as Social Signals'}, {'paperId': '4219f87d4f66c4e5656dda5694b4f7890df0ac02', 'title': 'Towards Understanding the Effect of Voice on Human-Agent Negotiation'}, {'paperId': '690632f500d36a89e0096bd6c925ccff645e6919', 'title': 'Human–Robot Cooperation in Economic Games: People Show Strong Reciprocity but Conditional Prosociality Toward Robots'}, {'paperId': 'c72bad42222dd9a1a720411b47505685b5492067', 'title': 'Virtual Big Heads: Analysis of Human Perception and Comfort of Head Scales in Social Virtual Reality'}, {'paperId': '89bc65e8a53a29be45e25e10e0de23e64dbd3eb3', 'title': 'Text-driven Mouth Animation for Human Computer Interaction With Personal Assistant'}, {'paperId': '8a8975186f89642708e49a17874c557e2fdd6e3e', 'title': 'The Effect of Virtual Agent Warmth on Human-Agent Negotiation'}, {'paperId': 'ffa4b19cdd1370bdd5e152103febe1a66a590857', 'title': 'Overfeed the Bold, Starve the Cowardly: A Legend or Reality?'}, {'paperId': '268d2ec8e5e6e01d36b9731727358156ab718569', 'title': 'A Systematic Survey of 15 Years of User Studies Published in the Intelligent Virtual Agents Conference'}, {'paperId': '93fa32a16fbf1049db819dd9ec4c18b45f105f46', 'title': ""Detecting User's Likes and Dislikes for a Virtual Negotiating Agent""}, {'paperId': '279ceb403e4ad687421122890c1957bfc54f2eee', 'title': ""Mood-affect congruency. Exploring the relation between learners' mood and the affective charge of educational videos""}, {'paperId': 'c6bf764d9c80c0273ec5ff3a2f0fa152f5616d2a', 'title': 'Toward affect-sensitive virtual human tutors: The influence of facial expressions on learning and emotion'}, {'paperId': '0708ac293051eebc025b0a972cb58a8c44b281db', 'title': 'Studying Gender Bias and Social Backlash via Simulated Negotiations with Virtual Agents'}, {'paperId': '5f1664247ca562ea9adc201a41d69654200c1e30', 'title': 'The Future of Frontline Research'}, {'paperId': '3c2360fbc612e8381ecda4f79c2c3cf52e681829', 'title': ""The Effects of a Pedagogical Agent's Smiling Expression on the Learner's Emotions and Motivation in a Virtual Learning Environment.""}, {'paperId': 'c37018adfa3bf4a8b5600a2e63141d212f93db9c', 'title': '“Why Aren’t You a Sassy Little Thing”: The Effects of Robot-Enacted Guilt Trips on Credibility and Consensus in a Negotiation'}, {'paperId': '60cfeda8f5731cf9afdd2f4eea4d19bf684a20e5', 'title': ""Emotional perception for updating agents' beliefs""}, {'paperId': '02410cf604c5b1d97ecfe589c1451c84e44fe2f1', 'title': 'The Effect of Wrinkles, Presentation Mode, and Intensity on the Perception of Facial Actions and Full-Face Expressions of Laughter'}, {'paperId': '89b23a7f939de527e5a917e85aa7ec61ecb590b4', 'title': 'Talking with a Virtual Human: Controlling the Human Experience and Behavior in a Virtual Conversation'}, {'paperId': '783f49505ef8ebee7c6ecc437ca066664198c9ae', 'title': 'Is That How Everyone Really Feels? Emotional Contagion with Masking for Virtual Crowds'}, {'paperId': '1c92ac050dc7df429f13c3e9d23df5bda4bdbfca', 'title': 'Conversations with a virtual human: Synthetic emotions and human responses'}, {'paperId': '63fbd37a24a23705df09cd91198106c0ca2cbe20', 'title': ""Évaluation de l'impact des facteurs émotionnels lors d'interactions physiques entre humains réels et virtuels""}, {'paperId': 'fad3485ffbaa80a3222ea0e871a1e8dbf9e95cee', 'title': 'Supervised Hybrid Expression Control Framework for a Lifelike Affective Avatar'}, {'paperId': '8c29378938814539193c90c4c3d52ee9b2c63e28', 'title': 'Subjective Perceptions in Wartime Negotiation'}, {'paperId': 'e96afe23bd651075504f73bb622aabd9cdc43925', 'title': 'Multimodal Emotion Expressions of Virtual Agents, Mimic and Vocal Emotion Expressions and Their Effects on Emotion Recognition'}, {'paperId': '9194330c9379def1e39a96984863027c5b26ba63', 'title': 'Walk with me: interactions in emotional walking situations, a pilot study'}, {'paperId': 'a9f479a799671c5cf6ae84805291ba5178bcac16', 'title': 'A Cross-Cultural Study of Playing Simple Economic Games Online with Humans and Virtual Humans'}, {'paperId': '0a855c61138453b4e00287ba978f27f98c213750', 'title': 'Modeling and Evaluating Emotions Impact on Cognition'}, {'paperId': 'ca540af76357225740a887bd000669057d3fd4f7', 'title': 'User modelling in adjustable control system'}, {'paperId': 'ba390eb2c1eae81bfc1e263328bcef4a23fe1f28', 'title': 'THE AGENT’S SMILE: IMPACTS OF ARTIFICIALLY GENERATED PEDAGOGICAL AGENTS ON RISK-TAKING'}, {'paperId': '4880434b8efc9a73db45b47b89f04df533420b51', 'title': 'Facial Expressions of Emotions for Virtual Characters'}, {'paperId': '77b4edc2a6274c85fd62bfad6614d3769ecca0e8', 'title': 'Examining the Effects of Robot-Enacted Guilt Appeals in a Human-Robot Negotiation'}, {'paperId': 'b690e186394beae5926c252fb90456db0b56f1f4', 'title': 'Human Behavior towards Virtual Humans'}, {'paperId': '2025fefb310e12351b14987792dc0157ab7690fd', 'title': 'Synthetic emotions and human responses'}, {'paperId': 'a65b62ab7934c6e41ae3cdc78676cfc678adb714', 'title': 'The interpersonal effect of emotion in decision-making and social dilemmas'}]",{'pages': '53-66'},31.0,The Effect of Virtual Agents' Emotion Displays and Appraisals on People's Decision Making in Negotiation,2012.0
521,2a09d4dbfd714722842272d237648c7382d72d4c,,"[{'authorId': '2309507', 'name': 'S. Majeski'}]",36.0,"{'bibtex': ""@Article{Majeski1984ArmsRA,\n author = {S. Majeski},\n journal = {Mathematical Social Sciences},\n pages = {253-266},\n title = {Arms races as iterated prisoner's dilemma games},\n volume = {7},\n year = {1984}\n}\n""}",,"{'volume': '7', 'pages': '253-266', 'name': 'Mathematical Social Sciences'}",21.0,Arms races as iterated prisoner's dilemma games,1984.0
522,2a13335bba46abe15afbf9d08b9caf72be90db30,"The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems. This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion. We propose that basic emotion theories no longer explain adequately the vast number of empirical observations from studies in affective neuroscience, and we suggest that a conceptual shift is needed in the empirical approaches taken to the study of emotion and affective psychopathologies. The circumplex model of affect is more consistent with many recent findings from behavioral, cognitive neuroscience, neuroimaging, and developmental studies of affect. Moreover, the model offers new theoretical and empirical approaches to studying the development of affective disorders as well as the genetic and cognitive underpinnings of affective processing within the central nervous system.This work was supported in part by NIMH Grants MH01232, MH59139, MH36197, MHK02-74677, and MH068318; a grant from the National Alliance for Research in Schizophrenia and Affective Disorders (NARSAD); NSF Grant BSC-0421702; and funding from the Thomas D. Klingenstein and Nancy D. Perlman Family Fund and the Suzanne Crosby Murphy Endowment at Columbia University.","[{'authorId': '49585246', 'name': 'J. Posner'}, {'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '3286633', 'name': 'B. Peterson'}]",1949.0,"{'bibtex': '@Article{Posner2005TheCM,\n author = {J. Posner and J. Russell and B. Peterson},\n journal = {Development and Psychopathology},\n pages = {715 - 734},\n title = {The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology},\n volume = {17},\n year = {2005}\n}\n'}",,"{'volume': '17', 'pages': '715 - 734', 'name': 'Development and Psychopathology'}",150.0,"The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",2005.0
523,2a1d0640a3e92879b2b68bf0ca25ee19374c422f,"This paper presents four exploratory studies of the potential use of robots for gathering customer feedback in the hospitality industry. To account for the viewpoints of both hotels and guests, we administered need finding interviews at five hotels and an online survey concerning hotel guest experiences with 60 participants. We then conducted the two deployment studies based on deploying software prototypes for Savioke Relay robots we designed to collect customer feedback: (i) a hotel deployment study (three hotels over three months) to explore the feasibility of robot use for gathering customer feedback as well as issues such deployment might pose and (ii) a hotel kitchen deployment study (at Savioke headquarters over three weeks) to explore the role of different robot behaviors (mobility and social attributes) in gathering feedback and understand the customers' thought process in the context that they experience a service. We found that hotels want to collect customer feedback in real-time to disseminate positive feedback immediately and to respond to unhappy customers while they are still on-site. Guests want to inform the hotel staff about their experiences without compromising their convenience and privacy. We also found that the robot users, e.g. hotel staff, use their domain knowledge to increase the response rate to customer feedback surveys at the hotels. Finally, environmental factors, such as robot's location in the building influenced customer response rates more than altering the behaviors of the robot collecting the feedback.","[{'authorId': '2677547', 'name': 'M. Chung'}, {'authorId': '35096370', 'name': 'M. Cakmak'}]",21.0,"{'bibtex': '@Article{Chung2018HowWY,\n author = {M. Chung and M. Cakmak},\n journal = {2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {947-954},\n title = {“How was Your Stay?”: Exploring the Use of Robots for Gathering Customer Feedback in the Hospitality Industry},\n year = {2018}\n}\n'}",,"{'pages': '947-954', 'name': '2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)'}",30.0,“How was Your Stay?”: Exploring the Use of Robots for Gathering Customer Feedback in the Hospitality Industry,2018.0
524,2a1f8cc1c02897b0aaffb5c410c1a38c404771c6,"BACKGROUND
Manifestations of core social deficits in autism are more pronounced in everyday settings than in explicit experimental tasks. To bring experimental measures in line with clinical observation, we report a novel method of quantifying atypical strategies of social monitoring in a setting that simulates the demands of daily experience. Enhanced ecological validity was intended to maximize between-group effect sizes and assess the predictive utility of experimental variables relative to outcome measures of social competence.


METHODS
While viewing social scenes, eye-tracking technology measured visual fixations in 15 cognitively able males with autism and 15 age-, sex-, and verbal IQ-matched control subjects. We reliably coded fixations on 4 regions: mouth, eyes, body, and objects. Statistical analyses compared fixation time on regions of interest between groups and correlation of fixation time with outcome measures of social competence (ie, standardized measures of daily social adjustment and degree of autistic social symptoms).


RESULTS
Significant between-group differences were obtained for all 4 regions. The best predictor of autism was reduced eye region fixation time. Fixation on mouths and objects was significantly correlated with social functioning: increased focus on mouths predicted improved social adjustment and less autistic social impairment, whereas more time on objects predicted the opposite relationship.


CONCLUSIONS
When viewing naturalistic social situations, individuals with autism demonstrate abnormal patterns of social visual pursuit consistent with reduced salience of eyes and increased salience of mouths, bodies, and objects. Fixation times on mouths and objects but not on eyes are strong predictors of degree of social competence.","[{'authorId': '6261339', 'name': 'A. Klin'}, {'authorId': '8137420', 'name': 'W. Jones'}, {'authorId': '145157155', 'name': 'R. Schultz'}, {'authorId': '2155644', 'name': 'F. Volkmar'}, {'authorId': '48858469', 'name': 'D. Cohen'}]",2001.0,"{'bibtex': '@Article{Klin2002VisualFP,\n author = {A. Klin and W. Jones and R. Schultz and F. Volkmar and D. Cohen},\n journal = {Archives of general psychiatry},\n pages = {\n          809-16\n        },\n title = {Visual fixation patterns during viewing of naturalistic social situations as predictors of social competence in individuals with autism.},\n volume = {59 9},\n year = {2002}\n}\n'}",,"{'volume': '59 9', 'pages': '\n          809-16\n        ', 'name': 'Archives of general psychiatry'}",56.0,Visual fixation patterns during viewing of naturalistic social situations as predictors of social competence in individuals with autism.,2002.0
525,2a378dc326f8ece1b741390673dbe4657d422750,"This paper presents a virtual rap dancer that is able to dance to the beat of music coming in from music recordings, beats obtained from music, voice or other input through a microphone, motion beats detected in the video stream of a human dancer, or motions detected from a dance mat. The rap dancer's moves are generated from a lexicon that was derived manually from the analysis of the video clips of rap songs performed by various rappers. The system allows for adaptation of the moves in the lexicon on the basis of style parameters. The rap dancer invites a user to dance along with the music.","[{'authorId': '2997504', 'name': 'D. Reidsma'}, {'authorId': '144483472', 'name': 'A. Nijholt'}, {'authorId': '1754666', 'name': 'R. Poppe'}, {'authorId': '6236777', 'name': 'R. Rienks'}, {'authorId': '2854168', 'name': 'H. Hondorp'}]",25.0,"{'bibtex': ""@Article{Reidsma2006VirtualRD,\n author = {D. Reidsma and A. Nijholt and R. Poppe and R. Rienks and H. Hondorp},\n journal = {CHI '06 Extended Abstracts on Human Factors in Computing Systems},\n title = {Virtual rap dancer: invitation to dance},\n year = {2006}\n}\n""}",,"{'name': ""CHI '06 Extended Abstracts on Human Factors in Computing Systems""}",16.0,Virtual rap dancer: invitation to dance,2006.0
527,2a69e806a42bf401017c9c9b6b1c1efb1430c541,"Realistic versus stylized depictions of virtual humans in simulated inter-personal situations and their ability to elicit emotional responses in users has been an open question for artists and researchers alike. We empirically evaluated the effects of near visually realistic vs. non-realistic stylized appearance of virtual humans on the emotional response of participants in a medical virtual reality system that was designed to educate users in recognizing the signs and symptoms of patient deterioration. In a between-subjects experiment protocol, participants interacted with one of three different appearances of a virtual patient, namely visually realistic, cartoon-shaded and charcoal-sketch like conditions in a mixed reality simulation. Emotional impact were measured via a combination of quantitative objective measures were gathered using skin Electrodermal Activity (EDA) sensors, and quantitative subjective measures such as the Differential Emotion Survey (DES IV), Positive and Negative Affect Schedule (PANAS), and Social Presence questionnaire. The emotional states of the participants were analyzed across four distinct time steps during which the medical condition of the virtual patient deteriorated (an emotionally stressful interaction), and were contrasted to a baseline affective state. Objective EDA results showed that in all three conditions, male participants exhibited greater levels of arousal as compared to female participants. We found that negative affect levels were significantly lower in the visually realistic condition, as compared to the stylized appearance conditions. Furthermore, in emotional dimensions of interest-excitement, surprise, anger, fear and guilt participants in all conditions responded similarly. However, in social emotional constructs of shyness, presence, perceived personality, and enjoyment-joy, we found that participants responded differently in the visually realistic condition as compared to the cartoon and sketch conditions. Our study suggests that virtual human appearance can affect not only critical emotional reactions in affective inter-oersonal trainina scenarios. but also users' oerceotions of oersonalitv and social characteristic of the virtual interlocutors.","[{'authorId': '51250937', 'name': 'Matias Volonte'}, {'authorId': '144403504', 'name': 'Sabarish V. Babu'}, {'authorId': '144291265', 'name': 'H. Chaturvedi'}, {'authorId': '2698078', 'name': 'Nathan D. Newsome'}, {'authorId': '2064300398', 'name': 'Elham Ebrahimi'}, {'authorId': '144455263', 'name': 'Tania Roy'}, {'authorId': '1959041', 'name': 'S. Daily'}, {'authorId': '3043236', 'name': 'Tracy Fasolino'}]",96.0,"{'bibtex': '@Article{Volonte2016EffectsOV,\n author = {Matias Volonte and Sabarish V. Babu and H. Chaturvedi and Nathan D. Newsome and Elham Ebrahimi and Tania Roy and S. Daily and Tracy Fasolino},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {1326-1335},\n title = {Effects of Virtual Human Appearance Fidelity on Emotion Contagion in Affective Inter-Personal Simulations},\n volume = {22},\n year = {2016}\n}\n'}",,"{'volume': '22', 'pages': '1326-1335', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",39.0,Effects of Virtual Human Appearance Fidelity on Emotion Contagion in Affective Inter-Personal Simulations,2016.0
528,2a75f34663a60ab1b04a0049ed1d14335129e908,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.","[{'authorId': '145387780', 'name': 'M. Pantic'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '2503276', 'name': 'R. Rademaker'}, {'authorId': '153828376', 'name': 'L. Maat'}]",1060.0,"{'bibtex': '@Article{Pantic2005WebbasedDF,\n author = {M. Pantic and M. Valstar and R. Rademaker and L. Maat},\n journal = {2005 IEEE International Conference on Multimedia and Expo},\n pages = {5 pp.-},\n title = {Web-based database for facial expression analysis},\n year = {2005}\n}\n'}",,"{'pages': '5 pp.-', 'name': '2005 IEEE International Conference on Multimedia and Expo'}",59.0,Web-based database for facial expression analysis,2005.0
529,2aa5da20c14ae3b7cbdc735d40407dcd5ec39a01,"Most of the work on reasoning and decision-making in virtual agents relates the choices with an exhaustive exploration, analysing every possible alternative and implication, and trying to maximize some utility measure in order to make the best decision. Humans, however, seem not to reason and make decisions naturally in this way. As authors such as Herbert A. Simon [21] have proposed, humans seem to develop a concept of bounded rationality, according to which human reasoning process and decision-making is bounded to a part of reality at a time as a focusing effect. According to some psychologists, focus of thought is one of the main purposes of emotions in humans. Using an abstract framework, in this work we propose an approach to consider emotions as an argument-selection heuristic towards the ability for an agent to reason and act in a believable manner. Influenced by emotions, the agent will produce a line of reasoning according to the evolution of its own emotional state.","[{'authorId': '2435674', 'name': 'S. E. F. Dalibón'}, {'authorId': '143987551', 'name': 'D. C. Martínez'}, {'authorId': '1790217', 'name': 'Guillermo R. Simari'}]",8.0,"{'bibtex': '@Article{Dalibón2012EmotiondirectedAA,\n author = {S. E. F. Dalibón and D. C. Martínez and Guillermo R. Simari},\n booktitle = {Inteligencia Artif.},\n journal = {Inteligencia Artif.},\n pages = {30-45},\n title = {Emotion-directed Argument Awareness for Autonomous Agent Reasoning},\n volume = {15},\n year = {2012}\n}\n'}","[{'paperId': '367257e297dc87a6c7288af071669ae30087aa73', 'title': 'EXPLAINABLE, TRUSTABLE AND EMPHATIC ARTIFICIAL INTELLIGENCE FROM FORMAL ARGUMENTATION THEORY TO ARGUMENTATION FOR HUMANS'}, {'paperId': '4920c9341c944bab9fa9854c7a040e300cdc40c2', 'title': 'Strategic Dialogical Argumentation Using Multi-criteria Decision Making with Application to Epistemic and Emotional Aspects of Arguments'}, {'paperId': 'd978d492cb7775471c89bce57399e2ab1c113e51', 'title': 'Persuasive Argumentation and Emotions: An Empirical Evaluation with Users'}, {'paperId': '7f01babc3116e24060d764abd1f02534f83e1fe0', 'title': 'Emotions in Argumentation: an Empirical Evaluation'}, {'paperId': '3a9dee00deed49e6cf5cc378b15e1271b9f23a75', 'title': 'An approach to knowledge dynamic maintenance for emotional agents'}, {'paperId': 'f60018a9519740b91afa4119534a9dcbcda3f086', 'title': '1 Empirical Cognitive Studies About Formal Argumentation'}, {'paperId': '86ae1210130a28827024c20b4ed659f4ae1c0b03', 'title': 'Université de Montréal-Thèse et mémoire'}, {'paperId': 'd9a603dc414cd333f83379d194265bbfff6a2fc9', 'title': 'Emotions and personality traits in argumentation: An empirical evaluation1'}]","{'name': 'Inteligencia Artif.', 'pages': '30-45', 'volume': '15'}",26.0,Emotion-directed Argument Awareness for Autonomous Agent Reasoning,2012.0
530,2ab047912a0e950f77bfcc2b38e4951ba1d8e30f,"Cortical blindness refers to the loss of vision that occurs after destruction of the primary visual cortex. Although there is no sensory cortex and hence no conscious vision, some cortically blind patients show amygdala activation in response to facial or bodily expressions of emotion. Here we investigated whether direction of gaze could also be processed in the absence of any functional visual cortex. A well-known patient with bilateral destruction of his visual cortex and subsequent cortical blindness was investigated in an fMRI paradigm during which blocks of faces were presented either with their gaze directed toward or away from the viewer. Increased right amygdala activation was found in response to directed compared with averted gaze. Activity in this region was further found to be functionally connected to a larger network associated with face and gaze processing. The present study demonstrates that, in human subjects, the amygdala response to eye contact does not require an intact primary visual cortex.","[{'authorId': '3119034', 'name': 'Nicolas Burra'}, {'authorId': '1404500022', 'name': 'Alexis Hervais-Adelman'}, {'authorId': '2577502', 'name': 'D. Kerzel'}, {'authorId': '3005853', 'name': 'M. Tamietto'}, {'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '1861118', 'name': 'A. Pegna'}]",88.0,"{'bibtex': '@Article{Burra2013AmygdalaAF,\n author = {Nicolas Burra and Alexis Hervais-Adelman and D. Kerzel and M. Tamietto and B. de Gelder and A. Pegna},\n journal = {The Journal of Neuroscience},\n pages = {10483 - 10489},\n title = {Amygdala Activation for Eye Contact Despite Complete Cortical Blindness},\n volume = {33},\n year = {2013}\n}\n'}",,"{'volume': '33', 'pages': '10483 - 10489', 'name': 'The Journal of Neuroscience'}",37.0,Amygdala Activation for Eye Contact Despite Complete Cortical Blindness,2013.0
531,2ab4bdd0477c70c680f5867d4ff45843bc21e27f,"Virtual agents hold great promise in human-computer interaction with their ability to afford embodied interaction using nonverbal human communicative cues. Gaze cues are particularly important to achieve significant high-level outcomes such as improved learning and feelings of rapport. Our goal is to explore how agents might achieve such outcomes through seemingly subtle changes in gaze behavior and what design variables for gaze might lead to such positive outcomes. Drawing on research in human physiology, we developed a model of gaze behavior to capture these key design variables. In a user study, we investigated how manipulations in these variables might improve affiliation with the agent and learning. The results showed that an agent using affiliative gaze elicited more positive feelings of connection, while an agent using referential gaze improved participants' learning. Our model and findings offer guidelines for the design of effective gaze behaviors for virtual agents.","[{'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '2633572', 'name': 'T. Pejsa'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}]",83.0,"{'bibtex': '@Article{Andrist2012DesigningEG,\n author = {Sean Andrist and T. Pejsa and Bilge Mutlu and Michael Gleicher},\n journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n title = {Designing effective gaze mechanisms for virtual agents},\n year = {2012}\n}\n'}",,{'name': 'Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'},61.0,Designing effective gaze mechanisms for virtual agents,2012.0
532,2b0f4b6d00cdb104b8bfe4fb367e47aea1f5bf66,"In the present research, we test the assumption that emotional mimicry and contagion are moderated by group membership. We report two studies using facial electromyography (EMG; Study 1), Facial Action Coding System (FACS; Study 2), and self-reported emotions (Study 2) as dependent measures. As predicted, both studies show that ingroup anger and fear displays were mimicked to a greater extent than outgroup displays of these emotions. The self-report data in Study 2 further showed specific divergent reactions to outgroup anger and fear displays. Outgroup anger evoked fear, and outgroup fear evoked aversion. Interestingly, mimicry increased liking for ingroup models but not for outgroup models. The findings are discussed in terms of the social functions of emotions in group contexts. (PsycINFO Database Record (c) 2011 APA, all rights reserved).","[{'authorId': '46845475', 'name': 'Job van der Schalk'}, {'authorId': '7444483', 'name': 'A. Fischer'}, {'authorId': '3534042', 'name': 'B. Doosje'}, {'authorId': '39854264', 'name': 'D. Wigboldus'}, {'authorId': '2390460', 'name': 'Skyler T. Hawk'}, {'authorId': '4837582', 'name': 'M. Rotteveel'}, {'authorId': '3067657', 'name': 'U. Hess'}]",231.0,"{'bibtex': '@Article{Schalk2011ConvergentAD,\n author = {Job van der Schalk and A. Fischer and B. Doosje and D. Wigboldus and Skyler T. Hawk and M. Rotteveel and U. Hess},\n journal = {Emotion},\n pages = {\n          286-98\n        },\n title = {Convergent and divergent responses to emotional displays of ingroup and outgroup.},\n volume = {11 2},\n year = {2011}\n}\n'}",,"{'volume': '11 2', 'pages': '\n          286-98\n        ', 'name': 'Emotion'}",53.0,Convergent and divergent responses to emotional displays of ingroup and outgroup.,2011.0
533,2b151e0fc03510315f641a5a50c65ba0f4e723fa,"Early theory phenomenological theory behavioural theory physiological theory cognitive theory ambitious theories specific emotions theory developmental theory social theory clinical theory theory from the individual, the environment and the culture theory outside psychology emotion themes.","[{'authorId': '4750917', 'name': 'K. Strongman'}]",115.0,"{'bibtex': '@Inproceedings{Strongman1996ThePO,\n author = {K. Strongman},\n title = {The Psychology of Emotion: Theories of Emotion in Perspective},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Psychology of Emotion: Theories of Emotion in Perspective,1996.0
534,2b5c5557d4ace8af2bef672309485cde27cb025c,"Software developers experience and share a wide range of emotions throughout a rich ecosystem of communication channels. A recent trend that has emerged in empirical software engineering studies is leveraging sentiment analysis of developers' communication traces. We release a dataset of 4,800 questions, answers, and comments from Stack Overflow, manually annotated for emotions. Our dataset contributes to the building of a shared corpus of annotated resources to support research on emotion awareness in software development.","[{'authorId': '1750021', 'name': 'Nicole Novielli'}, {'authorId': '1810643', 'name': 'Fabio Calefato'}, {'authorId': '1743022', 'name': 'F. Lanubile'}]",53.0,"{'bibtex': '@Article{Novielli2018AGS,\n author = {Nicole Novielli and Fabio Calefato and F. Lanubile},\n journal = {2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)},\n pages = {14-17},\n title = {A Gold Standard for Emotion Annotation in Stack Overflow},\n year = {2018}\n}\n'}",,"{'pages': '14-17', 'name': '2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)'}",20.0,A Gold Standard for Emotion Annotation in Stack Overflow,2018.0
535,2b6765d728b920451dd5dc8a92d8613d0ce2733a,,"[{'authorId': '118688586', 'name': 'J. Turner'}]",33.0,"{'bibtex': '@Inproceedings{Turner2000OnTO,\n author = {J. Turner},\n title = {On the Origins of Human Emotions},\n year = {2000}\n}\n'}",,,0.0,On the Origins of Human Emotions,2000.0
536,2b6991269b8c0c02930ef36ed509457d7f07533e,"Cross-lingual voice conversion (VC) is a task that aims to synthesize target voices with the same content while source and target speakers speak in different languages. Its challenge lies in the fact that the source and target data are naturally non-parallel, and it is even difficult to bridge the gaps between languages with no transcriptions provided. In this paper, we focus on knowledge transfer from monolin-gual ASR to cross-lingual VC, in order to address the con-tent mismatch problem. To achieve this, we first train a monolingual acoustic model for the source language, use it to extract phonetic features for all the speech in the VC dataset, and then train a Seq2Seq conversion model to pre-dict the mel-spectrograms. We successfully address cross-lingual VC without any transcription or language-specific knowledge for foreign speech. We experiment this on Voice Conversion Challenge 2020 datasets and show that our speaker-dependent conversion model outperforms the zero-shot baseline, achieving MOS of 3.83 and 3.54 in speech quality and speaker similarity for cross-lingual conversion. When compared to Cascade ASR-TTS method, our proposed one significantly reduces the MOS drop be-tween intra- and cross-lingual conversion.","[{'authorId': '2152342254', 'name': 'Che-Jui Chang'}]",4.0,"{'bibtex': '@Article{Chang2020TransferLF,\n author = {Che-Jui Chang},\n journal = {ArXiv},\n title = {Transfer Learning from Monolingual ASR to Transcription-free Cross-lingual Voice Conversion},\n volume = {abs/2009.14668},\n year = {2020}\n}\n'}",,"{'volume': 'abs/2009.14668', 'name': 'ArXiv'}",28.0,Transfer Learning from Monolingual ASR to Transcription-free Cross-lingual Voice Conversion,2020.0
537,2b7cded7ef57579034870db4c3850f1245a8b94b,,"[{'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '1884967', 'name': 'Yuyu Xu'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '2058717828', 'name': 'Alesia Egan'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",72.0,"{'bibtex': '@Inproceedings{Scherer2012PerceptionML,\n author = {Stefan Scherer and S. Marsella and Giota Stratou and Yuyu Xu and Fabrizio Morbini and Alesia Egan and A. Rizzo and Louis-Philippe Morency},\n pages = {455-463},\n title = {Perception Markup Language: Towards a Standardized Representation of Perceived Nonverbal Behaviors},\n year = {2012}\n}\n'}",,{'pages': '455-463'},25.0,Perception Markup Language: Towards a Standardized Representation of Perceived Nonverbal Behaviors,2012.0
539,2b82475826449ba43e49664cd185eadd35fdebfe,"Embodied conversational agents (ECAs) are designed to provide natural and intuitive communication with a human user. One major current topic in agent design consequently is to enhance their believability, often by incorporating internal models of emotions or motivations. As psychological theories often lack the necessary details for direct implementation, many agent modelers currently rely on models that are rather marginal in current psychological research, or models that are created ad hoc with little theoretical and empirical foundation. The goal of this article is both to raise psychologists' awareness of the central challenges in the process of creating psychologically believable agents, and to recommend existing psychological frameworks to the virtual agents community that seem particularly useful for implementation in ECAs. Special attention is paid to a computationally detailed model of basic social motives that seems particularly useful for implementation: the Zurich model of social motivation.","[{'authorId': '1844541', 'name': 'Felix D. Schönbrodt'}, {'authorId': '1901675', 'name': 'J. Asendorpf'}]",25.0,"{'bibtex': '@Article{Schönbrodt2011TheCO,\n author = {Felix D. Schönbrodt and J. Asendorpf},\n journal = {J. Media Psychol. Theor. Methods Appl.},\n pages = {100-107},\n title = {The Challenge of Constructing Psychologically Believable Agents},\n volume = {23},\n year = {2011}\n}\n'}",,"{'volume': '23', 'pages': '100-107', 'name': 'J. Media Psychol. Theor. Methods Appl.'}",66.0,The Challenge of Constructing Psychologically Believable Agents,2011.0
540,2b847220cae3406b0e76239f320703d08321df9e,,"[{'authorId': '2816538', 'name': 'T. Koike'}, {'authorId': '34748885', 'name': 'H. Tanabe'}, {'authorId': '3176946', 'name': 'S. Okazaki'}, {'authorId': '38381724', 'name': 'E. Nakagawa'}, {'authorId': '3168697', 'name': 'A. Sasaki'}, {'authorId': '2066525320', 'name': 'Koji Shimada'}, {'authorId': '25569909', 'name': 'Sho K. Sugawara'}, {'authorId': '2015756', 'name': 'Haruka K. Takahashi'}, {'authorId': '2189057', 'name': 'Kazufumi Yoshihara'}, {'authorId': '1412683480', 'name': 'J. Bosch-Bayard'}, {'authorId': '1843699', 'name': 'N. Sadato'}]",139.0,"{'bibtex': '@Article{Koike2016NeuralSO,\n author = {T. Koike and H. Tanabe and S. Okazaki and E. Nakagawa and A. Sasaki and Koji Shimada and Sho K. Sugawara and Haruka K. Takahashi and Kazufumi Yoshihara and J. Bosch-Bayard and N. Sadato},\n journal = {NeuroImage},\n pages = {401-412},\n title = {Neural substrates of shared attention as social memory: A hyperscanning functional magnetic resonance imaging study},\n volume = {125},\n year = {2016}\n}\n'}",,"{'volume': '125', 'pages': '401-412', 'name': 'NeuroImage'}",79.0,Neural substrates of shared attention as social memory: A hyperscanning functional magnetic resonance imaging study,2016.0
541,2b9315e0668a4cc674882d6450a8ba8d1aa78ac6,"Learning to cope with negative emotions is an important challenge, which has received considerable attention in domains like the military and law enforcement. Driven by the aim to develop better training in coping skills, this paper presents an adaptive computational model of emotion regulation strategies, which is inspired by recent neurological literature. The model can be used both to gain more insight in emotion regulation training itself and to develop intelligent virtual reality-based training environments. The behaviour of the model is illustrated by a number of simulation experiments and by a mathematical analysis. In addition, a preliminary validation points out that it is able to approximate empirical data obtained from an experiment with human participants.","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '144668050', 'name': 'C. Gerritsen'}, {'authorId': '144287490', 'name': 'J. D. Man'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",14.0,"{'bibtex': '@Article{Bosse2013LearningER,\n author = {T. Bosse and C. Gerritsen and J. D. Man and Jan Treur},\n booktitle = {2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},\n journal = {2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},\n pages = {245-252},\n title = {Learning Emotion Regulation Strategies: A Cognitive Agent Model},\n volume = {2},\n year = {2013}\n}\n'}","[{'paperId': 'aab001463ad517d55e906d205473672ddf4680d5', 'title': 'Letting It Go: Four Design Concepts to Support Emotion Regulation in Virtual Reality'}, {'paperId': '25036ff7211bc6045e5bceb94b3e3a9ca70cefab', 'title': 'Tahan atau pikir kembali: Strategi regulasi emosi dan kepuasan pernikahan pada masa pandemi Covid-19'}, {'paperId': 'a7d7b654f004f00adc9a1cbdc91eb36648139110', 'title': 'An Interoperable Framework for Computational Models of Emotion'}, {'paperId': 'e0910222c626c5b98a01e32b68b556a48aacfe24', 'title': 'Predicting Student Engagement in the Online Learning Environment'}, {'paperId': 'ffbbc28c675d0969e3cd2d9bbee0fc051f74b3e1', 'title': 'New Technologies for the Understanding, Assessment, and Intervention of Emotion Regulation'}, {'paperId': '0941af785830ce8847cb94da9307c24db70a26d1', 'title': 'A User-Centred Well-Being Home for the Elderly'}, {'paperId': '5492d95d4ebeba80470a6e5ff8f66978e3d6f1e2', 'title': 'Using data visualizations to foster emotion regulation during self-regulated learning with advanced learning technologies: a conceptual framework'}, {'paperId': '8b7b023650ac8c6eae1f03d7df881c6d453a9f0e', 'title': 'Agent-Based Simulation as a Tool for the Design of a Virtual Training Environment'}, {'paperId': '80e595eaa62ab8989580948fb18df5dab473934b', 'title': 'A Computational Model Of Aggression De-escalation'}, {'paperId': '41033c9a9650b863497cc494a8f9d578aa9cc8e9', 'title': 'Modelado de estrategias de regulación emocional en una arquitectura computacional de emociones'}, {'paperId': '2219a980717fb0e70629da64314853874b1fad9b', 'title': 'User-Centered Rating of Well-Being in Older Adults'}, {'paperId': '8e61b1da2f43007e5a23cc09f64e935e9de50666', 'title': 'BITALINO USE AND APLICATIONS FOR HEALTH , EDUCATION , HOME AUTOMATION AND INDUSTRY'}, {'paperId': '10396aeec79c2ef421f2431d15957b1f4bf7dab2', 'title': 'On Computational Models of Emotion Regulation and Their Applications Within HCI'}, {'paperId': '52312c7fd8b9d38fe0e3cdb5097e6b6d4e4f0c96', 'title': 'How Emotions Come in Between Everything'}]","{'name': '2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)', 'pages': '245-252', 'volume': '2'}",29.0,Learning Emotion Regulation Strategies: A Cognitive Agent Model,2013.0
542,2b942261c49553bba62c340b197cf6ef373fd5a4,"Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs. cmu.edu/intraface.","[{'authorId': '3182065', 'name': 'Xuehan Xiong'}, {'authorId': '143867160', 'name': 'F. D. L. Torre'}]",2013.0,"{'bibtex': '@Article{Xiong2013SupervisedDM,\n author = {Xuehan Xiong and F. D. L. Torre},\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {532-539},\n title = {Supervised Descent Method and Its Applications to Face Alignment},\n year = {2013}\n}\n'}",,"{'pages': '532-539', 'name': '2013 IEEE Conference on Computer Vision and Pattern Recognition'}",33.0,Supervised Descent Method and Its Applications to Face Alignment,2013.0
543,2b971138c0073e494788c89ba4c1fca9a901df4b,"Recent studies have shown that mimicry occurs unintentionally and even among strangers. In the present studies, we investigated the consequences of this automatic phenomenon in order to learn more about the adaptive function it serves. In three studies, we consistently found that mimicry increases pro-social behavior. Participants who had been mimicked were more helpful and generous toward other people than were non-mimicked participants. These beneficial consequences of mimicry were not restricted to behavior directed toward the mimicker, but included behavior directed toward people not directly involved in the mimicry situation. These results suggest that the effects of mimicry are not simply due to increased liking for the mimicker, but are due to increased prosocial orientation in general.","[{'authorId': '113458595', 'name': 'Rick B. van Baaren'}, {'authorId': '39821405', 'name': 'R. Holland'}, {'authorId': '4891987', 'name': 'Kerry Kawakami'}, {'authorId': '6511503', 'name': 'A. van Knippenberg'}]",656.0,"{'bibtex': '@Article{Baaren2004MimicryAP,\n author = {Rick B. van Baaren and R. Holland and Kerry Kawakami and A. van Knippenberg},\n journal = {Psychological Science},\n pages = {71 - 74},\n title = {Mimicry and Prosocial Behavior},\n volume = {15},\n year = {2004}\n}\n'}",,"{'volume': '15', 'pages': '71 - 74', 'name': 'Psychological Science'}",17.0,Mimicry and Prosocial Behavior,2004.0
544,2bfec947a52338c4da074ae8d5361923256aa3d8,"Social sharing of emotion is a very common long-term consequence of emotional experiences. Despite the fact that it reactivates the emotions associated with the experience, people are prone to talk about the negative events they face. So, why do people share their emotions? From an intrapersonal perspective, a widespread belief exists that verbalising an emotion alleviates the impact of an emotional event. The purpose of our research was to examine whether verbalisation of emotions effectively contributed to the recovery from the emotion. We review the correlative and experimental studies that were conducted to test this hypothesis. They consistently failed to support the view that mere talking about an emotional memory can lower its emotional load. Nevertheless, participants generally reported that they perceived the sharing process as beneficial. The question then remains as to why people share their emotions and report it is a beneficial process, if it does not bring emotional recovery. To answer this question, we shifted perspective and studied the interpersonal factors implied in the social sharing process. In the following of the chapter, we suggest that the effects of social sharing depend on the social context in which it occurs. We first consider types of sharing partners that are commonly chosen both as a function of age and according to the type of emotional situation experienced. Then, the types of helpful responses from sharing partners are examined. Finally, recent studies on the effects of specific sharing partner’s reactions on affiliation and cognitive benefits are presented. In the conclusion of this chapter, implications of the research on social sharing for the field of emotion regulation are considered.","[{'authorId': '4462136', 'name': 'E. Zech'}, {'authorId': '5722413', 'name': 'B. Rimé'}, {'authorId': '2083760609', 'name': 'F. Nils'}]",42.0,"{'bibtex': '@Inproceedings{Zech2004SocialSO,\n author = {E. Zech and B. Rimé and F. Nils},\n title = {Social sharing of emotion, emotional recovery, and interpersonal aspects},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",92.0,"Social sharing of emotion, emotional recovery, and interpersonal aspects",2004.0
545,2c02011c87565c77ce2048ab8192a105485a2f3d,,"[{'authorId': '69056502', 'name': 'J. Gibbons'}, {'authorId': '145692786', 'name': 'S. Chakraborti'}]",2248.0,"{'bibtex': '@Inproceedings{Gibbons2020NonparametricSI,\n author = {J. Gibbons and S. Chakraborti},\n pages = {977-979},\n title = {Nonparametric Statistical Inference},\n year = {2020}\n}\n'}",,{'pages': '977-979'},236.0,Nonparametric Statistical Inference,2020.0
546,2c0fc87bef123f5556a0a63959e0f004f7730b1e,"The evidence on universals in facial expression of emotion, renewed controversy about that evidence, and new findings on cultural differences are reviewed. New findings on the capability for voluntarily made facial expressions to generate changes in both autonomic and central nervous system activity are discussed, and possible mechanisms by which this could occur are outlined. Finally, new work which has identified how to distinguish the smile of enjoyment from other types of smiling is described.","[{'authorId': '21451088', 'name': 'P. Ekman'}]",697.0,"{'bibtex': '@Article{Ekman1992FacialEO,\n author = {P. Ekman},\n journal = {Psychological Science},\n pages = {34 - 38},\n title = {Facial Expressions of Emotion: New Findings, New Questions},\n volume = {3},\n year = {1992}\n}\n'}",,"{'volume': '3', 'pages': '34 - 38', 'name': 'Psychological Science'}",70.0,"Facial Expressions of Emotion: New Findings, New Questions",1992.0
547,2c28755adfb8ef67337a98b07dd4c80f96e82e83,"Key Points Question What aspects of psychotherapy content are significantly associated with clinical outcomes? Findings In this quality improvement study, a deep learning model was trained to automatically categorize therapist utterances from approximately 90 000 hours of internet-enabled cognitive behavior therapy (CBT). Increased quantities of CBT change methods were positively associated with reliable improvement in patient symptoms, and the quantity of nontherapy-related content showed a negative association. Meaning The findings support the key principles underlying CBT as a treatment and demonstrate that applying deep learning to large clinical data sets can provide valuable insights into the effectiveness of psychotherapy.","[{'authorId': '1984805', 'name': 'M. Ewbank'}, {'authorId': '153714664', 'name': 'R. Cummins'}, {'authorId': '2095029103', 'name': 'V. Tablan'}, {'authorId': '13827718', 'name': 'S. Bateup'}, {'authorId': '94314925', 'name': 'A. Catarino'}, {'authorId': '2111210811', 'name': 'Alan Martin'}, {'authorId': '7176472', 'name': 'A. Blackwell'}]",63.0,"{'bibtex': '@Article{Ewbank2019QuantifyingTA,\n author = {M. Ewbank and R. Cummins and V. Tablan and S. Bateup and A. Catarino and Alan Martin and A. Blackwell},\n journal = {JAMA Psychiatry},\n pages = {35 - 43},\n title = {Quantifying the Association Between Psychotherapy Content and Clinical Outcomes Using Deep Learning},\n volume = {77},\n year = {2019}\n}\n'}",,"{'volume': '77', 'pages': '35 - 43', 'name': 'JAMA Psychiatry'}",36.0,Quantifying the Association Between Psychotherapy Content and Clinical Outcomes Using Deep Learning,2019.0
548,2cc18c34e3e6375ac0472d0f79f789aec3986b68,,"[{'authorId': '3026039', 'name': 'C. Midden'}, {'authorId': '145960497', 'name': 'Jaap Ham'}]",23.0,"{'bibtex': '@Inproceedings{Midden2012TheIO,\n author = {C. Midden and Jaap Ham},\n pages = {90-99},\n title = {The Illusion of Agency: The Influence of the Agency of an Artificial Agent on Its Persuasive Power},\n year = {2012}\n}\n'}",,{'pages': '90-99'},21.0,The Illusion of Agency: The Influence of the Agency of an Artificial Agent on Its Persuasive Power,2012.0
549,2cd052ffde128ead5a220a67a8d23345c93be314,"Mental disorders have been a serious and growing problem for people around the planet in recent years and efficient ways of treatment are yet to be developed. Self-Attachment is a recently introduced psychotherapeutic method in which the patient creates a compassionate and affectional bond with their childhood-self using their childhood photos. The underlying theory and the promising results of the Self-Attachment therapy, in combination with its self-administrable nature, motivate further research in technologies that can enhance the procedure of the therapy. Virtual reality is a technology that has experienced rapid evolution over the last years, yet its ability to replace existing psychotherapeutic techniques remains fairly unexplored. The aim of this thesis is to create a user-friendly and highly interactive virtual reality platform that delivers the Self-Attachment therapy in an efficient way by creating an avatar of the patient’s childhood-self based on a 2D childhood photo. For the fulfillment of this goal, the implemented platform has been equipped with the latest features of the head-mounted Oculus Quest device and interactable components. In addition, the virtual environment features a virtual agent who acts as an assistant and is able to predict the emotional state of the user. The aforementioned achievements have led to the creation of a complete virtual platform, capable of autonomously delivering the Self-Attachment therapy in a personalised experience for the user. During an impact evaluation trial, a version of the platform was tested by seven healthy individuals and the results were promising. The high levels of immersiveness of the platform and its ability to attract and maintain the user’s engagement played a crucial role in obtaining positive feedback from the participants. Furthermore, the platform was successful in triggering participants’ emotions, who in the end preferred the virtual experience over the original therapy which is based on the use of their childhood photo. Consequently, we believe that this project is the beginning of the creation of a tool that will be able to autonomously and effectively treat people with mental disorders in the near future.","[{'authorId': '1694989', 'name': 'A. Edalat'}]",1.0,"{'bibtex': '@Inproceedings{Edalat2020VirtualRP,\n author = {A. Edalat},\n title = {Virtual reality platform for Self-Attachment therapy, assisted by a virtual agent with emotion recognition capabilities},\n year = {2020}\n}\n'}","[{'paperId': 'c0292ef46a4bed4f8146a654983683a206295f1d', 'title': 'Photorealistic avatars to enhance the efficacy of Selfattachment psychotherapy'}]",,50.0,"Virtual reality platform for Self-Attachment therapy, assisted by a virtual agent with emotion recognition capabilities",2020.0
550,2d08dbfb26d35399c7c9f68b6faa46f63488d499,"Objectives:Immersive virtual reality (IVR) therapy has been explored as an adjunct therapy for the management of acute pain among children and adults for several conditions. Therapeutic approaches have traditionally involved medication and physiotherapy but such approaches are limited over time by their cost and side effects. This review seeks to critically evaluate the evidence for and against IVR as an adjunctive therapy for acute clinical pain applications. Methods:A rapid evidence assessment (REA) strategy was used. CINAHL, Medline, Web of Science, IEEE Xplore Digital Library, and the Cochrane Library databases were screened in from December 2012 to March 2013 to identify studies exploring IVR therapies as an intervention to assist in the management of pain. Main outcome measures were for acute pain and functional impairment. Results:Seventeen research studies were included in total including 5 RCTs, 6 randomized crossover studies, 2 case series studies, and 4 single-patient case studies. This included a total of 337 patients. Of these studies only 4 had a low risk of bias. There was strong overall evidence for immediate and short-term pain reduction, whereas moderate evidence was found for short-term effects on physical function. Little evidence exists for longer-term benefits. IVR was not associated with any serious adverse events. Discussion:This review found moderate evidence for the reduction of pain and functional impairment after IVR in patients with acute pain. Further high-quality studies are required for the conclusive judgment of its effectiveness in acute pain, to establish potential benefits for chronic pain, and for safety.","[{'authorId': '34214879', 'name': 'Bernie Garrett'}, {'authorId': '3237686', 'name': 'T. Taverner'}, {'authorId': '15921526', 'name': 'Wendy Masinde'}, {'authorId': '1744767', 'name': 'D. Gromala'}, {'authorId': '145600106', 'name': 'Chris D. Shaw'}, {'authorId': '12200854', 'name': 'Michael Negraeff'}]",143.0,"{'bibtex': '@Article{Garrett2014ARE,\n author = {Bernie Garrett and T. Taverner and Wendy Masinde and D. Gromala and Chris D. Shaw and Michael Negraeff},\n journal = {The Clinical Journal of Pain},\n pages = {1089–1098},\n title = {A Rapid Evidence Assessment of Immersive Virtual Reality as an Adjunct Therapy in Acute Pain Management in Clinical Practice},\n volume = {30},\n year = {2014}\n}\n'}",,"{'volume': '30', 'pages': '1089–1098', 'name': 'The Clinical Journal of Pain'}",77.0,A Rapid Evidence Assessment of Immersive Virtual Reality as an Adjunct Therapy in Acute Pain Management in Clinical Practice,2014.0
551,2d2c0b2ca556f27f9adcecf636f7a2eb8f7f2a49,"A chatbot is software that is programmed and designed to simulate conversation with human users through Artificial Intelligence. During this pandemic, where students are confined to their homes with limited guidance from teachers, this chatbot provides a human-like interface that can solve academic queries of students, thus encouraging students to learn in an interesting manner. This paper demonstrates and presents the design and development of Student Chatbot software integrated with a user website that handles students' queries through defined intents. The paper covers the chatbot system with Recurrent Neural Network (RNN) for dealing language part, Convolutional Neural Network (CNN) to deal with the image part, Dialogflow, illustrating with precision the intent and entity representation and keyword matching techniques used by introducing an artificial brain into “Web-Based Bot”. The aim to have a balanced user interface that is easy to access and customized to all target users. Also two Visual Question Answering chatbots ie visualdialog.cloudcv and CloudCV are compared to find which performs better. It was found that chatbot serves accurate for education.","[{'authorId': '2127998124', 'name': 'J. Sophia'}, {'authorId': '40636037', 'name': 'T. Jacob'}]",4.0,"{'bibtex': '@Article{Sophia2021EDUBOTACF,\n author = {J. Sophia and T. Jacob},\n journal = {2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)},\n pages = {1707-1714},\n title = {EDUBOT-A Chatbot For Education in Covid-19 Pandemic and VQAbot Comparison},\n year = {2021}\n}\n'}",,"{'pages': '1707-1714', 'name': '2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)'}",17.0,EDUBOT-A Chatbot For Education in Covid-19 Pandemic and VQAbot Comparison,2021.0
552,2d5673caa9e6af3a7b82a43f19ee920992db07ad,"I propose to consider the question, “Can machines think?”♣ This should begin with definitions of the meaning of the terms “machine” and “think”. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words “machine” and “think” are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, “Can machines think?” is to be sought in a statistical survey such as a Gallup poll.","[{'authorId': '2262347', 'name': 'A. Turing'}]",9664.0,"{'bibtex': '@Article{Turing1950ComputingMA,\n author = {A. Turing},\n journal = {Mind},\n pages = {433-460},\n title = {Computing Machinery and Intelligence},\n volume = {LIX},\n year = {1950}\n}\n'}",,"{'volume': 'LIX', 'pages': '433-460', 'name': 'Mind'}",14.0,Computing Machinery and Intelligence,1950.0
553,2d7a3dd3ea386de50e13704c37b3c2a106ad571a,,"[{'authorId': '3253521', 'name': 'A. B. Altamimi'}, {'authorId': '8970271', 'name': 'R. Ramadan'}]",25.0,"{'bibtex': '@Article{Altamimi2016TowardsIO,\n author = {A. B. Altamimi and R. Ramadan},\n journal = {Complex Adaptive Systems Modeling},\n pages = {1-11},\n title = {Towards internet of things modeling: a gateway approach},\n volume = {4},\n year = {2016}\n}\n'}",,"{'volume': '4', 'pages': '1-11', 'name': 'Complex Adaptive Systems Modeling'}",60.0,Towards internet of things modeling: a gateway approach,2016.0
554,2d88e7e9c6cf3cc48f3d34c6f6b893799695bc43,,"[{'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '2144259493', 'name': 'Soyoung Kim'}]",148.0,"{'bibtex': '@Article{Baylor2009DesigningNC,\n author = {A. L. Baylor and Soyoung Kim},\n journal = {Comput. Hum. Behav.},\n pages = {450-457},\n title = {Designing nonverbal communication for pedagogical agents: When less is more},\n volume = {25},\n year = {2009}\n}\n'}",,"{'volume': '25', 'pages': '450-457', 'name': 'Comput. Hum. Behav.'}",60.0,Designing nonverbal communication for pedagogical agents: When less is more,2009.0
557,2d99dcb48f4f5f9cae977a9e7da9ecdc17b9b0c1,"Speech-driven lip synchronization, an important part of facial animation, is to animate a face model to render lip movements that are synchronized with the acoustic speech signal. It has many applications in human-computer interaction. In this paper, we present a framework that systematically addresses multimodal database collection and processing and real-time speech-driven lip synchronization using collaborative filtering which is a data-driven approach used by many online retailers to recommend products. Mel-frequency cepstral coefficients (MFCCs) with their delta and acceleration coefficients and Facial Animation Parameters (FAPs) supported by MPEG-4 for the visual representation of speech are utilized as acoustic features and animation parameters respectively. The proposed system is speaker independent and real-time capable. The subjective experiments show that the proposed approach generates a natural facial animation.","[{'authorId': '3295988', 'name': 'Kaihui Mu'}, {'authorId': '37670752', 'name': 'J. Tao'}, {'authorId': '2061534506', 'name': 'Jianfeng Che'}, {'authorId': '2740129', 'name': 'Minghao Yang'}]",6.0,"{'bibtex': '@Article{Mu2010RealtimeSL,\n author = {Kaihui Mu and J. Tao and Jianfeng Che and Minghao Yang},\n journal = {2010 4th International Universal Communication Symposium},\n pages = {378-382},\n title = {Real-time speech-driven lip synchronization},\n year = {2010}\n}\n'}",,"{'pages': '378-382', 'name': '2010 4th International Universal Communication Symposium'}",18.0,Real-time speech-driven lip synchronization,2010.0
558,2db6b1345d2084ce1503fd56618857cef04f6402,,"[{'authorId': '145884449', 'name': 'W. Buxton'}]",4190.0,"{'bibtex': '@Inproceedings{Buxton1988HumanComputerI,\n author = {W. Buxton},\n title = {Human-Computer Interaction},\n year = {1988}\n}\n'}",,,332.0,Human-Computer Interaction,1988.0
559,2dcb37b781b6286dc64fbec6218c229915e07de7,"Nowadays virtual humans such as non-player characters in computer games need to have a real autonomy in order to live their own life in persistent virtual worlds. When designing autonomous virtual humans, the action selection problem needs to be considered, as it is responsible for decision making at each moment in time. Action selection architectures for autonomous virtual humans should be individual, motivational, reactive and proactive to obtain a high degree of autonomy. This paper describes in detail our motivational model of action selection for autonomous virtual humans in which overlapping hierarchical classifier systems, working in parallel to generate coherent behavioral plans, are associated with the functionalities of a free flow hierarchy to give reactivity to the hierarchical system. Finally, results of our model in a complex simulated environment, with conflicting motivations, demonstrate that the model is sufficiently robust and flexible for designing motivational autonomous virtual humans in real-time.","[{'authorId': '1761859', 'name': 'E. D. Sevin'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}]",71.0,"{'bibtex': '@Article{Sevin2005AMM,\n author = {E. D. Sevin and D. Thalmann},\n journal = {International 2005 Computer Graphics},\n pages = {213-220},\n title = {A motivational model of action selection for virtual humans},\n year = {2005}\n}\n'}",,"{'pages': '213-220', 'name': 'International 2005 Computer Graphics'}",36.0,A motivational model of action selection for virtual humans,2005.0
560,2dced31a14401d465cd115902bf8f508d79de076,"Non-verbal signals expressed through body language play a crucial role in multi-modal human communication during social relations. Indeed, in all cultures, facial expressions are the most universal and direct signs to express innate emotional cues. A human face conveys important information in social interactions and helps us to better understand our social partners and establish empathic links. Latest researches show that humanoid and social robots are becoming increasingly similar to humans, both esthetically and expressively. However, their visual expressiveness is a crucial issue that must be improved to make these robots more realistic and intuitively perceivable by humans as not different from them. This study concerns the capability of a humanoid robot to exhibit emotions through facial expressions. More specifically, emotional signs performed by a humanoid robot have been compared with corresponding human facial expressions in terms of recognition rate and response time. The set of stimuli included standardized human expressions taken from an Ekman-based database and the same facial expressions performed by the robot. Furthermore, participants’ psychophysiological responses have been explored to investigate whether there could be differences induced by interpreting robot or human emotional stimuli. Preliminary results show a trend to better recognize expressions performed by the robot than 2D photos or 3D models. Moreover, no significant differences in the subjects’ psychophysiological state have been found during the discrimination of facial expressions performed by the robot in comparison with the same task performed with 2D photos and 3D models.","[{'authorId': '35440863', 'name': 'N. Lazzeri'}, {'authorId': '34573296', 'name': 'D. Mazzei'}, {'authorId': '145254408', 'name': 'A. Greco'}, {'authorId': '6284325', 'name': 'Annalisa Rotesi'}, {'authorId': '1730665', 'name': 'A. Lanatà'}, {'authorId': '46301802', 'name': 'D. De Rossi'}]",26.0,"{'bibtex': '@Article{Lazzeri2015CanAH,\n author = {N. Lazzeri and D. Mazzei and A. Greco and Annalisa Rotesi and A. Lanatà and D. De Rossi},\n journal = {Frontiers in Bioengineering and Biotechnology},\n title = {Can a Humanoid Face be Expressive? A Psychophysiological Investigation},\n volume = {3},\n year = {2015}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Bioengineering and Biotechnology'}",84.0,Can a Humanoid Face be Expressive? A Psychophysiological Investigation,2015.0
561,2ddfcc851a12ed34d8f0ad270c4b8286393796f4,"Avoidant behaviour is critical in social anxiety and social phobia, being a major factor in the maintenance of anxiety. However, almost all previous studies of social avoidance were restricted to using self-reports for the study of intentional aspects of avoidance. In contrast, the current study used immersive virtual reality technology to measure interpersonal distance as an index of avoidance, an unintentional behavioural indicator. In a virtual supermarket, twenty-three female participants differing in social anxiety approached computer-generated persons (avatars) under the pretext of a cover story. During the task, different aspects of approach and avoidance were measured. The results confirmed the hypotheses: The more anxious participants were, the more slowly they approached the avatars, and the larger the distance they kept from the avatars. This indicates that even sub-phobic social anxiety is related to unintentional avoidance behaviour in social situations.","[{'authorId': '2505736', 'name': 'M. Rinck'}, {'authorId': '113703375', 'name': 'T. Rörtgen'}, {'authorId': '46641514', 'name': 'Wolf-Gero Lange'}, {'authorId': '2365875', 'name': 'R. Dotsch'}, {'authorId': '39854264', 'name': 'D. Wigboldus'}, {'authorId': '3107071', 'name': 'E. Becker'}]",67.0,"{'bibtex': '@Article{Rinck2010SocialAP,\n author = {M. Rinck and T. Rörtgen and Wolf-Gero Lange and R. Dotsch and D. Wigboldus and E. Becker},\n journal = {Cognition and Emotion},\n pages = {1269 - 1276},\n title = {Social anxiety predicts avoidance behaviour in virtual encounters},\n volume = {24},\n year = {2010}\n}\n'}",,"{'volume': '24', 'pages': '1269 - 1276', 'name': 'Cognition and Emotion'}",25.0,Social anxiety predicts avoidance behaviour in virtual encounters,2010.0
562,2e0485f38581ad5c99055dbe0f313889ae7dfa78,"In this paper we propose the role of pedagogical agents as social models. We first framed our arguments in social cognitive perspectives and supported them with findings from several experimental studies we have conducted. In the studies, we found that learners perceived their agents socially and that the agents' social presence influenced the learner's cognitive and affective characteristics. Two on-going projects highlighting the agents' role as social models for enhancing females' motivation and attitudes toward math and engineering are briefly described. We have consistently found that a particularly effective use of a pedagogical agent is as a ""social model"" to enhance learners' motivation and attitudes. Given that such affective characteristics enable the learner to face challenges, to engage, and to persist in learning, implementing pedagogical agents for this purpose is of great value. In that an individual's learning and cognitive development are inevitably rooted in the social context where the individual has been placed, his/her positive or negative attitudes towards the learning task may also be mediated by simulated social relations and social interactions. Social modeling research illustrates how the presence and role of others (in this case, that of an anthropomorphic pedagogical agent) can influence one's self-efficacy beliefs and social and intellectual functioning. Social modeling refers to psychological and behavioral changes that result from observing others in social contexts. Through vicarious experience and/or social interaction, learners acquire resources or expertise mediated through the social models. For example, a pedagogical agent serving as a 'mastery model' may demonstrate positive attitudes towards the task and/or the desired levels of performance so that a learner can learn vicariously. Or an agent may work along with a learner as a companion (see Cassell & colleagues, and Goldman & colleagues, this issue) and even figuratively learn from the learner (see Schwartz & colleagues, this issue), serving as a 'coping model.' We first inquired into the design constituents of a pedagogical agent that would produce successful modeling effects. Relying on Bandura's (1997) concept of attribute similarity - that having similar personal characteristics of learner and social model is desirable, we (Kim & Baylor, in press) proposed seven design constituents important for a pedagogical agent serving as an effective social model: 1) agent competency, 2) interaction type, 3) affect, 4) gender, 5) ethnicity, 6) multiplicity, and 7) feedback. We have conducted several experimental studies examining the impact of varying permutations of the constituents on learners' perceptions, social judgments, and motivation.","[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '25550816', 'name': 'A. L. Baylor'}]",54.0,"{'bibtex': '@Article{Kim2007PedagogicalAA,\n author = {Yanghee Kim and A. L. Baylor},\n journal = {Educational Technology archive},\n pages = {23-28},\n title = {Pedagogical Agents as Social Models to Influence Learner Attitudes},\n volume = {47},\n year = {2007}\n}\n'}",,"{'volume': '47', 'pages': '23-28', 'name': 'Educational Technology archive'}",17.0,Pedagogical Agents as Social Models to Influence Learner Attitudes,2007.0
563,2e15c4feab6122750ea53583325ca450d9a70d0d,"A fire occurred on the night of Feb. 20, 2003, in The Station nightclub at 211 Cowesett Avenue, West Warwick, Rhode Island. A band that was on the platform that night, during its performance, used pyrotechnics that ignited polyurethane foam insulation lining the walls and ceiling of the platform. The fire spread quickly along the walls and ceiling area over the dance floor. Smoke was visible in the exit doorways in a little more than one minute, and flames were observed breaking through a portion of the roof in less than five minutes. Egress from the nightclub, which was not equipped with sprinklers, was hampered by crowding at the main entrance to the building. One hundred people lost their lives in the fire. On Feb. 27, 2003, under the authority of the National Construction Safety Team (NCST) Act, the National Institute of Standards and Technology (NIST) established a National Construction Safety Team to determine the likely technical cause or causes of the building failure that led to the high number of casualties in that fire. This report documents the procedures, findings, and issues that were raised by the investigation. Volume I contains the main report and Volume II contains appendix material. The investigation concluded that strict adherence to 2003 model codes available at the time of the fire would go a long way to preventing similar tragedies in the future. Changes to the codes subsequent to the fire made them stronger. By making some additional changes – and state and local agencies adopting and enforcing them – we can strengthen occupant safety even further. Ten recommendations to improve model building and fire codes, standards and practices (as they existed in February 2003) resulted from the investigation, including (i) urging state and local jurisdictions to (a) adopt and update building and fire codes covering nightclubs based on one of the model codes and (b) enforce those codes aggressively; (ii) strengthening the requirements for the installation of automatic fire sprinklers; (iii) increasing the factor of safety on the time for occupants to egress; (iv) tightening the restriction on the use of flexible polyurethane foam -and other materials that ignite as easily and propagate flames as rapidly as non-fire retarded foam -as an interior finish product; (v) further limiting the use of pyrotechnics; and (vi) conducting research in specific areas to underpin the recommended changes.","[{'authorId': '89869714', 'name': 'W. Grosshandler'}, {'authorId': '8871732', 'name': 'N. Bryner'}, {'authorId': '50853958', 'name': 'Daniel Madrzykowski'}, {'authorId': '97424509', 'name': 'K. Kuntz'}]",46.0,"{'bibtex': '@Inproceedings{Grosshandler2005ReportOT,\n author = {W. Grosshandler and N. Bryner and Daniel Madrzykowski and K. Kuntz},\n title = {Report of the technical investigation of The Station nightclub fire :: appendices},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",1.0,Report of the technical investigation of The Station nightclub fire :: appendices,2005.0
564,2e36998bb64688e3484c2d6f8431b86f04a1332e,,"[{'authorId': '1807752', 'name': 'F. D. Rosis'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1802126', 'name': 'I. Poggi'}, {'authorId': '1694255', 'name': 'V. Carofiglio'}, {'authorId': '1739256', 'name': 'B. D. Carolis'}]",312.0,"{'bibtex': ""@Article{Rosis2003FromGM,\n author = {F. D. Rosis and C. Pelachaud and I. Poggi and V. Carofiglio and B. D. Carolis},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {81-118},\n title = {From Greta's mind to her face: modelling the dynamics of affective states in a conversational embodied agent},\n volume = {59},\n year = {2003}\n}\n""}",,"{'volume': '59', 'pages': '81-118', 'name': 'Int. J. Hum. Comput. Stud.'}",91.0,From Greta's mind to her face: modelling the dynamics of affective states in a conversational embodied agent,2003.0
570,2e377bdd4104d2869d1cc53e0a7f6acb36a53d8e,,"[{'authorId': '112853075', 'name': 'Norbert Wiley'}]",21.0,"{'bibtex': '@Inproceedings{Wiley2003EMOTIONAF,\n author = {Norbert Wiley},\n pages = {169-187},\n title = {EMOTION AND FILM THEORY},\n volume = {26},\n year = {2003}\n}\n'}",,"{'volume': '26', 'pages': '169-187', 'name': ''}",39.0,EMOTION AND FILM THEORY,2003.0
571,2e3aa3170ddeca53d3cbb7ce0bbbba0479666e47,,"[{'authorId': '2334450', 'name': 'Clare Press'}]",112.0,"{'bibtex': '@Article{Press2011ActionOA,\n author = {Clare Press},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {1410-1418},\n title = {Action observation and robotic agents: Learning and anthropomorphism},\n volume = {35},\n year = {2011}\n}\n'}",,"{'volume': '35', 'pages': '1410-1418', 'name': 'Neuroscience & Biobehavioral Reviews'}",87.0,Action observation and robotic agents: Learning and anthropomorphism,2011.0
572,2e43a55fbdafac189615945fa17feaf6610b7339,,"[{'authorId': '2245673', 'name': 'A. Duchowski'}]",344.0,"{'bibtex': '@Inproceedings{Duchowski2003EyeTM,\n author = {A. Duchowski},\n pages = {1-366},\n title = {Eye Tracking Methodology - Theory and Practice, Third Edition},\n year = {2003}\n}\n'}",,{'pages': '1-366'},0.0,"Eye Tracking Methodology - Theory and Practice, Third Edition",2003.0
573,2e4ca3d95ffb83870661dd66deee143e782f0706,"Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto metrics to evaluate tasks such as image captioning and machine translation, despite their known limitations. This is partly due to ease of use, and partly because researchers expect to see them and know how to interpret them. In this paper, we urge the community for more careful consideration of how they automatically evaluate their models by demonstrating important failure cases on multiple datasets, language pairs and tasks. Our experiments show that metrics (i) usually prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set.","[{'authorId': '10791325', 'name': 'Ozan Caglayan'}, {'authorId': '3238408', 'name': 'P. Madhyastha'}, {'authorId': '1702974', 'name': 'Lucia Specia'}]",24.0,"{'bibtex': '@Article{Caglayan2020CuriousCO,\n author = {Ozan Caglayan and P. Madhyastha and Lucia Specia},\n journal = {ArXiv},\n title = {Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale},\n volume = {abs/2010.13588},\n year = {2020}\n}\n'}",,"{'volume': 'abs/2010.13588', 'name': 'ArXiv'}",27.0,Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale,2020.0
574,2e4f8e46898aa5229d079487a009ec88ed70294d,"Virtual reality (VR) is a valuable research tool offering advantages in terms of high experimenter control and standardization in the simulation of vivid personal and social experiences. It has been used for assessments and training in social cognition with the use of virtual agents instead of face-to-face interactions – but its potential for the study of social emotion regulation has, perhaps surprisingly, largely remained untapped. The present study evaluates a novel immersive VR scenario designed to study the efficacy of social support by a virtual agent using a modified version of Cyberball, an established paradigm to induce the feeling of ostracism. Participants embodied a new pupil in a virtual school environment and played Cyberball, after which they either did or did not receive emotional support. Self-reports and psychophysiological markers demonstrated that the negative impact of social exclusion in Cyberball was successfully replicated, while participants also reported a significant improvement in emotional state after being supported by the virtual agent. These results indicate the potential of the developed scenario for research on social emotion regulation in immersive VR. Future studies could aim to test the efficacy of social support for people with difficulties in self-regulation, for example individuals with high social anxiety, with a view to developing training programs in VR.","[{'authorId': '2124332447', 'name': 'Lina Stallmann'}, {'authorId': '2054039824', 'name': 'Michel Tran'}, {'authorId': '3090887', 'name': 'D. Rudrauf'}, {'authorId': '39779139', 'name': 'Daniel Dukes'}, {'authorId': '38707445', 'name': 'Andrea C. Samson'}]",1.0,"{'bibtex': '@Article{Stallmann2023SimulatingSE,\n author = {Lina Stallmann and Michel Tran and D. Rudrauf and Daniel Dukes and Andrea C. Samson},\n booktitle = {International Journal of Emerging Technologies in Learning (iJET)},\n journal = {Int. J. Emerg. Technol. Learn.},\n pages = {4-27},\n title = {Simulating Social Emotion Regulation in Virtual Reality: Effect of Virtual Social Support Following Ostracism in a Cyberball Game},\n volume = {18},\n year = {2023}\n}\n'}","[{'paperId': 'ea8467e1d26f472a5fc8579d0fc8372149283acc', 'title': 'Socially Supported by an Embodied Agent: The Development of a Virtual-Reality Paradigm to Study Social Emotion Regulation'}]","{'name': 'Int. J. Emerg. Technol. Learn.', 'pages': '4-27', 'volume': '18'}",0.0,Simulating Social Emotion Regulation in Virtual Reality: Effect of Virtual Social Support Following Ostracism in a Cyberball Game,2023.0
575,2e571bffc52137980f4c5b4385466435ea5c42d3,,"[{'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '2345401', 'name': 'Sathish Pammi'}, {'authorId': '1783043', 'name': 'S. Hyniewska'}, {'authorId': '144951065', 'name': 'M. Schröder'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",53.0,"{'bibtex': '@Inproceedings{Bevacqua2010MultimodalBF,\n author = {Elisabetta Bevacqua and Sathish Pammi and S. Hyniewska and M. Schröder and C. Pelachaud},\n pages = {194-200},\n title = {Multimodal Backchannels for Embodied Conversational Agents},\n year = {2010}\n}\n'}",,{'pages': '194-200'},17.0,Multimodal Backchannels for Embodied Conversational Agents,2010.0
576,2e62d1345b340d5fda3b092c460264b9543bc4b5,"From the Publisher: 
This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. 
 
Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.","[{'authorId': '1715339', 'name': 'D. Goldberg'}]",60120.0,"{'bibtex': '@Inproceedings{Goldberg1988GeneticAI,\n author = {D. Goldberg},\n title = {Genetic Algorithms in Search Optimization and Machine Learning},\n year = {1988}\n}\n'}",,"{'volume': '', 'name': ''}",1.0,Genetic Algorithms in Search Optimization and Machine Learning,1988.0
577,2e9d221c206e9503ceb452302d68d10e293f2a10,"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.","[{'authorId': '3308557', 'name': 'S. Hochreiter'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}]",70615.0,"{'bibtex': '@Article{Hochreiter1997LongSM,\n author = {S. Hochreiter and J. Schmidhuber},\n journal = {Neural Computation},\n pages = {1735-1780},\n title = {Long Short-Term Memory},\n volume = {9},\n year = {1997}\n}\n'}",,"{'volume': '9', 'pages': '1735-1780', 'name': 'Neural Computation'}",47.0,Long Short-Term Memory,1997.0
578,2ea33c152a6ec4b33c379eb4cea81b34edd84813,"Evacuation is a complex process influenced by multiple parameters that have significant impact on the design and execution of an efficient Active Evacuation Route (AER). Computer vision algorithms are critical for an effective AER, since it indicates the current situation awareness of the environment. Thermal imaging is an alternative effective computer vision mechanisms for the analysis of the crowd behavior either at the micro or macro scale. Thermal imaging allows efficient determination of people from the background even if highly dynamic scenes, illumination, occlusions or content alterations. This allows micro-scale analysis of the crowds resulting in an efficient active evacuation design. Experiments on thermal data from Athens International Airport indicate the assistive performance of our method.","[{'authorId': '120205775', 'name': 'N. Doulamis'}, {'authorId': '1746705', 'name': 'A. Doulamis'}, {'authorId': '2879272', 'name': 'Konstantinos Makantasis'}, {'authorId': '144542193', 'name': 'K. Karantzalos'}, {'authorId': '3044370', 'name': 'K. Loupos'}]",2.0,"{'bibtex': '@Article{Doulamis2015MicroscaleTB,\n author = {N. Doulamis and A. Doulamis and Konstantinos Makantasis and K. Karantzalos and K. Loupos},\n journal = {Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments},\n title = {Micro-scale thermal behavioral analysis for active evacuation routes},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments'},26.0,Micro-scale thermal behavioral analysis for active evacuation routes,2015.0
579,2ea93609216babae7f3a323089234e1305309037,"Participants performed a stop signal task in which an emotional picture preceded a neutral stimulus. They were asked to respond on the basis of the identity of the neutral stimulus unless an auditory tone was presented, in which case participants should try to withhold their response. In Experiment 1, we used positive, neutral and negative pictures. Results demonstrated that the presentation of an emotional stimulus prolonged both response and stopping latencies regardless of the valence of the emotional stimulus. This suggested that the degree of arousal could modulate the interference effect. In Experiment 2, high- and low-arousing pictures with a positive or negative valence were used. In line with the arousal hypothesis, high-arousal pictures interfered more with responding and stopping than low-arousing pictures whereas the valence of the pictures had little or no effect. These findings support the hypothesis that emotional stimuli interrupt ongoing cognitively controlled activities because they attract attention away from these ongoing activities.","[{'authorId': '2157435', 'name': 'F. Verbruggen'}, {'authorId': '3602422', 'name': 'J. de Houwer'}]",268.0,"{'bibtex': '@Article{Verbruggen2007DoES,\n author = {F. Verbruggen and J. de Houwer},\n journal = {Cognition and Emotion},\n pages = {391 - 403},\n title = {Do emotional stimuli interfere with response inhibition? Evidence from the stop signal paradigm},\n volume = {21},\n year = {2007}\n}\n'}",,"{'volume': '21', 'pages': '391 - 403', 'name': 'Cognition and Emotion'}",27.0,Do emotional stimuli interfere with response inhibition? Evidence from the stop signal paradigm,2007.0
580,2eb75ae0d50db8680d37f2eeaad5fa745fd3b4a1,"Objective:To examine the effectiveness of 2 affect recognition interventions (Faces and Stories) in people with a traumatic brain injury. Setting:Postacute rehabilitation facilities. Participants:A total of 203 participants with moderate to severe traumatic brain injury were screened; 71 were eligible and randomized to the Faces (n = 24), Stories (n = 23), and Control interventions (n = 24). Participants were an average of 39.8 years of age and 10.3 years postinjury; 74% of participants were male. Design:Randomized controlled trial with immediate, 3-month, and 6-month follow-up posttests. Interventions were 9 hours of computer-based training with a therapist. Measures:Diagnostic Assessment of Nonverbal Accuracy 2-Adult Faces; Emotional Inference From Stories Test; Empathy (Interpersonal Reactivity Index); and Irritability and Aggression (Neuropsychiatric Inventory). Results:The Faces Intervention did significantly better than the Control Intervention on the Diagnostic Assessment of Nonverbal Accuracy 2-Adult Faces (P = .031) posttreatment; no time effect or group interaction was observed. No other significant differences were noted for the Faces Intervention. No significant differences were observed between the Stories and the Control Interventions; however, a significant time effect was found for the Emotional Inference From Stories Test. Conclusion:The Faces Intervention effectively improved facial affect recognition in participants with chronic post–traumatic brain injury, and changes were maintained for 6 months. Future work should focus on generalizing this skill to functional behaviors.","[{'authorId': '33932391', 'name': 'Dawn Neumann'}, {'authorId': '4042102', 'name': 'D. Babbage'}, {'authorId': '5705546', 'name': 'Barbra Zupan'}, {'authorId': '8982797', 'name': 'B. Willer'}]",63.0,"{'bibtex': '@Article{Neumann2015ARC,\n author = {Dawn Neumann and D. Babbage and Barbra Zupan and B. Willer},\n journal = {Journal of Head Trauma Rehabilitation},\n pages = {E12–E23},\n title = {A Randomized Controlled Trial of Emotion Recognition Training After Traumatic Brain Injury},\n volume = {30},\n year = {2015}\n}\n'}",,"{'volume': '30', 'pages': 'E12–E23', 'name': 'Journal of Head Trauma Rehabilitation'}",49.0,A Randomized Controlled Trial of Emotion Recognition Training After Traumatic Brain Injury,2015.0
581,2ebbced3dd7d5806463079fa9080b420b3ed8520,"Nowadays, social media became a tracker of the COVID-19 disease which reflects the status of the COVID-19 outbreak in the world. Although it is important to know the impact of COVID- 19 on the sentiment of mass people for the government and the policymakers in order to address peoples’ needs and take emergent decisions during such crisis time, not many studies have been conducted regarding this issue. Moreover, very few studies were conducted on sentiment analysis during the COVID-19 pandemic in the context of Bangladesh. The purpose of this study is to estimate the impact of the COVID-19 outbreak on the sentiment of the Bangladeshi people through a machine learning approach. To achieve this goal, COVID-19 tweets were collected over a specific period and then build a deep learning classifier, having an average area under the curve (AUC) of 0.76. The study analyzes the spread and estimates various public emotions during the outbreak. And reveals that a significant number (55%) of people had negative sentiment regarding COVID-19, whereas, 38% and 7% of people had positive and neutral sentiment respectively. This study also found that people’s involvement with social media increases as the number of active COVID-19 cases increases. Moreover, this study identified people’s sentiment towards some important concerns regarding the COVID-19 pandemic.","[{'authorId': '9256301', 'name': 'M. Islam'}, {'authorId': '2045331161', 'name': 'N. Khan'}, {'authorId': '49274455', 'name': 'Ayon Roy'}, {'authorId': '2116363360', 'name': 'Md. Mahbubar Rahman'}, {'authorId': '103171103', 'name': 'Saddam Hossain Mukta'}, {'authorId': '1478810924', 'name': 'A. K. M. Najmul Islam'}]",5.0,"{'bibtex': '@Article{Islam2021SentimentAO,\n author = {M. Islam and N. Khan and Ayon Roy and Md. Mahbubar Rahman and Saddam Hossain Mukta and A. K. M. Najmul Islam},\n journal = {2021 62nd International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)},\n pages = {1-6},\n title = {Sentiment Analysis of Bangladesh-specific COVID-19 Tweets using Deep Neural Network},\n year = {2021}\n}\n'}",,"{'pages': '1-6', 'name': '2021 62nd International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)'}",0.0,Sentiment Analysis of Bangladesh-specific COVID-19 Tweets using Deep Neural Network,2021.0
582,2ec80f52f0edf7047ef43927a4a4f1333300c677,"How many members of a group need to express their anger in order to inﬂuence a deviant group member’s behavior? In two studies, we examine whether an increase in number of angry group members affects the extent to which a deviant individual feels rejected, and we investigate downstream effects on conformity. We show that each additional angry reaction linearly increases the extent to which a deviant individual feels rejected, and that this relation is independent of the total number of majority members (Study 1). This felt rejection is then shown to lead to anti-conformity unless two conditions are met: (1) the deviant is motivated to seek reacceptance in the group, and (2) conformity is instrumental in gaining reacceptance because it is observable by the majority (Study 2). These ﬁndings show that angry reactions are likely to trigger anti-conformity in a deviant, but they are also consistent with a motivational account of conformity, in which conformity is strategic behavior aimed at gaining reacceptance from the group.",[],6.0,"{'bibtex': '@Misc{None,\n title = {UvA-DARE (Digital Academic Repository) Emotional reactions to deviance in groups: the relation between number of angry reactions, felt rejection, and conformity}\n}\n'}",,,0.0,"UvA-DARE (Digital Academic Repository) Emotional reactions to deviance in groups: the relation between number of angry reactions, felt rejection, and conformity",
583,2eda53312d39fabad8b6816a563c483ed85d37b1,"Studied the turn-taking mechanism, whereby participants manage the smooth and appropriate exchange of speaking turns in face-to-face interaction in 2 videotapes showing a therapist-patient interview and a discussion between 2 therapists. 3 basic signals were noted: (a) turn-yielding signals by the s","[{'authorId': '48949057', 'name': 'Starkey Duncan'}]",1225.0,"{'bibtex': '@Article{Duncan1972SomeSA,\n author = {Starkey Duncan},\n journal = {Journal of Personality and Social Psychology},\n pages = {283-292},\n title = {Some Signals and Rules for Taking Speaking Turns in Conversations},\n volume = {23},\n year = {1972}\n}\n'}",,"{'volume': '23', 'pages': '283-292', 'name': 'Journal of Personality and Social Psychology'}",19.0,Some Signals and Rules for Taking Speaking Turns in Conversations,1972.0
584,2f0ee4567e99f0e417550892ff864f3632415c7d,,"[{'authorId': '143687087', 'name': 'Michael Neff'}, {'authorId': '145215255', 'name': 'Yingying Wang'}, {'authorId': '39764052', 'name': 'Rob Abbott'}, {'authorId': '2234088162', 'name': 'M. Walker'}]",130.0,"{'bibtex': '@Inproceedings{Neff2010EvaluatingTE,\n author = {Michael Neff and Yingying Wang and Rob Abbott and M. Walker},\n pages = {222-235},\n title = {Evaluating the Effect of Gesture and Language on Personality Perception in Conversational Agents},\n year = {2010}\n}\n'}",,{'pages': '222-235'},40.0,Evaluating the Effect of Gesture and Language on Personality Perception in Conversational Agents,2010.0
585,2f23b87170a860f984d38ec96e09e1bfd20e3a67,"The use of Augmented Reality (AR) to achieve educational inclusion has been not deeply explored. This systematic review describes the current state of using AR as an educational technology that takes into consideration the needs of all students including those with a disability. It is done through the analysis of factors, such as the advantages of AR, its limitations, uses, challenges, its scope in the educational field, the attended population and the positive or negative effects of its use in learning scenarios that involve students with diverse educational needs. A total of 50 studies between 2008 and 2018 were analyzed through searching in three interdisciplinary databases: Scopus, Web of Science, and Springer link. For this, the methodological stages considered were planning the review, search, analysis of literature and results report. After analyzing the results, it was possible to demonstrate that the use of AR for inclusive education in the field of sciences is where more studies have been conducted. In regard to the population with disabilities, among the most representative advantages reported were the motivation, interaction and generating interest on the part of the student. At the same time, an important methodological limitation identified was the size of the sample; some investigations were done with two or three subjects, some studies Single Subject Designs were found. In terms of the population attended, the studies generally included students with different impairments (hearing, visual, motor or cognitive), minorities (ethnic, vulnerable), leaving aside other groups excluded as exceptional talents and immigrants, which could be explored in the future. Despite different problems to be addressed, few frameworks to the diversity attention in education were reported, and there was no model and methodology in inclusive education considered in the studies. Finally, from this review we have identified open issues that could give rise to new research in the subject of using AR to favor the creation of inclusive learning scenarios.","[{'authorId': '2057637451', 'name': 'Jairo Quintero'}, {'authorId': '1728461', 'name': 'S. Baldiris'}, {'authorId': '10377566', 'name': 'R. Rubira'}, {'authorId': '152740360', 'name': 'Jhoni Cerón'}, {'authorId': '120193213', 'name': 'Gloria Vélez'}]",88.0,"{'bibtex': '@Article{Quintero2019AugmentedRI,\n author = {Jairo Quintero and S. Baldiris and R. Rubira and Jhoni Cerón and Gloria Vélez},\n journal = {Frontiers in Psychology},\n title = {Augmented Reality in Educational Inclusion. A Systematic Review on the Last Decade},\n volume = {10},\n year = {2019}\n}\n'}",,"{'volume': '10', 'name': 'Frontiers in Psychology'}",94.0,Augmented Reality in Educational Inclusion. A Systematic Review on the Last Decade,2019.0
586,2f3563bf21116a2332451532d486cd195fda316f,"Reliability and validity data are presented for three orthogonal measures of temperament: trait-pleasure, trait-arousal, and trait-dominance. The KR-20 reliability coefficients for the three scales were .91, .60, and .84, respectively. Intercorrelations among the three scales were insignificant and low. Thus, the expectation that these three measures could be used as a parsimonious base for the description of temperament was confirmed. The trait-pleasure scale was correlated positively with measures of social desirability, achieving tendency, extroversion, and affiliative tendency, and was correlated negatively with measures of trait-anxiety, neuroticism, and aggression. The trait-arousal scale was not related to the social desirability scale, and was correlated positively with measures of trait-anxiety, neuroticism, and aggression. The trait-dominance scale was also free of social desirability bias. It exhibited positive correlations with measures of dominance-submissiveness, autonomy, extroversion, achieving tendency, and aggression, and negative correlations with measures of trait-anxiety and neuroticism.","[{'authorId': '144102217', 'name': 'A. Mehrabian'}]",72.0,"{'bibtex': '@Article{Mehrabian1978MeasuresOI,\n author = {A. Mehrabian},\n journal = {Educational and Psychological Measurement},\n pages = {1105 - 1117},\n title = {Measures of Individual Differences in Temperament},\n volume = {38},\n year = {1978}\n}\n'}",,"{'volume': '38', 'pages': '1105 - 1117', 'name': 'Educational and Psychological Measurement'}",22.0,Measures of Individual Differences in Temperament,1978.0
587,2f36a05985fa13ff2e555215a8049a4212eb5a34,"Rapid growth in computational modeling of emotion and cognitive-affective architectures occurred over the past 15 years. Emotion models and architectures are built to elucidate the mechanisms of emotions and enhance believability and effectiveness of synthetic agents and robots. Despite the many emotion models developed to date, a lack of consistency and clarity regarding what exactly it means to 'model emotions' persists. There are no systematic guidelines for development of computational models of emotions. This paper deconstructs the often vague term 'emotion modeling' by suggesting the view of emotion models in terms of two fundamental categories of processes: emotion generation and emotion effects. Computational tasks necessary to implement these processes are also identified. The paper addresses how computational building blocks provide a basis for the development of more systematic guidelines for affective model development. The paper concludes with a description of an affective requirements analysis and design process for developing affective computational models in agent architectures.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",133.0,"{'bibtex': '@Article{Hudlicka2011GuidelinesFD,\n author = {E. Hudlicka},\n journal = {Int. J. Synth. Emot.},\n pages = {26-79},\n title = {Guidelines for Designing Computational Models of Emotions},\n volume = {2},\n year = {2011}\n}\n'}",,"{'volume': '2', 'pages': '26-79', 'name': 'Int. J. Synth. Emot.'}",92.0,Guidelines for Designing Computational Models of Emotions,2011.0
588,2f53c93c83c322137cbf1ebd8adbfc3121a97caf,"Theory and models of supervisory control - frameworks and fragments supervisory control of anthropomorphic teleoperators for space, undersea, and other applications supervisory control in transportation, process, and other automated systems social implications of telerobotics, automation, and supervisory control.","[{'authorId': '1712194', 'name': 'T. Sheridan'}]",1729.0,"{'bibtex': '@Inproceedings{Sheridan2003TeleroboticsAA,\n author = {T. Sheridan},\n title = {Telerobotics, Automation, and Human Supervisory Control},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Telerobotics, Automation, and Human Supervisory Control",2003.0
589,2f614c29d03090dd26f91d06573f5df8bd83ae1d,"The five-factor model of personality is a hierarchical organization of personality traits in terms of five basic dimensions: Extraversion, Agreeableness, Conscientiousness, Neuroticism, and Openness to Experience. Research using both natural language adjectives and theoretically based personality questionnaires supports the comprehensiveness of the model and its applicability across observers and cultures. This article summarizes the history of the model and its supporting evidence; discusses conceptions of the nature of the factors; and outlines an agenda for theorizing about the origins and operation of the factors. We argue that the model should prove useful both for individual assessment and for the elucidation of a number of topics of interest to personality psychologists.","[{'authorId': '6206591', 'name': 'R. McCrae'}, {'authorId': '2254103', 'name': 'O. John'}]",6129.0,"{'bibtex': '@Article{McCrae1992AnIT,\n author = {R. McCrae and O. John},\n journal = {Journal of personality},\n pages = {\n          175-215\n        },\n title = {An introduction to the five-factor model and its applications.},\n volume = {60 2},\n year = {1992}\n}\n'}",,"{'volume': '60 2', 'pages': '\n          175-215\n        ', 'name': 'Journal of personality'}",141.0,An introduction to the five-factor model and its applications.,1992.0
594,2f72169bc5f6d8ddad8c8463d139233fcee17468,"The study of narrative has become so popular that the French have honored it with a term-la narratologie. Given the escalating and sophisticated literature on the subject, its English counterpart, ""narratology,"" may not be as risible as it sounds. Modern narratology combines two powerful intellectual trends: the Anglo-American inheritance of Henry James, Percy Lubbock, E. M. Forster, and Wayne Booth; and the mingling of Russian formalist (Viktor Shklovsky, Boris Eichenbaum, Roman Jakobson, and Vladimir Propp) with French structuralist approaches (Claude Levi-Strauss, Roland Barthes, Gerard Genette, and Tzvetan Todorov). It's not accidental that narratology has developed during a period in which linguistics and cinema theory have also flourished. Linguistics, of course, is one basis for the field now called semiotics-the study of all meaning systems, not only natural language. Another basis is the work of the philosopher Charles S. Peirce and his continuator, Charles W. Morris. These trees have borne elegant fruit: we read fascinating semiotic analyses of facial communication, body language, fashion, the circus, architecture, and gastronomy. The most vigorous, if controversial, branch of cinema studies, the work of Christian Metz, is also semiotically based. One of the most important observations to come out of narratology is that narrative itself is a deep structure quite independent of its medium. In other words, narrative is basically a kind of text organization, and that organization, that schema, needs to be actualized: in written words, as in stories and novels; in spoken words combined with the movements of actors imitating characters against sets which imitate","[{'authorId': '103558925', 'name': 'S. Chatman'}]",235.0,"{'bibtex': ""@Article{Chatman1980WhatNC,\n author = {S. Chatman},\n journal = {Critical Inquiry},\n pages = {121 - 140},\n title = {What Novels Can Do That Films Can't (And Vice Versa)},\n volume = {7},\n year = {1980}\n}\n""}",,"{'volume': '7', 'pages': '121 - 140', 'name': 'Critical Inquiry'}",0.0,What Novels Can Do That Films Can't (And Vice Versa),1980.0
595,2fcb759731e2d72d742b17c5e7fdf96650791674,,"[{'authorId': '145109163', 'name': 'Ari Shapiro'}]",109.0,"{'bibtex': '@Inproceedings{Shapiro2011BuildingAC,\n author = {Ari Shapiro},\n pages = {98-109},\n title = {Building a Character Animation System},\n year = {2011}\n}\n'}",,{'pages': '98-109'},12.0,Building a Character Animation System,2011.0
597,30009a385847c5af26c103adc8d10b56d9159713,"We introduce the notion of self-attachment which, based on an interdisciplinary set of concepts, proposes a new psychotherapeutic technique. The underlying ideas include findings and paradigms in developmental psychology and neuroscience, neuroplasticity and long term term potentiation, fMRI studies on human bond making and religious experience, and experiments in energy based artificial neural networks. The proposed self-attachment therapeutic technique is distinguished by its intervention to create an internal and passionate affectional bond within the individual between the “adult self”, representing the logical and cognitive faculty, and the “inner child”, representing the unregulated and undeveloped emotional circuits. The aim is to create more optimal circuits for emotional regulation. The proposed self-attachment protocols internally emulate within the individual the interactions of a good enough primary care-giver and child in order to moderate the child's arousal level, minimise its negative affects and maximize its positive affects. These interactions are assumed, in developmental neuroscience and in developmental psychology, to be the basis of secure attachment of children with their parents, which leads to an optimal regulation of neurotransmitters, hormones, and the emotional dynamics of the individual. We report on several case studies of this technique in recent years. Finally, we propose a simple mathematical model to capture the impact of self-attachment protocols using the notion of strong patterns in energy based neural networks and employ a recently developed mathematical model to examine the impact of self-attachment using emotional and cognitive neural pathways for decision making.","[{'authorId': '1694989', 'name': 'A. Edalat'}]",19.0,"{'bibtex': '@Article{Edalat2015IntroductionTS,\n author = {A. Edalat},\n journal = {2015 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Introduction to self-attachment and its neural basis},\n year = {2015}\n}\n'}",,"{'pages': '1-8', 'name': '2015 International Joint Conference on Neural Networks (IJCNN)'}",71.0,Introduction to self-attachment and its neural basis,2015.0
600,30066fd55802d295974e28bd1d763af37762fc2f,"Mimicry plays an important role in social interaction. In human communication, it is used to establish rapport and bonding both with other humans, as well as robots and virtual characters. However, little is known about the underlying factors that elicit mimicry in humans when interacting with a robot. In this work, we study the influence of embodiment on participants' ability to mimic a social character. Participants were asked to intentionally mimic the laughing behavior of the Furhat mixed embodied robotic head and a 2D virtual version of the same character. To explore the effect of embodiment, we present two novel approaches to automatically assess people's ability to mimic based solely on videos of their facial expressions. In contrast to participants' self-assessment, the analysis of video recordings suggests a better ability to mimic when people interact with the 2D embodiment.","[{'authorId': '2710492', 'name': 'Maike Paetzel'}, {'authorId': '1958033', 'name': 'G. Varni'}, {'authorId': '32595768', 'name': 'Isabelle Hupont Torres'}, {'authorId': '1680828', 'name': 'M. Chetouani'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",3.0,"{'bibtex': '@Article{Paetzel2017InvestigatingTI,\n author = {Maike Paetzel and G. Varni and Isabelle Hupont Torres and M. Chetouani and Christopher E. Peters and Ginevra Castellano},\n journal = {2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {579-586},\n title = {Investigating the influence of embodiment on facial mimicry in HRI using computer vision-based measures},\n year = {2017}\n}\n'}",,"{'pages': '579-586', 'name': '2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)'}",32.0,Investigating the influence of embodiment on facial mimicry in HRI using computer vision-based measures,2017.0
601,30322f795de22fc6af6ae169549608f7cf816f44,"Prime pictures of emotional scenes appeared in parafoveal vision, followed by probe pictures either congruent or incongruent in affective valence. Participants responded whether the probe was pleasant or unpleasant (or whether it portrayed people or animals). Shorter latencies for congruent than for incongruent prime-probe pairs revealed affective priming. This occurred even when visual attention was focused on a concurrent verbal task and when foveal gaze-contingent masking prevented overt attention to the primes but only if these had been preexposed and appeared in the left visual field. The preexposure and laterality patterns were different for affective priming and semantic category priming. Affective priming was independent of the nature of the task (i.e., affective or category judgment), whereas semantic priming was not. The authors conclude that affective processing occurs without overt attention--although it is dependent on resources available for covert attention--and that prior experience of the stimulus is required and right-hemisphere dominance is involved.","[{'authorId': '144866673', 'name': 'M. Calvo'}, {'authorId': '2036051', 'name': 'L. Nummenmaa'}]",99.0,"{'bibtex': '@Article{Calvo2007ProcessingOU,\n author = {M. Calvo and L. Nummenmaa},\n journal = {Journal of experimental psychology. General},\n pages = {\n          347-69\n        },\n title = {Processing of unattended emotional visual scenes.},\n volume = {136 3},\n year = {2007}\n}\n'}",,"{'volume': '136 3', 'pages': '\n          347-69\n        ', 'name': 'Journal of experimental psychology. General'}",76.0,Processing of unattended emotional visual scenes.,2007.0
602,303445111ffa70f036d309d318d1b3e2a7ec43dd,"Negotiation Strategies with Incongruent Facial Expressions of Emotion Cause Cardiovascular Threat Peter Khooshabeh (khooshabeh@ict.usc.edu) 1, 3 Celso de Melo (demelo@usc.edu) 2 Brooks Volkman (volkman@psych.ucsb.edu) 1 Jonathan Gratch (gratch@ict.usc.edu) 3 Jim Blascovich (blascovi@psych.ucsb.edu) 1 Peter J. Carnevale (carnevale@usc.edu) 2 Department of Psychological and Brain Sciences, University of California, Santa Barbara, CA 93106 USA Marshall School of Business, University of Southern California, Los Angeles, CA 90089 Institute for Creative Technologies, University of Southern California, Los Angeles, CA 90094 affectively neutral; that is, they are associated with interactants’ positive or negative emotional states. Clearly, negotiations represent motivated performance situations to interested partners. And, experimental negotiation tasks are no exception, including those involving real human players (Van Kleef, De Dreu, & Manstead, 2004) and digital agents (i.e., player representations driven by computer algorithms, de Melo, Carnevale, & Gratch, 2011). The current work examines individuals’ motivational responses, using physiological indexes, to emotionally expressive virtual characters in a multi-issue negotiation task. Specifically, we focus on the question of how situational context affects emotion perception from facial expressions. In person-to-agent negotiation tasks, experimenters often insert communicative cues such as agent facial expressions intended to strategically manipulate user’s emotions. Agents that show emotion have now been used in several domains such as education, entertainment, training, therapy and commerce (for a review see Beale & Creed, 2009). In a multi-issue negotiation task, de Melo and colleagues (2011) reported that participants made more concessions to a virtual human that displayed an angry facial expression compared to a happy facial expression. Most research on the effects of virtual characters’ emotional facial expressions has relied on subjective responses from participants (e.g., Beale & Creed, 2009). However, given the evidence that emotion is processed via non-conscious pathways, perhaps more so than conscious pathways (Tamietto & De Gelder, 2010), validated physiological measures related to affect should provide confirmation of the operation of non-conscious emotional processes involved in motivated performance tasks such as negotiation (Blascovich & Mendes, 2010). Abstract Affect is important in motivated performance situations such as negotiation. Longstanding theories of emotion suggest that facial expressions provide enough information to perceive another person’s internal affective state. Alternatively, the contextual emotion hypothesis posits that situational factors bias the perception of emotion in others’ facial displays. This hypothesis predicts that individuals will have different perceptions of the same facial expression depending upon the context in which the expression is displayed. In this study, cardiovascular indexes of motivational states (i.e., challenge vs. threat) were recorded while players engaged in a multi- issue negotiation where the opposing negotiator (confederate) displayed emotional facial expressions (angry vs. happy); the confederate’s negotiation strategy (cooperative vs. competitive) was factorially crossed with his facial expression. During the game, participants’ eye fixations and cardiovascular responses, indexing task engagement and challenge/threat motivation, were recorded. Results indicated that participants playing confederates with incongruent facial expressions (e.g., cooperative strategy, angry face) exhibited a greater threat response, which arises due to increased uncertainty. Eye fixations also suggest that participants look at the face more in order to acquire information to reconcile their uncertainty in the incongruent condition. Taken together, these results suggest that context matters in the perception of emotion. Keywords: facial expressions, negotiation, context in emotion Introduction Negotiation is relatively common in personal and professional settings. A child might ask a parent whether she can leave the dinner table. The parent might sternly command the child to finish her vegetables and the child could make a counter offer to finish the peas but not the broccoli. This could ensue into a strategic and emotionally charged social interaction. Emotion is an important human factor in motivated performance situations (i.e., those that are self-relevant and therefore task engaging and require instrumental cognitive responses; Blascovich, 2008). Such interactions are rarely Psychophysiological Measurement of Motivational States Psychophysiological research is now a well-established technique to infer peoples’ affective reactions to various situations (Blascovich, Vanman, Mendes, & Dickerson,","[{'authorId': '2635945', 'name': 'P. Khooshabeh'}, {'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '3389003', 'name': 'Brooks Volkman'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '48755211', 'name': 'P. Carnevale'}]",11.0,"{'bibtex': '@Inproceedings{Khooshabeh2013NegotiationSW,\n author = {P. Khooshabeh and C. D. Melo and Brooks Volkman and J. Gratch and J. Blascovich and P. Carnevale},\n title = {Negotiation Strategies with Incongruent Facial Expressions of Emotion Cause Cardiovascular Threat},\n volume = {35},\n year = {2013}\n}\n'}",,"{'volume': '35', 'name': ''}",23.0,Negotiation Strategies with Incongruent Facial Expressions of Emotion Cause Cardiovascular Threat,2013.0
603,303f91aefc824a1c765b95c9b8ecb0e34c75aa9e,In this article we describe our approach to generating convincing and empathetic facial animation. Our goal is to develop a robust facial animation platform that is usable and can be easily extended. We also want to facilitate the integration of research in the area and to directly incorporate the characters in interactive applications such as embodied conversational agents and games. We have developed a framework capable of easily animating MPEG-4 parameterized faces through high-level description of facial actions and behaviors. The animations can be generated in real time for interactive applications. We present some case studies that integrate computer vision techniques in order to provide interaction between the user and a character that interacts with different facial actions according to events in the application.,"[{'authorId': '2786977', 'name': 'R. Queiroz'}, {'authorId': '2111362866', 'name': 'Marcelo Cohen'}, {'authorId': '1679516', 'name': 'S. Musse'}]",21.0,"{'bibtex': '@Article{Queiroz2009AnEF,\n author = {R. Queiroz and Marcelo Cohen and S. Musse},\n journal = {Comput. Entertain.},\n pages = {58:1-58:20},\n title = {An extensible framework for interactive facial animation with facial expressions, lip synchronization and eye behavior},\n volume = {7},\n year = {2009}\n}\n'}",,"{'volume': '7', 'pages': '58:1-58:20', 'name': 'Comput. Entertain.'}",39.0,"An extensible framework for interactive facial animation with facial expressions, lip synchronization and eye behavior",2009.0
604,305a0ff88a70b49f745e95bd5ef004a33f5afc4a,"determining not only that they pertain to emotion, but to which emotion . . . Appraisal is not always automatic. Sometimes the evaluation of what is happening is slow, deliberate and conscious. With such a more extended appraisal there may be some autonomic arousal, but perhaps not of a kind which is differentiated. The person could be said to be aroused or alerted, but no specific emotion is operative. Cognition plays the important role in determining what will transpire. During such extended appraisal the evaluation may match to the selective filters of the automatic appraiser . . . . It need not be, however; the experience may be diffuse rather than specific to one emotion” (pp. 58—59).","[{'authorId': '2193978', 'name': 'T. Dalgleish'}]",1950.0,"{'bibtex': '@Inproceedings{Dalgleish2004BasicE,\n author = {T. Dalgleish},\n title = {Basic Emotions},\n year = {2004}\n}\n'}",,,52.0,Basic Emotions,2004.0
606,3074530bec83310492592052f5adda383c0b7baf,Exciting research in the design of automated negotiators is making great progress.,"[{'authorId': '3042933', 'name': 'R. Lin'}, {'authorId': '1691597', 'name': 'Sarit Kraus'}]",161.0,"{'bibtex': '@Article{Lin2010CanAA,\n author = {R. Lin and Sarit Kraus},\n journal = {Commun. ACM},\n pages = {78-88},\n title = {Can automated agents proficiently negotiate with humans?},\n volume = {53},\n year = {2010}\n}\n'}",,"{'volume': '53', 'pages': '78-88', 'name': 'Commun. ACM'}",49.0,Can automated agents proficiently negotiate with humans?,2010.0
607,3077017ccc45855de3c9426158270d334b05a23d,"In literary and drama criticism, emotions, and moral emotions in particular, have been pointed out as one of characterizing features of stories. In this paper, we propose to model story characters as value-based emotional agents, who appraise their own and others' actions based on their desires and values, and feel the appropriate moral emotions in response to narrative situations that challenge their goals and values. 
 
In order to validate the appropriateness of the agent model for narrative characters, we ran an experiment with human participants aimed at comparing their expectations about characters' emotions with the predictions of the value-based model of emotional agent. The results of the experiment show that the participants' expectations meet the predictions of the model.","[{'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '144411873', 'name': 'R. Damiano'}]",6.0,"{'bibtex': '@Inproceedings{Battaglino2014ACM,\n author = {C. Battaglino and R. Damiano},\n pages = {24-41},\n title = {A Character Model with Moral Emotions: Preliminary Evaluation},\n year = {2014}\n}\n'}",,{'pages': '24-41'},45.0,A Character Model with Moral Emotions: Preliminary Evaluation,2014.0
608,308b16bec9f17784fed039ddf4f86a856b36a768,"This paper describes the development and testing of the Temple Presence Inventory. The TPI questionnaire is a multidimensional, literature-based measure of telepresence that has demonstrated sensitivity to media form and content in studies discussed here.","[{'authorId': '37189009', 'name': 'M. Lombard'}, {'authorId': '3332169', 'name': 'Theresa Ditton'}, {'authorId': '33671740', 'name': 'L. Weinstein'}, {'authorId': '2095050507', 'name': 'Temple'}]",217.0,"{'bibtex': '@Inproceedings{Lombard2009MeasuringPT,\n author = {M. Lombard and Theresa Ditton and L. Weinstein and Temple},\n title = {Measuring Presence: The Temple Presence Inventory},\n year = {2009}\n}\n'}",,,81.0,Measuring Presence: The Temple Presence Inventory,2009.0
609,30e7e25061b7ccd9a625548dd6836afcff85043b,"Abstract The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (x i , y i ), i = 1, …, n, in which the fitted value at z k is the value of a polynomial fit to the data using weighted least squares, where the weight for (x i , y i ) is large if x i is close to x k and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.","[{'authorId': '35157864', 'name': 'W. Cleveland'}]",10314.0,"{'bibtex': '@Article{Cleveland1979RobustLW,\n author = {W. Cleveland},\n journal = {Journal of the American Statistical Association},\n pages = {829-836},\n title = {Robust Locally Weighted Regression and Smoothing Scatterplots},\n volume = {74},\n year = {1979}\n}\n'}",,"{'volume': '74', 'pages': '829-836', 'name': 'Journal of the American Statistical Association'}",17.0,Robust Locally Weighted Regression and Smoothing Scatterplots,1979.0
610,30ef8f7cc73fac8c678fc8e8925f316f78079b15,"Virtual agents have been investigated as an educational tool for use with children on the autistic spectrum with positive results being gained for language skills with the use of autonomous agents and social skills with human-controlled agents. This project combines these ideas to investigate the utility of autonomous agents for teaching social skills. The virtual agent used in this project, known as the Thinking Head, has an ability to realistically portray facial expressions that lends it to this task. Two prototype modules were developed for this agent platform, one teaching basic conversation skills and the other dealing with bullying. In a pre-test-post-test evaluation, a group of children with autism who were exposed to the training modules obtained significantly higher post-test scores on their knowledge of these two topics. In addition, responses to a post-training survey indicated that participants found the virtual tutor enjoyable and useful.","[{'authorId': '26530439', 'name': 'Marissa Milne'}, {'authorId': '1776457', 'name': 'M. Luerssen'}, {'authorId': '145111765', 'name': 'T. Lewis'}, {'authorId': '2323617', 'name': 'Richard Leibbrandt'}, {'authorId': '144871539', 'name': 'D. Powers'}]",53.0,"{'bibtex': '@Article{Milne2010DevelopmentOA,\n author = {Marissa Milne and M. Luerssen and T. Lewis and Richard Leibbrandt and D. Powers},\n journal = {The 2010 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-9},\n title = {Development of a virtual agent based social tutor for children with autism spectrum disorders},\n year = {2010}\n}\n'}",,"{'pages': '1-9', 'name': 'The 2010 International Joint Conference on Neural Networks (IJCNN)'}",20.0,Development of a virtual agent based social tutor for children with autism spectrum disorders,2010.0
611,30ff7845c8b0de24f8e494b815f3041feeb508d4,"We present 3D Constrained Local Model (CLM-Z) for robust facial feature tracking under varying pose. Our approach integrates both depth and intensity information in a common framework. We show the benefit of our CLM-Z method in both accuracy and convergence rates over regular CLM formulation through experiments on publicly available datasets. Additionally, we demonstrate a way to combine a rigid head pose tracker with CLM-Z that benefits rigid head tracking. We show better performance than the current state-of-the-art approaches in head pose tracking with our extension of the generalised adaptive view-based appearance model (GAVAM).","[{'authorId': '1756344', 'name': 'T. Baltrušaitis'}, {'authorId': '2149814967', 'name': 'P. Robinson'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",315.0,"{'bibtex': '@Article{Baltrušaitis20123DCL,\n author = {T. Baltrušaitis and P. Robinson and Louis-Philippe Morency},\n journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {2610-2617},\n title = {3D Constrained Local Model for rigid and non-rigid facial tracking},\n year = {2012}\n}\n'}",,"{'pages': '2610-2617', 'name': '2012 IEEE Conference on Computer Vision and Pattern Recognition'}",33.0,3D Constrained Local Model for rigid and non-rigid facial tracking,2012.0
612,30ffee716e80cebab7f169b3a4fd8ce307417b7e,"The coronavirus disease 2019 (COVID-19) pandemic will present an unprecedented stressor to patients and health care systems across the globe. Because there is currently no vaccine or treatment for the underlying infection, current health efforts are focused on providing prevention and screening, maintaining continuity of treatment for other chronic conditions, and ensuring access to appropriately intensive services for those with the most severe symptoms.1 Disasters disproportionately affect poor and vulnerable populations, and patients with serious mental illness may be among the hardest hit. High rates of smoking in this population may raise the risk of infection and confer a worse prognosis among those who develop the illness.2 Residential instability and homelessness can raise the risk of infection and make it harder to identify, follow up, and treat those who are infected.3 Individuals with serious mental illnesses who are employed may have challenges taking time off from work and may lack sufficient insurance coverage to cover testing or treatment. Small social networks may limit opportunities to obtain support from friends and family members should individuals with serious mental illness become ill. Taken together, these factors may lead to elevated infection rates and worse prognoses in this population. What strategies are available to mitigate the outcome of this epidemic among patients with serious mental illness? Federal preparedness policies developed in the wake of complex disasters have increasingly embraced the notion of whole community preparedness, which supports building and supporting structures at multiple levels to prepare and respond, particularly for vulnerable populations.4 Within the public mental health care system, this includes engagement with mental health service users, clinicians, and federal and state policies.","[{'authorId': '5265093', 'name': 'B. Druss'}]",371.0,"{'bibtex': '@Article{Druss2020AddressingTC,\n author = {B. Druss},\n journal = {JAMA psychiatry},\n title = {Addressing the COVID-19 Pandemic in Populations With Serious Mental Illness.},\n year = {2020}\n}\n'}",,{'name': 'JAMA psychiatry'},5.0,Addressing the COVID-19 Pandemic in Populations With Serious Mental Illness.,2020.0
614,3108f96f80d129036f53684344f4058257b37c4b,"We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The dataset is available on http://yanran.li/dailydialog","[{'authorId': '3305402', 'name': 'Yanran Li'}, {'authorId': '2087042666', 'name': 'Hui Su'}, {'authorId': '2562211', 'name': 'Xiaoyu Shen'}, {'authorId': '50135338', 'name': 'Wenjie Li'}, {'authorId': '2314396', 'name': 'Ziqiang Cao'}, {'authorId': '2944201', 'name': 'Shuzi Niu'}]",953.0,"{'bibtex': '@Article{Li2017DailyDialogAM,\n author = {Yanran Li and Hui Su and Xiaoyu Shen and Wenjie Li and Ziqiang Cao and Shuzi Niu},\n journal = {ArXiv},\n title = {DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset},\n volume = {abs/1710.03957},\n year = {2017}\n}\n'}",,"{'volume': 'abs/1710.03957', 'name': 'ArXiv'}",32.0,DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset,2017.0
615,31295108ff1159f5a6ef4ab0eff56d1346af351d,,[],1497.0,"{'bibtex': '@Misc{None,\n title = {Opinion TRENDS in Cognitive Sciences Vol.10 No.10 The empathic brain: how, when and why?}\n}\n'}",,,0.0,"Opinion TRENDS in Cognitive Sciences Vol.10 No.10 The empathic brain: how, when and why?",
616,3159dc207966290c624d3868cf878a83ff8fb53e,"Abstract The reduction of aversive emotions by a conspecific’s presence—called social buffering—is a universal phenomenon in the mammalian world and a powerful form of human social emotion regulation. Animal and human studies on neural pathways underlying social buffering typically examined physiological reactions or regional brain activations. However, direct links between emotional and social stimuli, distinct neural processes and behavioural outcomes are still missing. Using data of 27 female participants, the current study delineated a large-scale process model of social buffering’s neural underpinnings, connecting changes in neural activity to emotional behaviour by means of voxel-wise multilevel mediation analysis. Our results confirmed that three processes underlie human social buffering: (i) social support-related reduction of activity in the orbitofrontal cortex, ventromedial and dorsolateral prefrontal cortices, anterior and mid-cingulate; (ii) downregulation of aversive emotion-induced brain activity in the superficial cortex-like amygdala and mediodorsal thalamus; and (iii) downregulation of reported aversive feelings. Results of the current study provide evidence for a distinct neural process model of aversive emotion regulation in humans by social buffering.","[{'authorId': '7775987', 'name': 'Satja Mulej Bratec'}, {'authorId': '2067212278', 'name': 'Teresa Bertram'}, {'authorId': '1751526633', 'name': 'G. Starke'}, {'authorId': '39741194', 'name': 'F. Brandl'}, {'authorId': '2141704', 'name': 'Xiyao Xie'}, {'authorId': '14505378', 'name': 'C. Sorg'}]",8.0,"{'bibtex': '@Article{Bratec2020YourPS,\n author = {Satja Mulej Bratec and Teresa Bertram and G. Starke and F. Brandl and Xiyao Xie and C. Sorg},\n journal = {Social Cognitive and Affective Neuroscience},\n pages = {561 - 570},\n title = {Your presence soothes me: a neural process model of aversive emotion regulation via social buffering},\n volume = {15},\n year = {2020}\n}\n'}",,"{'volume': '15', 'pages': '561 - 570', 'name': 'Social Cognitive and Affective Neuroscience'}",57.0,Your presence soothes me: a neural process model of aversive emotion regulation via social buffering,2020.0
617,315a63fb7fc3d56feb4d9e2d32fc631dc62d909d,"ABSTRACT Background: Apathy, characterized by diminished motivation, is a highly prevalent neuropsychiatric symptom in dementia. However, there is a substantial knowledge gap with regard to prevalence rates, neurobiological underpinnings, and effective treatments for apathy in pre-dementia states, including mild cognitive impairment (MCI) and mild behavioral impairment (MBI). Methods: We conducted a comprehensive literature search using MEDLINE, Embase, and PsycINFO databases to identify available research on apathy in prodromal dementia. Results: Apathy has consistently been detected in individuals with MCI with varying prevalence rates, and only recently has literature discussed the prevalence of apathy in MBI. Few pharmacological treatments have been utilized for apathy, with galantamine and risperidone showing mild reductions in apathetic behaviors. Non-pharmacological interventions in prodromal dementia are beginning to be explored and show promise, but few studies have replicated those results. Discussion: More comprehensive guidelines for diagnosing apathy and further research investigating neurobiological mechanisms of apathy in MCI and MBI are required in order to effectively treat apathetic patients in prodromal dementia.","[{'authorId': '22853228', 'name': 'Chelsea Sherman'}, {'authorId': '2107890195', 'name': 'Celina S. Liu'}, {'authorId': '144528209', 'name': 'N. Herrmann'}, {'authorId': '4797664', 'name': 'K. Lanctôt'}]",40.0,"{'bibtex': '@Article{Sherman2017PrevalenceNA,\n author = {Chelsea Sherman and Celina S. Liu and N. Herrmann and K. Lanctôt},\n journal = {International Psychogeriatrics},\n pages = {177 - 184},\n title = {Prevalence, neurobiology, and treatments for apathy in prodromal dementia},\n volume = {30},\n year = {2017}\n}\n'}",,"{'volume': '30', 'pages': '177 - 184', 'name': 'International Psychogeriatrics'}",64.0,"Prevalence, neurobiology, and treatments for apathy in prodromal dementia",2017.0
618,31677bcb8a8b3e17373008cc906eb7f07b4d3c3d,"A shared visual workspace allows multiple people to see similar views of objects and environments. Prior empirical literature demonstrates that visual information helps collaborators understand the current state of their task and enables them to communicate and ground their conversations efficiently. We present an empirical study that demonstrates how action replaces explicit verbal instruction in a shared visual workspace. Pairs performed a referential communication task with and without a shared visual space. A detailed sequential analysis of the communicative content reveals that pairs with a shared workspace were less likely to explicitly verify their actions with speech. Rather, they relied on visual information to provide the necessary communicative and coordinative cues.","[{'authorId': '2497393', 'name': 'Darren Gergle'}, {'authorId': '1702853', 'name': 'R. Kraut'}, {'authorId': '1692772', 'name': 'Susan R. Fussell'}]",152.0,"{'bibtex': '@Article{Gergle2004ActionAL,\n author = {Darren Gergle and R. Kraut and Susan R. Fussell},\n journal = {Proceedings of the 2004 ACM conference on Computer supported cooperative work},\n title = {Action as language in a shared visual space},\n year = {2004}\n}\n'}",,{'name': 'Proceedings of the 2004 ACM conference on Computer supported cooperative work'},40.0,Action as language in a shared visual space,2004.0
619,317d38b9bf4c5aa2e204f43194a85d901e8ce191,,"[{'authorId': '1708171', 'name': 'J. Sansonnet'}, {'authorId': '144869828', 'name': 'D. Leray'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}]",43.0,"{'bibtex': '@Inproceedings{Sansonnet2006ArchitectureOA,\n author = {J. Sansonnet and D. Leray and Jean-Claude Martin},\n pages = {145-156},\n title = {Architecture of a Framework for Generic Assisting Conversational Agents},\n year = {2006}\n}\n'}",,{'pages': '145-156'},26.0,Architecture of a Framework for Generic Assisting Conversational Agents,2006.0
620,31a2ea84d34c60130f8f2291b7650234e482933d,"(from the chapter) discuss the ways in which the sociocultural environment can be expected to influence the emotional processes, the roles and functions of these processes in social interaction, and the influences of the sociocultural environment upon those roles and functions / discuss the modes of influence on emotions of the immediate context of social interaction in which emotions arise and of the values, norms, and cognitive customs prevalent in a given culture / briefly outline the conception of emotions that guides our analysis","[{'authorId': '49584958', 'name': 'N. Frijda'}, {'authorId': '5935785', 'name': 'B. Mesquita'}]",593.0,"{'bibtex': '@Inproceedings{Frijda1994TheSR,\n author = {N. Frijda and B. Mesquita},\n pages = {51-87},\n title = {The social roles and functions of emotions},\n year = {1994}\n}\n'}",,"{'volume': '', 'pages': '51-87', 'name': ''}",43.0,The social roles and functions of emotions,1994.0
621,31b144791f19e5bef94f73e86c70e415acd22ce4,,"[{'authorId': '50075430', 'name': 'D. Cicchetti'}]",5509.0,"{'bibtex': '@Article{Cicchetti1993EmotionAA,\n author = {D. Cicchetti},\n journal = {American Journal of Psychiatry},\n title = {Emotion and Adaptation},\n volume = {150},\n year = {1993}\n}\n'}",,"{'volume': '150', 'name': 'American Journal of Psychiatry'}",0.0,Emotion and Adaptation,1993.0
622,31b71374346cf3414a7ca11e2d36158a1b67f52c,"This article builds on insights into how humans cope with emotion to guide the design of virtual humans. Although coping is increasingly viewed in the psychological literature as having a central role in human adaptive behavior, it has been largely ignored in computational models of emotion. In this paper, we show how psychological research on the interplay between human emotion, cognition and coping behavior can serve as a central organizing principle for the behavior of human-like autonomous agents. We present a detailed domain-independent model of coping based on this framework that significantly extends our previous work. We argue that this perspective provides novel insights into realizing adaptive behavior.","[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",166.0,"{'bibtex': ""@Inproceedings{Marsella2003ModelingCB,\n author = {S. Marsella and J. Gratch},\n pages = {313-320},\n title = {Modeling coping behavior in virtual humans: don't worry, be happy},\n year = {2003}\n}\n""}",,{'pages': '313-320'},30.0,"Modeling coping behavior in virtual humans: don't worry, be happy",2003.0
624,31d7f4f2a9f48a74f22fe2fa80822bf70231ccc7,"Serotonin (5-hydroxytryptamine, 5-HT) is an important modulatory neurotransmitter and functions as a key neurodevelopmental signal in the mammalian brain. 5-HT plays a prominent role in regulating various types of psychological processes and functions, including mood and emotion, particularly anxiety, but also in regulating social behavior. Consequently, the 5-HT system is implicated in various neuropsychiatric disorders, such as anxiety disorders and depression or autism spectrum disorders (ASD), with selective 5-HT reuptake inhibitors being the frontline medication. Mice and rats perceive and emit ultrasonic vocalizations (USV). It is widely believed that the various distinct USV types reflect the animal’s affective state, such as anxiety or pleasure. Furthermore, they serve communicative functions, for instance, as alarm calls or social contact calls. Manipulations targeting the 5-HT system alter affective ultrasonic communication in rodents throughout life, probably because of its important role in regulating anxiety and social behavior. Ample evidence indicates the involvement of the 5-HT system in modulating isolation-induced USV in pups. Later in life, the 5-HT system plays a strong modulatory role in the emission of aversive 22-kHz USV in rats. So far, little is known about the role of 5-HT in the production of interaction-induced USV in mice and appetitive 50-kHz USV in rats, although recent findings also suggest a modulatory effect of the 5-HT system. Assessment of rodent USV is a valuable method to investigate mood and emotion, and to enhance our understanding of, and develop novel pharmacological therapies for neuropsychiatric disorders, such as anxiety disorders and depression or ASD.","[{'authorId': '5765908', 'name': 'M. Wöhr'}, {'authorId': '81334342', 'name': 'M. V. van Gaalen'}, {'authorId': '20679477', 'name': 'R. Schwarting'}]",30.0,"{'bibtex': '@Article{Wöhr2015AffectiveCI,\n author = {M. Wöhr and M. V. van Gaalen and R. Schwarting},\n journal = {Behavioural Pharmacology},\n pages = {506–521},\n title = {Affective communication in rodents: serotonin and its modulating role in ultrasonic vocalizations},\n volume = {26},\n year = {2015}\n}\n'}",,"{'volume': '26', 'pages': '506–521', 'name': 'Behavioural Pharmacology'}",189.0,Affective communication in rodents: serotonin and its modulating role in ultrasonic vocalizations,2015.0
625,31e3a593195fb8d1b7c146353b77e27035dbb3fb,"Embodied conversational agents become more and more realistic concerning their conversational and their nonverbal behaviors. But if the information conveyed nonverbally exhibits clues that are not consistent with the verbal part of an agent’s action, how will the user react to such a discrepancy? Masking ones real emotions with a smile is a naturally occuring example of such a discrepancy. But such masks are often deﬁcient and thus subtle clues of lying and deceiving manifest themselves in facial expressions. The questions is how users will react to these clues if they are conveyed by an agent. Will they render an application unattractive or on the contrary more human-like? In this paper, we examine such facial clues to deception and present the results of two empirical studies: i.) lies in monologues by a talking head presenting movies, ii.) lies in an interactive game of dice.","[{'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '2262115677', 'name': 'Elisabeth Andr´e'}]",88.0,"{'bibtex': '@Inproceedings{Rehm2004CatchMI,\n author = {M. Rehm and Elisabeth Andr´e},\n title = {Catch Me If You Can — Exploring Lying Agents in Social Settings},\n year = {2004}\n}\n'}",,,31.0,Catch Me If You Can — Exploring Lying Agents in Social Settings,2004.0
626,31edae2a7810324c82a53a86904090406d5505e9,"In this paper we integrate claims and ® ndings concerning the social functions of emotions at the individual, dyadic, group, and cultural levels of analysis. Across levels of analysis theorists assume that emotions solve problems important to social relationships in the context of ongoing interactions. Theorists diverge, however, in their assumptions about the origins, de® ning characteristics, and consequences of emotions, and in their preferred forms of data. We illustrate the differences and compatibilities among these levels of analysis for the speci® c case of embarrassment. We close by suggesting research strategies that incorporate a social-functional perspective.","[{'authorId': '3990536', 'name': 'D. Keltner'}, {'authorId': '2480714', 'name': 'J. Haidt'}]",1563.0,"{'bibtex': '@Article{Keltner1999SocialFO,\n author = {D. Keltner and J. Haidt},\n journal = {Cognition & Emotion},\n pages = {505-521},\n title = {Social Functions of Emotions at Four Levels of Analysis},\n volume = {13},\n year = {1999}\n}\n'}",,"{'volume': '13', 'pages': '505-521', 'name': 'Cognition & Emotion'}",90.0,Social Functions of Emotions at Four Levels of Analysis,1999.0
627,322ac47267c4e056298d8acfc88dc18a95114a26,,"[{'authorId': '2123508', 'name': 'F. Serón'}, {'authorId': '3019186', 'name': 'Carlos Bobed'}]",16.0,"{'bibtex': '@Article{Serón2014VOXSA,\n author = {F. Serón and Carlos Bobed},\n journal = {Multimedia Tools and Applications},\n pages = {381-404},\n title = {VOX system: a semantic embodied conversational agent exploiting linked data},\n volume = {75},\n year = {2014}\n}\n'}",,"{'volume': '75', 'pages': '381-404', 'name': 'Multimedia Tools and Applications'}",54.0,VOX system: a semantic embodied conversational agent exploiting linked data,2014.0
628,324199805cc345460714dd33b11e5bc5abcc0c70,"Elementary school classrooms are emotionally stressful environments, for both students and teachers. Successful teachers use strategies that regulate students' emotions and behaviors while also controlling their own emotions (stress, nervousness). To prepare teachers for the challenges of teaching, teacher training should include emotional and behavioral management strategies. Virtual Training Environments (VTEs) are effective at providing experiences and increasing learning in many domains. Creating VTEs for teachers can improve student learning and teacher retention. We introduce our current research aimed at integrating emotionally-intelligent virtual students within a 3D classroom training system. In our simulation, virtual students' emotional states will be determined from an appraisal process of actions taken by the teacher trainee in the virtual classroom. Virtual students will then display the appropriate non-verbal behaviors and react to the teacher accordingly. We present the first steps required to implement our proposed architecture which are based on appraisal theory of emotions and emotion regulation theory.","[{'authorId': '17849026', 'name': 'A. Delamarre'}, {'authorId': '1753287', 'name': 'Cédric Buche'}, {'authorId': '1779199', 'name': 'C. Lisetti'}]",2.0,"{'bibtex': '@Article{Delamarre2019AIMERAI,\n author = {A. Delamarre and Cédric Buche and C. Lisetti},\n journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},\n title = {AIMER: Appraisal Interpersonal Model of Emotion Regulation, Affective Virtual Students to Support Teachers Training},\n year = {2019}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents'},37.0,"AIMER: Appraisal Interpersonal Model of Emotion Regulation, Affective Virtual Students to Support Teachers Training",2019.0
629,326a37c760c665dec07420f5eb6dca681f012db1,"Affect sensitivity is of the utmost importance for a robot companion to be able to display socially intelligent behaviour, a key requirement for sustaining long-term interactions with humans. This paper explores a naturalistic scenario in which children play chess with the iCat, a robot companion. A person-independent, Bayesian approach to detect the user's engagement with the iCat robot is presented. Our framework models both causes and effects of engagement: features related to the user's non-verbal behaviour, the task and the companion's affective reactions are identified to predict the children's level of engagement. An experiment was carried out to train and validate our model. Results show that our approach based on multimodal integration of task and social interaction-based features outperforms those based solely on non-verbal behaviour or contextual information (94.79 % vs. 93.75 % and 78.13 %).","[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '2803283', 'name': 'P. McOwan'}]",173.0,"{'bibtex': '@Inproceedings{Castellano2009DetectingUE,\n author = {Ginevra Castellano and André Pereira and Iolanda Leite and Ana Paiva and P. McOwan},\n pages = {119-126},\n title = {Detecting user engagement with a robot companion using task and social interaction-based features},\n year = {2009}\n}\n'}",,{'pages': '119-126'},21.0,Detecting user engagement with a robot companion using task and social interaction-based features,2009.0
630,32ba17ac7b02627b7f0d07213b7e0a00822206c1,"Much discussion of emotions and related topics is riddled with confusion because different authors use the key expressions with different meanings. Some confuse the concept of ""emotion"" with the more general concept of ""affect"", which covers other things besides emotions, including moods, attitudes, desires, preferences, intentions, dislikes, etc. Moreover researchers have different goals: some are concerned with understanding natural phenomena, while others are more concerned with producing useful artifacts, e.g. synthetic entertainment agents, sympathetic machine interfaces, and the like. We address this confusion by showing how ""architecture-based"" concepts can extend and refine our pre-theoretical concepts in ways that make them more useful both for expressing scientific questions and theories, and for specifying engineering objectives. An implication is that different information-processing architectures support different classes of emotions, different classes of consciousness, different varieties of perception, and so on. We start with high level concepts applicable to a wide variety of types of natural and artificial systems, including very simple organisms, namely concepts such as ""need"", ""function"", ""information-user"", ""affect"", ""information-processing architecture"". For more complex architectures, we offer the CogAff schema as a generic framework which distinguishes types of components that may be in a architecture, operating concurrently with different functional roles. We also sketch H-Cogaff, a richly-featured special case of CogAff, conjectured as a type of architecture that can explain or replicate human mental phenomena. We show how the concepts that are definable in terms of such architectures can clarify and enrich research on human emotions. If successful for the purposes of science and philosophy the architecture is also likely to be useful for engineering purposes, though many engineering goals can be achieved using shallow concepts and shallow theories, e.g., producing ""believable"" agents for computer entertainments. The more human-like robot emotions will emerge, as they do in humans, from the interactions of many mechanisms serving different purposes, not from a particular, dedicated ""emotion mechanism"".","[{'authorId': '145788442', 'name': 'A. Sloman'}, {'authorId': '1693259', 'name': 'Ron Chrisley'}, {'authorId': '1793014', 'name': 'Matthias Scheutz'}]",123.0,"{'bibtex': '@Inproceedings{Sloman2005TheAB,\n author = {A. Sloman and Ron Chrisley and Matthias Scheutz},\n pages = {203-244},\n title = {The Architectural Basis of Affective States and Processes},\n year = {2005}\n}\n'}",,{'pages': '203-244'},67.0,The Architectural Basis of Affective States and Processes,2005.0
631,32c99b4728b81253a250f5711deb0ee47645ac18,"A great deal of research has been performed recently on robots that feature functions for communicating with humans in daily life, i.e., communication robots. We consider it important to develop methods to measure humans’ attitudes and emotions that may prevent them from interaction with communication robots, as indices to study short-term and long-term interaction between humans and communication robots. This study is aimed at exploring the influence of negative attitudes toward robots, focusing on applications of communication robots to daily-life services. First, a scale of negative attitudes toward robots consisting of three subordinate scales, “negative attitudes toward situations of interaction with robots,” “negative attitudes toward the social influence of robots,” and “negative attitudes toward emotions in interaction with robots,” was developed based on a data sample comprising of 263 Japanese university students. This scale was administered to 240 Japanese university students to confirm its validity and reliability. In this paper, we report on the results of analyses of these data samples. Moreover, we discuss some future problems including a comparison of attitudes toward robots between nations.","[{'authorId': '1768404', 'name': 'T. Nomura'}, {'authorId': '2108797118', 'name': 'Tomohiro Suzuki'}, {'authorId': '48309591', 'name': 'T. Kanda'}, {'authorId': '2804491', 'name': 'Kensuke Kato'}]",301.0,"{'bibtex': '@Article{Nomura2006MeasurementON,\n author = {T. Nomura and Tomohiro Suzuki and T. Kanda and Kensuke Kato},\n journal = {Interaction Studies},\n pages = {437-454},\n title = {Measurement of negative attitudes toward robots},\n volume = {7},\n year = {2006}\n}\n'}",,"{'volume': '7', 'pages': '437-454', 'name': 'Interaction Studies'}",0.0,Measurement of negative attitudes toward robots,2006.0
632,32f14fab6396a31c48c659d6936e2888662223b0,,"[{'authorId': '39824145', 'name': 'D. Bernhardt'}, {'authorId': '2149814967', 'name': 'P. Robinson'}]",170.0,"{'bibtex': '@Inproceedings{Bernhardt2007DetectingAF,\n author = {D. Bernhardt and P. Robinson},\n pages = {59-70},\n title = {Detecting Affect from Non-stylised Body Motions},\n year = {2007}\n}\n'}",,{'pages': '59-70'},15.0,Detecting Affect from Non-stylised Body Motions,2007.0
634,32fba2469859478811e979ad20345ed2737efdab,,"[{'authorId': '50652909', 'name': 'Xiang Wei'}, {'authorId': '143844117', 'name': 'W. Lu'}, {'authorId': '2112271512', 'name': 'Lili Zhu'}, {'authorId': '145767614', 'name': 'Weiwei Xing'}]",23.0,"{'bibtex': '@Article{Wei2018LearningMR,\n author = {Xiang Wei and W. Lu and Lili Zhu and Weiwei Xing},\n journal = {Neurocomputing},\n pages = {125-134},\n title = {Learning motion rules from real data: Neural network for crowd simulation},\n volume = {310},\n year = {2018}\n}\n'}",,"{'volume': '310', 'pages': '125-134', 'name': 'Neurocomputing'}",34.0,Learning motion rules from real data: Neural network for crowd simulation,2018.0
635,3303b8482f5cfcea58740d6ab33afb8afef34a8f,"We describe a hybrid real and virtual system that monitors and teaches children in an everyday classroom environment without requiring any special virtual reality set ups or any knowledge that there is a computer involved. This system is truly pervasive in that it interacts with a child who is playing with normal physical toys using speech. A simulated virtual head provides a focus and the opportunity for microphonological language teaching, whilst a simulated world allows the teacher to demonstrate using her set of blocks – much as a teacher might demonstrate at the front of the class. However this system allows individual students, or pairs of students, to interact with the task in a computer-free way and receive feedback from PETA, the Teaching Head, as if from their own private tutor.","[{'authorId': '144871539', 'name': 'D. Powers'}, {'authorId': '2323617', 'name': 'Richard Leibbrandt'}, {'authorId': '1776457', 'name': 'M. Luerssen'}, {'authorId': '145111765', 'name': 'T. Lewis'}, {'authorId': '48275227', 'name': 'M. Lawson'}]",14.0,"{'bibtex': '@Misc{None,\n author = {D. Powers and Richard Leibbrandt and M. Luerssen and T. Lewis and M. Lawson},\n title = {Peta – a Pedagogical Embodied Teaching Agent}\n}\n'}",,,27.0,Peta – a Pedagogical Embodied Teaching Agent,
636,33161ce8d17a405354d20c7d0c552abc6cf189f7,"We present and describe CiF-CK — a social agent architecture that models reasoning about persistent social interactions to improve narrative engagement and play experience for human interactors. The architecture is inspired by McCoy et al's Comme il-Faut (CiF) architecture that represented rich social interactions between agents that included emotions, social and relationship contexts, and longer term mood. The key contribution of this work is in adapting the richness of social interactions from CiF to a first-person interaction experience and a released distribution of its implementation on the Skyrim game engine. The released modification has been successful in the player community for the popular game.","[{'authorId': '28004507', 'name': 'Manuel Guimarães'}, {'authorId': '2068679306', 'name': 'Pedro Santos'}, {'authorId': '1763814', 'name': 'A. Jhala'}]",19.0,"{'bibtex': '@Article{Guimarães2017CiFCKAA,\n author = {Manuel Guimarães and Pedro Santos and A. Jhala},\n journal = {2017 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {126-133},\n title = {CiF-CK: An architecture for social NPCS in commercial games},\n year = {2017}\n}\n'}",,"{'pages': '126-133', 'name': '2017 IEEE Conference on Computational Intelligence and Games (CIG)'}",35.0,CiF-CK: An architecture for social NPCS in commercial games,2017.0
637,3356f537149f49a24f6a0002de062d60958e1f3e,,"[{'authorId': '3065464', 'name': 'A. V. D. Pütten'}, {'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '34728215', 'name': 'Sin-Hwa Kang'}]",303.0,"{'bibtex': '@Article{Pütten2010ItDM,\n author = {A. V. D. Pütten and N. Krämer and J. Gratch and Sin-Hwa Kang},\n journal = {Comput. Hum. Behav.},\n pages = {1641-1650},\n title = {""It doesn\'t matter what you are!"" Explaining social effects of agents and avatars},\n volume = {26},\n year = {2010}\n}\n'}",,"{'volume': '26', 'pages': '1641-1650', 'name': 'Comput. Hum. Behav.'}",79.0,"""It doesn't matter what you are!"" Explaining social effects of agents and avatars",2010.0
638,336e291572583c58c0209c3dd7d374ee0927dcad,"We propose that face-to-face contact fosters the development of rapport and thereby helps negotiators coordinate on mutually beneficial settlements in mixed-motive conflicts. Specifically, we investigate whether, in a cooperative climate, negotiators' visual access to each other's nonverbal behavior fosters a dyadic state of rapport that facilitates mutual cooperation. Experiment 1 manipulated whether negotiators stood face-to-face or side-by-side (unable to see each other) in a simulated strike negotiation. Face-to-face dyads were more likely to coordinate on a settlement early in the strike, resulting in higher joint gains. An alternative interpretation in terms of an anticipatory effect of face-to-face contact was not supported. Experiment 2 manipulated whether previously unacquainted negotiators conversed face-to-face or by telephone before separating to play a conflict game with the structure of a Prisoner's Dilemma game. Face-to-face dyads were more likely to coordinate on high joint gain outcomes. The facilitatory effect of face-to-face contact was statistically mediated by a measure of dyadic rapport. Results did not support alternative interpretations based on individual-level positive affect or expectations about opponents. We conclude with a discussion of the role of affective and dyad-level processes in social psychological models of conflict resolution.","[{'authorId': '48718478', 'name': 'A. Drolet'}, {'authorId': '145440944', 'name': 'M. Morris'}]",420.0,"{'bibtex': '@Article{Drolet2000RapportIC,\n author = {A. Drolet and M. Morris},\n journal = {Journal of Experimental Social Psychology},\n pages = {26-50},\n title = {Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixed-motive conflicts.},\n volume = {36},\n year = {2000}\n}\n'}",,"{'volume': '36', 'pages': '26-50', 'name': 'Journal of Experimental Social Psychology'}",71.0,Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixed-motive conflicts.,2000.0
639,336ef90827eeb209540070fa0f6ce15ad6c300a5,"Perception of emotion through facial expressions and head motion is of interest to both psychology and affective computing researchers. However, very little is known about the importance of each modality individually, as they are often treated together rather than separately. We present a study which isolates the effect of head motion from facial expression in the perception of complex emotions in videos. We demonstrate that head motions carry emotional information that is complementary rather than redundant to the emotion content in facial expressions. Finally, we show that emotional expressivity in head motion is not limited to nods and shakes and that additional gestures (such as head tilts, raises and general amount of motion) could be beneficial to automated recognition systems.","[{'authorId': '31639770', 'name': 'Andra Adams'}, {'authorId': '97930679', 'name': 'M. Mahmoud'}, {'authorId': '1756344', 'name': 'T. Baltrušaitis'}, {'authorId': '2149814967', 'name': 'P. Robinson'}]",42.0,"{'bibtex': '@Article{Adams2015DecouplingFE,\n author = {Andra Adams and M. Mahmoud and T. Baltrušaitis and P. Robinson},\n journal = {2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {274-280},\n title = {Decoupling facial expressions and head motions in complex emotions},\n year = {2015}\n}\n'}",,"{'pages': '274-280', 'name': '2015 International Conference on Affective Computing and Intelligent Interaction (ACII)'}",36.0,Decoupling facial expressions and head motions in complex emotions,2015.0
640,337c5b072f0bc8b30e03276f02e3fc81e3c0ff2d,,"[{'authorId': '1388551183', 'name': 'I. M. Soboĺ'}]",3691.0,"{'bibtex': '@Article{Soboĺ2001GlobalSI,\n author = {I. M. Soboĺ},\n journal = {Mathematics and Computers in Simulation},\n pages = {271-280},\n title = {Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates},\n volume = {55},\n year = {2001}\n}\n'}",,"{'volume': '55', 'pages': '271-280', 'name': 'Mathematics and Computers in Simulation'}",9.0,Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates,2001.0
641,33a8acdcf7685129d3cf62c3dbc04daed98beabc,"Emotion embedding space learned from references is a straight-forward approach for emotion transfer in encoder-decoder structured emotional text to speech (TTS) systems. However, the transferred emotion in the synthetic speech is not accurate and expressive enough with emotion category confusions. Moreover, it is hard to select an appropriate reference to deliver desired emotion strength. To solve these problems, we propose a novel approach based on Tacotron. First, we plug two emotion classifiers – one after the reference encoder, one after the decoder output – to enhance the emotion-discriminative ability of the emotion embedding and the predicted mel-spectrum. Second, we adopt style loss to measure the difference between the generated and reference mel-spectrum. The emotion strength in the synthetic speech can be controlled by adjusting the value of the emotion embedding as the emotion embedding can be viewed as the feature map of the mel-spectrum. Experiments on emotion transfer and strength control have shown that the synthetic speech of the proposed method is more accurate and expressive with less emotion category confusions and the control of emotion strength is more salient to listeners.","[{'authorId': '50289773', 'name': 'Tao Li'}, {'authorId': '50591589', 'name': 'Shan Yang'}, {'authorId': '46426991', 'name': 'Liumeng Xue'}, {'authorId': '144206962', 'name': 'Lei Xie'}]",56.0,"{'bibtex': '@Article{Li2020ControllableET,\n author = {Tao Li and Shan Yang and Liumeng Xue and Lei Xie},\n journal = {2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP)},\n pages = {1-5},\n title = {Controllable Emotion Transfer For End-to-End Speech Synthesis},\n year = {2020}\n}\n'}",,"{'pages': '1-5', 'name': '2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP)'}",35.0,Controllable Emotion Transfer For End-to-End Speech Synthesis,2020.0
642,33b0036074b2493794cfbefdc589a869574620eb,,"[{'authorId': '9005262', 'name': 'F. Gobet'}, {'authorId': '152148629', 'name': 'H. Richman'}, {'authorId': '32422330', 'name': 'J. Staszewski'}, {'authorId': '2259532335', 'name': 'H. Simon'}]",27.0,"{'bibtex': '@Article{Gobet1997GoalsRA,\n author = {F. Gobet and H. Richman and J. Staszewski and H. Simon},\n journal = {Psychology of Learning and Motivation},\n pages = {265-290},\n title = {Goals, Representations, and Strategies in a Concept Attainment Task: the EPAM Model},\n volume = {37},\n year = {1997}\n}\n'}",,"{'volume': '37', 'pages': '265-290', 'name': 'Psychology of Learning and Motivation'}",16.0,"Goals, Representations, and Strategies in a Concept Attainment Task: the EPAM Model",1997.0
643,340921f48de9ac42de4cc5dc3329e8c872fbb1d6,,[],50.0,"{'bibtex': '@Article{None,\n journal = {Encyclopedic Dictionary of Archaeology},\n title = {Oculus},\n year = {2021}\n}\n'}",,{'name': 'Encyclopedic Dictionary of Archaeology'},0.0,Oculus,2021.0
644,340f48901f72278f6bf78a04ee5b01df208cc508,,"[{'authorId': '3255983', 'name': 'Volodymyr Mnih'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '2228824', 'name': 'Andrei A. Rusu'}, {'authorId': '144056327', 'name': 'J. Veness'}, {'authorId': '1792298', 'name': 'Marc G. Bellemare'}, {'authorId': '1753223', 'name': 'Alex Graves'}, {'authorId': '3137672', 'name': 'Martin A. Riedmiller'}, {'authorId': '145600108', 'name': 'A. Fidjeland'}, {'authorId': '2273072', 'name': 'Georg Ostrovski'}, {'authorId': '48348688', 'name': 'Stig Petersen'}, {'authorId': '50388928', 'name': 'Charlie Beattie'}, {'authorId': '49813280', 'name': 'Amir Sadik'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '143776287', 'name': 'Helen King'}, {'authorId': '2106164', 'name': 'D. Kumaran'}, {'authorId': '1688276', 'name': 'Daan Wierstra'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",22104.0,"{'bibtex': '@Article{Mnih2015HumanlevelCT,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},\n journal = {Nature},\n pages = {529-533},\n title = {Human-level control through deep reinforcement learning},\n volume = {518},\n year = {2015}\n}\n'}",,"{'volume': '518', 'pages': '529-533', 'name': 'Nature'}",38.0,Human-level control through deep reinforcement learning,2015.0
645,341ea24cc5456da43c0c0b8d1a6e362b31815d8f,"&NA; As a highly social species, humans are equipped with a powerful tool for social communication—the face. Although seemingly simple, the human face can elicit multiple social perceptions due to the rich variations of its movements, morphology, and complexion. Consequently, identifying precisely what face information elicits different social perceptions is a complex empirical challenge that has largely remained beyond the reach of traditional methods. In the past decade, the emerging field of social psychophysics has developed new methods to address this challenge, with the potential to transfer psychophysical laws of social perception to the digital economy via avatars and social robots. At this exciting juncture, it is timely to review these new methodological developments. In this article, we introduce and review the foundational methodological developments of social psychophysics, present work done in the past decade that has advanced understanding of the face as a tool for social communication, and discuss the major challenges that lie ahead.","[{'authorId': '2143019', 'name': 'Rachael E. Jack'}, {'authorId': '2287417', 'name': 'P. Schyns'}]",123.0,"{'bibtex': '@Article{Jack2017TowardAS,\n author = {Rachael E. Jack and P. Schyns},\n journal = {Annual Review of Psychology},\n pages = {269–297},\n title = {Toward a Social Psychophysics of Face Communication},\n volume = {68},\n year = {2017}\n}\n'}",,"{'volume': '68', 'pages': '269–297', 'name': 'Annual Review of Psychology'}",134.0,Toward a Social Psychophysics of Face Communication,2017.0
646,3421020c64a04af023669b26cc4cba2b615ea501,"To endow virtual agents with more realistic affective behavior, the notion of expectation-based emotions plays an important role: emotional states of agents should not only be triggered by present stimuli, but also by anticipation on future stimuli, and evaluation of past stimuli in the context of these anticipations. Within this study, an extension of the belief-desire-intention (BDI) model with expectation-based emotions is proposed. The model has been implemented in the modeling language LEADSTO. In addition, a game application has been developed, in which a user can play a dice game against an agent that is equipped with the emotion-based model. An empirical evaluation indicates that the model significantly enhances the agent's believability, in particular concerning its involvement in the situation.","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '2984887', 'name': 'E. Zwanenburg'}]",23.0,"{'bibtex': ""@Article{Bosse2009TheresAH,\n author = {T. Bosse and E. Zwanenburg},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-8},\n title = {There's always hope: Enhancing agent believability through expectation-based emotions},\n year = {2009}\n}\n""}",,"{'pages': '1-8', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",28.0,There's always hope: Enhancing agent believability through expectation-based emotions,2009.0
648,342e20a754969961e5f2ce8ac232bd26e41439a1,,"[{'authorId': '21451088', 'name': 'P. Ekman'}]",436.0,"{'bibtex': '@Inproceedings{Ekman1970UniversalFE,\n author = {P. Ekman},\n title = {Universal facial expressions of emotion.},\n year = {1970}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Universal facial expressions of emotion.,1970.0
649,344b00021c1ff66cb0efdd2648dd735976e7d582,"How do mistakes made by a robot affect its trustworthiness and acceptance in human-robot collaboration? We investigate how the perception of erroneous robot behavior may influence human interaction choices and the willingness to cooperate with the robot by following a number of itsunusual requests. For this purpose, we conducted an experiment in which participants interacted with a home companion robot in one of two experimental conditions: (1) the correct modeor (2) the faulty mode. Our findings reveal that, while significantly affecting subjective perceptions of the robot and assessments of its reliability and trustworthiness, the robot’s performance does not seem to substantially influence participants’ decisions to (not) comply with its requests. However, our results further suggest that the nature of the task requested by the robot, e.g. whether its effects are revocable as opposed to irrevocable, has a significant impact on participants’ willingness to follow its instructions.","[{'authorId': '144426526', 'name': 'Maha Salem'}, {'authorId': '32464892', 'name': 'G. Lakatos'}, {'authorId': '3029288', 'name': 'F. Amirabdollahian'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}]",465.0,"{'bibtex': '@Article{Salem2015WouldYT,\n author = {Maha Salem and G. Lakatos and F. Amirabdollahian and K. Dautenhahn},\n journal = {2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {1-8},\n title = {Would You Trust a (Faulty) Robot? Effects of Error, Task Type and Personality on Human-Robot Cooperation and Trust},\n year = {2015}\n}\n'}",,"{'pages': '1-8', 'name': '2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",23.0,"Would You Trust a (Faulty) Robot? Effects of Error, Task Type and Personality on Human-Robot Cooperation and Trust",2015.0
650,345c39e52c3d09f61612407a0a1c5cda5e73e9f9,"Culture is not the first aspect that comes to mind when discussing human robot interaction. But our cultural upbringing does to a large degree influence our patterns of behavior and interpretation. Thus, culture is present in the development of robotic systems right from the start, unconsciously influencing how robots look, what we envision with them to do, and how they are programmed to interact with the user. In this paper we argue that is is beneficial to make this unconscious influence explicit and take it into account during the development (and evaluation) of humanoid robots. To this end we present a principled approach of capturing various cultural influences during the development process of humanoid robots and exemplify this approach with a case study of affective body movements.","[{'authorId': '39957689', 'name': 'M. Rehm'}]",8.0,"{'bibtex': '@Article{Rehm2012ExperimentalDF,\n author = {M. Rehm},\n journal = {2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012)},\n pages = {78-83},\n title = {Experimental designs for cross-cultural interactions: A case study on affective body movements for HRI},\n year = {2012}\n}\n'}",,"{'pages': '78-83', 'name': '2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012)'}",32.0,Experimental designs for cross-cultural interactions: A case study on affective body movements for HRI,2012.0
651,34b9a8f0715ff16a5118209530634fd4e83ee5b9,,"[{'authorId': '145028923', 'name': 'A. Bork'}, {'authorId': '2372781', 'name': 'S. Gunnarsdottir'}]",3391.0,"{'bibtex': '@Inproceedings{Bork2001MultimediaIL,\n author = {A. Bork and S. Gunnarsdottir},\n pages = {63-70},\n title = {Multimedia in Learning},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '63-70', 'name': ''}",0.0,Multimedia in Learning,2001.0
652,34be3feaa59ddf191563381793a7a235f33a295e,"When speakers talk, they gesture. The goal of this review is to investigate the contribution that these gestures make to how we communicate and think. Gesture can play a role in communication and thought at many timespans. We explore, in turn, gesture's contribution to how language is produced and understood in the moment; its contribution to how we learn language and other cognitive skills; and its contribution to how language is created over generations, over childhood, and on the spot. We find that the gestures speakers produce when they talk are integral to communication and can be harnessed in a number of ways. (a) Gesture reflects speakers' thoughts, often their unspoken thoughts, and thus can serve as a window onto cognition. Encouraging speakers to gesture can thus provide another route for teachers, clinicians, interviewers, etc., to better understand their communication partners. (b) Gesture can change speakers' thoughts. Encouraging gesture thus has the potential to change how students, patients, witnesses, etc., think about a problem and, as a result, alter the course of learning, therapy, or an interchange. (c) Gesture provides building blocks that can be used to construct a language. By watching how children and adults who do not already have a language put those blocks together, we can observe the process of language creation. Our hands are with us at all times and thus provide researchers and learners with an ever-present tool for understanding how we talk and think.","[{'authorId': '115377287', 'name': 'S. Goldin‐Meadow'}, {'authorId': '3177547', 'name': 'M. Alibali'}]",331.0,"{'bibtex': ""@Article{Goldin‐Meadow2013GesturesRI,\n author = {S. Goldin‐Meadow and M. Alibali},\n journal = {Annual review of psychology},\n pages = {\n          257-83\n        },\n title = {Gesture's role in speaking, learning, and creating language.},\n volume = {64},\n year = {2013}\n}\n""}",,"{'volume': '64', 'pages': '\n          257-83\n        ', 'name': 'Annual review of psychology'}",149.0,"Gesture's role in speaking, learning, and creating language.",2013.0
653,34c1d8ce112bf496f3433bfecb431d7720e426b8,,"[{'authorId': '7234960', 'name': 'A. Cowell'}, {'authorId': '1701555', 'name': 'K. Stanney'}]",125.0,"{'bibtex': '@Article{Cowell2005ManipulationON,\n author = {A. Cowell and K. Stanney},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {281-306},\n title = {Manipulation of non-verbal interaction style and demographic embodiment to increase anthropomorphic computer character credibility},\n volume = {62},\n year = {2005}\n}\n'}",,"{'volume': '62', 'pages': '281-306', 'name': 'Int. J. Hum. Comput. Stud.'}",78.0,Manipulation of non-verbal interaction style and demographic embodiment to increase anthropomorphic computer character credibility,2005.0
654,34e2cb7a4fef0651fb5c0a120c8e70ebab9f0749,,"[{'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '32722492', 'name': 'Aisha King'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",526.0,"{'bibtex': ""@Article{Lucas2014ItsOA,\n author = {Gale M. Lucas and J. Gratch and Aisha King and Louis-Philippe Morency},\n journal = {Comput. Hum. Behav.},\n pages = {94-100},\n title = {It's only a computer: Virtual humans increase willingness to disclose},\n volume = {37},\n year = {2014}\n}\n""}",,"{'volume': '37', 'pages': '94-100', 'name': 'Comput. Hum. Behav.'}",28.0,It's only a computer: Virtual humans increase willingness to disclose,2014.0
656,34fbe38622194cae32166f926e65ac082c921706,"This study was designed to investigate the relations between emotional expression and the movement characteristics. For this purpose, we used kinematic data related to three factors of the movement characteristics: Speed, Force, and Directness. In Exp. 1, we examined how the dancers expressed emotions when they used a certain body action and body part, and how they altered the movement characteristics. In Exp. 1, 10 female dancers were instructed to express three emotions, joy, sadness, and anger, by altering arm-movement characteristics. Analysis of variance indicated that the three exhibited emotional expressions had different movement characteristics. Discriminant analysis indicated that kinematic data for evaluation of movement characteristics are useful for discrimination of the three emotional expressions in dance. In Exp. 2, we investigated how naive observers perceived the type of emotion from the arm-movement characteristics. Analysis of variance showed that 22 observers accurately perceived each emotion distinguished from other emotions. Multiple regression analysis showed that specific movement characteristics influenced the perception of particular emotion.","[{'authorId': '3487152', 'name': 'Misako Sawada'}, {'authorId': '92882753', 'name': 'K. Suda'}, {'authorId': '3473156', 'name': 'Motonobu Ishii'}]",132.0,"{'bibtex': '@Article{Sawada2003ExpressionOE,\n author = {Misako Sawada and K. Suda and Motonobu Ishii},\n journal = {Perceptual and Motor Skills},\n pages = {697 - 708},\n title = {Expression of Emotions in Dance: Relation between Arm Movement Characteristics and Emotion},\n volume = {97},\n year = {2003}\n}\n'}",,"{'volume': '97', 'pages': '697 - 708', 'name': 'Perceptual and Motor Skills'}",19.0,Expression of Emotions in Dance: Relation between Arm Movement Characteristics and Emotion,2003.0
657,3531a94f5d8728745916cadb7d9de0a801b2e596,,"[{'authorId': '1708171', 'name': 'J. Sansonnet'}, {'authorId': '40845119', 'name': 'François Bouchet'}]",5.0,"{'bibtex': '@Inproceedings{Sansonnet2010ExpressionOB,\n author = {J. Sansonnet and François Bouchet},\n pages = {413-419},\n title = {Expression of Behaviors in Assistant Agents as Influences on Rational Execution of Plans},\n year = {2010}\n}\n'}",,{'pages': '413-419'},13.0,Expression of Behaviors in Assistant Agents as Influences on Rational Execution of Plans,2010.0
658,3548b6ac3f044cc226681ab98df0253fddda7367,,"[{'authorId': '1991013052', 'name': 'Qian Hu'}, {'authorId': '144038070', 'name': 'Yao-bin Lu'}, {'authorId': '2069543203', 'name': 'Zhao Pan'}, {'authorId': '40653187', 'name': 'Y. Gong'}, {'authorId': '2109513074', 'name': 'Zhiling Yang'}]",108.0,"{'bibtex': '@Article{Hu2021CanAA,\n author = {Qian Hu and Yao-bin Lu and Zhao Pan and Y. Gong and Zhiling Yang},\n journal = {Int. J. Inf. Manag.},\n pages = {102250},\n title = {Can AI artifacts influence human cognition? The effects of artificial autonomy in intelligent personal assistants},\n volume = {56},\n year = {2021}\n}\n'}",,"{'volume': '56', 'pages': '102250', 'name': 'Int. J. Inf. Manag.'}",92.0,Can AI artifacts influence human cognition? The effects of artificial autonomy in intelligent personal assistants,2021.0
659,355226f590fca31add067e8a1020edf27f3b4e2f,"The spontaneous mimicry of others' emotional facial expressions constitutes a rudimentary form of empathy and facilitates social understanding. Here, we show that human participants spontaneously match facial expressions of an android physically present in the room with them. This mimicry occurs even though these participants find the android unsettling and are fully aware that it lacks intentionality. Interestingly, a video of that same android elicits weaker mimicry reactions, occurring only in participants who find the android “humanlike.” These findings suggest that spontaneous mimicry depends on the salience of humanlike features highlighted by face-to-face contact, emphasizing the role of presence in human-robot interaction. Further, the findings suggest that mimicry of androids can dissociate from knowledge of artificiality and experienced emotional unease. These findings have implications for theoretical debates about the mechanisms of imitation. They also inform creation of future robots that effectively build rapport and engagement with their human users.","[{'authorId': '3092566', 'name': 'Galit Hofree'}, {'authorId': '12114845', 'name': 'P. Ruvolo'}, {'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '3122131', 'name': 'P. Winkielman'}]",41.0,"{'bibtex': '@Article{Hofree2014BridgingTM,\n author = {Galit Hofree and P. Ruvolo and M. Bartlett and P. Winkielman},\n journal = {PLoS ONE},\n title = {Bridging the Mechanical and the Human Mind: Spontaneous Mimicry of a Physically Present Android},\n volume = {9},\n year = {2014}\n}\n'}",,"{'volume': '9', 'name': 'PLoS ONE'}",50.0,Bridging the Mechanical and the Human Mind: Spontaneous Mimicry of a Physically Present Android,2014.0
661,357e6c27d79c4b0d3e2997ec24dc3d2d3c16c700,,"[{'authorId': '3070352', 'name': 'G. Loewenstein'}, {'authorId': '5209814', 'name': 'J. Lerner'}]",787.0,"{'bibtex': '@Inproceedings{Loewenstein2003TheRO,\n author = {G. Loewenstein and J. Lerner},\n title = {The role of affect in decision making.},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The role of affect in decision making.,2003.0
662,3586d02338c87f8a7da08aed18625745fd1b3c46,"We describe an implemented system which automatically generates and animates conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures. Conversation is created by a dialogue planner that produces the text as well as the intonation of the utterances. The speaker/listener relationship, the text, and the intonation in turn drive facial expressions, lip motions, eye gaze, head motion, and arm gestures generators. Coordinated arm, wrist, and hand motions are invoked to create semantically meaningful gestures. Throughout we will use examples from an actual synthesized, fully animated conversation.","[{'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '145332819', 'name': 'Mark Steedman'}, {'authorId': '1792667', 'name': 'Brett Achorn'}, {'authorId': '2955637', 'name': 'Tripp Becket'}, {'authorId': '3024972', 'name': 'Brett Douville'}, {'authorId': '35219353', 'name': 'Scott Prevost'}, {'authorId': '144884556', 'name': 'Matthew Stone'}]",781.0,"{'bibtex': '@Article{Cassell1994AnimatedCR,\n author = {Justine Cassell and C. Pelachaud and N. Badler and Mark Steedman and Brett Achorn and Tripp Becket and Brett Douville and Scott Prevost and Matthew Stone},\n journal = {Proceedings of the 21st annual conference on Computer graphics and interactive techniques},\n title = {Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents},\n year = {1994}\n}\n'}",,{'name': 'Proceedings of the 21st annual conference on Computer graphics and interactive techniques'},45.0,"Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents",1994.0
664,358f092645d60e74a0d917c147a33076037cf23e,"Human beings can be proactive and engaged or, alternatively, passive and alienated, largely as a function of the social conditions in which they develop and function. Accordingly, research guided by self-determination theory has focused on the social-contextual conditions that facilitate versus forestall the natural processes of self-motivation and healthy psychological development. Specifically, factors have been examined that enhance versus undermine intrinsic motivation, self-regulation, and well-being. The findings have led to the postulate of three innate psychological needs--competence, autonomy, and relatedness--which when satisfied yield enhanced self-motivation and mental health and when thwarted lead to diminished motivation and well-being. Also considered is the significance of these psychological needs and processes within domains such as health care, education, work, sport, religion, and psychotherapy.","[{'authorId': '4064306', 'name': 'R. Ryan'}, {'authorId': '2740774', 'name': 'E. Deci'}]",31201.0,"{'bibtex': '@Article{Ryan2000SelfdeterminationTA,\n author = {R. Ryan and E. Deci},\n journal = {The American psychologist},\n pages = {\n          68-78\n        },\n title = {Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.},\n volume = {55 1},\n year = {2000}\n}\n'}",,"{'volume': '55 1', 'pages': '\n          68-78\n        ', 'name': 'The American psychologist'}",183.0,"Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.",2000.0
665,35900a1ba8b267a144be21f7e016fe5d113cd549,,"[{'authorId': '1838029', 'name': 'Sigal G. Barsade'}, {'authorId': '35547971', 'name': 'D. Gibson'}]",407.0,"{'bibtex': '@Inproceedings{Barsade1998GroupEA,\n author = {Sigal G. Barsade and D. Gibson},\n title = {Group emotion: A view from top and bottom.},\n year = {1998}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Group emotion: A view from top and bottom.,1998.0
666,35aa24b141a401fefbd5f5c4fa307dc6f7c6f772,"Digital immersive virtual environment technology (IVET) enables behavioral scientists to conduct ecologically realistic experiments with near-perfect experimental control. The authors employed IVET to study the interpersonal distance maintained between participants and virtual humans. In Study 1, participants traversed a three-dimensional virtual room in which a virtual human stood. In Study 2, a virtual human approached participants. In both studies, participant gender, virtual human gender, virtual human gaze behavior, and whether virtual humans were allegedly controlled by humans (i.e., avatars) or computers (i.e., agents) were varied. Results indicated that participants maintained greater distance from virtual humans when approaching their fronts compared to their backs. In addition, participants gave more personal space to virtual agents who engaged them in mutual gaze. Moreover, when virtual humans invaded their personal space, participants moved farthest from virtual human agents. The advantages and disadvantages of IVET for the study of human behavior are discussed.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2386187', 'name': 'J. Loomis'}]",699.0,"{'bibtex': '@Article{Bailenson2003InterpersonalDI,\n author = {J. Bailenson and J. Blascovich and A. Beall and J. Loomis},\n journal = {Personality and Social Psychology Bulletin},\n pages = {819 - 833},\n title = {Interpersonal Distance in Immersive Virtual Environments},\n volume = {29},\n year = {2003}\n}\n'}",,"{'volume': '29', 'pages': '819 - 833', 'name': 'Personality and Social Psychology Bulletin'}",38.0,Interpersonal Distance in Immersive Virtual Environments,2003.0
668,35c691ff05d65ef5d67c272a6828ee1791de9e8c,"The evolution of multicellular organisms from monocellular ancestors represents one of the greatest advances of the history of life. The assembly of such multicellular organisms requires signalling and response between cells: over millions of years these signalling processes have become extremely sophisticated and refined by evolution, such that study of modern organisms may not be able to shed much light on the original ancient processes . Here we are interested in determining how simple a signalling method can be, while still achieving self-assembly. In 2D a coupled cellular automaton/differential equation approach models organisms and chemotaxic chemicals, producing spiralling aggregation. In 3D Lennard-Jones-like particles are used to represent single cells, and their evolution in response to signalling is followed by molecular dynamics. It is found that if a single cell is able to emit a signal which induces others to move towards it, then a colony of single-cell organisms can assemble into shapes as complex as a tower, a ball atop a stalk, or a fast-moving slug. The similarity with the behaviour of modern Dictyostelium slime molds signalling with cyclic adenosine monophosphate (cAMP) is striking.","[{'authorId': '2216630', 'name': 'G. Ackland'}, {'authorId': '1436799167', 'name': 'Richard D.L.Hanes'}, {'authorId': '1664624778', 'name': 'M. Cohen'}]",3.0,"{'bibtex': '@Article{Ackland2007SelfAO,\n author = {G. Ackland and Richard D.L.Hanes and M. Cohen},\n journal = {arXiv: Cell Behavior},\n title = {Self assembly of a model multicellular organism resembling the Dictyostelium slime molds},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': 'arXiv: Cell Behavior'}",25.0,Self assembly of a model multicellular organism resembling the Dictyostelium slime molds,2007.0
669,366151c9363395783ab5fdb20814689c248b2575,"is a lot of evidence for the phenomenon describing the spread of emotion from one person to another, called emotional contagion. Although there is a large body of research on this topic, research containing evidence for factors that moderate the process of emotional contagion, is limited and inconclusive. Furthermore most of these studies are done in a dyadic lab-setting and consequently little is known about emotional contagion in groups. This paper presents, for the first time, a dynamic computational model of contagion in groups of agents based on factors that moderate contagion. These factors are strictly based on experimental evidence in the psychological literature. In this paper we first present our review of the psychological literature. We then present our computational model as well as a pilot study investigating several group contagion cases showing the flexibility and potential of this strategy.","[{'authorId': '2059510738', 'name': 'R. Coenen'}, {'authorId': '1735303', 'name': 'J. Broekens'}]",18.0,"{'bibtex': '@Inproceedings{Coenen2012ModelingEC,\n author = {R. Coenen and J. Broekens},\n title = {Modeling emotional contagion based on experimental evidence for moderating factors},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",39.0,Modeling emotional contagion based on experimental evidence for moderating factors,2012.0
670,36670493aa690e49e7ade1747a419079395ab4bc,"A revolution in the science of emotion has emerged in recent decades, with the potential to create a paradigm shift in decision theories. The research reveals that emotions constitute potent, pervasive, predictable, sometimes harmful and sometimes beneficial drivers of decision making. Across different domains, important regularities appear in the mechanisms through which emotions influence judgments and choices. We organize and analyze what has been learned from the past 35 years of work on emotion and decision making. In so doing, we propose the emotion-imbued choice model, which accounts for inputs from traditional rational choice theory and from newer emotion research, synthesizing scientific models.","[{'authorId': '5209814', 'name': 'J. Lerner'}, {'authorId': '2110764517', 'name': 'Ye Li'}, {'authorId': '49421872', 'name': 'Piercarlo Valdesolo'}, {'authorId': '7014924', 'name': 'K. Kassam'}]",1698.0,"{'bibtex': '@Article{Lerner2015EmotionAD,\n author = {J. Lerner and Ye Li and Piercarlo Valdesolo and K. Kassam},\n journal = {Annual review of psychology},\n pages = {\n          799-823\n        },\n title = {Emotion and decision making.},\n volume = {66},\n year = {2015}\n}\n'}",,"{'volume': '66', 'pages': '\n          799-823\n        ', 'name': 'Annual review of psychology'}",235.0,Emotion and decision making.,2015.0
671,36a8de88bddab9218e5208da6889dfb7088df217,,"[{'authorId': '49603388', 'name': 'Atef Ben Youssef'}, {'authorId': '40325099', 'name': 'Mathieu Chollet'}, {'authorId': '31600786', 'name': 'H. Jones'}, {'authorId': '1731432', 'name': 'N. Sabouret'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1724289', 'name': 'M. Ochs'}]",24.0,"{'bibtex': '@Article{Youssef2015TowardsAS,\n author = {Atef Ben Youssef and Mathieu Chollet and H. Jones and N. Sabouret and C. Pelachaud and M. Ochs},\n booktitle = {International Conference on Intelligent Virtual Agents},\n pages = {3-16},\n title = {Towards a Socially Adaptive Virtual Agent},\n year = {2015}\n}\n'}","[{'paperId': '54faa01dc3dd82d4c6ff0c38c8c44f91c35dfe14', 'title': 'The Long-Term Efficacy of “Social Buffering” in Artificial Social Agents: Contextual Affective Perception Matters'}, {'paperId': 'bc46125f8208e07ba64302ff95cd93d2362a710c', 'title': 'User interactions with chatbot interfaces vs. Menu-based interfaces: An empirical study'}, {'paperId': '88c5d878bb1cc2be6329023f12d207a29647f2d4', 'title': 'Reactive Virtual Agents: A Viewpoint-Driven Approach for Bodily Nonverbal Communication'}, {'paperId': 'f46ec853e9fdf2e4499cd17456719fcde30a6769', 'title': 'Communication Skills Training Intervention Based on Automated Recognition of Nonverbal Signals'}, {'paperId': '8f28ce66c49ee040ffe4372351efff09e7f99b08', 'title': 'Friends from the Future: A Scoping Review of Research into Robots and Computer Agents to Combat Loneliness in Older People'}, {'paperId': 'bae607617a09b099d48984289d73cd0f13b332f2', 'title': 'A Taxonomy of Social Cues for Conversational Agents'}, {'paperId': '3ef9fc07c1feb5783a1b46f9f6e6eabcc1c02140', 'title': 'A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes'}, {'paperId': '0bfbd54c2ed38d580fd8c6fb2baf5ee59525dabc', 'title': 'Domain Authoring Assistant for Intelligent Virtual Agents'}, {'paperId': '268d2ec8e5e6e01d36b9731727358156ab718569', 'title': 'A Systematic Survey of 15 Years of User Studies Published in the Intelligent Virtual Agents Conference'}, {'paperId': '645abd8c2cc71be28e78f0bdf0195059253e3a4d', 'title': 'Challenges in Exploiting Conversational Memory in Human-Agent Interaction'}, {'paperId': '29c5663973f8a9803174592b6678dc043bed5920', 'title': 'A Text-Based Chat System Embodied with an Expressive Agent'}, {'paperId': '7c96d03d60a4108e3b2f90d730d8024db1a1c043', 'title': 'Get Your Virtual Hands Off Me! - Developing Threatening IVAs Using Haptic Feedback'}, {'paperId': '0b10529fe7777bfc8f5e3e64a5af46f63352ee08', 'title': 'Towards Interactive Agents that Infer Emotions from Voice and Context Information'}, {'paperId': '0777ad80dc0a3e60e79407e19a511619ea28e953', 'title': 'Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent'}, {'paperId': '65e6da8ea3525079ee80405673a230bdff31d47c', 'title': 'A Conventional Dialogue Model Based on Dialogue Patterns'}, {'paperId': '8de690ecc54990792d0990fa9a815f852341be57', 'title': 'An Intelligent System for Aggression De-Escalation Training'}, {'paperId': 'ccceb3844ef5618958e0fc3f57772d92f7a98574', 'title': 'A Conversational Agent that Reacts to Vocal Signals'}, {'paperId': '9f743c5a68cdae0b44344e2b096a85a8b0874c51', 'title': ""Don't Lose Sight of the Forest: Why the Big Picture of Social Intelligence is Essential""}, {'paperId': '80b55846401f3dda674b09d267825c9cd5cb85dd', 'title': 'A Multimodal Corpus for the Assessment of Public Speaking Ability and Anxiety'}, {'paperId': '1939dba26c2645ca389e042c05445dddf18d4df7', 'title': 'AI as a Methodology for Supporting Educational Praxis and Teacher Metacognition'}, {'paperId': None, 'title': 'CIA_A_282709 941..971'}, {'paperId': '20003dd049afec7526750583ea9dd5d34d53b070', 'title': 'The role of eye gaze in virtual characters on their perceived believability, social presence and the application of Theory of Mind'}, {'paperId': 'ac8f98674c65ed076227ad849bf455d4c63df241', 'title': ""Analyse de signaux sociaux multimodaux: application à la synthèse d'attitudes sociales chez un agent conversationnel animé. (Multimodal analysis and recognition of social signals: application to social stance generation in virtual agents)""}, {'paperId': 'de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36', 'title': 'Artificial Intelligence'}]",{'pages': '3-16'},40.0,Towards a Socially Adaptive Virtual Agent,2015.0
672,36b84360b64a53c057b3fb59b646fd9c8b7a05ab,"In the quarter-century that Social & Legal Studies has been published, regulation has emerged as a new, and for many exciting, interdisciplinary field. The concept itself requires a wider view of normativity than the narrow positivist one of law as command. It is certainly protean, ranging over many fundamental questions about the changing nature of the public sphere of politics and the state, and its interactions with the ‘private’ sphere of economic activity and social relations, as well as the mediation of these interactions, especially through law. This survey aims to outline and evaluate some of the main contours of the field as it has developed in this recent period, focusing on the regulation of economic activity. Regulation is seen as having emerged with the withdrawal by governments from direct provision of many economic and social services, to be replaced by corporatist bureaucracies and quasi-public agencies managing the complex public–private interactions of financialized capitalism. The arguments for ‘smart’ regulation have, in an era fixated on neo-liberalism, generally legitimized delegation of responsibility to big business. Its advocates, having been drawn into policy fields, have perhaps too often lost their critical edge, and allowed it to become instrumentalized, reflecting the technicist character of its practice.","[{'authorId': '35720444', 'name': 'S. Picciotto'}]",561.0,"{'bibtex': '@Article{Picciotto2017Regulation,\n author = {S. Picciotto},\n journal = {Social & Legal Studies},\n pages = {676 - 699},\n title = {Regulation},\n volume = {26},\n year = {2017}\n}\n'}",,"{'volume': '26', 'pages': '676 - 699', 'name': 'Social & Legal Studies'}",0.0,Regulation,2017.0
673,36bd16db07dac7d23ea84c7712f94f7858427a5e,"Composite facial expressions were prepared by aligning the top half of one expression (e.g., anger) with the bottom half of another (e.g., happiness). Experiment 1 shows that participants are slower to identify the expression in either half of these composite images relative to a ""noncomposite"" control condition in which the 2 halves are misaligned. This parallels the composite effect for facial identity (A. W. Young, D. Hellawell, & D. C. Hay, 1987), and like its identity counterpart, the effect is disrupted by inverting the stimuli (Experiment 2). Experiment 3 shows that no composite effect is found when the top and bottom sections contain different models' faces posing the same expression; this serves to exclude many nonconfigural interpretations of the composite effect (e.g., that composites are more ""attention-grabbing"" than noncomposites). Finally, Experiment 4 demonstrates that the composite effects for identity and expression operate independently of one another.","[{'authorId': '2825775', 'name': 'A. Calder'}, {'authorId': '2423497', 'name': 'A. Young'}, {'authorId': '40020056', 'name': 'Jill Keane'}, {'authorId': '2052648474', 'name': 'M. P. Dean'}]",637.0,"{'bibtex': '@Article{Calder2000ConfiguralII,\n author = {A. Calder and A. Young and Jill Keane and M. P. Dean},\n journal = {Journal of experimental psychology. Human perception and performance},\n pages = {\n          527-51\n        },\n title = {Configural information in facial expression perception.},\n volume = {26 2},\n year = {2000}\n}\n'}",,"{'volume': '26 2', 'pages': '\n          527-51\n        ', 'name': 'Journal of experimental psychology. Human perception and performance'}",64.0,Configural information in facial expression perception.,2000.0
674,36c2e086bf91d0293e6c760ea0c99e60757749de,"This paper describes Steve , an animated agent that helps students learn to perform physical , procedural tasks . The student and Steve cohabit a three - dimensional , simulated mock - up of the student's work environment . Steve can demonstrate how to perform tasks and can also monitor students while they practice tasks , providing assistance when needed . This paper describes Steve's architecture in detail , including perception , cognition , and motor control . The perception module monitors the state of the virtual world , maintains a coherent representation of it , and provides this information to the cognition and motor control modules . The cognition module interprets its perceptual input , chooses appropriate goals , constructs and executes plans to achieve those goals , and sends out motor commands . The motor control module implements these motor commands , controlling Steve's voice , locomotion , gaze , and gestures , allowing Steve to manipulate objects in the virtual world .","[{'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '2247993128', 'name': 'W. Lewis Johnson'}]",564.0,"{'bibtex': '@Article{Rickel1999AnimatedAF,\n author = {J. Rickel and W. Lewis Johnson},\n journal = {Appl. Artif. Intell.},\n pages = {343-382},\n title = {Animated Agents for Procedural Training in Virtual Reality: Perception, Cognition, and Motor Control},\n volume = {13},\n year = {1999}\n}\n'}",,"{'volume': '13', 'pages': '343-382', 'name': 'Appl. Artif. Intell.'}",52.0,"Animated Agents for Procedural Training in Virtual Reality: Perception, Cognition, and Motor Control",1999.0
675,36e3a68e57ee0de629ebcd536b95b0b8128987c8,"This paper describes a substantial effort to build a real-time interactive multimodal dialogue system with a focus on emotional and nonverbal interaction capabilities. The work is motivated by the aim to provide technology with competences in perceiving and producing the emotional and nonverbal behaviors required to sustain a conversational dialogue. We present the Sensitive Artificial Listener (SAL) scenario as a setting which seems particularly suited for the study of emotional and nonverbal behavior since it requires only very limited verbal understanding on the part of the machine. This scenario allows us to concentrate on nonverbal capabilities without having to address at the same time the challenges of spoken language understanding, task modeling, etc. We first report on three prototype versions of the SAL scenario in which the behavior of the Sensitive Artificial Listener characters was determined by a human operator. These prototypes served the purpose of verifying the effectiveness of the SAL scenario and allowed us to collect data required for building system components for analyzing and synthesizing the respective behaviors. We then describe the fully autonomous integrated real-time system we created, which combines incremental analysis of user behavior, dialogue management, and synthesis of speaker and listener behavior of a SAL character displayed as a virtual agent. We discuss principles that should underlie the evaluation of SAL-type systems. Since the system is designed for modularity and reuse and since it is publicly available, the SAL system has potential as a joint research tool in the affective computing research community.","[{'authorId': '144951065', 'name': 'M. Schröder'}, {'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '1751126', 'name': 'F. Eyben'}, {'authorId': '1781916', 'name': 'H. Gunes'}, {'authorId': '1678537', 'name': 'D. Heylen'}, {'authorId': '2975858', 'name': 'M. Maat'}, {'authorId': '2228246', 'name': 'G. McKeown'}, {'authorId': '2345401', 'name': 'Sathish Pammi'}, {'authorId': '145387780', 'name': 'M. Pantic'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1761859', 'name': 'E. D. Sevin'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '2103575', 'name': 'M. Wöllmer'}]",204.0,"{'bibtex': '@Article{Schröder2012BuildingAS,\n author = {M. Schröder and Elisabetta Bevacqua and R. Cowie and F. Eyben and H. Gunes and D. Heylen and M. Maat and G. McKeown and Sathish Pammi and M. Pantic and C. Pelachaud and Björn Schuller and E. D. Sevin and M. Valstar and M. Wöllmer},\n journal = {IEEE Transactions on Affective Computing},\n pages = {165-183},\n title = {Building Autonomous Sensitive Artificial Listeners},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'pages': '165-183', 'name': 'IEEE Transactions on Affective Computing'}",143.0,Building Autonomous Sensitive Artificial Listeners,2012.0
676,36f01c8daeb55f06e1b84f785aa84049e07911c7,"An earlier study showed that listeners in conversations insert brief responses (""mm-hmm,"" ""I see,"" and the like) almost exclusively at the ends of rhythmical units in the talker's speech (Dittmann & Llewellyn, 1967). In this study these vocal responses were compared with a visible one, the head nod, and it was found that the 2 occurred together more often than chance would predict. Content analysis showed that these co-occurrences usually serve an interpersonal function: the wish of the listener to speak or the wish of the talker for feedback. When they did occur together, nods were found to precede the vocal response slightly. Apparently the listener must hold a vocal response politely until the speaker has finished a unit, but may nod before then.","[{'authorId': '3956276', 'name': 'A. Dittmann'}, {'authorId': '2335983', 'name': 'L. Llewellyn'}]",175.0,"{'bibtex': '@Article{Dittmann1968RelationshipBV,\n author = {A. Dittmann and L. Llewellyn},\n journal = {Journal of personality and social psychology},\n pages = {\n          79-84\n        },\n title = {Relationship between vocalizations and head nods as listener responses.},\n volume = {9 1},\n year = {1968}\n}\n'}",,"{'volume': '9 1', 'pages': '\n          79-84\n        ', 'name': 'Journal of personality and social psychology'}",6.0,Relationship between vocalizations and head nods as listener responses.,1968.0
677,373ba28a9790dc9df676548b21e2123bfe5d15b4,"The term “believability” is often used to describe expectations concerning virtual agents. In this paper, we analyze which factors influence the believability of the agent acting as the software assistant. We consider several factors such as embodiment, communicative behavior, and emotional capabilities. We conduct a perceptive study where we analyze the role of plausible and/or appropriate emotional displays in relation to believability. We also investigate how people judge the believability of the agent, and whether it provokes social reactions of humans toward it. Finally, we evaluate the respective impact of embodiment and emotion over believability judgments. The results of our study show that (a) appropriate emotions lead to higher perceived believability, (b) the notion of believability is closely correlated with the two major socio-cognitive variables, namely competence and warmth, and (c) considering an agent as believable can be different from having a human-like attitude toward it. Finally, a primacy of emotion behavior over embodiment while judging believability is also hypothesized from free responses given by the participants of this experiment.","[{'authorId': '2539387', 'name': 'Virginie Demeure'}, {'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",71.0,"{'bibtex': '@Article{Demeure2011HowIB,\n author = {Virginie Demeure and Radoslaw Niewiadomski and C. Pelachaud},\n booktitle = {PRESENCE: Teleoperators and Virtual Environments},\n journal = {PRESENCE: Teleoperators and Virtual Environments},\n pages = {431-448},\n title = {How Is Believability of a Virtual Agent Related to Warmth, Competence, Personification, and Embodiment?},\n volume = {20},\n year = {2011}\n}\n'}","[{'paperId': '3c47636003c86163a4e1dfec13ada354823ae419', 'title': 'Effects of Body Type and Voice Pitch on Perceived Audio-Visual Correspondence and Believability of Virtual Characters'}, {'paperId': '937cebbd4a78f933b6f90cbf2c70c4811246c1c0', 'title': 'Towards an architectural framework for intelligent virtual agents using probabilistic programming'}, {'paperId': '2b1a6afbf5f87f21f86138d903c11abbd09accd9', 'title': 'Mediated Social Support for Distress Reduction: AI Chatbots vs. Human'}, {'paperId': '2a406df0e53079cc60f07f6124800403147a906b', 'title': 'Believability, Anticipation, and... Timing Improving believability through timing manipulation'}, {'paperId': '4abd59ea0d92962d37747c3e4071fce83f510091', 'title': 'Let voice assistants sound like a machine: Voice and task type effects on perceived fluency, competence, and consumer attitude'}, {'paperId': '8ff88dfa0fd4aeb1c78153fd4e6711f2405977fb', 'title': 'Beyond Self-diagnosis: How a Chatbot-based Symptom Checker Should Respond'}, {'paperId': 'fef931f4dcc3e1808d0975c97882a84cb11fc20f', 'title': 'User experience of human-robot long-term interactions'}, {'paperId': '5f861f96d0234da728de00122934bce792e553fb', 'title': 'Design and user experience analysis of AR intelligent virtual agents on smartphones'}, {'paperId': '44fb9cca6c785d7e5b505eb580fed3cfab362d36', 'title': 'A robotizáció térnyerésével kapcsolatos attitűdök Magyarországon = Attitudes towards the rise of robotization in Hungary'}, {'paperId': '996cc13dfd70a9d6df5e3e0d509dd195d71d73eb', 'title': 'Toward a design theory for virtual companionship'}, {'paperId': '0a4380de627d506b440520929caa041fb864da6b', 'title': '“I Am in Your Computer While We Talk to Each Other” a Content Analysis on the Use of Language-Based Strategies by Humans and a Social Chatbot in Initial Human-Chatbot Interactions'}, {'paperId': 'aef9217ce9f373b4fec893f684cd84a820dc5101', 'title': 'TRUST AND CYBERSICKNESS IN VR-MARKETING - INVESTIGATING IPD AND CYBERSICKNESS, AND THEIR EFFECTS ON TRUST, CUSTOMER VALUE, NPS, CROSS- AND UP-SELLING'}, {'paperId': 'd53e0c833c6104fd0eae7aef919e1bfbeb0360e9', 'title': 'Select your character: Individual needs and avatar choice.'}, {'paperId': 'f3f112c6c3273e6ae16568664135ab64497cfb38', 'title': 'Power of Gijinka: Designing Virtual Teachers for Ecosystem Conservation Education'}, {'paperId': '9230d75e83ea2c4c4af354aec800d6481f68f883', 'title': 'Using a computer-tailored COPD screening assessment to promote advice-seeking behaviors'}, {'paperId': '7b02ef63a9822e54e53cdb14d165dde8921f943f', 'title': 'Effects of Social Competence on Social Use of AI Chatbots and Self-Disclosure : Mediation Effects of Loneliness and Perceived Humanlikeness of AI Chatbots'}, {'paperId': '2e4ddd75d4de91402d4705ba4f186214deb67790', 'title': 'Effect of Hidden Vector on the Speech of PRVA'}, {'paperId': 'ceeff4c559bedc9e158fa1da7ba348914ca6995c', 'title': ""The Effects of Warmth and Competence Perceptions on Users' Choice of an AI System""}, {'paperId': '54fe3df11c93a777c95fa932cb1784a98f55c345', 'title': 'Use of offensive language in human-artificial intelligence chatbot interaction: The effects of ethical ideology, social competence, and perceived humanlikeness'}, {'paperId': 'ac1338719a47ac345eb6af8fed66513013184c66', 'title': ""User Perceptions of an Intelligent Personal Assistant's Personality: The Role of Interaction Context""}, {'paperId': '7a78375386c798f2ddcbb86fea1599e60068df2e', 'title': 'WEBVR VS. PICTURES OF PRODUCTS IN E-COMMERCE: EFFECTS ON TRUST, PERCEIVED EASE OF USE, PERCEIVED USEFULNESS, AND INTENTION TO USE'}, {'paperId': 'e783ba47bea4b121673c74f2757399ab82e53f43', 'title': 'Robots are friends as well as foes: Ambivalent attitudes toward mindful and mindless AI robots in the United States and China'}, {'paperId': '570f88934ac2b29911d626a07f30d425980d8ffc', 'title': 'Age-related changes in gaze behaviour during social interaction: An eye-tracking study with an embodied conversational agent'}, {'paperId': 'a9eb4941298462c4a91f2ff128ddbd8f4a658100', 'title': 'Can we be friends with Mitsuku? A longitudinal study on the process of relationship formation between humans and a social chatbot'}, {'paperId': '2bf3571ff7ff872991802da583a24d8a7105e42a', 'title': 'Impact of auditory sense on trust and brand affect through auditory social interaction and control'}, {'paperId': '11db02b4f2a77c0464b6091c6ee073a4541971ea', 'title': 'The impact of chatbot conversational skill on engagement and perceived humanness'}, {'paperId': '58751dabf6812284610e1123b3c10db852497a43', 'title': 'Computational Study of Primitive Emotional Contagion in Dyadic Interactions'}, {'paperId': '7598e7b5957342fd848d59185b30224d6aee2eb2', 'title': 'Reducing Task Load with an Embodied Intelligent Virtual Assistant for Improved Performance in Collaborative Decision Making'}, {'paperId': 'be5f512824d6df7ba76a04d38cae4e3128b3b739', 'title': 'Fostering fashion retail experiences through virtual reality and voice assistants'}, {'paperId': '24a07f6f1f30477e64ae1c49a662e4ecd29797dc', 'title': 'Artificial Intelligence in Conversational Agents: A Study of Factors Related to Perceived Humanness in Chatbots'}, {'paperId': 'bae607617a09b099d48984289d73cd0f13b332f2', 'title': 'A Taxonomy of Social Cues for Conversational Agents'}, {'paperId': 'bd9df4e5bd98d31e0af099a5114e962426d37ff9', 'title': 'Blended Agents: Manipulation of Physical Objects within Mixed Reality Environments and Beyond'}, {'paperId': '07f542173fac4914a55c859975e93cfed8058f89', 'title': 'The black sheep effect: The case of the deviant ingroup robot'}, {'paperId': '01349916f9050ddda26fc46004f7b0776be1bb3a', 'title': 'Effects of virtual agent and object representation on experiencing exhibited artifacts'}, {'paperId': '3545fcc053d80b3861d9d268f06790efd3f19286', 'title': 'I Believe in a Thing Called Bot: Perceptions of the Humanness of “Chatbots”'}, {'paperId': '5d09c012108ce1710b1119e1564698cdbab3bffb', 'title': 'Social Touch in Human-agent Interactions in an Immersive Virtual Environment'}, {'paperId': 'c3cb6cc94cd3145e18e994d25b2bd8f499cb1ae3', 'title': 'Does a Digital Assistant Need a Body? The Influence of Visual Embodiment and Social Behavior on the Perception of Intelligent Virtual Agents in AR'}, {'paperId': 'bf750dffb4385685f59bebb085e5eaae6d20786a', 'title': 'Perceptual Validation for the Generation of Expressive Movements from End-Effector Trajectories'}, {'paperId': 'ea3ce10544768bcaba93e6ffc61e3389cf1513c6', 'title': ""Data-centered persuasion: Nudging user's prosocial behavior and designing social innovation""}, {'paperId': '1e02905d765faa8cb485fa67030ebd7883d3a594', 'title': ""Interaction corporelle évolutive entre un humain et un personnage virtuel. Proposition d'un modèle théorique et de son évaluation dans le cadre d'un exergame de fitness""}, {'paperId': '942f519cd7751981d4e173a4e8af1e8cf0d4d990', 'title': 'Reflecting and Shaping the Self through Avatars: The Relationship between Avatars, Identity, and Personal Needs'}, {'paperId': 'f909c439b27f6a4036f51712e910de77828d7aa9', 'title': 'Neural systems for evaluating speaker (Un)believability'}, {'paperId': '61e529e1259f0bdb6b3650d7bae173f51b373af9', 'title': 'Towards Interactional Symbiosis: Epistemic Balance and Co-presence in a Quantified Self Experiment'}, {'paperId': '9cd7f779c28d022339b657e2b01cc1f8e4dc9d7d', 'title': 'Identifying Human-Like Qualities for Non-player Characters'}, {'paperId': '5e5609d01c737e6e74e14aae38d262c7d6b30439', 'title': 'Position-based Skin Deformations for Interactive Character Animation'}, {'paperId': '764e1560eb5a6d05cfca459adf52a8fbd989509a', 'title': 'Affect in Embodied Pedagogical Agents'}, {'paperId': '24126a960721633c48704a5cd790964c7bb8f86e', 'title': 'Human Capacities for Emotion Recognition and their Implications for Computer Vision'}, {'paperId': '16845d447bbaa91580a22eb9934d6105ffdd5ded', 'title': ""Amae and agency appraisal as japanese emotional behavior: influences on agent's believability""}, {'paperId': 'de4ec182980a04ae4d6a2d432f324e121de8f5b9', 'title': 'A Joint Activity Theory Analysis of Body Interactions in Multiplayer Virtual Basketball'}, {'paperId': '01ee5ba471fbe8f0e57af5f0a565ed2bf069e5d2', 'title': 'Investigating Macroexpressions and Microexpressions in Computer Graphics Animated Faces'}, {'paperId': 'd78501d68466ede97e6d5f3c20713344076f2f87', 'title': 'Encyclopedia of Information Science and Technology, Third Edition'}, {'paperId': 'a7f283d5fa316b4bde353c320a9848c82332af9a', 'title': 'A framework for the assessment of synthetic personalities according to user perception'}, {'paperId': 'b81899031b20e4c41c52e6c4ec084d5c35789dd5', 'title': 'Sharing Secrets with Robots'}, {'paperId': 'd3dc6af64d4311e66ca53c9880f44418f8c2174f', 'title': 'Stereotyping human-like virtual influencers in retailing: Does warmth prevail over competence?'}, {'paperId': 'f6d056921f22c5bab5e04da2b555551f8b27742d', 'title': 'How Can I Help You? The Influence of Situation and Hostile Sexism on Perception of Appropriate Gender of Conversational Agents'}, {'paperId': '6af48eee056e58abc204516d981f14661abb307e', 'title': 'Chatbots, Conversational Interfaces, and the Stereotype Content Model'}, {'paperId': 'b4dc57b4aa3ae46c65a389ef2f4d4e5d6ba62217', 'title': 'Dynamic Creation of Points of View'}, {'paperId': '47d198c161a72ec2fbfd6575ab74fba1f54455f8', 'title': 'Theory: Foundations of Quality in Natural and Synthesized Speech'}, {'paperId': 'c1e3b89480ddf56c4015c524933eb00da3379ff0', 'title': 'An Investigation of Conversational Agent Relevance, Presence, and Engagement'}, {'paperId': '0543c09bcb1fa68936102d893a4f3ff91ae1a432', 'title': 'Effects of Embodiment on Generic and Content-Specific Intelligent Virtual Agents as Exhibition Guides'}, {'paperId': '8c7211aafa0cf6feabba64f7c9ff45e6e40295f3', 'title': 'A more human side of a chatbot . Analysing anthropomorphism in conversations with a virtual agent depending on the level of elicited agent knowledge'}, {'paperId': 'cfe08285e7a146ce7ddafd6e077cb402915d00d0', 'title': 'Believability and Co-presence in Human-Virtual Character Interaction'}, {'paperId': '9d840a1ffb3ce51c611cd543e1f31a96d3efbf0e', 'title': 'The Effects of Anticipation in WALL·E (2008)'}, {'paperId': '4880434b8efc9a73db45b47b89f04df533420b51', 'title': 'Facial Expressions of Emotions for Virtual Characters'}, {'paperId': '5c106fb350db83bc8a6e7531504d47332d5e000a', 'title': 'Understanding Educational Potential and Value of Affective Computing'}, {'paperId': 'ce8950d526e215b62175a81876a7a3428baa50ad', 'title': 'Predicting intention to work with social robots'}, {'paperId': '825f5b44876a681467ff786c37733cc854b661ce', 'title': 'Students’ Experiences of Emotional Connection with Pedagogical Agents'}, {'paperId': '6639f01446384470401677d7bec44743c9a2de62', 'title': 'Un ACA sincère, affectif et expressif comme compagnon artificiel'}, {'paperId': '0ae349aeaa7c370ce3468fcf6d08c5a110401ca2', 'title': 'Interaction style and specification of the occasional user of digital interfaces : perspectives from interdisciplinary assessment of virtual agent spatial guidance'}, {'paperId': '8e0df3e86481595ada70120aecb99b9283a1cd49', 'title': 'Beyond embodiment and social presence: preferences for virtual assistant gender and clothing style'}, {'paperId': None, 'title': 'AP-PPMJ210052 1..10'}]","{'name': 'PRESENCE: Teleoperators and Virtual Environments', 'pages': '431-448', 'volume': '20'}",44.0,"How Is Believability of a Virtual Agent Related to Warmth, Competence, Personification, and Embodiment?",2011.0
678,373c53f423fbfd4726f333d8a4c8ce1556cf758a,,"[{'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '2803283', 'name': 'P. McOwan'}]",10.0,"{'bibtex': '@Inproceedings{Mancini2011EvaluatingTC,\n author = {M. Mancini and Ginevra Castellano and Christopher E. Peters and P. McOwan},\n pages = {215-224},\n title = {Evaluating the Communication of Emotion via Expressive Gesture Copying Behaviour in an Embodied Humanoid Agent},\n year = {2011}\n}\n'}",,{'pages': '215-224'},25.0,Evaluating the Communication of Emotion via Expressive Gesture Copying Behaviour in an Embodied Humanoid Agent,2011.0
679,375243fc58bf2319894703111e52b6d8ad2ec111,"Issues of reliability, item latent structure, and faking on the Holden Psychological Screening Inventory (HPSI), the Brief Symptom Inventory (BSI), and the Balanced Inventory of Desirable Responding (BIDR) were examined with a sample of 300 university undergraduates. Reliability analyses indicated that scales from all inventories had acceptable internal consistency. Confirmatory item principal component analyses supported the structures and scoring keys of the HPSI and the BIDR, but not the BSI. Although all inventories were susceptible to faking, validity indices of the HPSI and the BIDR could correctly classify over two-thirds of test respondents as either responding honestly or as faking.","[{'authorId': '40252192', 'name': 'R. Holden'}, {'authorId': '38956355', 'name': 'Katherine B. Starzyk'}, {'authorId': '114697267', 'name': 'L. McLeod'}, {'authorId': '153196106', 'name': 'Melanie J. Edwards'}]",48.0,"{'bibtex': '@Article{Holden2000ComparisonsAT,\n author = {R. Holden and Katherine B. Starzyk and L. McLeod and Melanie J. Edwards},\n journal = {Assessment},\n pages = {163 - 175},\n title = {Comparisons among the Holden Psychological Screening Inventory (HPSI), the Brief Symptom Inventory (BSI), and the Balanced Inventory of Desirable Responding (BIDR)},\n volume = {7},\n year = {2000}\n}\n'}",,"{'volume': '7', 'pages': '163 - 175', 'name': 'Assessment'}",21.0,"Comparisons among the Holden Psychological Screening Inventory (HPSI), the Brief Symptom Inventory (BSI), and the Balanced Inventory of Desirable Responding (BIDR)",2000.0
680,375e201aa93dcff804959e70c0ae7a2d44593ae6,"Experiencing emotional distress is the number one reason why people who are undergoing behaviour modification (e.g. quitting smoking, dieting) suffer from relapses. Providing emotional support is an effective way to help them overcome the unpleasant effects of negative affect and adhere to their regimen. Building computers with such ability has grabbed the attention of the HCI community in recent years. Early research has shown some promising results when adopting strategies of how we comfort others, but many questions on how to build such systems remain unanswered. This paper presents the results of a 2 (modality: animated vs. no visual) by 3 (intervention: non-empathy vs. empathy vs. empathy and expressivity) between-subjects study that investigates the impact of two important factors and their interaction in the design of such systems: (1) different ways of expressing empathy, and (2) the modality of delivering such content. Findings and implications for the design of empathic computer systems are discussed and directions for future research are suggested.","[{'authorId': '2110579362', 'name': 'Hien Nguyen'}, {'authorId': '145428594', 'name': 'J. Masthoff'}]",52.0,"{'bibtex': '@Inproceedings{Nguyen2009DesigningEC,\n author = {Hien Nguyen and J. Masthoff},\n pages = {7},\n title = {Designing empathic computers: the effect of multimodal empathic feedback using animated agent},\n year = {2009}\n}\n'}",,{'pages': '7'},16.0,Designing empathic computers: the effect of multimodal empathic feedback using animated agent,2009.0
681,376973681a3e717736625bd4e680515b909cf339,,"[{'authorId': '11269472', 'name': 'I. Masi'}, {'authorId': '2077431021', 'name': 'A. Tran'}, {'authorId': '1756099', 'name': 'Tal Hassner'}, {'authorId': '2955822', 'name': 'J. Leksut'}, {'authorId': '3463966', 'name': 'G. Medioni'}]",352.0,"{'bibtex': '@Article{Masi2016DoWR,\n author = {I. Masi and A. Tran and Tal Hassner and J. Leksut and G. Medioni},\n journal = {ArXiv},\n title = {Do We Really Need to Collect Millions of Faces for Effective Face Recognition?},\n volume = {abs/1603.07057},\n year = {2016}\n}\n'}",,"{'volume': 'abs/1603.07057', 'name': 'ArXiv'}",50.0,Do We Really Need to Collect Millions of Faces for Effective Face Recognition?,2016.0
682,376cf14b5bf4d0e636233d4344757e5e44cda285,,"[{'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",828.0,"{'bibtex': '@Article{Picard2003AffectiveCC,\n author = {Rosalind W. Picard},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {55-64},\n title = {Affective computing: challenges},\n volume = {59},\n year = {2003}\n}\n'}",,"{'volume': '59', 'pages': '55-64', 'name': 'Int. J. Hum. Comput. Stud.'}",18.0,Affective computing: challenges,2003.0
683,37bdb0c35d00d3af6201b6f5f56e751ff09e601a,"Objective: We tested the hypothesis that automation errors on tasks easily performed by humans undermine trust in automation. Background: Research has revealed that the reliability of imperfect automation is frequently misperceived. We examined the manner in which the easiness and type of imperfect automation errors affect trust and dependence. Method: Participants performed a target detection task utilizing an automated aid. In Study 1, the aid missed targets either on easy trials (easy miss group) or on difficult trials (difficult miss group). In Study 2, we manipulated both easiness and type of error (miss vs. false alarm). The aid erred on either difficult trials alone (difficult errors group) or on difficult and easy trials (easy miss group; easy false alarm group). Results: In both experiments, easy errors led to participants mistrusting and disagreeing more with the aid on difficult trials, as compared with those using aids that generated only difficult errors. This resulted in a downward shift in decision criterion for the former, leading to poorer overall performance. Misses and false alarms led to similar effects. Conclusion: Automation errors on tasks that appear “easy” to the operator severely degrade trust and reliance. Application: Potential applications include the implementation of system design solutions that circumvent the negative effects of easy automation errors.","[{'authorId': '2820508', 'name': 'P. Madhavan'}, {'authorId': '2732870', 'name': 'D. Wiegmann'}, {'authorId': '2823736', 'name': 'Frank C. Lacson'}]",231.0,"{'bibtex': '@Article{Madhavan2006AutomationFO,\n author = {P. Madhavan and D. Wiegmann and Frank C. Lacson},\n journal = {Human Factors: The Journal of Human Factors and Ergonomics Society},\n pages = {241 - 256},\n title = {Automation Failures on Tasks Easily Performed by Operators Undermine Trust in Automated Aids},\n volume = {48},\n year = {2006}\n}\n'}",,"{'volume': '48', 'pages': '241 - 256', 'name': 'Human Factors: The Journal of Human Factors and Ergonomics Society'}",44.0,Automation Failures on Tasks Easily Performed by Operators Undermine Trust in Automated Aids,2006.0
684,37c02744688401c6624d2de49ba22b9d6506caaf,"Recent progress in the study of attention and performance is discussed, focusing on the nature of attentional control and the effects of practice. Generally speaking, the effects of mental set are proving more pervasive than was previously suspected, whereas automaticity is proving less robust. Stimulus attributes (e.g. onsets, transients) thought to have a ""wired-in"" ability to capture attention automatically have been shown to capture attention only as a consequence of voluntarily adopted task sets. Recent research suggests that practice does not have as dramatic effects as is commonly believed. While it may turn out that some mental operations are automatized in the strongest sense, this may be uncommon. Recent work on task switching is also described; optimal engagement in a task set is proving to be intimately tied to learning operations triggered by the actual performance of a new task, not merely the anticipation of such performance.","[{'authorId': '2310380', 'name': 'H. Pashler'}, {'authorId': '143607120', 'name': 'J. C. Johnston'}, {'authorId': '2159745', 'name': 'E. Ruthruff'}]",637.0,"{'bibtex': '@Article{Pashler2001AttentionAP,\n author = {H. Pashler and J. C. Johnston and E. Ruthruff},\n journal = {Annual review of psychology},\n pages = {\n          629-51\n        },\n title = {Attention and performance.},\n volume = {52},\n year = {2001}\n}\n'}",,"{'volume': '52', 'pages': '\n          629-51\n        ', 'name': 'Annual review of psychology'}",84.0,Attention and performance.,2001.0
685,37d116d50af56473b58c0c79a7ea6276cb142680,"Comme il Faut is an artificial intelligence system and authoring strategy for creating game-based interactive stories about relationships and social interactions between characters. Using the abstraction of the social game, Comme il Faut creates experiences where specific dramatic interactions between characters arise from play. This paper describes the process of authoring for Comme il Faut. Specifically, we will describe the authoring and design considerations for Comme il Faut's inaugural game, The Prom. We discuss how we extracted and encoded an exaggerated social logic from pre-existing media experiences to create its intended story space, developed an idiosyncratic local culture for its story world, defined a set of character histories and personalities to be revealed through play, and authored the specific lines of dialogue and motivating social situations that give the audience experience of The Prom its particular character. Together, these produce an experience with much greater fictional specificity than in open-ended simulation games and many more options for what happens (and how things happen) than in traditional game stories.","[{'authorId': '38523727', 'name': 'Joshua Mccoy'}, {'authorId': '2952013', 'name': 'Mike Treanor'}, {'authorId': '144253835', 'name': 'B. Samuel'}, {'authorId': '2663075', 'name': 'B. Tearse'}, {'authorId': '114402462', 'name': 'Michael Mateas'}]",56.0,"{'bibtex': '@Inproceedings{Mccoy2010AuthoringGI,\n author = {Joshua Mccoy and Mike Treanor and B. Samuel and B. Tearse and Michael Mateas},\n title = {Authoring Game-based Interactive Narrative using Social Games and Comme il},\n year = {2010}\n}\n'}",,,7.0,Authoring Game-based Interactive Narrative using Social Games and Comme il,2010.0
686,37f2ba71dd16f26971e78ea824e3e36bece9ba3c,,"[{'authorId': '13474685', 'name': 'Christopher K. Frantz'}, {'authorId': '2251921', 'name': 'G. Pigozzi'}]",5.0,"{'bibtex': '@Article{Frantz2018ModelingND,\n author = {Christopher K. Frantz and G. Pigozzi},\n journal = {FLAP},\n pages = {491-564},\n title = {Modeling Norm Dynamics in Multiagent Systems},\n volume = {5},\n year = {2018}\n}\n'}",,"{'volume': '5', 'pages': '491-564', 'name': 'FLAP'}",0.0,Modeling Norm Dynamics in Multiagent Systems,2018.0
687,38313dd4fbe9108e602cc70b76e35fd425cea5a2,"In this paper, an intelligent navigation system for autonomous land vehicles (ALV) using hierarchical terrain representation has been developed which can successfully negotiate an obstacle and threat-laden terrain, even if nothing is known beforehand about the terrain. The ALV stores new information in its memory as it travels, has the ability to backtrack out of unexpected dead ends, and performs spontaneous decision-making in the field based on local sensor readings. The optimal global route of the ALV journey is obtained using dynamic programming, and decision-making is accomplished via a production rule-based system. Execution examples demonstrate the power of the prototype system to solving navigation problems. This establishes the feasibility of constructing a valid ALV by combining search techniques with artificial intelligence tools such as production rule-based systems.","[{'authorId': '3453694', 'name': 'Nark B. Metea'}, {'authorId': '145118476', 'name': 'J. Tsai'}]",22.0,"{'bibtex': '@Article{Metea1987RoutePF,\n author = {Nark B. Metea and J. Tsai},\n journal = {Proceedings. 1987 IEEE International Conference on Robotics and Automation},\n pages = {1947-1952},\n title = {Route planning for intelligent autonomous land vehicles using hierarchical terrain representation},\n volume = {4},\n year = {1987}\n}\n'}",,"{'volume': '4', 'pages': '1947-1952', 'name': 'Proceedings. 1987 IEEE International Conference on Robotics and Automation'}",6.0,Route planning for intelligent autonomous land vehicles using hierarchical terrain representation,1987.0
688,3839a6a41e863151e380e3aa80fdbe594231a8dc,,"[{'authorId': '2239842555', 'name': 'W. Scott Neal Reilly'}]",44.0,"{'bibtex': '@Inproceedings{Reilly1997AMF,\n author = {W. Scott Neal Reilly},\n pages = {114-121},\n title = {A methodology for building believable social agents},\n year = {1997}\n}\n'}",,{'pages': '114-121'},14.0,A methodology for building believable social agents,1997.0
690,3840ecff1e0a35359be3409612df7f12b251aad8,"Apathy is a common neuropsychiatric symptom in Alzheimer's disease dementia and amnestic mild cognitive impairment and is associated with cortical atrophy in Alzheimer's disease dementia. This study investigated possible correlations between apathy and cortical atrophy in 47 individuals with mild cognitive impairment and 19 clinically normal elderly. Backward elimination multivariate linear regression was used to evaluate the cross-sectional relationship between scores on the Apathy Evaluation Scale and thickness of several cortical regions and covariates. Lower inferior temporal cortical thickness was predictive of greater apathy. Greater anterior cingulate cortical thickness was also predictive of greater apathy, suggesting an underlying reactive process.","[{'authorId': '6855495', 'name': 'B. Guercio'}, {'authorId': '4181598', 'name': 'Nancy J. Donovan'}, {'authorId': '1946495146', 'name': 'Andrew M. Ward'}, {'authorId': '2442664', 'name': 'A. Schultz'}, {'authorId': '4001133', 'name': 'Natacha Lorius'}, {'authorId': '5217438', 'name': 'R. Amariglio'}, {'authorId': '2147040', 'name': 'D. Rentz'}, {'authorId': '32207254', 'name': 'Keith A. Johnson'}, {'authorId': '2289371', 'name': 'R. Sperling'}, {'authorId': '4205480', 'name': 'G. Marshall'}]",53.0,"{'bibtex': '@Article{Guercio2015ApathyIA,\n author = {B. Guercio and Nancy J. Donovan and Andrew M. Ward and A. Schultz and Natacha Lorius and R. Amariglio and D. Rentz and Keith A. Johnson and R. Sperling and G. Marshall},\n journal = {The Journal of neuropsychiatry and clinical neurosciences},\n pages = {\n          e22-7\n        },\n title = {Apathy is associated with lower inferior temporal cortical thickness in mild cognitive impairment and normal elderly individuals.},\n volume = {27 1},\n year = {2015}\n}\n'}",,"{'volume': '27 1', 'pages': '\n          e22-7\n        ', 'name': 'The Journal of neuropsychiatry and clinical neurosciences'}",41.0,Apathy is associated with lower inferior temporal cortical thickness in mild cognitive impairment and normal elderly individuals.,2015.0
691,385b930425a8094fcad9823391f86d46b797b475,"The game nicknamed ‘Prisoner’s Dilemma’ by A.W. Tucker has attracted wide attention, doubtless because it has raised doubts about the universal applicability of the so called Sure-thing Principle as a principle of rational decision.","[{'authorId': '48184297', 'name': 'A. Rapoport'}, {'authorId': '48698892', 'name': 'Albert M. Chammah'}]",861.0,"{'bibtex': ""@Inproceedings{Rapoport1965PrisonersD,\n author = {A. Rapoport and Albert M. Chammah},\n title = {Prisoner's Dilemma},\n year = {1965}\n}\n""}",,"{'volume': '', 'name': ''}",1.0,Prisoner's Dilemma,1965.0
692,386a9e874a485f9152d6bab21691454aa9912d49,"The “behavioral approach system” (BAS) (Gray, 1990) has been primarily associated with reward processing and positive affect. However, additional research has demonstrated that the BAS plays a role in aggressive behavior, heightened experience of anger, and increased attention to facial signals of aggression. Using functional magnetic resonance imaging, we show that variation in the BAS trait in healthy participants predicts activation in neural regions implicated in aggression when participants view facial signals of aggression in others. Increased BAS drive (appetitive motivation) was associated with increased amygdala activation and decreased ventral anterior cingulate and ventral striatal activation to facial signals of aggression, relative to sad and neutral expressions. In contrast, increased behavioral inhibition was associated with increased activation in the dorsal anterior cingulate, a region involved in the perception of fear and threat. Our results provide the first demonstration that appetitive motivation constitutes a significant factor governing the function of neural regions implicated in aggression, and have implications for understanding clinical disorders of aggression.","[{'authorId': '2814252', 'name': 'J. Beaver'}, {'authorId': '145422979', 'name': 'A. Lawrence'}, {'authorId': '2946383', 'name': 'L. Passamonti'}, {'authorId': '2825775', 'name': 'A. Calder'}]",138.0,"{'bibtex': '@Article{Beaver2008AppetitiveMP,\n author = {J. Beaver and A. Lawrence and L. Passamonti and A. Calder},\n journal = {The Journal of Neuroscience},\n pages = {2719 - 2725},\n title = {Appetitive Motivation Predicts the Neural Response to Facial Signals of Aggression},\n volume = {28},\n year = {2008}\n}\n'}",,"{'volume': '28', 'pages': '2719 - 2725', 'name': 'The Journal of Neuroscience'}",80.0,Appetitive Motivation Predicts the Neural Response to Facial Signals of Aggression,2008.0
693,38a0fa90a395661f51f749cf42ce5f5de2c41d98,"That being said, the book does contain information that deals in all facets of animation, and, sometimes, on the wider scale of dramatic and literary techniques relating to the visual medium. Since animation is essentially a unification of film, acting, drawing, and writing, techniques from all four are important. This book does contain the twelve principles of animation essentially, several key elements to any good animation, which are employed by any good animator, in anything from Disneys Steamboat Willie to Pixars Up.","[{'authorId': '145094550', 'name': 'F. Thomas'}, {'authorId': '145812012', 'name': 'Ollie Johnston'}]",868.0,"{'bibtex': '@Inproceedings{Thomas1981TheIO,\n author = {F. Thomas and Ollie Johnston},\n title = {The illusion of life : Disney animation},\n year = {1981}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The illusion of life : Disney animation,1981.0
694,38a20b885d52d364d388f9f83d3941c0f874f322,"OBJECTIVE
This research focuses on the potential ability of animated avatars (a digital representation of the user) and virtual agents (a digital representation of a coach, buddy, or teacher) to deliver computer-based interventions for adolescents' chronic weight management. An exploration of the acceptance and desire of teens to interact with avatars and virtual agents for self-management and behavioral modification was undertaken.


MATERIALS AND METHODS
The utilized approach was inspired by community-based participatory research. Data was collected from 2 phases: Phase 1) focus groups with teens, provider interviews, parent interviews; and Phase 2) mid-range prototype assessment by teens and providers.


RESULTS
Data from all stakeholder groups expressed great interest in avatars and virtual agents assisting self-management efforts. Adolescents felt the avatars and virtual agents could: 1) reinforce guidance and support, 2) fit within their lifestyle, and 3) help set future goals, particularly after witnessing the effect of their current behavior(s) on the projected physical appearance (external and internal organs) of avatars. Teens wanted 2 virtual characters: a virtual agent to act as a coach or teacher and an avatar (extension of themselves) to serve as a ""buddy"" for empathic support and guidance and as a surrogate for rewards. Preferred modalities for use include both mobile devices to accommodate access and desktop to accommodate preferences for maximum screen real estate to support virtualization of functions that are more contemplative and complex (e.g., goal setting). Adolescents expressed a desire for limited co-user access, which they could regulate. Data revealed certain barriers and facilitators that could affect adoption and use.


DISCUSSION
The current study extends the support of teens, parents, and providers for adding avatars or virtual agents to traditional computer-based interactions. Data supports the desire for a personal relationship with a virtual character in support of previous studies. The study provides a foundation for further work in the area of avatar-driven motivational interviewing.


CONCLUSIONS
This study provides evidence supporting the use of avatars and virtual agents, designed using participatory approaches, to be included in the continuum of care. Increased probability of engagement and long-term retention of overweight, obese adolescent users and suggests expanding current chronic care models toward more comprehensive, socio-technical representations.","[{'authorId': '1750842', 'name': 'C. LeRouge'}, {'authorId': '3343024', 'name': 'Kathryn Dickhut'}, {'authorId': '1779199', 'name': 'C. Lisetti'}, {'authorId': '3459376', 'name': 'Savitha Sangameswaran'}, {'authorId': '3459243', 'name': 'T. Malasanos'}]",51.0,"{'bibtex': '@Article{LeRouge2016EngagingAI,\n author = {C. LeRouge and Kathryn Dickhut and C. Lisetti and Savitha Sangameswaran and T. Malasanos},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n pages = {\n          19-28\n        },\n title = {Engaging adolescents in a computer-based weight management program: avatars and virtual coaches could help},\n volume = {23 1},\n year = {2016}\n}\n'}",,"{'volume': '23 1', 'pages': '\n          19-28\n        ', 'name': 'Journal of the American Medical Informatics Association : JAMIA'}",40.0,Engaging adolescents in a computer-based weight management program: avatars and virtual coaches could help,2016.0
695,38a9f63f23f33084ddeb08006fb118fe751480fe,"In this paper we introduce ALMA - A Layered Model of Affect. It integrates three major affective characteristics: emotions, moods and personality that cover short, medium, and long term affect. The use of this model consists of two phases: In the preparation phase appraisal rules and personality profiles for characters must be specified with the help of AffectML - our XML based affect modeling language. In the runtime phase, the specified appraisal rules are used to compute real-time emotions and moods as results of a subjective appraisal of relevant input. The computed affective characteristics are represented in AffectML and can be processed by sub-sequent modules that control the cognitive processes and physical behavior of embodied conversational characters. ALMA is part of the VirtualHuman project which develops interactive virtual characters that serve as dialog partners with human-like conversational skills. ALMA provides our virtual humans with a personality profile and with real-time emotions and moods. These are used by the multimodal behavior generation module to enrich the lifelike and believable qualities.","[{'authorId': '48785659', 'name': 'Patrick Gebhard'}]",466.0,"{'bibtex': '@Inproceedings{Gebhard2005ALMAAL,\n author = {Patrick Gebhard},\n pages = {29-36},\n title = {ALMA: a layered model of affect},\n year = {2005}\n}\n'}",,{'pages': '29-36'},31.0,ALMA: a layered model of affect,2005.0
702,38bb261f5c0f7d5d150e7b306deec46544729908,,"[{'authorId': '2064945', 'name': 'A. Moors'}, {'authorId': '4367292', 'name': 'P. Ellsworth'}, {'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '49584958', 'name': 'N. Frijda'}]",934.0,"{'bibtex': '@Article{Moors2013AppraisalTO,\n author = {A. Moors and P. Ellsworth and K. Scherer and N. Frijda},\n journal = {Emotion Review},\n pages = {119 - 124},\n title = {Appraisal Theories of Emotion: State of the Art and Future Development},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '119 - 124', 'name': 'Emotion Review'}",71.0,Appraisal Theories of Emotion: State of the Art and Future Development,2013.0
703,38c465d415ca2b3f5225f36230347c8ebbd89768,,"[{'authorId': '33432486', 'name': 'Mei Si'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '1748597', 'name': 'D. Pynadath'}]",57.0,"{'bibtex': '@Inproceedings{Si2006ThespianMS,\n author = {Mei Si and S. Marsella and D. Pynadath},\n pages = {369-382},\n title = {Thespian: Modeling Socially Normative Behavior in a Decision-Theoretic Framework},\n year = {2006}\n}\n'}",,{'pages': '369-382'},26.0,Thespian: Modeling Socially Normative Behavior in a Decision-Theoretic Framework,2006.0
704,3906485099c38d66ca4190e89fb308b08ad03bde,"Gandalf is a fully autonomous, humanoid agent that perceives a user's multimodal actsNspeech, prosody, manual gesture, body language, gazeNand generates appropriate multimodal responses to these in real-time (speech, gaze, facial & manual gesture & head movement). Gandalf has knowledge about the solar system and can travel to and tell users about the planets and moons with speech and gesture. Gandalf demonstrates a coherent framework for psychosoical dialogue skills which enables the production of concurrent reactive and reflective behaviors and planning of communicative acts, with response cycles analogous to those found in human face-to-face dialogue, from 1/6th of a second and up. Gandalf has been tested in interaction with humans and shown to be capable of supporting and sustaining multimodal, situated, real-time dialogue. Content Areas: Interaction between people and agents, face-to-face communication, action selection and planning, real-time performance, synthetic agents.","[{'authorId': '1727838', 'name': 'K. Thórisson'}]",97.0,"{'bibtex': '@Inproceedings{Thórisson1997GandalfAE,\n author = {K. Thórisson},\n pages = {536-537},\n title = {Gandalf: an embodied humanoid capable of real-time multimodal dialogue with people},\n year = {1997}\n}\n'}",,{'pages': '536-537'},7.0,Gandalf: an embodied humanoid capable of real-time multimodal dialogue with people,1997.0
705,3921f459a9ee26827963abc4abf013b4cc9cbd32,"We present an example‐based crowd simulation technique. Most crowd simulation techniques assume that the behavior exhibited by each person in the crowd can be defined by a restricted set of rules. This assumption limits the behavioral complexity of the simulated agents. By learning from real‐world examples, our autonomous agents display complex natural behaviors that are often missing in crowd simulations. Examples are created from tracked video segments of real pedestrian crowds. During a simulation, autonomous agents search for examples that closely match the situation that they are facing. Trajectories taken by real people in similar situations, are copied to the simulated agents, resulting in seemingly natural behaviors.","[{'authorId': '39149037', 'name': 'Alon Lerner'}, {'authorId': '1706408', 'name': 'Y. Chrysanthou'}, {'authorId': '1684384', 'name': 'Dani Lischinski'}]",894.0,"{'bibtex': '@Article{Lerner2007CrowdsBE,\n author = {Alon Lerner and Y. Chrysanthou and Dani Lischinski},\n journal = {Computer Graphics Forum},\n title = {Crowds by Example},\n volume = {26},\n year = {2007}\n}\n'}",,"{'volume': '26', 'name': 'Computer Graphics Forum'}",33.0,Crowds by Example,2007.0
706,3922d7f8862f89248c2a09533e8ccd130b35d1ed,"Numerous studies in social psychology have shown that familiarization across repeated interactions improves people’s perception of the other. If and how these findings relate to human-robot interaction (HRI) is not well understood, even though such knowledge is crucial when pursuing long-term interactions. In our work, we investigate the persistence of first impressions by asking 49 participants to play a geography game with a robot. We measure how their perception of the robot changes over three sessions with three to ten days of zero exposure in between. Our results show that different perceptual dimensions stabilize within different time frames, with the robot’s competence being the fastest to stabilize and perceived threat the most fluctuating over time. We also found evidence that perceptual differences between robots with varying levels of humanlikeness persist across repeated interactions. This study has important implications for HRI design as it sheds new light on the influence of robots’ embodiment and interaction abilities. Moreover, it also impacts HRI theory as it presents novel findings contributing to research on the uncanny valley and robot perception in general. CCS CONCEPTS •Human-centered computing → Empirical studies in HCI; Natural language interfaces; •Computer systems organization →Robotics; •Computing methodologies →Intelligent agents. ACM Reference Format: Maike Paetzel, Giulia Perugia, and Ginevra Castellano. 2020. The Persistence of First Impressions: The Effect of Repeated Interactions on the Perception of a Social Robot. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (HRI ’20), March 23–26, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3319502.3374786","[{'authorId': '2710492', 'name': 'Maike Paetzel'}, {'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",55.0,"{'bibtex': '@Article{Paetzel2020ThePO,\n author = {Maike Paetzel and G. Perugia and Ginevra Castellano},\n journal = {2020 15th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {73-82},\n title = {The Persistence of First Impressions: The Effect of Repeated Interactions on the Perception of a Social Robot},\n year = {2020}\n}\n'}",,"{'pages': '73-82', 'name': '2020 15th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",61.0,The Persistence of First Impressions: The Effect of Repeated Interactions on the Perception of a Social Robot,2020.0
707,392446af8e704efcc2b795309275b3006a21ee5e,"ABSTRACT Emotional events tend to be remembered better than neutral events, but emotional states and stimuli may also interfere with cognitive processes that underlie memory performance. The current study investigated the effects of emotional content on working memory capacity (WMC), which involves both short term storage and executive attention control. We tested competing hypotheses in a preregistered experiment (N = 297). The emotional enhancement hypothesis predicts that emotional stimuli attract attention and additional processing resources relative to neutral stimuli, thereby making it easier to encode and store emotional information in WMC. The emotional impairment hypothesis, by contrast, predicts that emotional stimuli interfere with attention control and the active maintenance of information in working memory. Participants completed a common measure of WMC (the operation span task; Turner, M. L., & Engle, R. W. [1989]. Is working memory capacity task dependent? Journal of Memory and Language, 28, 127–154) that included either emotional or neutral words. Results revealed that WMC was reduced for emotional words relative to neutral words, consistent with the emotional impairment hypothesis.","[{'authorId': '19194781', 'name': 'Katie E Garrison'}, {'authorId': '4555303', 'name': 'B. Schmeichel'}]",22.0,"{'bibtex': '@Article{Garrison2019EffectsOE,\n author = {Katie E Garrison and B. Schmeichel},\n journal = {Cognition and Emotion},\n pages = {370 - 377},\n title = {Effects of emotional content on working memory capacity},\n volume = {33},\n year = {2019}\n}\n'}",,"{'volume': '33', 'pages': '370 - 377', 'name': 'Cognition and Emotion'}",33.0,Effects of emotional content on working memory capacity,2019.0
708,39271ddb2c8a40e5d4d8430d7e24b1498ef4f944,"Multiplicity of behavior features gives rise to its different interpretations (in addition to behavior vagueness and ambiguity typically studied in social cognition research). Particularly, identical actions are construable both in moral and competence-related categories due to distinct behavioral features underlying each of these interpretations. It was hypothesized that the two construals are alternatively used by the perceiver. Because of perspective-dependent differences in accessibility and applicability of competence and moral categories, it was hypothesized that actors interpret their own behavior in competence terms, whereas observers interpret it in moral categories, and that within the actor perspective, competence construal is used to a higher degree by male than female perceivers, but the opposite is true for moral construal","[{'authorId': '2585858', 'name': 'B. Wojciszke'}]",302.0,"{'bibtex': '@Article{Wojciszke1994MultipleMO,\n author = {B. Wojciszke},\n journal = {Journal of Personality and Social Psychology},\n pages = {222-232},\n title = {Multiple meanings of behavior: Construing actions in terms of competence or morality.},\n volume = {67},\n year = {1994}\n}\n'}",,"{'volume': '67', 'pages': '222-232', 'name': 'Journal of Personality and Social Psychology'}",34.0,Multiple meanings of behavior: Construing actions in terms of competence or morality.,1994.0
709,392917176507d775d840e9532f1beadd0b74cf27,,"[{'authorId': '144931305', 'name': 'R. Cooper'}, {'authorId': '144380860', 'name': 'M. Press'}]",11.0,"{'bibtex': '@Inproceedings{Cooper2016TheDE,\n author = {R. Cooper and M. Press},\n title = {The Design Experience},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Design Experience,2016.0
710,39447676a45d3ddb31bf19ab0c9885ed24a2d57d,,"[{'authorId': '25550816', 'name': 'A. L. Baylor'}]",162.0,"{'bibtex': '@Article{Baylor2011TheDO,\n author = {A. L. Baylor},\n journal = {Educational Technology Research and Development},\n pages = {291-300},\n title = {The design of motivational agents and avatars},\n volume = {59},\n year = {2011}\n}\n'}",,"{'volume': '59', 'pages': '291-300', 'name': 'Educational Technology Research and Development'}",52.0,The design of motivational agents and avatars,2011.0
712,394a647e390ab215cb5a30e861f80d3295a5a9bb,"The concepts of psychosocial and psychomotor inhibition characteristic of major depression are based primarily on clinical observations. It is possible to describe and define these two types of inhibition by means of a systematic, quantitative ethological (behavioral) approach, which singles out precise and significant behavior markers. This investigation focuses on the behavioral features of psychosocial and psychomotor inhibition in 11 hospitalized depressed subjects and their changes during clinical recovery. The hypothesis that major depression is characterized by a significant reduction of social interaction is tested (psycho-intellectual inhibition is not addressed). Results show significant behavioral differences between depressed and recovered subjects with depression being characterized by a significant reduction of social interaction, whereas self occupation and body mobility are reduced to a lesser degree. Behavior markers for depression include nonspecific gaze, withdrawal, no mouth movements, no eye region movements, and social inactivity. Behavior markers for recovery include socially interested, social smile, verbal social initiative, speech, nod, raised eyebrows, wrinkled eyebrows, social laughter, gesticulation, drum one's fingers, point, and help. Findings point to tendencies toward two types of major depression and two types of recovery. A companion paper (Schelde, this journal) addresses theoretical issues.","[{'authorId': '5450623', 'name': 'J. Schelde'}]",34.0,"{'bibtex': '@Article{Schelde1998MajorDB,\n author = {J. Schelde},\n journal = {The Journal of nervous and mental disease},\n pages = {\n          133-40\n        },\n title = {Major depression: behavioral markers of depression and recovery.},\n volume = {186 3},\n year = {1998}\n}\n'}",,"{'volume': '186 3', 'pages': '\n          133-40\n        ', 'name': 'The Journal of nervous and mental disease'}",0.0,Major depression: behavioral markers of depression and recovery.,1998.0
713,3974ebb9db25ae9e4644d7ceb5831e3376190d0a,"In this paper we present our work toward the creation of a multimodal expressive Embodied Conversational Agent (ECA). Our agent, called Greta, exhibits nonverbal behaviors synchronized with speech. We are using the taxonomy of communicative functions developed by Isabella Poggi [22] to specify the behavior of the agent. Based on this taxonomy a representation language, Affective Presentation Markup Language, APML has been defined to drive the animation of the agent [4]. Lately, we have been working on creating no longer a generic agent but an agent with individual characteristics. We have been concentrated on the behavior specification for an individual agent. In particular we have defined a set of parameters to change the expressivity of the agent's behaviors. Six parameters have been defined and implemented to encode gesture and face expressivity. We have performed perceptual studies of our expressivity model.","[{'authorId': '1703084', 'name': 'C. Pelachaud'}]",137.0,"{'bibtex': '@Article{Pelachaud2005MultimodalEE,\n author = {C. Pelachaud},\n journal = {Proceedings of the 13th annual ACM international conference on Multimedia},\n title = {Multimodal expressive embodied conversational agents},\n year = {2005}\n}\n'}",,{'name': 'Proceedings of the 13th annual ACM international conference on Multimedia'},29.0,Multimodal expressive embodied conversational agents,2005.0
715,399da68d3b97218b6c80262df7963baa89dcc71b,"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.","[{'authorId': '1762744', 'name': 'A. Stolcke'}]",5128.0,"{'bibtex': '@Inproceedings{Stolcke2002SRILMA,\n author = {A. Stolcke},\n pages = {901-904},\n title = {SRILM - an extensible language modeling toolkit},\n year = {2002}\n}\n'}",,{'pages': '901-904'},37.0,SRILM - an extensible language modeling toolkit,2002.0
716,39ddd2b1b678fadfa0e84c94ba319425c93ab97e,"Interfaces that talk and listen are populating computers, cars, call centers, and even home appliances and toys, but voice interfaces invariably frustrate rather than help. In Wired for Speech, Clifford Nass and Scott Brave reveal how interactive voice technologies can readily and effectively tap into the automatic responses all speech -- whether from human or machine -- evokes. Wired for Speech demonstrates that people are ""voice-activated"": we respond to voice technologies as we respond to actual people and behave as we would in any social situation. By leveraging this powerful finding, voice interfaces can truly emerge as the next frontier for efficient, user-friendly technology.Wired for Speech presents new theories and experiments and applies them to critical issues concerning how people interact with technology-based voices. It considers how people respond to a female voice in e-commerce (does stereotyping matter?), how a car's voice can promote safer driving (are ""happy"" cars better cars?), whether synthetic voices have personality and emotion (is sounding like a person always good?), whether an automated call center should apologize when it cannot understand a spoken request (""To Err is Interface; To Blame, Complex""), and much more. Nass and Brave's deep understanding of both social science and design, drawn from ten years of research at Nass's Stanford laboratory, produces results that often challenge conventional wisdom and common design practices. These insights will help designers and marketers build better interfaces, scientists construct better theories, and everyone gain better understandings of the future of the machines that speak with us.","[{'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '2739604', 'name': 'Scott Brave'}]",228.0,"{'bibtex': '@Inproceedings{Nass2005WiredFS,\n author = {C. Nass and Scott Brave},\n title = {Wired for Speech: How Voice Activates and Advances the Human-Computer Relationship},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Wired for Speech: How Voice Activates and Advances the Human-Computer Relationship,2005.0
717,39df7432dc7e401c9d3043e8d7619e888af6eca0,"When avoiding a group, a walker has two possibilities: either he goes through it or around it. Going through very dense groups or around huge ones would not seem natural and could break any sense of presence in a virtual environment. This paper aims to enable crowd simulators to handle such situations correctly. To this end, we need to understand how real humans decide to go through or around groups. As a first hypothesis, we apply the Principle of Minimum Energy (PME) on different group sizes and density. According to this principle, a walker should go around small and dense groups whereas he should go through large and sparse groups. Such principle has already been used for crowd simulation; the novelty here is to apply it to decide on a global avoidance strategy instead of local adaptations only. Our study quantifies decision thresholds. However, PME leaves some inconclusive situations for which the two solutions paths have similar energetic costs. In a second part, we propose an experiment to corroborate PME decisions thresholds with real observations. As controlling the factors of an experiment with many people is extremely hard, we propose to use Virtual Reality as a new method to observe human behavior. This work represents the first crowd simulation algorithm component directly designed from a VR-based study. We also consider the role of secondary factors in inconclusive situations. We show the influence of the group appearance and direction of relative motion in the decision process. Finally, we draw some guidelines to integrate our conclusions to existing crowd simulators and show an example of such integration. We evaluate the achieved improvements.","[{'authorId': '2123864', 'name': 'Julien Bruneau'}, {'authorId': '1851306', 'name': 'A. Olivier'}, {'authorId': '2235773', 'name': 'J. Pettré'}]",84.0,"{'bibtex': '@Article{Bruneau2015GoingTG,\n author = {Julien Bruneau and A. Olivier and J. Pettré},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {520-528},\n title = {Going Through, Going Around: A Study on Individual Avoidance of Groups},\n volume = {21},\n year = {2015}\n}\n'}",,"{'volume': '21', 'pages': '520-528', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",42.0,"Going Through, Going Around: A Study on Individual Avoidance of Groups",2015.0
718,39fc78bc0ee863a18a202efd684bc8c12ba65501,,"[{'authorId': '37670752', 'name': 'J. Tao'}, {'authorId': '1729563', 'name': 'Yongguo Kang'}]",48.0,"{'bibtex': '@Inproceedings{Tao2005FeaturesIA,\n author = {J. Tao and Yongguo Kang},\n pages = {449-457},\n title = {Features Importance Analysis for Emotional Speech Classification},\n year = {2005}\n}\n'}",,{'pages': '449-457'},13.0,Features Importance Analysis for Emotional Speech Classification,2005.0
719,3a1a2cff2b70fb84a7ca7d97f8adcc5855851795,"We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state automata (using the freely available OpenFst), together with detailed documentation and a comprehensive set of scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.","[{'authorId': '1792214', 'name': 'Daniel Povey'}, {'authorId': '2268620', 'name': 'Arnab Ghoshal'}, {'authorId': '2541218', 'name': 'Gilles Boulianne'}, {'authorId': '1816892', 'name': 'L. Burget'}, {'authorId': '3075141', 'name': 'O. Glembek'}, {'authorId': '46356878', 'name': 'N. Goel'}, {'authorId': '2592983', 'name': 'M. Hannemann'}, {'authorId': '2745667', 'name': 'P. Motlícek'}, {'authorId': '2480051', 'name': 'Y. Qian'}, {'authorId': '35455336', 'name': 'Petr Schwarz'}, {'authorId': '3330139', 'name': 'J. Silovský'}, {'authorId': '1708033', 'name': 'G. Stemmer'}, {'authorId': '2459598', 'name': 'Karel Veselý'}]",5947.0,"{'bibtex': '@Inproceedings{Povey2011TheKS,\n author = {Daniel Povey and Arnab Ghoshal and Gilles Boulianne and L. Burget and O. Glembek and N. Goel and M. Hannemann and P. Motlícek and Y. Qian and Petr Schwarz and J. Silovský and G. Stemmer and Karel Veselý},\n title = {The Kaldi Speech Recognition Toolkit},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",26.0,The Kaldi Speech Recognition Toolkit,2011.0
720,3a46c26d15fd3ad9e4ec3a70bfeb3f22f7af9ccd,,"[{'authorId': '3865216', 'name': 'Alice M. Isen'}]",1439.0,"{'bibtex': '@Inproceedings{Isen1993PositiveAA,\n author = {Alice M. Isen},\n pages = {261-277},\n title = {Positive affect and decision making.},\n year = {1993}\n}\n'}",,"{'volume': '', 'pages': '261-277', 'name': ''}",0.0,Positive affect and decision making.,1993.0
721,3a8ca154126cfbdfa6ab1787a0264c8739cfea4f,,"[{'authorId': '1695172', 'name': 'C. Park'}, {'authorId': '2079306', 'name': 'Hifza Javed'}, {'authorId': '2572836', 'name': 'M. Jeon'}]",2.0,"{'bibtex': '@Article{Park2019ConsensusBasedHI,\n author = {C. Park and Hifza Javed and M. Jeon},\n booktitle = {Interacción},\n pages = {295-301},\n title = {Consensus-Based Human-Agent Interaction Model for Emotion Regulation in ASD},\n year = {2019}\n}\n'}","[{'paperId': 'a125bbd9a95f458f96f4a5b8ddb920b934578749', 'title': 'Embodied technologies for stress management in children: A systematic review'}, {'paperId': '819291dc87c6ee06b3d224279b400d2f4dd60157', 'title': 'Survey of Emotions in Human–Robot Interactions: Perspectives from Robotic Psychology on 20 Years of Research'}]",{'pages': '295-301'},7.0,Consensus-Based Human-Agent Interaction Model for Emotion Regulation in ASD,2019.0
722,3aa2e8da97e430389ca37e122bb41fb2f4ae3b61,"Facial expressions reflect internal emotional states of a character or in response to social communications. Though much effort has been taken to generate realistic facial expressions, it still remains a challenging topic due to human being’s sensitivity to subtle facial movements. In this paper, we present a method for facial animation generation, which reflects true facial muscle movements with high fidelity. An intermediate model space is introduced to transfer captured static AU peak frames based on FACS to the conformed target face. And then dynamic parameters derived using a psychophysics method is integrated to generate facial animation, which is assumed to represent natural correlation of multiple AUs. Finally, the animation sequence in the intermediate model space is mapped to the target face to produce final animation.","[{'authorId': '2118681985', 'name': 'Hui Yu'}, {'authorId': '48522841', 'name': 'Oliver G. B. Garrod'}, {'authorId': '2143019', 'name': 'Rachael E. Jack'}, {'authorId': '2287417', 'name': 'P. Schyns'}]",7.0,"{'bibtex': '@Inproceedings{Yu2014RealisticFA,\n author = {Hui Yu and Oliver G. B. Garrod and Rachael E. Jack and P. Schyns},\n title = {Realistic facial animation generation based on facial expression mapping},\n volume = {9069},\n year = {2014}\n}\n'}",,{'volume': '9069'},32.0,Realistic facial animation generation based on facial expression mapping,2014.0
723,3aa4404958b5cb69bf4e4c93fda6cff1648d4ffb,"Background Computer-generated virtual faces become increasingly realistic including the simulation of emotional expressions. These faces can be used as well-controlled, realistic and dynamic stimuli in emotion research. However, the validity of virtual facial expressions in comparison to natural emotion displays still needs to be shown for the different emotions and different age groups. Methodology/Principal Findings Thirty-two healthy volunteers between the age of 20 and 60 rated pictures of natural human faces and faces of virtual characters (avatars) with respect to the expressed emotions: happiness, sadness, anger, fear, disgust, and neutral. Results indicate that virtual emotions were recognized comparable to natural ones. Recognition differences in virtual and natural faces depended on specific emotions: whereas disgust was difficult to convey with the current avatar technology, virtual sadness and fear achieved better recognition results than natural faces. Furthermore, emotion recognition rates decreased for virtual but not natural faces in participants over the age of 40. This specific age effect suggests that media exposure has an influence on emotion recognition. Conclusions/Significance Virtual and natural facial displays of emotion may be equally effective. Improved technology (e.g. better modelling of the naso-labial area) may lead to even better results as compared to trained actors. Due to the ease with which virtual human faces can be animated and manipulated, validated artificial emotional expressions will be of major relevance in future research and therapeutic applications.","[{'authorId': '3108830', 'name': 'M. Dyck'}, {'authorId': '3660635', 'name': 'Maren Winbeck'}, {'authorId': '2178712', 'name': 'Susanne Leiberg'}, {'authorId': '2115868005', 'name': 'Yuhan Chen'}, {'authorId': '40217273', 'name': 'R. C. Gur'}, {'authorId': '2448724', 'name': 'K. Mathiak'}]",109.0,"{'bibtex': '@Article{Dyck2008RecognitionPO,\n author = {M. Dyck and Maren Winbeck and Susanne Leiberg and Yuhan Chen and R. C. Gur and K. Mathiak},\n journal = {PLoS ONE},\n title = {Recognition Profile of Emotions in Natural and Virtual Faces},\n volume = {3},\n year = {2008}\n}\n'}",,"{'volume': '3', 'name': 'PLoS ONE'}",46.0,Recognition Profile of Emotions in Natural and Virtual Faces,2008.0
725,3aca0ab1638e6d05b4811d54ff547794678d3fb4,"Several studies have established that facial expressions of children with autism are often perceived as atypical, awkward or less engaging by typical adult observers. Despite this clear deficit in the quality of facial expression production, very little is understood about its underlying mechanisms and characteristics. This paper takes a computational approach to studying details of facial expressions of children with high functioning autism (HFA). The objective is to uncover those characteristics of facial expressions, notably distinct from those in typically developing children, and which are otherwise difficult to detect by visual inspection. We use motion capture data obtained from subjects with HFA and typically developing subjects while they produced various facial expressions. This data is analyzed to investigate how the overall and local facial dynamics of children with HFA differ from their typically developing peers. Our major observations include reduced complexity in the dynamic facial behavior of the HFA group arising primarily from the eye region.","[{'authorId': '1720741', 'name': 'T. Guha'}, {'authorId': '3161887', 'name': 'Zhaojun Yang'}, {'authorId': '35711564', 'name': 'R. Grossman'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",53.0,"{'bibtex': '@Article{Guha2018ACS,\n author = {T. Guha and Zhaojun Yang and R. Grossman and Shrikanth S. Narayanan},\n journal = {IEEE Transactions on Affective Computing},\n pages = {14-20},\n title = {A Computational Study of Expressive Facial Dynamics in Children with Autism},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'pages': '14-20', 'name': 'IEEE Transactions on Affective Computing'}",35.0,A Computational Study of Expressive Facial Dynamics in Children with Autism,2018.0
726,3aca676954859d12d1b3b927bb21c82b951c390d,,"[{'authorId': '143977043', 'name': 'R. Hogan'}, {'authorId': '49948286', 'name': 'N. Henley'}]",16.0,"{'bibtex': '@Inproceedings{Hogan1970ATO,\n author = {R. Hogan and N. Henley},\n title = {A Test of the Empathy-Effective Communication Hypothesis.},\n year = {1970}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,A Test of the Empathy-Effective Communication Hypothesis.,1970.0
727,3ad11696a552d86f3903efc593968e2492cc6b22,"This paper addresses the formation of infant attachment types within the context of active inference: a holistic account of action, perception and learning in the brain. We show how the organised forms of attachment (secure, avoidant and ambivalent) might arise in (Bayesian) infants. Specifically, we show that these distinct forms of attachment emerge from a minimisation of free energy—over interoceptive states relating to internal stress levels—when seeking proximity to caregivers who have a varying impact on these interoceptive states. In line with empirical findings in disrupted patterns of affective communication, we then demonstrate how exteroceptive cues (in the form of caregiver-mediated AMBIANCE affective communication errors, ACE) can result in disorganised forms of attachment in infants of caregivers who consistently increase stress when the infant seeks proximity, but can have an organising (towards ambivalence) effect in infants of inconsistent caregivers. In particular, we differentiate disorganised attachment from avoidance in terms of the high epistemic value of proximity seeking behaviours (resulting from the caregiver’s misleading exteroceptive cues) that preclude the emergence of coherent and organised behavioural policies. Our work, the first to formulate infant attachment in terms of active inference, makes a new testable prediction with regards to the types of affective communication errors that engender ambivalent attachment.","[{'authorId': '2585368', 'name': 'David Cittern'}, {'authorId': '38852400', 'name': 'T. Nolte'}, {'authorId': '1737497', 'name': 'Karl J. Friston'}, {'authorId': '1694989', 'name': 'A. Edalat'}]",10.0,"{'bibtex': '@Article{Cittern2018IntrinsicAE,\n author = {David Cittern and T. Nolte and Karl J. Friston and A. Edalat},\n journal = {PLoS ONE},\n title = {Intrinsic and extrinsic motivators of attachment under active inference},\n volume = {13},\n year = {2018}\n}\n'}",,"{'volume': '13', 'name': 'PLoS ONE'}",124.0,Intrinsic and extrinsic motivators of attachment under active inference,2018.0
728,3ad8e10d867e7338195545780556fe695b85eccb,"Dual-process and dual-system theories in both cognitive and social psychology have been subjected to a number of recently published criticisms. However, they have been attacked as a category, incorrectly assuming there is a generic version that applies to all. We identify and respond to 5 main lines of argument made by such critics. We agree that some of these arguments have force against some of the theories in the literature but believe them to be overstated. We argue that the dual-processing distinction is supported by much recent evidence in cognitive science. Our preferred theoretical approach is one in which rapid autonomous processes (Type 1) are assumed to yield default responses unless intervened on by distinctive higher order reasoning processes (Type 2). What defines the difference is that Type 2 processing supports hypothetical thinking and load heavily on working memory.",[],2585.0,"{'bibtex': '@Misc{None,\n title = {Science Perspectives on Psychological}\n}\n'}",,,124.0,Science Perspectives on Psychological,
729,3adc859834fac7ba48c2d20aec53a8d299aca9cb,,"[{'authorId': '1390848733', 'name': 'Yan Mao'}, {'authorId': '2115299380', 'name': 'Shanwen Yang'}, {'authorId': '49970067', 'name': 'Zuning Li'}, {'authorId': '2110516274', 'name': 'Yongjian Li'}]",39.0,"{'bibtex': '@Article{Mao2018PersonalityTA,\n author = {Yan Mao and Shanwen Yang and Zuning Li and Yongjian Li},\n booktitle = {Multimedia tools and applications},\n journal = {Multimedia Tools and Applications},\n pages = {3077 - 3104},\n title = {Personality trait and group emotion contagion based crowd simulation for emergency evacuation},\n volume = {79},\n year = {2018}\n}\n'}","[{'paperId': '356f2957dc1f8bd21153708f78977897e1d0aeab', 'title': 'A roadmap for the future of crowd safety research and practice: Introducing the Swiss Cheese Model of Crowd Safety and the imperative of a Vision Zero target'}, {'paperId': 'c934d02783018262c7895f7876ba640c333465e2', 'title': 'Modified social force model considering emotional contagion for crowd evacuation simulation'}, {'paperId': '1e849f89d8e05546bf6b6160dc43d7f176f4a88e', 'title': 'An agent based modeling approach to evaluate crowd movement strategies and density at bathing areas during Kumbh Mela-2019'}, {'paperId': 'e3b8167cc969697fe0cf69b6df3edc817b0cd0a2', 'title': 'GMAP 2023: 2nd Workshop on Group Modeling, Adaptation and Personalization'}, {'paperId': '3ddaea596f01d9d22e30fdb52e5d07d17b9237d8', 'title': 'Application of navigation grid corner point algorithm in virtual reality simulation images of indoor fire evacuation'}, {'paperId': 'e6445d34ffbb2cdc0c011c7c8fb72da5f2b2b227', 'title': ""Students' evacuation behavior during an emergency at schools: A systematic literature review""}, {'paperId': '4faad939f3e2767bbe74558085c46e10b8c6ad92', 'title': '3D indoor environments in pedestrian evacuation simulations'}, {'paperId': '40f466c8a2c2e13f5decf4676133300078ddb240', 'title': 'Heterogeneous crowd dynamics considering the impact of personality traits under a fire emergency: A questionnaire & simulation-based approach'}, {'paperId': '7d4c8e95560617ee59f82b1d879618955fe5279d', 'title': 'Emotion contagion in agent-based simulations of crowds: a systematic review'}, {'paperId': '1bc87602f14f5e82733c5471f508f9f838fc97c1', 'title': 'Developing Fire Evacuation Simulation Through Emotion-based BDI Methodology'}, {'paperId': '327e79ffe4a193a31e92ccbc069d6be84b4445a9', 'title': ""A deep reinforcement learning based decision‐making approach for avoiding crowd situation within the case of Covid'19 pandemic""}, {'paperId': '198c51a662101775e6d8d56c17759d98ea1d9197', 'title': 'Emergency Team Communication: Adaptive Sensemaking in Turbulent Environments'}, {'paperId': '4830644f34e8ed835a4b49a926c559bf92b74efd', 'title': 'A behavior model for improving realism in a VR team sport'}, {'paperId': 'f125f9f905ab9dea15d4d340db6bcff39fe58e3d', 'title': 'Adaptive Intervention for Crowd Negative Emotional Contagion'}, {'paperId': '9e5e8e46d8d0c8c2f2ed7235dedc0d6fb75f5ab6', 'title': 'Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation'}, {'paperId': 'a7715c2dcd434d7bcf835a51ba17297d6c918fde', 'title': 'On the Synergy between Virtual Reality and Multi-Agent Systems'}, {'paperId': '1f622b8c75d4273f8dff27938b3c010fab2845df', 'title': 'Group emotional contagion and simulation in large-scale flight delays based on the two-layer network model'}, {'paperId': '56ba9ece280b7158306a0ed06eb9c6a7d5926acb', 'title': 'Hybrid-driven Trajectory Prediction Based on Group Emotion'}, {'paperId': '2d7681241fedcf695cda5585ec30df59e2bac4e1', 'title': 'Knowledge and emotion dual-driven method for crowd evacuation'}, {'paperId': 'c26d5914b29fab8342c813399e424fb5161fa8e7', 'title': 'An integration of enhanced social force and crowd control models for high-density crowd simulation'}, {'paperId': '45c2cfa5c214c5f700d13487acd5dc420225ec63', 'title': 'A review on crowd simulation and modeling'}, {'paperId': 'f11b89fdd7008d802025f47833dc8b52e6496194', 'title': 'Individual differences and personalized learning: a review and appraisal'}, {'paperId': '255eebfe45593be0c9443ad2257db539a6b98377', 'title': 'Individual differences and personalized learning: a review and appraisal'}, {'paperId': 'c46bf7f39fb01bbb8bad52b4f7e8059020846b6a', 'title': 'Towards the cognitive and psychological perspectives of crowd behaviour: a vision-based analysis'}, {'paperId': '0d856a70b73b993c814a999a9612b5a8f49937ad', 'title': 'An emotional contagion based simulation for emergency evacuation peer behavior decision'}, {'paperId': '34458880f02232a3a77dd8a48ad88484f775f05f', 'title': 'Modeling Group Structures With Emotion in Crowd Evacuation'}, {'paperId': '39ee99e8046ed24c01ed5b36b538e1e4b9175a68', 'title': 'Amygdala-inspired affective computing: To realize personalized intracranial emotions with accurately observed external emotions'}, {'paperId': '7d38e6876e3cdff119efe3d8ddd3a81b1b0ed55e', 'title': 'Why Do We Have Emotions? The Social Functions of Emotions'}, {'paperId': '1cbf05ea4120f6b8462da81b2d009051db4bb359', 'title': ""The research of human individual's conformity behavior in emergency situations""}, {'paperId': '98c932d00aa7f903e084db6897c6a84a5025ae86', 'title': 'Analysis of Lifeboat Embarkation Efficiency for Cruise Passengers under Multiple Scenarios'}, {'paperId': 'a5bef250f09f5ecde84e3ed4050a367435ad09b3', 'title': 'Simulation of Crowd Motion Combined with Individual Emotional Factors'}, {'paperId': '2ace44a73e2c01aebbc439e7b2206ea695f39765', 'title': 'Developing an Adaptive Building Evacuation Simulation and Decision Support Framework using Cognitive Agent-Based Modelling'}, {'paperId': 'a3a87b443dd9bb716f4a8087e5d091e2c32bacb4', 'title': 'The Mechanism of Crowd Stampede Based on Case Statistics through SNA Method'}, {'paperId': '9555d2c7aa27db26d08d3dec971f18b51506261c', 'title': 'Crowd Cognitive Modeling as a Vital Process for Collaborative Disaster Management'}, {'paperId': '6177633a55c424308e27688a9500c305c7a5eca1', 'title': 'A Review on Multi-agent Systems and Virtual Reality'}, {'paperId': 'ac512b9e3751943139d734b9fc8dafa6e2b23ed0', 'title': 'An Emergency Evacuation Behavior Simulation Method Combines Personality Traits and Emotion Contagion'}, {'paperId': '7804798d73b34d3e1beaddc922baa3af0d245331', 'title': 'Asymmetric Intimacy Based Positive Emotions Contagion Peer Public Safety Evacuation Behavior'}, {'paperId': '4c1da588ea6a7d541bda916b9ad75053d53de39a', 'title': 'SIMUL 2019 Proceedings'}, {'paperId': '0479c89ac9af4f33301a46dc92f40807dc483c84', 'title': 'Simulating Strain and Motivation in Human Work Performance: An Agent-Based Modeling Approach Using the Job Demands-Resources Model'}]","{'name': 'Multimedia Tools and Applications', 'pages': '3077 - 3104', 'volume': '79'}",53.0,Personality trait and group emotion contagion based crowd simulation for emergency evacuation,2018.0
730,3b10877ea37c3c8b0960a65263343940c8f648c1,,"[{'authorId': '40635785', 'name': 'S. Tomkins'}]",910.0,"{'bibtex': '@Article{Tomkins1963IlluminatingAS,\n author = {S. Tomkins},\n journal = {Science},\n title = {Illuminating and Stimulating. (Book Reviews: Affect, Imagery, Consciousness. vol. 1, The Positive Affects)},\n year = {1963}\n}\n'}",,"{'volume': '', 'name': 'Science'}",0.0,"Illuminating and Stimulating. (Book Reviews: Affect, Imagery, Consciousness. vol. 1, The Positive Affects)",1963.0
731,3b22ce4c3243a0fc2ff68d91c7e1ae085f276452,"In this paper, we propose a new concept - the ""Reciprocal Velocity Obstacle""- for real-time multi-agent navigation. We consider the case in which each agent navigates independently without explicit communication with other agents. Our formulation is an extension of the Velocity Obstacle concept [3], which was introduced for navigation among (passively) moving obstacles. Our approach takes into account the reactive behavior of the other agents by implicitly assuming that the other agents make a similar collision-avoidance reasoning. We show that this method guarantees safe and oscillation- free motions for each of the agents. We apply our concept to navigation of hundreds of agents in densely populated environments containing both static and moving obstacles, and we show that real-time and scalable performance is achieved in such challenging scenarios.","[{'authorId': '144721873', 'name': 'J. V. D. Berg'}, {'authorId': '144247566', 'name': 'M. Lin'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",1386.0,"{'bibtex': '@Article{Berg2008ReciprocalVO,\n author = {J. V. D. Berg and M. Lin and Dinesh Manocha},\n journal = {2008 IEEE International Conference on Robotics and Automation},\n pages = {1928-1935},\n title = {Reciprocal Velocity Obstacles for real-time multi-agent navigation},\n year = {2008}\n}\n'}",,"{'pages': '1928-1935', 'name': '2008 IEEE International Conference on Robotics and Automation'}",25.0,Reciprocal Velocity Obstacles for real-time multi-agent navigation,2008.0
733,3b2524c884400cdbe72217c1576817babf3ce543,,"[{'authorId': '2087504899', 'name': 'P. Mitchell'}, {'authorId': '144674468', 'name': 'S. Parsons'}, {'authorId': '117398139', 'name': 'A. Leonard'}]",356.0,"{'bibtex': '@Article{Mitchell2007UsingVE,\n author = {P. Mitchell and S. Parsons and A. Leonard},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {589-600},\n title = {Using Virtual Environments for Teaching Social Understanding to 6 Adolescents with Autistic Spectrum Disorders},\n volume = {37},\n year = {2007}\n}\n'}",,"{'volume': '37', 'pages': '589-600', 'name': 'Journal of Autism and Developmental Disorders'}",36.0,Using Virtual Environments for Teaching Social Understanding to 6 Adolescents with Autistic Spectrum Disorders,2007.0
734,3b5247ada3a14bde5c78db59f1726ab1c385a904,"Interpersonal attitudes are expressed by non-verbal behaviors on a variety of different modalities. The perception of these behaviors is influenced by how they are sequenced with other behaviors from the same person and behaviors from other interactants. In this paper, we present a method for extracting and generating sequences of non-verbal signals expressing interpersonal attitudes. These sequences are used as part of a framework for non-verbal expression with Embodied Conversational Agents that considers different features of non-verbal behavior: global behavior tendencies, interpersonal reactions, sequencing of non-verbal signals, and communicative intentions. Our method uses a sequence mining technique on an annotated multimodal corpus to extract sequences characteristic of different attitudes. New sequences of non-verbal signals are generated using a probabilistic model, and evaluated using the previously mined sequences.","[{'authorId': '40325099', 'name': 'Mathieu Chollet'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",15.0,"{'bibtex': '@Inproceedings{Chollet2014MiningAM,\n author = {Mathieu Chollet and M. Ochs and C. Pelachaud},\n pages = {3417-3424},\n title = {Mining a multimodal corpus for non-verbal behavior sequences conveying attitudes},\n year = {2014}\n}\n'}",,{'pages': '3417-3424'},32.0,Mining a multimodal corpus for non-verbal behavior sequences conveying attitudes,2014.0
735,3b6163ad3cb47fa7120a67f828094a0f8d4302d5,"For many years, face recognition has been one of the most important domains in pattern recognition. Nowadays, face recognition is more required to be used in video actually. So moving facial capture must be studied firstly because of performance requirement. Since classic facial capture method is not so suitable in a moving environment, in this paper, we present a novel facial capture method in a moving environment. Firstly, continuous frames are extracted from detecting videos by similar characteristics. Then, we present an algorithm to extract the moving object and restructure background. Meanwhile, with analysis of skin color in both moving and static areas, we use the classic faces capture method to catch all faces. Finally, experimental results show that this method has better robustness and accuracy.","[{'authorId': '50152289', 'name': 'Shuai Liu'}, {'authorId': '1712119', 'name': 'Weina Fu'}, {'authorId': '98902637', 'name': 'Wen Zhao'}, {'authorId': '66102996', 'name': 'Jiantao Zhou'}, {'authorId': '2143524664', 'name': 'Qianzhong Li'}]",35.0,"{'bibtex': '@Article{Liu2013ANF,\n author = {Shuai Liu and Weina Fu and Wen Zhao and Jiantao Zhou and Qianzhong Li},\n journal = {Mathematical Problems in Engineering},\n pages = {1-6},\n title = {A Novel Fusion Method by Static and Moving Facial Capture},\n volume = {2013},\n year = {2013}\n}\n'}",,"{'volume': '2013', 'pages': '1-6', 'name': 'Mathematical Problems in Engineering'}",15.0,A Novel Fusion Method by Static and Moving Facial Capture,2013.0
736,3ba18fdb9d251c9dcd54d8630abe12f581b67c9c,"The present research aimed to assess how people use knowledge about the emotional reactions of others to make inferences about their character. Specifically, we postulate that people can reconstruct or “reverse engineer” the appraisals underlying an emotional reaction and use this appraisal information to draw person perception inferences. As predicted, a person who reacted with anger to blame was perceived as more aggressive, and self-confident, but also as less warm and gentle than a person who reacted with sadness (Study 1). A person who reacted with a smile (Study 1) or remained neutral (Study 2) was perceived as self-confident but also as unemotional. These perceptions were mediated by perceived appraisals.","[{'authorId': '3141618', 'name': 'Shlomo Hareli'}, {'authorId': '3067657', 'name': 'U. Hess'}]",278.0,"{'bibtex': '@Article{Hareli2010WhatER,\n author = {Shlomo Hareli and U. Hess},\n journal = {Cognition and Emotion},\n pages = {128 - 140},\n title = {What emotional reactions can tell us about the nature of others: An appraisal perspective on person perception},\n volume = {24},\n year = {2010}\n}\n'}",,"{'volume': '24', 'pages': '128 - 140', 'name': 'Cognition and Emotion'}",41.0,What emotional reactions can tell us about the nature of others: An appraisal perspective on person perception,2010.0
737,3bae8459c54858453e4ea1a74831927c2c9aecbf,"The authors have been developing humanoid robots in order to develop new mechanisms and functions for a humanoid robot that has the ability to communicate naturally with a human by expressing human-like emotion. We considered that human arms play an important role. And, in 2003, we developed the 9-DOFs emotion expression humanoid arm. Then, we developed the emotion expression humanoid robot WE-4R (Waseda Eye No.4 Refined) by integrating the new arms into the human-like head robot WE-4. Moreover, we introduced the new control algorithm for robot arm with redundant DOF into WE-4R. The new control algorithm was described by the junction of geometric constraint and robot emotion. We describe the mechanical design and the control algorithm of 9-DOFs emotion expression humanoid arm.","[{'authorId': '2065285579', 'name': 'H. Miwa'}, {'authorId': '46677342', 'name': 'K. Itoh'}, {'authorId': '2057391878', 'name': 'D. Ito'}, {'authorId': '1678499', 'name': 'H. Takanobu'}, {'authorId': '1737432', 'name': 'A. Takanishi'}]",26.0,"{'bibtex': ""@Article{Miwa2004DesignAC,\n author = {H. Miwa and K. Itoh and D. Ito and H. Takanobu and A. Takanishi},\n journal = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},\n pages = {128-133 Vol.1},\n title = {Design and control of 9-DOFs emotion expression humanoid arm},\n volume = {1},\n year = {2004}\n}\n""}",,"{'volume': '1', 'pages': '128-133 Vol.1', 'name': ""IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004""}",9.0,Design and control of 9-DOFs emotion expression humanoid arm,2004.0
738,3bd53f4a4918324cd8c4eb2d8012f03b01cd2d5b,"We study the notion of a strong attractor of a Hopfield neural model as a pattern that has been stored multiple times in the network, and examine its properties using basic mathematical techniques as well as a variety of simulations. It is proposed that strong attractors can be used to model attachment types in developmental psychology as well as behavioural patterns in psychology and psychotherapy. We study the stability and basins of attraction of strong attractors in the presence of other simple attractors and show that they are indeed more stable with a larger basin of attraction compared with simple attractors. We also show that the perturbation of a strong attractor by random noise results in a cluster of attractors near the original strong attractor measured by the Hamming distance. We investigate the stability and basins of attraction of such clusters as the noise increases and establish that the unfolding of the strong attractor, leading to its breakup, goes through three different stages. Finally the relation between strong attractors of different multiplicity and their influence on each other are studied and we show how the impact of a strong attractor can be replaced with that of a new strong attractor. This retraining of the network is proposed as a model of how attachment types and behavioural patterns can undergo change.","[{'authorId': '1694989', 'name': 'A. Edalat'}, {'authorId': '2894014', 'name': 'Federico Mancinelli'}]",20.0,"{'bibtex': '@Article{Edalat2013StrongAO,\n author = {A. Edalat and Federico Mancinelli},\n journal = {The 2013 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-10},\n title = {Strong attractors of Hopfield neural networks to model attachment types and behavioural patterns},\n year = {2013}\n}\n'}",,"{'pages': '1-10', 'name': 'The 2013 International Joint Conference on Neural Networks (IJCNN)'}",29.0,Strong attractors of Hopfield neural networks to model attachment types and behavioural patterns,2013.0
739,3bd7b18b165cc9bedebe6d92a092e3f2aa8c12f5,,"[{'authorId': '40428623', 'name': 'Luis-Felipe Rodríguez'}, {'authorId': '145956015', 'name': 'Félix F. Ramos'}]",65.0,"{'bibtex': '@Article{Rodríguez2014DevelopmentOC,\n author = {Luis-Felipe Rodríguez and Félix F. Ramos},\n journal = {Cognitive Computation},\n pages = {351-375},\n title = {Development of Computational Models of Emotions for Autonomous Agents: A Review},\n volume = {6},\n year = {2014}\n}\n'}",,"{'volume': '6', 'pages': '351-375', 'name': 'Cognitive Computation'}",126.0,Development of Computational Models of Emotions for Autonomous Agents: A Review,2014.0
741,3bdfafa0fc0bc514ccc251f1a8b75e510fa6361b,"Gaze following occurs automatically in social interactions, but the degree to which gaze is followed depends on whether an agent is perceived to have a mind, making its behavior socially more relevant for the interaction. Mind perception also modulates the attitudes we have toward others, and determines the degree of empathy, prosociality, and morality invested in social interactions. Seeing mind in others is not exclusive to human agents, but mind can also be ascribed to non-human agents like robots, as long as their appearance and/or behavior allows them to be perceived as intentional beings. Previous studies have shown that human appearance and reliable behavior induce mind perception to robot agents, and positively affect attitudes and performance in human–robot interaction. What has not been investigated so far is whether different triggers of mind perception have an independent or interactive effect on attitudes and performance in human–robot interaction. We examine this question by manipulating agent appearance (human vs. robot) and behavior (reliable vs. random) within the same paradigm and examine how congruent (human/reliable vs. robot/random) versus incongruent (human/random vs. robot/reliable) combinations of these triggers affect performance (i.e., gaze following) and attitudes (i.e., agent ratings) in human–robot interaction. The results show that both appearance and behavior affect human–robot interaction but that the two triggers seem to operate in isolation, with appearance more strongly impacting attitudes, and behavior more strongly affecting performance. The implications of these findings for human–robot interaction are discussed.","[{'authorId': '23148511', 'name': 'A. Abubshait'}, {'authorId': '2658383', 'name': 'E. Wiese'}]",74.0,"{'bibtex': '@Article{Abubshait2017YouLH,\n author = {A. Abubshait and E. Wiese},\n journal = {Frontiers in Psychology},\n title = {You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human–Robot Interaction},\n volume = {8},\n year = {2017}\n}\n'}",,"{'volume': '8', 'name': 'Frontiers in Psychology'}",100.0,"You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human–Robot Interaction",2017.0
742,3c1acc34dfd7de6feac56c37ad0a3a0d0dae98fe,,"[{'authorId': '11984844', 'name': 'Yu-Hung Chien'}, {'authorId': '1700936', 'name': 'V. Soo'}]",3.0,"{'bibtex': '@Article{Chien2011InferringPF,\n author = {Yu-Hung Chien and V. Soo},\n booktitle = {International Workshop on Agents for Educational Games and Simulations},\n pages = {123-138},\n title = {Inferring Pragmatics from Dialogue Contexts in Simulated Virtual Agent Games},\n year = {2011}\n}\n'}","[{'paperId': '4f2de43c49220610c50c3063eab795daedb61b23', 'title': 'Speech Act Theory as an Evaluation Tool for Human-Agent Communication'}, {'paperId': '848863d261cb114dda4612eba1bf6a14b760ac17', 'title': 'A Dempster-Shafer Theoretic Approach to Understanding Indirect Speech Acts'}, {'paperId': '901e7a0151542cc5e58c8f69db53754d3031b1bb', 'title': 'A Human-Agent Teamwork Communication Model (HAT-COM) for collaborative activity in virtual learning environments'}]",{'pages': '123-138'},17.0,Inferring Pragmatics from Dialogue Contexts in Simulated Virtual Agent Games,2011.0
743,3c2360fbc612e8381ecda4f79c2c3cf52e681829,"The present study aimed to test the hypothesis that a smiling expression on the face of a talking pedagogical agent could positively affect a learner’s emotions, motivation, and learning outcomes in a virtual learning environment. Contrary to the hypothesis, results from Experiment 1 demonstrated that the pedagogical agent’s smile induced negative emotional and motivational responses in learners. Experiment 2 showed that the social meaning of a pedagogical agent’s smile might be perceived by learners as polite or fake. In addition, qualitative data provided insights into factors that may cause negative perceptions of a pedagogical agent’s smile, which in turn lead to negative affective (emotional and motivational) states in learners. Theoretical and design implications for pedagogical agents in virtual learning environment are discussed in the concluding section of the paper.","[{'authorId': '3091348', 'name': 'Tze Wei Liew'}, {'authorId': '3211229', 'name': 'N. Zin'}, {'authorId': '49357391', 'name': 'N. Sahari'}, {'authorId': '3311426', 'name': 'Su-Mae Tan'}]",30.0,"{'bibtex': ""@Article{Liew2016TheEO,\n author = {Tze Wei Liew and N. Zin and N. Sahari and Su-Mae Tan},\n journal = {The International Review of Research in Open and Distributed Learning},\n pages = {248-266},\n title = {The Effects of a Pedagogical Agent's Smiling Expression on the Learner's Emotions and Motivation in a Virtual Learning Environment.},\n volume = {17},\n year = {2016}\n}\n""}",,"{'volume': '17', 'pages': '248-266', 'name': 'The International Review of Research in Open and Distributed Learning'}",52.0,The Effects of a Pedagogical Agent's Smiling Expression on the Learner's Emotions and Motivation in a Virtual Learning Environment.,2016.0
744,3c3700886d3f661f041075a32a35047489c6cf35,"This paper investigates emotion communication via copying behaviour in an embodied virtual agent. Copying takes place at the expressive level, where motion qualities are considered, rather than exact low-level motion matching. We present an experiment that investigates (1) the extent to which people can recognise the emotion expressed by the agent’s copying behaviour, and (2) whether and how the type of gesture performed by the agent affects the perception of emotion. Results suggest that a combination of type of movement performed and its quality are important for successfully communicating emotions.","[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}]",1.0,"{'bibtex': '@Inproceedings{Castellano2011EmotionCV,\n author = {Ginevra Castellano and Christopher E. Peters},\n title = {Emotion Communication via Copying Behaviour : A Case Study with the Greta Embodied Agent},\n year = {2011}\n}\n'}","[{'paperId': 'a37a0dd110ac067f671bfa8fcb02e989a07471c5', 'title': 'Effects of Gender Mapping on the Perception of Emotion from Upper Body Movement in Virtual Characters'}]",,17.0,Emotion Communication via Copying Behaviour : A Case Study with the Greta Embodied Agent,2011.0
745,3c5864d0d40434dd2e3e9dcb60df3c5dbf593a85,,"[{'authorId': '1873244', 'name': 'Ouarda Zedadra'}, {'authorId': '2882109', 'name': 'Nicolas Jouandeau'}, {'authorId': '2305167', 'name': 'Hamid Seridi'}, {'authorId': '1691577', 'name': 'G. Fortino'}]",40.0,"{'bibtex': '@Article{Zedadra2017MultiAgentFS,\n author = {Ouarda Zedadra and Nicolas Jouandeau and Hamid Seridi and G. Fortino},\n journal = {Complex Adaptive Systems Modeling},\n pages = {1-24},\n title = {Multi-Agent Foraging: state-of-the-art and research challenges},\n volume = {5},\n year = {2017}\n}\n'}",,"{'volume': '5', 'pages': '1-24', 'name': 'Complex Adaptive Systems Modeling'}",79.0,Multi-Agent Foraging: state-of-the-art and research challenges,2017.0
746,3c79e6ec5344181151905b20d4b4f4cca680a2ee,"There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.","[{'authorId': '1699868', 'name': 'Satinder Singh'}, {'authorId': '46328485', 'name': 'Richard L. Lewis'}, {'authorId': '1730590', 'name': 'A. Barto'}, {'authorId': '144123639', 'name': 'Jonathan Sorg'}]",388.0,"{'bibtex': '@Article{Singh2010IntrinsicallyMR,\n author = {Satinder Singh and Richard L. Lewis and A. Barto and Jonathan Sorg},\n journal = {IEEE Transactions on Autonomous Mental Development},\n pages = {70-82},\n title = {Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective},\n volume = {2},\n year = {2010}\n}\n'}",,"{'volume': '2', 'pages': '70-82', 'name': 'IEEE Transactions on Autonomous Mental Development'}",65.0,Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective,2010.0
747,3c7cf782e4094a7d66d8ad5da80a8083a8e72307,,"[{'authorId': '144714949', 'name': 'E. Proctor'}, {'authorId': '6428578', 'name': 'Hiie Silmere'}, {'authorId': '35464599', 'name': 'R. Raghavan'}, {'authorId': '2132087', 'name': 'P. Hovmand'}, {'authorId': '145349578', 'name': 'G. Aarons'}, {'authorId': '144046643', 'name': 'Alicia C. Bunger'}, {'authorId': '19206773', 'name': 'R. Griffey'}, {'authorId': '1485300590', 'name': 'Melissa A. Hensley'}]",3972.0,"{'bibtex': '@Article{Proctor2010OutcomesFI,\n author = {E. Proctor and Hiie Silmere and R. Raghavan and P. Hovmand and G. Aarons and Alicia C. Bunger and R. Griffey and Melissa A. Hensley},\n journal = {Administration and Policy in Mental Health},\n pages = {65 - 76},\n title = {Outcomes for Implementation Research: Conceptual Distinctions, Measurement Challenges, and Research Agenda},\n volume = {38},\n year = {2010}\n}\n'}",,"{'volume': '38', 'pages': '65 - 76', 'name': 'Administration and Policy in Mental Health'}",73.0,"Outcomes for Implementation Research: Conceptual Distinctions, Measurement Challenges, and Research Agenda",2010.0
748,3c900f9f5c9e259f4a91a29412ffad303e9bc3c8,"Abstract Humor is a complex and multi-faceted phenomenon composed of a variety of cognitive, social, and emotional processes. This paper will discuss humor appreciation in individuals with autism spectrum disorder (ASD) and individuals with Williams syndrome (WS), a rare genetic disorder mainly characterized by intellectual disabilities, high social approach tendencies and high positive emotions. Drawing on research on the comprehension and appreciation of humor in individuals with ASD, this paper aims to better understand how the particular cognitive, social, and emotional profile of individuals with WS might affect their appreciation of humor and how such research could ultimately lead to a greater understanding of the nature of humor.","[{'authorId': '2124359121', 'name': 'Noémie Treichel'}, {'authorId': '39779139', 'name': 'Daniel Dukes'}, {'authorId': '4021172', 'name': 'K. Barisnikov'}, {'authorId': '38707445', 'name': 'Andrea C. Samson'}]",5.0,"{'bibtex': '@Article{Treichel2021HowCS,\n author = {Noémie Treichel and Daniel Dukes and K. Barisnikov and Andrea C. Samson},\n journal = {HUMOR},\n pages = {113 - 133},\n title = {How cognitive, social, and emotional profiles impact humor appreciation: sense of humor in autism spectrum disorder and Williams syndrome},\n volume = {35},\n year = {2021}\n}\n'}",,"{'volume': '35', 'pages': '113 - 133', 'name': 'HUMOR'}",51.0,"How cognitive, social, and emotional profiles impact humor appreciation: sense of humor in autism spectrum disorder and Williams syndrome",2021.0
749,3cab85489dada987db53f391690bbaf0284b9563,,"[{'authorId': '8181364', 'name': 'Susan Williams White'}, {'authorId': '4171300', 'name': 'Kathleen Keonig'}, {'authorId': '144260517', 'name': 'L. Scahill'}]",879.0,"{'bibtex': '@Article{White2007SocialSD,\n author = {Susan Williams White and Kathleen Keonig and L. Scahill},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {1858-1868},\n title = {Social Skills Development in Children with Autism Spectrum Disorders: A Review of the Intervention Research},\n volume = {37},\n year = {2007}\n}\n'}",,"{'volume': '37', 'pages': '1858-1868', 'name': 'Journal of Autism and Developmental Disorders'}",67.0,Social Skills Development in Children with Autism Spectrum Disorders: A Review of the Intervention Research,2007.0
750,3cbe855bfad50102061b4c5bde1e0dc41df5cf15,"Social psychological and developmental research revealed that imitation serves a fundamental social function. It has been shown that human beings have the tendency to automatically mirror the behavior of others—the so-called chameleon effect. Furthermore, it has been demonstrated that being imitated leads to positive feelings toward the imitator. But why do we feel more positive about someone who imitates us? In the current fMRI study we aimed at exploring the neural correlates of the positive consequences of being imitated by means of an observation paradigm. Our results indicate that being imitated compared to not being imitated activates brain areas that have been associated with emotion and reward processing, namely medial orbitofrontal cortex/ventromedial prefrontal cortex (mOFC/vmPFC, GLM whole-brain contrast). Moreover mOFC/vmPFC shows higher effective connectivity with striatum and mid-posterior insula during being imitated compared to not being imitated.","[{'authorId': '1895071', 'name': 'S. Kühn'}, {'authorId': '40517064', 'name': 'Barbara C. N. Müller'}, {'authorId': '113458595', 'name': 'Rick B. van Baaren'}, {'authorId': '6198748', 'name': 'A. Wietzker'}, {'authorId': '4730036', 'name': 'A. Dijksterhuis'}, {'authorId': '2425526', 'name': 'M. Brass'}]",74.0,"{'bibtex': '@Article{Kühn2010WhyDI,\n author = {S. Kühn and Barbara C. N. Müller and Rick B. van Baaren and A. Wietzker and A. Dijksterhuis and M. Brass},\n journal = {Social Neuroscience},\n pages = {384 - 392},\n title = {Why do I like you when you behave like me? Neural mechanisms mediating positive consequences of observing someone being imitated},\n volume = {5},\n year = {2010}\n}\n'}",,"{'volume': '5', 'pages': '384 - 392', 'name': 'Social Neuroscience'}",58.0,Why do I like you when you behave like me? Neural mechanisms mediating positive consequences of observing someone being imitated,2010.0
751,3cc7e9d49d07b24fd50962f3100e2acc7a157b3e,,"[{'authorId': '21451088', 'name': 'P. Ekman'}]",2005.0,"{'bibtex': '@Inproceedings{Ekman1975UnmaskingTF,\n author = {P. Ekman},\n title = {Unmasking The Face},\n year = {1975}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Unmasking The Face,1975.0
753,3d0bcd30f4e569dde73522d574c2a1adda90c451,"To satisfy the need in personality research for factorially univocal measures of each of the 5 domains that subsume most English-language terms for personality-traits, new sets of Big-Five factor markers were investigated. In studies of adjective-anchored bipolar rating scales, a transparent format was found to produce factor markers that were more univocal than the same scales administered in the traditional format. Nonetheless, even the transparent bipolar scales proved less robust as factor markers than did parallel sets of adjectives administered in unipolar format. A set of 100 unipolar terms proved to be highly robust across quite diverse samples of self and peer descriptions. These new markers were compared with previously developed ones based on far larger sets of trait adjectives, as well as with the scales from the NEO and Hogan personality inventories.","[{'authorId': '30316209', 'name': 'L. R. Goldberg'}]",5052.0,"{'bibtex': '@Article{Goldberg1992THEDO,\n author = {L. R. Goldberg},\n journal = {Psychological Assessment},\n pages = {26-42},\n title = {THE DEVELOPMENT OF MARKERS FOR THE BIG-FIVE FACTOR STRUCTURE},\n volume = {4},\n year = {1992}\n}\n'}",,"{'volume': '4', 'pages': '26-42', 'name': 'Psychological Assessment'}",64.0,THE DEVELOPMENT OF MARKERS FOR THE BIG-FIVE FACTOR STRUCTURE,1992.0
754,3d0debd877ba219ff85e3597fd21a54cea955ded,"Previous studies have investigated the effect of empathy on relations. However, there have been few studies that found the effect of empathy on subjective well-being. This study aimed to investigate relations between empathy and subjective well-being. It was hypothesized that empathy had effects on subjective well-being in the mediation of self-esteem. Heo, J.H. and Lee, C.J. (2010) investigated the psychometric properties of EQ. However they did not do confirmatory analysis. So confirmatory analysis needs to be done in this study. Data was collected from 421 College student (male 192, female 225), and analyzed by SEM and path analysis. Results revealed that 3 factors model of EQ was proper, and EQ had effects on subjective well-being in the mediation of self-esteem. This suggested that empathy be a very important factor in our culture, and fulfillment of cultural task enhance self-esteem, which improve subjectve well-being. And cultural features should be considered in the study of empathy.","[{'authorId': '93868035', 'name': 'Jae-hong Heo'}, {'authorId': '67043433', 'name': 'Chan-Jong Lee'}]",2.0,"{'bibtex': '@Article{Heo2010TheEO,\n author = {Jae-hong Heo and Chan-Jong Lee},\n journal = {The Journal of the Acoustical Society of Korea},\n pages = {332-338},\n title = {The Effects of Empathy on Self-Esteem and Subjective Well-Being},\n volume = {29},\n year = {2010}\n}\n'}",,"{'volume': '29', 'pages': '332-338', 'name': 'The Journal of the Acoustical Society of Korea'}",0.0,The Effects of Empathy on Self-Esteem and Subjective Well-Being,2010.0
755,3d2185dcd4ac42e6ef5a389545963e289102f8ab,"While several research works have shown that virtual agents are able to generate natural and social behaviors from users, few of them have compared these social reactions to those expressed dur- ing a human-human mediated communication. In this paper, we propose to explore the social cues expressed by a user during a mediated communication either with an embodied conversational agent or with another human. For this purpose, we have exploited a machine learning method to identify the facial and head social cues characteristics in each interaction type and to construct a model to automatically determine if the user is interacting with a virtual agent or another human. e results show that, in fact, the users do not express the same facial and head movements during a communication with a virtual agent or another user. Based on these results, we propose to use such a machine learning model to automatically measure the social capability of a virtual agent to generate a social behavior in the user comparable to a human- human interaction. e resulting model can detect automatically if the user is communicating with a virtual or real interlocutor, looking only at the user’s face and head during one second.","[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '2104943740', 'name': 'Nathan Libermann'}, {'authorId': '2078341555', 'name': 'Axel Boidin'}, {'authorId': '1728769', 'name': 'T. Chaminade'}]",8.0,"{'bibtex': '@Article{Ochs2017DoYS,\n author = {M. Ochs and Nathan Libermann and Axel Boidin and T. Chaminade},\n journal = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},\n title = {Do you speak to a human or a virtual agent? automatic analysis of user’s social cues during mediated communication},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Multimodal Interaction'},35.0,Do you speak to a human or a virtual agent? automatic analysis of user’s social cues during mediated communication,2017.0
756,3d319b724ab3c6ddce66d58652fde0218a65d410,"Autism Spectrum Disorder (ASD) is a universal and often lifelong neuro-developmental disorder. Individuals with ASD often present comorbidities such as epilepsy, depression, and anxiety. In the United States, in 2014, 1 out of 68 people was affected by autism, but worldwide, the number of affected people drops to 1 in 160. This disparity is primarily due to underdiagnosis and unreported cases in resource-constrained environments. Wiggins et al. 1 found that, in the US, children of color are under-identified with ASD. Missing a diagnosis is not without consequences; approximately 26% of adults with ASD are under-employed, and are under-enrolled in higher education. Unfortunately, ASD diagnosis is not straightforward and involves a subjective assessment of the patient's behavior. Because such assessments can be noisy and even non-existent in low-resource environments, many cases go unidentified. Many such cases remain undiagnosed even when the patient reaches adolescence or adulthood. There is a need for an objective, low-cost, and ubiquitous approach to diagnose ASD. Autism is often characterized by symptoms such as limited interpersonal and social communication skills, and difficulty in face recognition and emotion interpretation. When watching video media, these symptoms can manifest as reduced eye fixation, resulting in characteristic gaze behaviors. Thus, we developed an approach to screen patients with ASD using their gaze behavior while they watch videos on a laptop screen. We used a dedicated eye tracker to record the participant's gaze. With data from 60 participants (35 with ASD and 25 without ASD), our algorithm demonstrates 92.5% classification accuracy after the participants watched 15 seconds of the video. We also developed a proof-of-concept regression model that estimates the severity of the condition and achieves a mean absolute error of 2.03 on the Childhood Autism Rating Scale (CARS). One of the most common approaches to identify individuals with ASD involves studying family home videos and investigating an infant's gaze and interactions with their families. However, having an expert carefully inspect hours of home video is expensive and unscalable. Our approach is more accessible and ubiquitous as we can directly sense the gaze of the user while they watch videos. Such sensing can be directly deployed on billions of smartphones around the world that are equipped with a front-facing camera. In our current exploration, we use a dedicated eye-tracker but achieving similar performance using an unmodified s martphone c amera is not far-fetched. Our results demonstrate that passively tracking a user's gaze pattern while they watch videos on a screen can enable robust identification of individuals with ASD. Past work has used specially-created visual content to detect ASD, but getting large sets of the population to watch specific videos is hard. Thus, we focus on generic content and selected four prosaic video scenes as a proof of concept. Our research team includes experienced psychologists to inform the study design and contextualize the performance of the final system. Although our gaze tracking approach cannot yet replace a clinical assessment, we believe it could be valuable for screening individuals passively, as they consume media content on computing devices (e.g., YouTube, Netflix, in-game cut scenes). We believe our efforts in estimating condition severity is also an essential first step towards building an entirely automated, in-home screening, and condition management tool. With rapid advancements in gaze tracking on consumer devices (e.g., Apple iPhone, HTC Vive), autism detection could be included on modern computing devices as a downloadable app or background feature, and potentially reduce the number of undiagnosed cases. Such a system could also track the efficacy of treatment and interventions. Additionally, ASD detection could be used to automatically adapt user interfaces, which has been shown to improve accessibility.","[{'authorId': '3451315', 'name': 'Karan Ahuja'}, {'authorId': '2069221594', 'name': 'A. Bose'}, {'authorId': '2089551047', 'name': 'Mohit Jain'}, {'authorId': '144710196', 'name': 'K. Dey'}, {'authorId': '2066730374', 'name': 'Anil Joshi'}, {'authorId': '144171754', 'name': 'K. Achary'}, {'authorId': '48619892', 'name': 'Blessin Varkey'}, {'authorId': '145078227', 'name': 'Chris Harrison'}, {'authorId': '4646339', 'name': 'Mayank Goel'}]",4.0,"{'bibtex': '@Article{Ahuja2020GazebasedSO,\n author = {Karan Ahuja and A. Bose and Mohit Jain and K. Dey and Anil Joshi and K. Achary and Blessin Varkey and Chris Harrison and Mayank Goel},\n journal = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},\n title = {Gaze-based Screening of Autistic Traits for Adolescents and Young Adults using Prosaic Videos},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies'},41.0,Gaze-based Screening of Autistic Traits for Adolescents and Young Adults using Prosaic Videos,2020.0
757,3d52d429b4d83d096dd354e8470bf3655e8b67bc,"Developing intelligent persuasive conversational agents to change people’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals’ demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals’ personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.","[{'authorId': '47120703', 'name': 'Xuewei Wang'}, {'authorId': '8299781', 'name': 'Weiyan Shi'}, {'authorId': '2054541342', 'name': 'Richard Kim'}, {'authorId': '2072731142', 'name': 'Y. Oh'}, {'authorId': '2108973978', 'name': 'Sijia Yang'}, {'authorId': '50561415', 'name': 'Jingwen Zhang'}, {'authorId': '1564034697', 'name': 'Zhou Yu'}]",168.0,"{'bibtex': '@Article{Wang2019PersuasionFG,\n author = {Xuewei Wang and Weiyan Shi and Richard Kim and Y. Oh and Sijia Yang and Jingwen Zhang and Zhou Yu},\n journal = {ArXiv},\n title = {Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good},\n volume = {abs/1906.06725},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1906.06725', 'name': 'ArXiv'}",41.0,Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good,2019.0
758,3d65c2418f0ce39b6b7228ddc9c1dc6fd19fa0ab,,"[{'authorId': '1717473', 'name': 'C. Stevens'}, {'authorId': '35318213', 'name': 'Bronwyn Pinchbeck'}, {'authorId': '145111765', 'name': 'T. Lewis'}, {'authorId': '1776457', 'name': 'M. Luerssen'}, {'authorId': '1692446', 'name': 'D. Pfitzner'}, {'authorId': '144871539', 'name': 'D. Powers'}, {'authorId': '1804967', 'name': 'Arman Abrahamyan'}, {'authorId': '31685492', 'name': 'Yvonne Leung'}, {'authorId': '2493088', 'name': 'G. Gibert'}]",25.0,"{'bibtex': '@Article{Stevens2016MimicryAE,\n author = {C. Stevens and Bronwyn Pinchbeck and T. Lewis and M. Luerssen and D. Pfitzner and D. Powers and Arman Abrahamyan and Yvonne Leung and G. Gibert},\n journal = {Computational Cognitive Science},\n title = {Mimicry and expressiveness of an ECA in human-agent interaction: familiarity breeds content!},\n volume = {2},\n year = {2016}\n}\n'}",,"{'volume': '2', 'name': 'Computational Cognitive Science'}",46.0,Mimicry and expressiveness of an ECA in human-agent interaction: familiarity breeds content!,2016.0
759,3d73ef7f4637e0b169cd8e3d7f5835136916adff,,"[{'authorId': '5628684', 'name': 'R. Lazarus'}]",6619.0,"{'bibtex': '@Inproceedings{Lazarus1970PsychologicalSA,\n author = {R. Lazarus},\n title = {Psychological stress and the coping process},\n year = {1970}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Psychological stress and the coping process,1970.0
760,3d80f96f8e3e72b4a6244e615b16d783b6115973,,"[{'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",14.0,"{'bibtex': '@Article{Saha2020TowardsSM,\n author = {Tulika Saha and S. Saha and P. Bhattacharyya},\n journal = {Cognitive Computation},\n pages = {246-260},\n title = {Towards Sentiment-Aware Multi-Modal Dialogue Policy Learning},\n volume = {14},\n year = {2020}\n}\n'}",,"{'volume': '14', 'pages': '246-260', 'name': 'Cognitive Computation'}",21.0,Towards Sentiment-Aware Multi-Modal Dialogue Policy Learning,2020.0
761,3dd0afb37060bf162b93fa0d5b9620cb07187ddd,"An ability to generate and express emotions constitutes an integral part of an artificial intelligent system be it cognitive architecture, virtual conversational agent or social robot. It is important to have an emotional component in those systems to enable the system to exhibit emotionally intelligent behaviour. This is in part achievable by integrating computational models of emotion into such cognitive systems. However, it is challenging to develop a computational model of emotion that embraces a wide range of cognitive aspects related to the process of emotion generation. As evident from the literature, mood and personality are the two aspects that are inseparable from the mechanism of emotion generation. In other words, mood and personality of an individual play a crucial role in determining the emotional state of an individual to a large extent. Thus, an emotion model that incorporates the notion of mood and personality is necessary to achieve a desirable level of emotional intelligence in cognitive systems. In this paper, we demonstrate how this integration has been achieved in our emotion model – EEGS. We also present how an artificial agent can show variation in emotion dynamics because of the influence of such factors, thus validating our theory.","[{'authorId': '101673582', 'name': 'Uman'}, {'authorId': '116149969', 'name': 'O. .'}, {'authorId': '2061665224', 'name': 'Jha'}]",3.0,"{'bibtex': '@Inproceedings{Uman2019ImplementingTD,\n author = {Uman and O. . and Jha},\n title = {Implementing the Dynamic Role of Mood and Personality in Emotion Processing of Cognitive Agents},\n year = {2019}\n}\n'}","[{'paperId': 'bfd65fad8833a3758ef3669a02aac846d851082c', 'title': 'Computational Emotion Models: A Thematic Review'}, {'paperId': '4c24d89680c8769c8950f11e147f080991b110d1', 'title': 'EEGS: A Transparent Model of Emotions'}, {'paperId': '3882ebd11f6bc4c61d5f5d2efce7f30798811704', 'title': 'Cognitive Agents and Ethical Behavior in Collaborative Teams'}]",,34.0,Implementing the Dynamic Role of Mood and Personality in Emotion Processing of Cognitive Agents,2019.0
762,3ddda3145dbb20a63c6fc36661ac3e464d0ab295,,"[{'authorId': '145213999', 'name': 'D. Watson'}, {'authorId': '10034636', 'name': 'L. Clark'}, {'authorId': '116114697', 'name': 'A. Tellegen'}]",101.0,"{'bibtex': '@Inproceedings{Watson2011PositiveAN,\n author = {D. Watson and L. Clark and A. Tellegen},\n title = {Positive and Negative Affect Schedule},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Positive and Negative Affect Schedule,2011.0
763,3de5d40b60742e3dfa86b19e7f660962298492af,"We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.","[{'authorId': '32538203', 'name': 'P. Brown'}, {'authorId': '39944066', 'name': 'V. D. Pietra'}, {'authorId': '144856857', 'name': 'P. D. Souza'}, {'authorId': '3853032', 'name': 'J. Lai'}, {'authorId': '2474650', 'name': 'R. Mercer'}]",3460.0,"{'bibtex': '@Article{Brown1992ClassBasedNM,\n author = {P. Brown and V. D. Pietra and P. D. Souza and J. Lai and R. Mercer},\n journal = {Comput. Linguistics},\n pages = {467-479},\n title = {Class-Based n-gram Models of Natural Language},\n volume = {18},\n year = {1992}\n}\n'}",,"{'volume': '18', 'pages': '467-479', 'name': 'Comput. Linguistics'}",15.0,Class-Based n-gram Models of Natural Language,1992.0
764,3deb202c71646b47d6e61699b82a5ce9e4412ec6,"In this paper we report about the use of computer generated affect to control body and mind of cognitively modeled virtual characters. We use the computational model of affect ALMA that is able to simulate three different affect types in real-time. The computation of affect is based on a novel approach of an appraisal language. Both the use of elements of the appraisal language and the simulation of different affect types has been evaluated. Affect is used to control facial expressions, facial complexions, affective animations, posture, and idle behavior on the body layer and the selection of dialogue strategies on the mind layer. To enable a fine-grained control of these aspects a Player Markup Language (PML) has been developed. The PML is player-independent and allows a sophisticated control of character actions coordinated by high-level temporal constraints. An Action Encoder module maps the output of ALMA to PML actions using affect display rules. These actions drive the real-time rendering of affect, gesture and speech parameters of virtual characters, which we call Virtual Humans.","[{'authorId': '2922093', 'name': 'Martin Klesen'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}]",5.0,"{'bibtex': '@Article{Klesen2020AffectiveMC,\n author = {Martin Klesen and Patrick Gebhard},\n journal = {Int. J. Virtual Real.},\n pages = {43-54},\n title = {Affective Multimodal Control of Virtual},\n volume = {6},\n year = {2020}\n}\n'}",,"{'volume': '6', 'pages': '43-54', 'name': 'Int. J. Virtual Real.'}",26.0,Affective Multimodal Control of Virtual,2020.0
765,3df8c316a6a6f1cb68b29abb86b3b09e1f1b5569,"Family history data on 99 autistic and 36 Down's syndrome probands are reported. They confirmed a raised familial loading for both autism and more broadly defined pervasive developmental disorders in siblings (2.9% and 2.9%, respectively, vs 0% in the Down's group) and also evidence for the familial aggregation of a lesser variant of autism, comprising more subtle communication/social impairments or stereotypic behaviours, but not mental retardation alone. Between 12.4 and 20.4% of the autism siblings and 1.6% and 3.2% of the Down's siblings exhibited this lesser variant, depending on the stringency of its definition. Amongst autistic probands with speech, various features of their disorder (increased number of autistic symptoms; reduced verbal and performance ability) as well as a history of obstetric complications, indexed an elevation in familial loading. No such association was seen in the probands without speech, even though familial loading for the lesser variant in this subgroup, was significantly higher than in the Down's controls. The findings suggest that the autism phenotype extends beyond autism as traditionally diagnosed; that aetiology involves several genes; that autism is genetically heterogeneous; and that obstetric abnormalities in autistic subjects may derive from abnormality in the foetus.","[{'authorId': '5897982', 'name': 'P. Bolton'}, {'authorId': '33193167', 'name': 'Hope Macdonald'}, {'authorId': '2066480', 'name': 'A. Pickles'}, {'authorId': '2067807870', 'name': 'P. Rios'}, {'authorId': '34178191', 'name': 'S. Goode'}, {'authorId': '49150998', 'name': 'M. Crowson'}, {'authorId': '2356031', 'name': 'A. Bailey'}, {'authorId': '2779929', 'name': 'M. Rutter'}]",1114.0,"{'bibtex': '@Article{Bolton1994ACF,\n author = {P. Bolton and Hope Macdonald and A. Pickles and P. Rios and S. Goode and M. Crowson and A. Bailey and M. Rutter},\n journal = {Journal of child psychology and psychiatry, and allied disciplines},\n pages = {\n          877-900\n        },\n title = {A case-control family history study of autism.},\n volume = {35 5},\n year = {1994}\n}\n'}",,"{'volume': '35 5', 'pages': '\n          877-900\n        ', 'name': 'Journal of child psychology and psychiatry, and allied disciplines'}",29.0,A case-control family history study of autism.,1994.0
766,3e06f9b4e4679d23f99fabab6b94105afa4705a7,,"[{'authorId': '145714168', 'name': 'T. Sandholm'}, {'authorId': '2748374', 'name': 'Robert H. Crites'}]",355.0,"{'bibtex': ""@Article{Sandholm1996MultiagentRL,\n author = {T. Sandholm and Robert H. Crites},\n journal = {Bio Systems},\n pages = {\n          147-66\n        },\n title = {Multiagent reinforcement learning in the Iterated Prisoner's Dilemma.},\n volume = {37 1-2},\n year = {1996}\n}\n""}",,"{'volume': '37 1-2', 'pages': '\n          147-66\n        ', 'name': 'Bio Systems'}",42.0,Multiagent reinforcement learning in the Iterated Prisoner's Dilemma.,1996.0
767,3e13193efbd8466ccec27bd808a30b37fcad0156,"Based on the assumptions that relational messages are multidimensional and that they are frequently communicated by nonverbal cues, this experiment manipulated five nonverbal cues -eye contact, proximity, body lean, smiling, and touch - to determine what meanings they convey along four relational message dimensions. Subjects (N= 150) observed 2 out of 40 videotaped conversational segments in which a male-female dyad presented various combinations of the nonverbal cues. High eye contact, close proximity, forward body lean, and smiling all conveyed greater intimacy, attraction, and trust. Low eye contact, a distal position, backward body lean, and the absence of smiling and touch communicated greater detachment. High eye contact, close proximity, and smiling also communicated less emotional arousal and greater composure, while high eye contact and close proximity alone conveyed greater dominance and control. Effects of combinations of cues and sex-differences are also reported.","[{'authorId': '2896960', 'name': 'J. Burgoon'}, {'authorId': '2461312', 'name': 'D. Buller'}, {'authorId': '39973716', 'name': 'Jerold L. Hale'}, {'authorId': '39842046', 'name': 'M. Turck'}]",370.0,"{'bibtex': '@Article{Burgoon1984RelationalMA,\n author = {J. Burgoon and D. Buller and Jerold L. Hale and M. Turck},\n journal = {Human Communication Research},\n pages = {351-378},\n title = {Relational Messages Associated with Nonverbal Behaviors.},\n volume = {10},\n year = {1984}\n}\n'}",,"{'volume': '10', 'pages': '351-378', 'name': 'Human Communication Research'}",61.0,Relational Messages Associated with Nonverbal Behaviors.,1984.0
768,3e49fae6e7529a5db404ad9ab8d04912c151e726,"This paper reports the results of the first study comparing subjects' responses to robotic emotional facial displays and human emotional facial displays. It describes step by step the building of believable emotional expressions in a robotic head, the problems raised by a comparative approach of robotic and human expressions, and the solutions found in order to ensure a valid comparison. Twenty adults and 15 children aged 3 were presented static (photos) and dynamic (2-D videoclips, or 3-D live) displays of emotional expressions presented by a robot or a person. The study compares two dependent variables: emotional resonance (automatic facial feedback during an emotional display) and emotion recognition (emotion labeling) according to partners (robot or person) and to the nature of the display (static or dynamic). Results for emotional resonance were similar with young children and with adults. Both groups resonated significantly more to dynamic displays than to static displays, be they robotic expressions or human expressions. In both groups, emotion recognition was easier for human expressions than for robotic ones. Unlike children that recognized more easily emotional expressions dynamically displayed, adults scored higher with static displays thus reflecting a cognitive strategy independent from emotional resonance. Results are discussed in the perspective of the therapeutic use of this comparative approach with children with autism that are described as impaired in emotion sharing and communication.","[{'authorId': '2433022', 'name': 'J. Nadel'}, {'authorId': '2114372735', 'name': 'M. Simon'}, {'authorId': '2077272329', 'name': 'P. Canet'}, {'authorId': '4122160', 'name': 'R. Soussignan'}, {'authorId': '3501675', 'name': 'P. Blancard'}, {'authorId': '46219050', 'name': 'Lola Cañamero'}, {'authorId': '3305788', 'name': 'P. Gaussier'}]",47.0,"{'bibtex': '@Inproceedings{Nadel2006HumanRT,\n author = {J. Nadel and M. Simon and P. Canet and R. Soussignan and P. Blancard and Lola Cañamero and P. Gaussier},\n title = {Human responses to an expressive robot},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",35.0,Human responses to an expressive robot,2006.0
769,3e4bc1aa55c752918ae99b1a125f6adef61afad2,,"[{'authorId': '143903370', 'name': 'Murray Campbell'}, {'authorId': '32498609', 'name': 'A. J. Hoane'}, {'authorId': '144844988', 'name': 'Feng-hsiung Hsu'}]",1114.0,"{'bibtex': '@Article{Campbell2002DeepB,\n author = {Murray Campbell and A. J. Hoane and Feng-hsiung Hsu},\n journal = {Artif. Intell.},\n pages = {57-83},\n title = {Deep Blue},\n volume = {134},\n year = {2002}\n}\n'}",,"{'volume': '134', 'pages': '57-83', 'name': 'Artif. Intell.'}",32.0,Deep Blue,2002.0
770,3e557968d8df23d52b1fe37c1f118c35b832f8a2,,"[{'authorId': '88215038', 'name': 'S. Brison'}]",2575.0,"{'bibtex': '@Article{Brison1989TheIS,\n author = {S. Brison},\n journal = {Philosophical Books},\n pages = {169-172},\n title = {The Intentional Stance},\n volume = {30},\n year = {1989}\n}\n'}",,"{'volume': '30', 'pages': '169-172', 'name': 'Philosophical Books'}",0.0,The Intentional Stance,1989.0
771,3e58205f859f6fd13623fb1dbedc3f4ade5c49c4,,"[{'authorId': '1403827243', 'name': 'C. Becker-Asano'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",59.0,"{'bibtex': '@Inproceedings{Becker-Asano2008AffectSW,\n author = {C. Becker-Asano and I. Wachsmuth},\n pages = {15-28},\n title = {Affect Simulation with Primary and Secondary Emotions},\n year = {2008}\n}\n'}",,{'pages': '15-28'},42.0,Affect Simulation with Primary and Secondary Emotions,2008.0
772,3e5e3ce6f1ccf669e4c75ecf3dce2baf8d9f19b2,,"[{'authorId': '49132154', 'name': 'B. Lee'}, {'authorId': '15450398', 'name': 'E. C. Kao'}, {'authorId': '1700936', 'name': 'V. Soo'}]",12.0,"{'bibtex': '@Inproceedings{Lee2006FeelingAA,\n author = {B. Lee and E. C. Kao and V. Soo},\n pages = {329-342},\n title = {Feeling Ambivalent: A Model of Mixed Emotions for Virtual Agents},\n year = {2006}\n}\n'}",,{'pages': '329-342'},15.0,Feeling Ambivalent: A Model of Mixed Emotions for Virtual Agents,2006.0
773,3e71fb7f01a623d6c4b89bdbb5e9227ecdeb2e4a,,[],75.0,"{'bibtex': '@Misc{None,\n title = {Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008) An Integrated Reasoning Approach to Moral Decision-Making}\n}\n'}",,,0.0,Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008) An Integrated Reasoning Approach to Moral Decision-Making,
774,3e7da5b3a59b12ff1b29a790a1c0d354de76bb4f,"Summary To explore the possibility of developing a valid abbreviated form of the Test of Facial Recognition of Benton and Van Allen for clinical use, the test performances of 185 adult patients with established diagnoses of brain disease and 151 adult patients without history or evidence of brain disease were studied. Stepwise multiple discriminant analysis on subsamples of the brain-diseased and control patients identified discriminating items which were then subjected to cross-validation on independent subsamples. The resulting short form of the test consisted of 16 items requiring 27 responses, representing a 50 percent reduction in length. Part-whole correlation coefficients between the short and long forms of the test ranged from .884 to .940 in five different samples of patients. With use of the smoothed equipercentile method of Flanagan, a table for predicting long form scores from obtained short form scores was constructed. Guidelines for the employment of the short form in clinical evaluation wer...","[{'authorId': '1910665', 'name': 'H. Levin'}, {'authorId': '3563344', 'name': 'K. D. Hamsher'}, {'authorId': '2203961', 'name': 'A. Benton'}]",110.0,"{'bibtex': '@Article{Levin1975ASF,\n author = {H. Levin and K. D. Hamsher and A. Benton},\n journal = {The Journal of Psychology},\n pages = {223-228},\n title = {A Short Form of the Test of Facial Recognition for Clinical Use},\n volume = {91},\n year = {1975}\n}\n'}",,"{'volume': '91', 'pages': '223-228', 'name': 'The Journal of Psychology'}",6.0,A Short Form of the Test of Facial Recognition for Clinical Use,1975.0
775,3e9754f7acdc7e96f252df8055bd8313b4411ab1,,"[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '2755130', 'name': 'Santiago D. Villalba'}, {'authorId': '3141704', 'name': 'A. Camurri'}]",361.0,"{'bibtex': '@Inproceedings{Castellano2007RecognisingHE,\n author = {Ginevra Castellano and Santiago D. Villalba and A. Camurri},\n pages = {71-82},\n title = {Recognising Human Emotions from Body Movement and Gesture Dynamics},\n year = {2007}\n}\n'}",,{'pages': '71-82'},29.0,Recognising Human Emotions from Body Movement and Gesture Dynamics,2007.0
776,3ea11e4fc90fffc4932ccaac262e8f42e68572d2,"This paper presents a model for generating French sign language gestures, which is based on both a semi-formal modelling approach, and on a specification formalism yielding to the translation of an utterance into a continuous data flow for the control of a virtual character. This approach benefits from knowledge of structural linguistics proper to sign language, and results of motion capture analysis.","[{'authorId': '2066213664', 'name': 'Sylvie Gibet'}, {'authorId': '2064000639', 'name': 'Alexis Héloir'}]",5.0,"{'bibtex': '@Article{Gibet2007FormalismeDD,\n author = {Sylvie Gibet and Alexis Héloir},\n journal = {Trait. Autom. des Langues},\n pages = {115-149},\n title = {Formalisme de description des gestes de la langue des signes française pour la génération du mouvement de signeurs virtuels [French Sign Language Gesture Description Formalism for the Generation of Virtual Signer Motion]},\n volume = {48},\n year = {2007}\n}\n'}",,"{'volume': '48', 'pages': '115-149', 'name': 'Trait. Autom. des Langues'}",37.0,Formalisme de description des gestes de la langue des signes française pour la génération du mouvement de signeurs virtuels [French Sign Language Gesture Description Formalism for the Generation of Virtual Signer Motion],2007.0
777,3ea76b808abc87297ad1ba45cdfdca65ba4d704c,,"[{'authorId': '7766514', 'name': 'Rebecca E. Martin'}, {'authorId': '2669604', 'name': 'K. Ochsner'}]",82.0,"{'bibtex': '@Article{Martin2016TheNO,\n author = {Rebecca E. Martin and K. Ochsner},\n journal = {Current Opinion in Behavioral Sciences},\n pages = {142-148},\n title = {The neuroscience of emotion regulation development: implications for education},\n volume = {10},\n year = {2016}\n}\n'}",,"{'volume': '10', 'pages': '142-148', 'name': 'Current Opinion in Behavioral Sciences'}",60.0,The neuroscience of emotion regulation development: implications for education,2016.0
778,3eb068252f341ace3b29c7b88f93cf491330a3d2,"The objective of our current work was to create a model for agent memory retrieval of emotionally relevant episodes. We analyzed agent architectures that support memory retrieval realizing that none fulfilled all of our requirements. We designed an episodic memory retrieval model consisting of two main steps: location ecphory, in which the agent's current location is matched against stored memories associated locations; and recollective experience, in which memories that had a positive match are re-appraised. We implemented our model and used it to drive the behavior of characters in a game application. We recorded the application running and used the videos to create a non-interactive evaluation. The evaluation's results are consistent with our hypothesis that agents with memory retrieval of emotionally relevant episodes would be perceived as more believable than similar agents without it.","[{'authorId': '1784861', 'name': 'P. Gomes'}, {'authorId': '145813496', 'name': 'C. Martinho'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",18.0,"{'bibtex': ""@Inproceedings{Gomes2011IveBH,\n author = {P. Gomes and C. Martinho and Ana Paiva},\n pages = {1039-1046},\n title = {I've been here before!: location and appraisal in memory retrieval},\n year = {2011}\n}\n""}",,{'pages': '1039-1046'},30.0,I've been here before!: location and appraisal in memory retrieval,2011.0
780,3ebdcef1c13a3f83a8d755c61815cc6c6ba0653f,"To explore the mechanisms of speech articulation, which is one of the most sophisticated human motor skills controlled by the central nervous system, we investigated the force-generation dynamics of the human speech articulator muscles [orbicularis oris superior (OOS) and inferior (OOI) muscles of the lips]. Short-pulse electrical stimulation (300 micros) with approximately three or four times the sensation threshold intensity of each subject induced the muscle response. The responses of these muscles were modeled as second-order dynamics with a time delay (TD), and the model parameters [natural frequency (NF), damping ratio (DR), and TD] were identified with a nonlinear least mean squares method. The OOS (NF: 6.1 Hz, DR: 0.71, TD: 14.5 ms) and OOI (NF: 6.1 Hz, DR: 0.68, TD: 15.6 ms) showed roughly similar characteristics in eight subjects. The dynamics in the tongue (generated by combined muscles) also showed similar characteristics (NF: 6.1 Hz, DR: 0.68, TD: 17.4 ms) in two subjects. The NF was higher, and the DR was lower than results measured for arm muscles (NF: 4.25 Hz, DR: 1.05, TD: 23.8 ms for triceps long head), indicating that articulatory organs adapt for more rapid movement. In contrast, slower response dynamics was estimated when muscle force data by voluntarily contraction task were used for force-generation dynamics modeling. We discuss methodological problems in estimating muscle dynamics when different kinds of muscle contraction methods are used.","[{'authorId': '145214184', 'name': 'Takayuki Ito'}, {'authorId': '2391497', 'name': 'E. Murano'}, {'authorId': '2561820', 'name': 'H. Gomi'}]",45.0,"{'bibtex': '@Article{Ito2004FastFD,\n author = {Takayuki Ito and E. Murano and H. Gomi},\n journal = {Journal of applied physiology},\n pages = {\n          2318-24; discussion 2317\n        },\n title = {Fast force-generation dynamics of human articulatory muscles.},\n volume = {96 6},\n year = {2004}\n}\n'}",,"{'volume': '96 6', 'pages': '\n          2318-24; discussion 2317\n        ', 'name': 'Journal of applied physiology'}",31.0,Fast force-generation dynamics of human articulatory muscles.,2004.0
781,3ecf1498f7f0da3146c508f3ff0f04c5e766cb42,"The emergence of, what is now called, ‘emotional intellige nce’ has revealed yet another aspect of human intelligence. Emot ions were shown to have a major impact on many of our everyday tasks, including decision-making, planning, communication, and behavior. Researchers have recently acknowledged this major role that emotions play, and thus we see a variety of models bei ng presented on simulating emotions in agents. However, emotion is not a simple process, it is often linked with many other pr ocesses, one of which is learning. As it has long been emphasized throug h psychology literature, memory and experience help shape and build the dynamic nature of the emotional process. In this pa per, we introduce PETEEI (a PET with Evolving Emotional Intelligence). PETEEI is a general model for simulating emotions in agents, with a particular emphasis on incorporating vario us learning mechanisms so that it can produce emotions accor ding to its own experience. Furthermore, it was modeled to recogniz e and cope with the various mood and emotional changes of its owne r. We have implemented PETEEI using fuzzy logic. An evaluation involving twenty-one subjects indicated that simulating the dynamic emotional process through learning provides a significantly more believable agent.","[{'authorId': '1381933697', 'name': 'M. S. El-Nasr'}, {'authorId': '1681317', 'name': 'T. Ioerger'}, {'authorId': '143674364', 'name': 'J. Yen'}]",85.0,"{'bibtex': '@Inproceedings{El-Nasr1999PETEEIAP,\n author = {M. S. El-Nasr and T. Ioerger and J. Yen},\n pages = {9-15},\n title = {PETEEI: a PET with evolving emotional intelligence},\n year = {1999}\n}\n'}",,{'pages': '9-15'},37.0,PETEEI: a PET with evolving emotional intelligence,1999.0
782,3f24bb4bf632cb05df08039cb8b7139a273f3338,"An experiment was carried out to examine the impact on electrodermal activity of people when approached by groups of one or four virtual characters at varying distances. It was premised on the basis of proxemics theory that the closer the approach of the virtual characters to the participant, the greater the level of physiological arousal. Physiological arousal was measured by the number of skin conductance responses within a short time period after the approach, and the maximum change in skin conductance level 5 s after the approach. The virtual characters were each either female or a cylinder of human size, and one or four characters approached each subject a total of 12 times. Twelve male subjects were recruited for the experiment. The results suggest that the number of skin conductance responses after the approach and the change in skin conductance level increased the closer the virtual characters approached toward the participants. Moreover, these response variables were inversely correlated with the number of visits, showing a typical adaptation effect. There was some evidence to suggest that the number of characters who simultaneously approached (one or four) was positively associated with the responses. Surprisingly there was no evidence of a difference in response between the humanoid characters and cylinders on the basis of this physiological data. It is suggested that the similarity in this quantitative arousal response to virtual characters and virtual objects might mask a profound difference in qualitative response, an interpretation supported by questionnaire and interview results. Overall the experiment supported the premise that people exhibit heightened physiological arousal the closer they are approached by virtual characters.","[{'authorId': '48334647', 'name': 'J. Llobera'}, {'authorId': '2891686', 'name': 'B. Spanlang'}, {'authorId': '144679534', 'name': 'G. Ruffini'}, {'authorId': '144931212', 'name': 'M. Slater'}]",133.0,"{'bibtex': '@Article{Llobera2010ProxemicsWM,\n author = {J. Llobera and B. Spanlang and G. Ruffini and M. Slater},\n journal = {ACM Trans. Appl. Percept.},\n pages = {3:1-3:12},\n title = {Proxemics with multiple dynamic characters in an immersive virtual environment},\n volume = {8},\n year = {2010}\n}\n'}",,"{'volume': '8', 'pages': '3:1-3:12', 'name': 'ACM Trans. Appl. Percept.'}",32.0,Proxemics with multiple dynamic characters in an immersive virtual environment,2010.0
784,3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b,,"[{'authorId': '1400326437', 'name': 'Pablo Hernandez-Leal'}, {'authorId': '35224631', 'name': 'Bilal Kartal'}, {'authorId': '39286677', 'name': 'Matthew E. Taylor'}]",420.0,"{'bibtex': '@Article{Hernandez-Leal2018ASA,\n author = {Pablo Hernandez-Leal and Bilal Kartal and Matthew E. Taylor},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {750 - 797},\n title = {A survey and critique of multiagent deep reinforcement learning},\n volume = {33},\n year = {2018}\n}\n'}",,"{'volume': '33', 'pages': '750 - 797', 'name': 'Autonomous Agents and Multi-Agent Systems'}",393.0,A survey and critique of multiagent deep reinforcement learning,2018.0
785,3f7803ced283b37a193e616c43c598e4c1720541,"Attentional bias is a central feature of many cognitive theories of psychopathology. One of the most frequent methods of investigating such bias has been an emotional analog of the Stroop task. In this task, participants name the colors in which words are printed, and the words vary in their relevance to each theme of psychopathology. The authors review research showing that patients are often slower to name the color of a word associated with concerns relevant to their clinical condition. They address the causes and mechanisms underlying the phenomenon, focusing on J.D. Cohen, K. Dunbar, and J.L. McClelland's (1990) parallel distributed processing model.","[{'authorId': '2248803009', 'name': 'G. W I L L I A M S A N D R E W M'}, {'authorId': '2248872775', 'name': 'D. Cohen'}, {'authorId': '2248802661', 'name': 'K. Dun-Bar'}, {'authorId': '2248803920', 'name': 'J. L. Mcclelland'}, {'authorId': '2239436018', 'name': 'J. Mark'}, {'authorId': '2248831872', 'name': 'G. Williams'}, {'authorId': '2248806247', 'name': 'Brendan Bradley'}, {'authorId': '2193978', 'name': 'T. Dalgleish'}, {'authorId': '2248803633', 'name': 'Andy Macleod'}, {'authorId': '2248802574', 'name': 'Karen Mogg'}, {'authorId': '2249058514', 'name': 'Mark G Williams'}, {'authorId': '2248805785', 'name': 'Williams'}]",2443.0,"{'bibtex': '@Article{M1996TheES,\n author = {G. W I L L I A M S A N D R E W M and D. Cohen and K. Dun-Bar and J. L. Mcclelland and J. Mark and G. Williams and Brendan Bradley and T. Dalgleish and Andy Macleod and Karen Mogg and Mark G Williams and Williams},\n journal = {Psychological bulletin},\n pages = {\n          3-24\n        },\n title = {The emotional Stroop task and psychopathology.},\n volume = {120 1},\n year = {1996}\n}\n'}",,"{'volume': '120 1', 'pages': '\n          3-24\n        ', 'name': 'Psychological bulletin'}",103.0,The emotional Stroop task and psychopathology.,1996.0
786,3fb3e336684791fb42f5444769a5f1e5484d1522,,"[{'authorId': '81486204', 'name': 'Qi Dunsworth'}, {'authorId': '1737845', 'name': 'R. Atkinson'}]",165.0,"{'bibtex': ""@Article{Dunsworth2007FosteringML,\n author = {Qi Dunsworth and R. Atkinson},\n journal = {Comput. Educ.},\n pages = {677-690},\n title = {Fostering multimedia learning of science: Exploring the role of an animated agent's image},\n volume = {49},\n year = {2007}\n}\n""}",,"{'volume': '49', 'pages': '677-690', 'name': 'Comput. Educ.'}",25.0,Fostering multimedia learning of science: Exploring the role of an animated agent's image,2007.0
787,3fdbf4e4980464abd2e85defcfe4ebefa4fe0a00,"Crowdsourcing has become a popular paradigm in data curation, annotation and evaluation for many artificial intelligence and information retrieval applications. Considerable efforts have gone into devising effective quality control mechanisms that identify or discourage cheat submissions in an attempt to improve the quality of noisy crowd judgments. Besides purposeful cheating, there is another source of noise that is often alluded to but insufficiently studied: Cognitive biases. This paper investigates the prevalence and effect size of a range of common cognitive biases on a standard relevance judgment task. Our experiments are based on three sizable publicly available document collections and note significant detrimental effects on annotation quality, system ranking and the performance of derived rankers when task design does not account for such biases.","[{'authorId': '1764160', 'name': 'Carsten Eickhoff'}]",112.0,"{'bibtex': '@Article{Eickhoff2018CognitiveBI,\n author = {Carsten Eickhoff},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Cognitive Biases in Crowdsourcing},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining'},64.0,Cognitive Biases in Crowdsourcing,2018.0
788,3ff82ae4683a4eea69d8150a5ebad76932664c04,"The fourth Audio-Visual Emotion Challenge and workshop AVEC 2014 was held in conjunction ACM Multimedia'14. Like the 2013 edition of AVEC, the workshop/challenge addresses the interpretation of social signals represented in both audio and video in terms of high-level continuous dimensions from a large number of clinically depressed patients and controls, with a sub-challenge in self-reported severity of depression estimation. In this summary, we mainly describe participation and its conditions.","[{'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '34030213', 'name': 'J. Krajewski'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",30.0,"{'bibtex': '@Article{Valstar2014AVEC2T,\n author = {M. Valstar and Björn Schuller and J. Krajewski and R. Cowie and M. Pantic},\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\n title = {AVEC 2014: the 4th international audio/visual emotion challenge and workshop},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the 22nd ACM international conference on Multimedia'},7.0,AVEC 2014: the 4th international audio/visual emotion challenge and workshop,2014.0
789,3ff96435f510c67b8f715def7c244d6bece0e7af,"Computer-generated anthropomorphic characters are a growing type of communicator that is deployed in digital communication environments. An essential theoretical question is how people identify humanlike but clearly artificial, hence humanoid, entities in comparison to natural human ones. This identity categorization inquiry was approached under the framework of consistency and tested through examining inconsistency effects from mismatching categories. Study 1 (N = 80), incorporating a self-disclosure task, tested participants’ responses to a talking-face agent, which varied in four combinations of human versus humanoid faces and voices. In line with the literature on inconsistency, the pairing of a human face with a humanoid voice or a humanoid face with a human voice led to longer processing time in making judgment of the agent and less trust than the pairing of a face and a voice from either the human or the humanoid category. Female users particularly showed negative attitudes toward inconsistently paired talking faces. Study 2 (N = 80), using a task that stressed comprehension demand, replicated the inconsistency effects on judging time and females’ negative attitudes but not for comprehension-related outcomes. Voice clarity overshadowed the consistency concern for comprehension-related responses. The overall inconsistency effects suggest that people treat humanoid entities in a different category from natural human ones.","[{'authorId': '2056813456', 'name': 'Li Gong'}, {'authorId': '2029850', 'name': 'C. Nass'}]",122.0,"{'bibtex': '@Article{Gong2007WhenAT,\n author = {Li Gong and C. Nass},\n journal = {Human Communication Research},\n pages = {163-193},\n title = {When a Talking-Face Computer Agent Is Half-Human and Half-Humanoid: Human Identity and Consistency Preference.},\n volume = {33},\n year = {2007}\n}\n'}",,"{'volume': '33', 'pages': '163-193', 'name': 'Human Communication Research'}",60.0,When a Talking-Face Computer Agent Is Half-Human and Half-Humanoid: Human Identity and Consistency Preference.,2007.0
790,4033d0775fc4a02c05866918399949346580efd6,"The idea of autonomous social robots capable of assisting us in our daily lives is becoming more real every day. However, there are still many open issues regarding the social capabilities that those robots should have in order to make daily interactions with humans more natural. For example, the role of affective interactions is still unclear. This paper presents an ethnographic study conducted in an elementary school where 40 children interacted with a social robot capable of recognising and responding empathically to some of the children's affective states. The findings suggest that the robot's empathic behaviour affected positively how children perceived the robot. However, the empathic behaviours should be selected carefully, under the risk of having the opposite effect. The target application scenario and the particular preferences of children seem to influence the “degree of empathy” that social robots should be endowed with.","[{'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '145813496', 'name': 'C. Martinho'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",117.0,"{'bibtex': '@Article{Leite2012ModellingEB,\n author = {Iolanda Leite and Ginevra Castellano and André Pereira and C. Martinho and Ana Paiva},\n journal = {2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {367-374},\n title = {Modelling empathic behaviour in a robotic game companion for children: An ethnographic study in real-world settings},\n year = {2012}\n}\n'}",,"{'pages': '367-374', 'name': '2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",30.0,Modelling empathic behaviour in a robotic game companion for children: An ethnographic study in real-world settings,2012.0
791,404694adbee46926ebadf2d4a02021ca75c73ce9,,"[{'authorId': '1723194', 'name': 'T. Komatsu'}, {'authorId': '9618507', 'name': 'Yukari Abe'}]",22.0,"{'bibtex': '@Inproceedings{Komatsu2008ComparingAO,\n author = {T. Komatsu and Yukari Abe},\n pages = {498-504},\n title = {Comparing an On-Screen Agent with a Robotic Agent in Non-Face-to-Face Interactions},\n year = {2008}\n}\n'}",,{'pages': '498-504'},8.0,Comparing an On-Screen Agent with a Robotic Agent in Non-Face-to-Face Interactions,2008.0
792,404929279543584a71f40d7db8ce5ca977a852d3,"One motive for behaving as the agent of another's aggression appears to be anchored in as yet unelucidated mechanisms of obedience to authority. In a recent partial replication of Milgram's obedience paradigm within an immersive virtual environment, participants administered pain to a female virtual human and observed her suffering. Whether the participants’ response to the latter was more akin to other-oriented empathic concern for her well-being or to a self-oriented aversive state of personal distress in response to her distress is unclear. Using the stimuli from that study, this event-related fMRI-based study analysed brain activity during observation of the victim in pain versus not in pain. This contrast revealed activation in pre-defined brain areas known to be involved in affective processing but not in those commonly associated with affect sharing (e.g., ACC and insula). We then examined whether different dimensions of dispositional empathy predict activity within the same pre-defined brain regions: While personal distress and fantasy (i.e., tendency to transpose oneself into fictional situations and characters) predicted brain activity, empathic concern and perspective taking predicted no change in neuronal response associated with pain observation. These exploratory findings suggest that there is a distinct pattern of brain activity associated with observing the pain-related behaviour of the victim within the context of this social dilemma, that this observation evoked a self-oriented aversive state of personal distress, and that the objective “reality” of pain is of secondary importance for this response. These findings provide a starting point for experimentally more rigorous investigation of obedience.","[{'authorId': '3442607', 'name': 'M. Cheetham'}, {'authorId': '40665260', 'name': 'Andreas Pedroni'}, {'authorId': '1705895', 'name': 'Angus Antley'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '3162334', 'name': 'L. Jäncke'}]",90.0,"{'bibtex': '@Article{Cheetham2009VirtualME,\n author = {M. Cheetham and Andreas Pedroni and Angus Antley and M. Slater and L. Jäncke},\n journal = {Frontiers in Human Neuroscience},\n title = {Virtual Milgram: Empathic Concern or Personal Distress? Evidence from Functional MRI and Dispositional Measures},\n volume = {3},\n year = {2009}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Human Neuroscience'}",111.0,Virtual Milgram: Empathic Concern or Personal Distress? Evidence from Functional MRI and Dispositional Measures,2009.0
793,4061bca09d27481190d4b9bbc62fe6a0ad016278,,"[{'authorId': '4226466', 'name': 'S. Porges'}]",669.0,"{'bibtex': '@Article{Porges2003ThePT,\n author = {S. Porges},\n journal = {Physiology & Behavior},\n pages = {503-513},\n title = {The Polyvagal Theory: phylogenetic contributions to social behavior},\n volume = {79},\n year = {2003}\n}\n'}",,"{'volume': '79', 'pages': '503-513', 'name': 'Physiology & Behavior'}",51.0,The Polyvagal Theory: phylogenetic contributions to social behavior,2003.0
794,407533185267c881f6b30d3e8212dd03b00085d9,"Emotional expressivity can boost trust in human-human and human-machine interaction. As a multimodal phenomenon, previous research argued that a mismatch in the expressive channels provides evidence of joint audio-video emotional processing. However, while previous work studied this from the point of view of emotion recognition and processing, not much is known about what effect a multimodal agent would have on a human-agent interaction task. Also, agent appearance could influence this interaction too. Here we manipulated the agent’s multimodal emotional expression (”smiling face” and ”smiling voice”, or both) and agent type (photorealistic or cartoon-like virtual human) and assessed people’s trust toward this agent. We measured trust using a mixed-methods approach, combining behavioural data from a survival task, questionnaire ratings and qualitative comments. These methods gave different results: while people commented on the importance of emotional expressivity in the agent’s voice, this factor had limited influence on trusting behaviours; while people rated the cartoon-like agent on several traits higher than the photorealistic one, the agent’s style also was not the most influential feature on people’s trusting behaviour. These results highlight the contribution of a mixed-methods approach in human-machine interaction, as both explicit and implicit perception and behaviour will contribute to the success of the interaction.","[{'authorId': '1996210170', 'name': 'Ilaria Torre'}, {'authorId': '3493587', 'name': 'E. Carrigan'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '1869130', 'name': 'Katarina Domijan'}, {'authorId': '51216086', 'name': 'Killian McCabe'}, {'authorId': '144686633', 'name': 'N. Harte'}]",21.0,"{'bibtex': '@Article{Torre2019TheEO,\n author = {Ilaria Torre and E. Carrigan and R. Mcdonnell and Katarina Domijan and Killian McCabe and N. Harte},\n booktitle = {Motion in Games},\n journal = {Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games},\n title = {The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interaction},\n year = {2019}\n}\n'}","[{'paperId': '1cc442f482a9436b260483facf39420a7c3567dc', 'title': 'Comparing Mixed Reality Agent Representations: Studies in the Lab and in the Wild'}, {'paperId': '33ace0e7844e942e17da45bd3e7925ff24f4546f', 'title': 'The Effects of Emotions on Trust in Human-Computer Interaction: A Survey and Prospect'}, {'paperId': '67aa7904c964cd7fa0bcd56c92d955a4f49f3a63', 'title': ""“I See You”: Comparing the Effects of Affective Empathy and Cognitive Empathy on Drivers' Affective States and Driving Behavior in Frustrating Driving Contexts.""}, {'paperId': 'c71e723389e84e152531c38fc32f60604251cc62', 'title': 'Do We Trust Embodied Agents who Look Like us?'}, {'paperId': 'c856d76c777f641b9467a7f97bfe6dc1a9bd0098', 'title': 'Exploring the Social Influence of Virtual Humans Unintentionally Conveying Conflicting Emotions'}, {'paperId': '5f861f96d0234da728de00122934bce792e553fb', 'title': 'Design and user experience analysis of AR intelligent virtual agents on smartphones'}, {'paperId': '288ae770b973f521f9961ad66f87f86d794d671d', 'title': 'Uncanny Valley Effects on Chatbot Trust, Purchase Intention, and Adoption Intention in the Context of E-Commerce: The Moderating Role of Avatar Familiarity'}, {'paperId': '19189c58d52054441a4fbb2b6c8d832c3d280973', 'title': 'Forming robot trust in heterogeneous agents during a multimodal interactive game'}, {'paperId': '3335ff9fb4ab0d3f1ed73921f13ab584dc6a720c', 'title': 'Feeling close to a Crab-Thing in virtual reality: Does avatar appearance always matter in forming meaningful connections? A case study'}, {'paperId': '5a3c3cf6c5c518254e945ab45536153d36209d9d', 'title': 'To smile or not to smile: The effect of mismatched emotional expressions in a Human-Robot cooperative task'}, {'paperId': '512df5eb736411c4941ab54f2f987f6993a0159a', 'title': 'Investigating how speech and animation realism influence the perceived personality of virtual characters and agents'}, {'paperId': '3041b15b1f2f08426585440eb52d2ea3156287bb', 'title': 'How to Evaluate Trust in AI-Assisted Decision Making? A Survey of Empirical Methodologies'}, {'paperId': 'e69dec802c264042d813332a41def172289d1c74', 'title': 'Intelligent autonomous agents and trust in virtual reality'}, {'paperId': '247f9147f0fa07896e677549495fb6cd4a789cb4', 'title': 'Human or Robot?: Investigating voice, appearance and gesture motion realism of conversational social agents'}, {'paperId': 'fe57daf4d7fa523433f58ad8a271e3c000af0054', 'title': 'Appearance'}, {'paperId': 'c5e2c8649c0b22bfdcbdddb7a4d5bd76dbee7bc3', 'title': 'Intelligent Adaptive Agents and Trust in Virtual and Augmented Reality'}, {'paperId': '081c802012114bff8f9a9b0a3853c5ddefa841b0', 'title': 'AAEC: An Adversarial Autoencoder-based Classifier for Audio Emotion Recognition'}, {'paperId': '84f94f3ec0acd9158735cb463d306d3e647996ba', 'title': 'The Impact of Virtual Assistant Advice on Human Trust: An Investigation in a game scenario'}, {'paperId': 'aa805cbe97f40e5026117d586a5e826053f9f0a0', 'title': 'Appearance Rachel McDonnell and Bilge Mutlu'}, {'paperId': '9f3e1b001250ab96aa77621f767835ab20d80ef0', 'title': 'Influence of Visual Appearance of Agents on Presence, Attractiveness, and Agency in Virtual Reality'}, {'paperId': '4cd24c8f92a29589df7f695786cacde43fb4d654', 'title': 'The Uncanny Valley: The human-likeness of chatbots and its paradoxical impact on consumers’ purchase intention in e-commerce'}]","{'name': 'Proceedings of the 12th ACM SIGGRAPH Conference on Motion, Interaction and Games'}",37.0,The Effect of Multimodal Emotional Expression and Agent Appearance on Trust in Human-Agent Interaction,2019.0
795,40c5441aad96b366996e6af163ca9473a19bb9ad,,"[{'authorId': '2109503620', 'name': 'Temple F. Smith'}, {'authorId': '2398669', 'name': 'M. Waterman'}]",10359.0,"{'bibtex': '@Article{Smith1981IdentificationOC,\n author = {Temple F. Smith and M. Waterman},\n journal = {Journal of molecular biology},\n pages = {\n          195-7\n        },\n title = {Identification of common molecular subsequences.},\n volume = {147 1},\n year = {1981}\n}\n'}",,"{'volume': '147 1', 'pages': '\n          195-7\n        ', 'name': 'Journal of molecular biology'}",19.0,Identification of common molecular subsequences.,1981.0
796,40da87c0fea4f05377326b98fb2ca1e962fb7355,"Voicemail has become an integral part of our personal and professional communication. The number of messages that accumulate in our voice mailboxes necessitate new ways of prioritizing them. Currently, we are forced to actively listen to all messages in order to find out which ones are important and which ones can be attended to later on. In this paper, we describe Emotive Alert, a system that can detect some of the significant emotions in a new message and notify the account owner along various affective axes, including urgency, formality, valence (happy vs. sad) and arousal (calm vs. excited). We have used a purely acoustic, HMM-based approach for identifying the emotions, which allows application of this system to all messages independent of language.","[{'authorId': '2571389', 'name': 'Zeynep Inanoglu'}, {'authorId': '2020387', 'name': 'R. Caneel'}]",40.0,"{'bibtex': '@Article{Inanoglu2005EmotiveAH,\n author = {Zeynep Inanoglu and R. Caneel},\n journal = {Proceedings of the 10th international conference on Intelligent user interfaces},\n title = {Emotive alert: HMM-based emotion detection in voicemail messages},\n year = {2005}\n}\n'}",,{'name': 'Proceedings of the 10th international conference on Intelligent user interfaces'},7.0,Emotive alert: HMM-based emotion detection in voicemail messages,2005.0
797,417337dd50423a1957eed79ec8bb2b1fae772bbf,"This paper considers how we feel about the content we see or hear. As opposed to the cognitive content information composed of the facts about the genre, temporal content structures and spatiotemporal content elements, we are interested in obtaining the information about the feelings, emotions, and moods evoked by a speech, audio, or video clip. We refer to the latter as the affective content, and to the terms such as happy or exciting as the affective labels of an audiovisual signal. In the first part of the paper, we explore the possibilities for representing and modeling the affective content of an audiovisual signal to effectively bridge the affective gap. Without loosing generality, we refer to this signal simply as video, which we see as an image sequence with an accompanying soundtrack. Then, we show the high potential of the affective video content analysis for enhancing the content recommendation functionalities of the future PVRs and VOD systems. We conclude this paper by outlining some interesting research challenges in the field","[{'authorId': '1718099', 'name': 'A. Hanjalic'}]",216.0,"{'bibtex': '@Article{Hanjalic2006ExtractingMF,\n author = {A. Hanjalic},\n journal = {IEEE Signal Processing Magazine},\n pages = {90-100},\n title = {Extracting moods from pictures and sounds: towards truly personalized TV},\n volume = {23},\n year = {2006}\n}\n'}",,"{'volume': '23', 'pages': '90-100', 'name': 'IEEE Signal Processing Magazine'}",37.0,Extracting moods from pictures and sounds: towards truly personalized TV,2006.0
798,417b37b3256b2ca7567f0dc2ca3c96c61f3a8376,"Abstract Objective: Understanding patient responses to psychotherapy is important in developing effective interventions. However, coding patient language is a resource-intensive exercise and difficult to perform at scale. Our aim was to develop a deep learning model to automatically identify patient utterances during text-based internet-enabled Cognitive Behavioural Therapy and to determine the association between utterances and clinical outcomes. Method: Using 340 manually annotated transcripts we trained a deep learning model to categorize patient utterances into one or more of five categories. The model was used to automatically code patient utterances from our entire data set of transcripts (∼34,000 patients), and logistic regression analyses used to determine the association between both reliable improvement and engagement, and patient responses. Results: Our model reached human-level agreement on three of the five patient categories. Regression analyses revealed that increased counter change-talk (movement away from change) was associated with lower odds of both reliable improvement and engagement, while increased change-talk (movement towards change or self-exploration) was associated with increased odds of improvement and engagement. Conclusions: Deep learning provides an effective means of automatically coding patient utterances at scale. This approach enables the development of a data-driven understanding of the relationship between therapist and patient during therapy.","[{'authorId': '1984805', 'name': 'M. Ewbank'}, {'authorId': '153714664', 'name': 'R. Cummins'}, {'authorId': '2095029103', 'name': 'V. Tablan'}, {'authorId': '94314925', 'name': 'A. Catarino'}, {'authorId': '122886979', 'name': 'S. Buchholz'}, {'authorId': '7176472', 'name': 'A. Blackwell'}]",19.0,"{'bibtex': '@Article{Ewbank2020UnderstandingTR,\n author = {M. Ewbank and R. Cummins and V. Tablan and A. Catarino and S. Buchholz and A. Blackwell},\n journal = {Psychotherapy Research},\n pages = {300 - 312},\n title = {Understanding the relationship between patient language and outcomes in internet-enabled cognitive behavioural therapy: A deep learning approach to automatic coding of session transcripts},\n volume = {31},\n year = {2020}\n}\n'}",,"{'volume': '31', 'pages': '300 - 312', 'name': 'Psychotherapy Research'}",36.0,Understanding the relationship between patient language and outcomes in internet-enabled cognitive behavioural therapy: A deep learning approach to automatic coding of session transcripts,2020.0
799,4182e7d996577fffe8aada56a7439c29fe07b2c6,,"[{'authorId': '2110202993', 'name': 'Craig A. Smith'}, {'authorId': '39504235', 'name': 'L. D. Kirby'}]",166.0,"{'bibtex': '@Inproceedings{Smith2000ConsequencesRA,\n author = {Craig A. Smith and L. D. Kirby},\n title = {Consequences require antecedents: Toward a process model of emotion elicitation.},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Consequences require antecedents: Toward a process model of emotion elicitation.,2000.0
800,41fb40a942f6d2cc59021d393d5283462a0f7f1f,"This paper describes a simulation of attention behaviour aimed at computer-animated characters. Attention is the focusing of a 
person’s perception on a particular object. This is useful for computer animation as it determines which objects the character is aware 
of: information that can be used in the simulation of the character’s behaviour in order to automatically animate the character. The 
simulation of attention also determines where the character is looking and so is used to produce gaze behaviour","[{'authorId': '2589934', 'name': 'M. Gillies'}, {'authorId': '1743917', 'name': 'N. Dodgson'}]",46.0,"{'bibtex': '@Article{Gillies2002EyeMA,\n author = {M. Gillies and N. Dodgson},\n journal = {Comput. Animat. Virtual Worlds},\n pages = {287-300},\n title = {Eye movements and attention for behavioural animation},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '287-300', 'name': 'Comput. Animat. Virtual Worlds'}",13.0,Eye movements and attention for behavioural animation,2002.0
801,42088b0e81007c52be6c0a0a3a4dff254668d59c,"Significance Humans spend a large percentage of their time perceiving the appearance, actions, and intentions of others, and extensive previous research has identified multiple brain regions engaged in these functions. However, social life depends on the ability to understand not just individuals, but also groups and their interactions. Here we show that a specific region of the posterior superior temporal sulcus responds strongly and selectively when viewing social interactions between two other agents. This region also contains information about whether the interaction is positive (helping) or negative (hindering), and may underlie our ability to perceive, understand, and navigate within our social world. Primates are highly attuned not just to social characteristics of individual agents, but also to social interactions between multiple agents. Here we report a neural correlate of the representation of social interactions in the human brain. Specifically, we observe a strong univariate response in the posterior superior temporal sulcus (pSTS) to stimuli depicting social interactions between two agents, compared with (i) pairs of agents not interacting with each other, (ii) physical interactions between inanimate objects, and (iii) individual animate agents pursuing goals and interacting with inanimate objects. We further show that this region contains information about the nature of the social interaction—specifically, whether one agent is helping or hindering the other. This sensitivity to social interactions is strongest in a specific subregion of the pSTS but extends to a lesser extent into nearby regions previously implicated in theory of mind and dynamic face perception. This sensitivity to the presence and nature of social interactions is not easily explainable in terms of low-level visual features, attention, or the animacy, actions, or goals of individual agents. This region may underlie our ability to understand the structure of our social world and navigate within it.","[{'authorId': '2166242', 'name': 'Leyla Isik'}, {'authorId': '2649806', 'name': 'Kami Koldewyn'}, {'authorId': '2081627829', 'name': 'David Beeler'}, {'authorId': '1931482', 'name': 'N. Kanwisher'}]",163.0,"{'bibtex': '@Article{Isik2017PerceivingSI,\n author = {Leyla Isik and Kami Koldewyn and David Beeler and N. Kanwisher},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {E9145 - E9152},\n title = {Perceiving social interactions in the posterior superior temporal sulcus},\n volume = {114},\n year = {2017}\n}\n'}",,"{'volume': '114', 'pages': 'E9145 - E9152', 'name': 'Proceedings of the National Academy of Sciences'}",53.0,Perceiving social interactions in the posterior superior temporal sulcus,2017.0
802,42146f07e40724ce8ec25d0bc41f14b7c78ac799,,"[{'authorId': '93969432', 'name': 'P. Young'}, {'authorId': '67090615', 'name': 'M. B. Arnold'}]",1817.0,"{'bibtex': '@Article{Young1963EmotionAP,\n author = {P. Young and M. B. Arnold},\n journal = {American Journal of Psychology},\n pages = {516},\n title = {Emotion and personality},\n volume = {76},\n year = {1963}\n}\n'}",,"{'volume': '76', 'pages': '516', 'name': 'American Journal of Psychology'}",0.0,Emotion and personality,1963.0
803,421d7bc45195dae4645f63a99d76e8e0d66c9b87,"Virtual characters contribute strongly to the entire visuals of 3D animated films. However, designing believable characters remains a challenging task. Artists rely on stylization to increase appeal or expressivity, exaggerating or softening specific features. In this paper we analyze two of the most influential factors that define how a character looks: shape and material. With the help of artists, we design a set of carefully crafted stimuli consisting of different stylization levels for both parameters, and analyze how different combinations affect the perceived realism, appeal, eeriness, and familiarity of the characters. Moreover, we additionally investigate how this affects the perceived intensity of different facial expressions (sadness, anger, happiness, and surprise). Our experiments reveal that shape is the dominant factor when rating realism and expression intensity, while material is the key component for appeal. Furthermore our results show that realism alone is a bad predictor for appeal, eeriness, or attractiveness.","[{'authorId': '39800733', 'name': 'E. Zell'}, {'authorId': '2371211', 'name': 'Carlos Aliaga'}, {'authorId': '2208378', 'name': 'A. Jarabo'}, {'authorId': '1710384', 'name': 'Katja Zibrek'}, {'authorId': '143876232', 'name': 'D. Gutierrez'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '1716234', 'name': 'M. Botsch'}]",94.0,"{'bibtex': '@Article{Zell2015ToSO,\n author = {E. Zell and Carlos Aliaga and A. Jarabo and Katja Zibrek and D. Gutierrez and R. Mcdonnell and M. Botsch},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 12},\n title = {To stylize or not to stylize?},\n volume = {34},\n year = {2015}\n}\n'}",,"{'volume': '34', 'pages': '1 - 12', 'name': 'ACM Transactions on Graphics (TOG)'}",44.0,To stylize or not to stylize?,2015.0
804,42420cf7a07d089a24944f802bcae71606516af2,"To design virtual agents that simulate humans in repeated decision making under uncertainty, we seek to quantitatively characterize the actual human behavior in these settings. We collect our data from 800 real human subjects through a large-scale randomized online experiment. We evaluate the performance of a wide range of computational models in fitting the data by both conducting a scalable search through the space of two-component models (i.e. inference + selection model) and investigating a few rules of thumb. 
 
Our results suggest that across different decision-making environment, an average human decision maker can be best described by a two-component model, which is composed of an inference model that relies heavily on more recent information (i.e. displays recency bias) and a selection model which assumes cost-proportional errors and reluctance to change in subsequent trials (i.e. displays status-quo bias). Additionally, while a large portion of individuals behave like the average decision maker, how they differ from each other is greatly influenced by the environment. These results imply the possibility of constructing agents with a single type of model that is robust against the context, and provide insights into adjusting heterogeneity among multiple agents based on the context.","[{'authorId': '2053888438', 'name': 'Ming Yin'}, {'authorId': '2108705456', 'name': 'Yu-An Sun'}]",3.0,"{'bibtex': '@Inproceedings{Yin2015HumanBM,\n author = {Ming Yin and Yu-An Sun},\n pages = {581-589},\n title = {Human Behavior Models for Virtual Agents in Repeated Decision Making under Uncertainty},\n year = {2015}\n}\n'}",,{'pages': '581-589'},29.0,Human Behavior Models for Virtual Agents in Repeated Decision Making under Uncertainty,2015.0
805,4246caa136ede0b59b4a762f24e9907f0f0e969c,,"[{'authorId': '144109100', 'name': 'B. Wicker'}, {'authorId': '46646879', 'name': 'C. Keysers'}, {'authorId': '1816716', 'name': 'J. Plailly'}, {'authorId': '2936760', 'name': 'J. Royet'}, {'authorId': '2914469', 'name': 'V. Gallese'}, {'authorId': '2460061', 'name': 'G. Rizzolatti'}]",2138.0,"{'bibtex': '@Article{Wicker2003BothOU,\n author = {B. Wicker and C. Keysers and J. Plailly and J. Royet and V. Gallese and G. Rizzolatti},\n journal = {Neuron},\n pages = {655-664},\n title = {Both of Us Disgusted in My Insula The Common Neural Basis of Seeing and Feeling Disgust},\n volume = {40},\n year = {2003}\n}\n'}",,"{'volume': '40', 'pages': '655-664', 'name': 'Neuron'}",76.0,Both of Us Disgusted in My Insula The Common Neural Basis of Seeing and Feeling Disgust,2003.0
806,42692324ae66e59d9b61877055a4d122487793fe,"Mental health problems are inseparable from the environment. With virtual reality (VR), computer-generated interactive environments, individuals can repeatedly experience their problematic situations and be taught, via evidence-based psychological treatments, how to overcome difficulties. VR is moving out of specialist laboratories. Our central aim was to describe the potential of VR in mental health, including a consideration of the first 20 years of applications. A systematic review of empirical studies was conducted. In all, 285 studies were identified, with 86 concerning assessment, 45 theory development, and 154 treatment. The main disorders researched were anxiety (n = 192), schizophrenia (n = 44), substance-related disorders (n = 22) and eating disorders (n = 18). There are pioneering early studies, but the methodological quality of studies was generally low. The gaps in meaningful applications to mental health are extensive. The most established finding is that VR exposure-based treatments can reduce anxiety disorders, but there are numerous research and treatment avenues of promise. VR was found to be a much-misused term, often applied to non-interactive and non-immersive technologies. We conclude that VR has the potential to transform the assessment, understanding and treatment of mental health problems. The treatment possibilities will only be realized if – with the user experience at the heart of design – the best immersive VR technology is combined with targeted translational interventions. The capability of VR to simulate reality could greatly increase access to psychological therapies, while treatment outcomes could be enhanced by the technology's ability to create new realities. VR may merit the level of attention given to neuroimaging.","[{'authorId': '2136711807', 'name': 'Daniel Freeman'}, {'authorId': '29021285', 'name': 'S. Reeve'}, {'authorId': '48325879', 'name': 'A. Robinson'}, {'authorId': '145848941', 'name': 'A. Ehlers'}, {'authorId': '2117734702', 'name': 'D. Clark'}, {'authorId': '2891686', 'name': 'B. Spanlang'}, {'authorId': '144931212', 'name': 'M. Slater'}]",628.0,"{'bibtex': '@Article{Freeman2017VirtualRI,\n author = {Daniel Freeman and S. Reeve and A. Robinson and A. Ehlers and D. Clark and B. Spanlang and M. Slater},\n journal = {Psychological Medicine},\n pages = {2393 - 2400},\n title = {Virtual reality in the assessment, understanding, and treatment of mental health disorders},\n volume = {47},\n year = {2017}\n}\n'}",,"{'volume': '47', 'pages': '2393 - 2400', 'name': 'Psychological Medicine'}",67.0,"Virtual reality in the assessment, understanding, and treatment of mental health disorders",2017.0
808,427f1ec3a8dea8fdd6a50ac2bbb46d983de26e79,"The decision-making by agents in games is commonly based on reinforcement learning. To improve the quality of agents, it is necessary to solve the problems of the time and state space that are required for learning. Such problems can be solved by Macro-Actions, which are defined and executed by a sequence of primitive actions. In this line of research, the learning time is reduced by cutting down the number of policy decisions by agents. Macro-Actions were originally defined as combinations of the same primitive actions. Based on studies that showed the generation of Macro-Actions by learning, Macro-Actions are now thought to consist of diverse kinds of primitive actions. However an enormous amount of learning time and state space are required to generate Macro-Actions. To resolve these issues, we can apply insights from studies on the learning of tasks through Programming by Demonstration (PbD) to generate Macro- Actions that reduce the learning time and state space. In this paper, we propose a method to define and execute Macro-Actions. Macro-Actions are learned from a human subject via PbD and a policy is learned by reinforcement learning. In an experiment, the proposed method was applied to a car simulation to verify the scalability of the proposed method. Data was collected from the driving control of a human subject, and then the Macro- Actions that are required for running a car were generated. Furthermore, the policy that is necessary for driving on a track was learned. The acquisition of Macro-Actions by PbD reduced the driving time by about 16% compared to the case in which Macro-Actions were directly defined by a human subject. In addition, the learning time was also reduced by a faster convergence of the optimum policies.","[{'authorId': '38968784', 'name': 'Yunsick Sung'}, {'authorId': '2818294', 'name': 'Kyungeun Cho'}]",3.0,"{'bibtex': '@Article{Sung2012AMF,\n author = {Yunsick Sung and Kyungeun Cho},\n journal = {J. Inf. Process. Syst.},\n pages = {409-420},\n title = {A Method for Learning Macro-Actions for Virtual Characters Using Programming by Demonstration and Reinforcement Learning},\n volume = {8},\n year = {2012}\n}\n'}",,"{'volume': '8', 'pages': '409-420', 'name': 'J. Inf. Process. Syst.'}",14.0,A Method for Learning Macro-Actions for Virtual Characters Using Programming by Demonstration and Reinforcement Learning,2012.0
809,4288cbc1fbc4f620045ca193086c1d476145dfae,"As we articulate speech, we usually move the head and exhibit various facial expressions. This visual aspect of speech aids understanding and helps communicating additional information, such as the speaker's mood. We analyze quantitatively head and facial movements that accompany speech and investigate how they relate to the text's prosodic structure. We recorded several hours of speech and measured the locations of the speakers' main facial features as well as their head poses. The text was evaluated with a prosody prediction tool, identifying phrase boundaries and pitch accents. Characteristic for most speakers are simple motion patterns that are repeatedly applied in synchrony with the main prosodic events. Direction and strength of head movements vary widely from one speaker to another, yet their timing is typically well synchronized with the spoken text. Understanding quantitatively the correlations between head movements and spoken text is important for synthesizing photo-realistic talking heads. Talking heads appear much more engaging when they exhibit realistic motion patterns.","[{'authorId': '1775043', 'name': 'H. Graf'}, {'authorId': '3165487', 'name': 'E. Cosatto'}, {'authorId': '3151603', 'name': 'V. Strom'}, {'authorId': '13919023', 'name': 'Fu Jie Huang'}]",208.0,"{'bibtex': '@Article{Graf2002VisualPF,\n author = {H. Graf and E. Cosatto and V. Strom and Fu Jie Huang},\n journal = {Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition},\n pages = {396-401},\n title = {Visual prosody: facial movements accompanying speech},\n year = {2002}\n}\n'}",,"{'pages': '396-401', 'name': 'Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition'}",11.0,Visual prosody: facial movements accompanying speech,2002.0
810,428d4aa8c331bb4609126443c942463aa8a5d74b,"Emotional expressions influence social judgments of personality traits. The goal of the present research was to show that it is of interest to assess the impact of neutral expressions in this context. In 2 studies using different methodologies, the authors found that participants perceived men who expressed neutral and angry emotions as higher in dominance when compared with men expressing sadness or shame. Study 1 showed that this is also true for men expressing happiness. In contrast, women expressing either anger or happiness were perceived as higher in dominance than were women showing a neutral expression who were rated as less dominant. However, sadness expressions by both men and women clearly decreased the extent to which they were perceived as dominant, and a trend in this direction emerged for shame expressions by men in Study 2. Thus, neutral expressions seem to be perceived as a sign of dominance in men but not in women. The present findings extend our understanding of the way different emotional expressions affect perceived dominance and the signal function of neutral expressions-which in the past have often been ignored.","[{'authorId': '3141618', 'name': 'Shlomo Hareli'}, {'authorId': '48082188', 'name': 'Noga Shomrat'}, {'authorId': '3067657', 'name': 'U. Hess'}]",132.0,"{'bibtex': '@Article{Hareli2009EmotionalVN,\n author = {Shlomo Hareli and Noga Shomrat and U. Hess},\n journal = {Emotion},\n pages = {\n          378-84\n        },\n title = {Emotional versus neutral expressions and perceptions of social dominance and submissiveness.},\n volume = {9 3},\n year = {2009}\n}\n'}",,"{'volume': '9 3', 'pages': '\n          378-84\n        ', 'name': 'Emotion'}",35.0,Emotional versus neutral expressions and perceptions of social dominance and submissiveness.,2009.0
811,429ed4c9845d0abd1f8204e1d7705919559bc2a2,"Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. 
We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.","[{'authorId': '2206490', 'name': 'Marcin Andrychowicz'}, {'authorId': '150074096', 'name': 'Dwight Crow'}, {'authorId': '2064770039', 'name': 'Alex Ray'}, {'authorId': '2113526509', 'name': 'Jonas Schneider'}, {'authorId': '2062025076', 'name': 'Rachel Fong'}, {'authorId': '2930640', 'name': 'P. Welinder'}, {'authorId': '39593364', 'name': 'Bob McGrew'}, {'authorId': '2052880384', 'name': 'Joshua Tobin'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}]",1790.0,"{'bibtex': '@Article{Andrychowicz2017HindsightER,\n author = {Marcin Andrychowicz and Dwight Crow and Alex Ray and Jonas Schneider and Rachel Fong and P. Welinder and Bob McGrew and Joshua Tobin and P. Abbeel and Wojciech Zaremba},\n journal = {ArXiv},\n title = {Hindsight Experience Replay},\n volume = {abs/1707.01495},\n year = {2017}\n}\n'}",,"{'volume': 'abs/1707.01495', 'name': 'ArXiv'}",47.0,Hindsight Experience Replay,2017.0
812,42ab21d97bd55f4dc0a2afe049f66b0c0b508f44,,"[{'authorId': '47397964', 'name': 'E. S. Knowles'}]",51.0,"{'bibtex': '@Article{Knowles1976GroupSA,\n author = {E. S. Knowles},\n journal = {Journal of Personality and Social Psychology},\n pages = {647-654},\n title = {Group size and the extension of social space boundaries.},\n volume = {33},\n year = {1976}\n}\n'}",,"{'volume': '33', 'pages': '647-654', 'name': 'Journal of Personality and Social Psychology'}",9.0,Group size and the extension of social space boundaries.,1976.0
813,42b39a9906eb3bceaeb0c4e88d6071405d9daf00,"""A tour de force. If you read this book, you'll never look at other people in quite the same way again."" Malcolm GladwellRenowned psychologist Paul Ekman explains the roots of our emotions anger, fear, disgust, sadness, and happiness and shows how they cascade across our faces, providing clear signals to those who can identify the clues. As featured in Malcolm Gladwell's bestseller ""Blink,"" Ekman's Facial Action Coding System offers intense training in recognizing feelings in spouses, children, colleagues, even strangers on the street. In ""Emotions Revealed,"" Ekman distills decades of research into a practical, mind-opening, and life-changing guide to reading the emotions of those around us. He answers such questions as: How does our body signal to others whether we are slightly sad or anguished, peeved or enraged? Can we learn to distinguish between a polite smile and the genuine thing? Can we ever truly control our emotions? Packed with unique exercises and photographs, and a new chapter on emotions and lying that encompasses security and terrorism as well as gut decisions, ""Emotions Revealed"" is an indispensable resource for navigating our emotional world.""","[{'authorId': '21451088', 'name': 'P. Ekman'}]",1445.0,"{'bibtex': '@Inproceedings{Ekman2003EmotionsRR,\n author = {P. Ekman},\n title = {Emotions Revealed: Recognizing Faces and Feelings to Improve Communication and Emotional Life},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",18.0,Emotions Revealed: Recognizing Faces and Feelings to Improve Communication and Emotional Life,2003.0
814,42dd521dbf8ec1c7c1cffba0bccf08bd93908566,"If humanoid robots are to become commonplace in our society, it is important to understand how they are perceived by humans. An influent model in social cognitive neuroscience posits that in human face-to-face interaction, the observation of another individual performing an action facilitates the execution of a similar action, and interferes with the execution of different action. In one interference experiment, null interference was reported when subjects observed an industrial robotic arm moving at a constant velocity perform an incongruent task, suggesting that this effect may be specific to interacting with other humans. This experimental paradigm was adapted to investigate how humanoid robots interfere with humans. Subjects performed rhythmic arm movements while observing either a human agent or humanoid robot performing either congruent or incongruent movements with comparable kinematics. The variance of the executed movements was used as a measure of the amount of interference in the movements. In a previous report, we reported that in contrast to the robotic arm, the humanoid robot caused a significant increase of the variance of the movement during the incongruent condition. In the present report we investigate the effect of the movement kinematics on the interference. The humanoid robot moved either with a biological motion, based on a realistic model of human motion, or with an artificial motion. We investigated the variance of the subjects' movement during the incongruent condition, with the hypothesis that it should be reduced for the artificial movement in comparison to the biological movement. We found a significant effect of the factors defining the experimental conditions, congruency and type of movements' kinematics, on the subjects' variation. Congruency was found to have the expected effect on the area, but the increase in incongruent conditions was only significant when the robot movements followed biological motion. This result implies that motion is a significant factor for the interference effect","[{'authorId': '1728769', 'name': 'T. Chaminade'}, {'authorId': '1681646', 'name': 'D. W. Franklin'}, {'authorId': '2528981', 'name': 'E. Oztop'}, {'authorId': '1704474', 'name': 'G. Cheng'}]",84.0,"{'bibtex': '@Article{Chaminade2005MotorIB,\n author = {T. Chaminade and D. W. Franklin and E. Oztop and G. Cheng},\n journal = {Proceedings. The 4nd International Conference on Development and Learning, 2005.},\n pages = {96-101},\n title = {Motor interference between Humans and Humanoid Robots: Effect of Biological and Artificial Motion},\n year = {2005}\n}\n'}",,"{'pages': '96-101', 'name': 'Proceedings. The 4nd International Conference on Development and Learning, 2005.'}",26.0,Motor interference between Humans and Humanoid Robots: Effect of Biological and Artificial Motion,2005.0
815,42e1b4c2b60baed1ee771a08946e2f430e0a6f15,"The prefrontal cortex (PFC) plays a critical role in the generation and regulation of emotion. However, we lack an integrative framework for understanding how different emotion-related functions are organized across the entire expanse of the PFC, as prior reviews have generally focused on specific emotional processes (e.g., decision making) or specific anatomical regions (e.g., orbitofrontal cortex). Additionally, psychological theories and neuroscientific investigations have proceeded largely independently because of the lack of a common framework. Here, we provide a comprehensive review of functional neuroimaging, electrophysiological, lesion, and structural connectivity studies on the emotion-related functions of 8 subregions spanning the entire PFC. We introduce the appraisal-by-content model, which provides a new framework for integrating the diverse range of empirical findings. Within this framework, appraisal serves as a unifying principle for understanding the PFC’s role in emotion, while relative content-specialization serves as a differentiating principle for understanding the role of each subregion. A synthesis of data from affective, social, and cognitive neuroscience studies suggests that different PFC subregions are preferentially involved in assigning value to specific types of inputs: exteroceptive sensations, episodic memories and imagined future events, viscero-sensory signals, viscero-motor signals, actions, others’ mental states (e.g., intentions), self-related information, and ongoing emotions. We discuss the implications of this integrative framework for understanding emotion regulation, value-based decision making, emotional salience, and refining theoretical models of emotion. This framework provides a unified understanding of how emotional processes are organized across PFC subregions and generates new hypotheses about the mechanisms underlying adaptive and maladaptive emotional functioning.","[{'authorId': '4296355', 'name': 'Matthew L. Dixon'}, {'authorId': '6635627', 'name': 'Ravi Thiruchselvam'}, {'authorId': '38124822', 'name': 'R. Todd'}, {'authorId': '2788051', 'name': 'K. Christoff'}]",383.0,"{'bibtex': '@Article{Dixon2017EmotionAT,\n author = {Matthew L. Dixon and Ravi Thiruchselvam and R. Todd and K. Christoff},\n journal = {Psychological Bulletin},\n pages = {1033–1081},\n title = {Emotion and the Prefrontal Cortex: An Integrative Review},\n volume = {143},\n year = {2017}\n}\n'}",,"{'volume': '143', 'pages': '1033–1081', 'name': 'Psychological Bulletin'}",499.0,Emotion and the Prefrontal Cortex: An Integrative Review,2017.0
816,42e416a3448c3825bd20675b8d3e527a1b98d593,"Two experiments examined the processes by which positive and negative mood states produce attitude change under high elaboration conditions. We hypothesized that under high elaboration conditions, mood would influence attitudes by affecting the perceived likelihood of occurrence for consequences presented in message arguments. In Experiment I, arguments were framed positively, and positive mood led to greater perceived likelihood of the consequences and more favourable attitudes than negative mood for subjects high in need for cognition (NC). In Experiment 2, arguments were framed either positively or negatively, and a mood × frame interaction was obtained on attitude and likelihood judgments for high-NC subjects. That is, positive mood led to marginally greater perceived likelihood of positive consequences but to lower likelihood of negative consequences as compared to negative mood. As a result, positive mood tended to lead to more persuasion than negative mood when the message was framed positively, but to less persuasion when the message was framed negatively In both experiments, path analyses supported the prediction that likelihood judgments mediated the impact of mood on attitudes for high-NC individuals.","[{'authorId': '32464299', 'name': 'D. Wegener'}, {'authorId': '32761043', 'name': 'R. Petty'}, {'authorId': '116959163', 'name': 'David J. Klein'}]",181.0,"{'bibtex': '@Article{Wegener1994EffectsOM,\n author = {D. Wegener and R. Petty and David J. Klein},\n journal = {European Journal of Social Psychology},\n pages = {25-43},\n title = {Effects of mood on high elaboration attitude change: The mediating role of likelihood judgments},\n volume = {24},\n year = {1994}\n}\n'}",,"{'volume': '24', 'pages': '25-43', 'name': 'European Journal of Social Psychology'}",36.0,Effects of mood on high elaboration attitude change: The mediating role of likelihood judgments,1994.0
817,42f0e4fe7b1e3e15e3c5597c398cfd950571eb24,"In interactive narratives, experience management is used to control the world and the nonplayer characters (NPCs) a player interacts with, encouraging particular types of stories or discouraging others. The space of all stories in a narrative can be understood as a story graph, with story states as nodes and actions in the story as directed edges. In this article, we present experience management as a graph pruning problem. Starting with the full story graph, edges representing NPC actions may be pruned until there is at most one action per NPC per state. With the full graph available, the choice of what to prune may consider all possible futures, and we can ensure that undesirable stories are not reachable. By never pruning player actions, we ensure the player may make any choice and still be accommodated in the story. When this method was used to manage the story of an adventure game, players found our technique generally produced higher agency and more-believable NPC behaviors than a control. Finally, we discuss scaling the results of this method for practical use.","[{'authorId': '34810994', 'name': 'Stephen G. Ware'}, {'authorId': '2111388875', 'name': 'E. T. Garcia'}, {'authorId': '2188306705', 'name': 'Mira Fisher'}, {'authorId': '144650313', 'name': 'A. Shirvani'}, {'authorId': '38518031', 'name': 'Rachelyn Farrell'}]",20.0,"{'bibtex': '@Article{Ware2019MultiagentNE,\n author = {Stephen G. Ware and E. T. Garcia and Mira Fisher and A. Shirvani and Rachelyn Farrell},\n journal = {IEEE Transactions on Games},\n pages = {378-387},\n title = {Multiagent Narrative Experience Management as Story Graph Pruning},\n volume = {15},\n year = {2019}\n}\n'}",,"{'volume': '15', 'pages': '378-387', 'name': 'IEEE Transactions on Games'}",40.0,Multiagent Narrative Experience Management as Story Graph Pruning,2019.0
818,42f1e01b8e405dd70318f61b323fa433cf94cb95,"We examined the effects of a robots' nonverbal response on evaluations of anthropomorphism and other dimensions (e.g., liking, closeness, pleasantness of human-robot interaction) in a case study. Our work both conceptually replicates and extends previous research: On the one hand, we replicated previous findings and generalized them to a different robot type, the iCat. On the other hand, our work extends existing research in that it includes a wider range of dependent variables, with a particular focus on perceptions of anthropomorphism. Taken together, most of our results support the experimental hypotheses for the dependent measures: That is, a robot that provided emotional feedback during the interaction was perceived to be superior to a robot that responded neutrally. Thus, our findings highlight the importance of the interplay of form and function in the attribution of humanness to robots.","[{'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '2065189', 'name': 'F. Hegel'}, {'authorId': '1935524', 'name': 'G. Horstmann'}, {'authorId': '145391245', 'name': 'Claudia Wagner'}]",94.0,"{'bibtex': '@Article{Eyssel2010AnthropomorphicIF,\n author = {F. Eyssel and F. Hegel and G. Horstmann and Claudia Wagner},\n journal = {19th International Symposium in Robot and Human Interactive Communication},\n pages = {646-651},\n title = {Anthropomorphic inferences from emotional nonverbal cues: A case study},\n year = {2010}\n}\n'}",,"{'pages': '646-651', 'name': '19th International Symposium in Robot and Human Interactive Communication'}",25.0,Anthropomorphic inferences from emotional nonverbal cues: A case study,2010.0
819,4311d7793e86eda5c34e2ae6047cb3e0617656a4,,"[{'authorId': '2724558', 'name': 'Dieta Kuchenbrandt'}, {'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '3120498', 'name': 'Simon Bobinger'}, {'authorId': '2074270742', 'name': 'Maria Neufeld'}]",93.0,"{'bibtex': '@Article{Kuchenbrandt2013WhenAR,\n author = {Dieta Kuchenbrandt and F. Eyssel and Simon Bobinger and Maria Neufeld},\n journal = {International Journal of Social Robotics},\n pages = {409-417},\n title = {When a Robot’s Group Membership Matters},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '409-417', 'name': 'International Journal of Social Robotics'}",43.0,When a Robot’s Group Membership Matters,2013.0
820,433aa68fa1834093bd1e0696f8b832c33d0030a0,"Abstract The big-five personality factors were investigated using the trait pleasure-arousability-dominance (PAD) temperament model to assess overlap, and, specifically, similarities and differences, among the five dimensions. Results showed that extraverts were primarily dominant and secondarily pleasant, Agreeableness resembled dependency with pleasant, arousable, and submissive characteristics, but involved greater pleasantness. Conscientiousness included equal degrees of pleasant and dominant qualities. Emotional stability involved almost equal degrees of pleasant and unarousable characteristics, lacking the important dominant feature in this trait. Sophistication was weighted primarily by dominant, and secondarily by arousable, characteristics. The PAD scales explained approximately 75% of the reliable variance in three of the factors (extraversion emotional stability, agreeableness) that have been identified, albeit sometimes with differing labels, in alternative general approaches to personality de...","[{'authorId': '144102217', 'name': 'A. Mehrabian'}]",176.0,"{'bibtex': '@Article{Mehrabian1996AnalysisOT,\n author = {A. Mehrabian},\n journal = {Australian Journal of Psychology},\n pages = {86-92},\n title = {Analysis of the Big‐five Personality Factors in Terms of the PAD Temperament Model},\n volume = {48},\n year = {1996}\n}\n'}",,"{'volume': '48', 'pages': '86-92', 'name': 'Australian Journal of Psychology'}",34.0,Analysis of the Big‐five Personality Factors in Terms of the PAD Temperament Model,1996.0
823,43422c66da26cbcf8315eb916cfc833bbbb4954e,,"[{'authorId': '4053415', 'name': 'S. Raffard'}, {'authorId': '6877243', 'name': 'C. Bortolon'}, {'authorId': '3286774', 'name': 'Mahdi Khoramshahi'}, {'authorId': '48407582', 'name': 'R. Salesse'}, {'authorId': '4654800', 'name': 'Marianna Burca'}, {'authorId': '143653907', 'name': 'L. Marin'}, {'authorId': '1803605', 'name': 'B. Bardy'}, {'authorId': '1807928', 'name': 'A. Billard'}, {'authorId': '3813018', 'name': 'V. Macioce'}, {'authorId': '3768760', 'name': 'D. Capdevielle'}]",34.0,"{'bibtex': '@Article{Raffard2016HumanoidRV,\n author = {S. Raffard and C. Bortolon and Mahdi Khoramshahi and R. Salesse and Marianna Burca and L. Marin and B. Bardy and A. Billard and V. Macioce and D. Capdevielle},\n journal = {Schizophrenia Research},\n pages = {506-513},\n title = {Humanoid robots versus humans: How is emotional valence of facial expressions recognized by individuals with schizophrenia? An exploratory study},\n volume = {176},\n year = {2016}\n}\n'}",,"{'volume': '176', 'pages': '506-513', 'name': 'Schizophrenia Research'}",54.0,Humanoid robots versus humans: How is emotional valence of facial expressions recognized by individuals with schizophrenia? An exploratory study,2016.0
824,434e121afd0e189853775ea31cbab0396417739c,,"[{'authorId': '2471431', 'name': 'E. Phelps'}, {'authorId': '144118279', 'name': 'M. Delgado'}, {'authorId': '5021619', 'name': 'K. Nearing'}, {'authorId': '2332694', 'name': 'Joseph E LeDoux'}]",1697.0,"{'bibtex': '@Article{Phelps2004ExtinctionLI,\n author = {E. Phelps and M. Delgado and K. Nearing and Joseph E LeDoux},\n journal = {Neuron},\n pages = {897-905},\n title = {Extinction Learning in Humans Role of the Amygdala and vmPFC},\n volume = {43},\n year = {2004}\n}\n'}",,"{'volume': '43', 'pages': '897-905', 'name': 'Neuron'}",35.0,Extinction Learning in Humans Role of the Amygdala and vmPFC,2004.0
825,4364057f7c278a6067c807bca953a7a1587520e9,"Old age represents a new frontier. The number of older people is increasing throughout the world. This changing demography affects individuals, but also families, communities and societies. The focus of this issue is the well–being of older adults on different continents. Scientists from around the world address this issue using a wide array of research designs and methodologies to provide a broad perspective on aging. Five topics are considered: Well–Being Among Older Adults; Social Support; Functional Status, Well–Being and Successful Aging; Cross–Cultural Approaches to the Study of Aging; and Research Perspectives in Aging. This issue clearly demonstrates that scientists have much to contribute to the goal of optimizing the experience of aging and creating a society for all ages.","[{'authorId': '3765960', 'name': 'T. Antonucci'}, {'authorId': '118239350', 'name': 'Corann Okorodudu'}, {'authorId': '48495017', 'name': 'H. Akiyama'}]",40.0,"{'bibtex': '@Article{Antonucci2002WellbeingAO,\n author = {T. Antonucci and Corann Okorodudu and H. Akiyama},\n journal = {Journal of Social Issues},\n pages = {617-626},\n title = {Well-being among older adults on different continents},\n volume = {58},\n year = {2002}\n}\n'}",,"{'volume': '58', 'pages': '617-626', 'name': 'Journal of Social Issues'}",0.0,Well-being among older adults on different continents,2002.0
826,436833fa62ebde03b0dab25fe18f0e3609cc7f95,"Humans, infants and adults alike, automatically mimic a variety of behaviors. Such mimicry facilitates social functioning, including establishment of interpersonal rapport and understanding of other minds. This fundamental social process may thus be impaired in disorders such as autism characterized by socio-emotional and communicative deficits. We examined automatic and voluntary mimicry of emotional facial expression among adolescents and adults with autistic spectrum disorders (ASD) and a typical sample matched on age, gender and verbal intelligence. Participants viewed pictures of happy and angry expressions while the activity over their cheek and brow muscle region was monitored with electromyography (EMG). ASD participants did not automatically mimic facial expressions whereas the typically developing participants did. However, both groups showed evidence of successful voluntary mimicry. The data suggest that autism is associated with an impairment of a basic automatic social-emotion process. Results have implications for understanding typical and atypical social cognition.","[{'authorId': '21464189', 'name': 'D. McIntosh'}, {'authorId': '1403429333', 'name': 'Aimee Reichmann-Decker'}, {'authorId': '3122131', 'name': 'P. Winkielman'}, {'authorId': '5795843', 'name': 'J. Wilbarger'}]",485.0,"{'bibtex': '@Article{McIntosh2006WhenTS,\n author = {D. McIntosh and Aimee Reichmann-Decker and P. Winkielman and J. Wilbarger},\n journal = {Developmental science},\n pages = {\n          295-302\n        },\n title = {When the social mirror breaks: deficits in automatic, but not voluntary, mimicry of emotional facial expressions in autism.},\n volume = {9 3},\n year = {2006}\n}\n'}",,"{'volume': '9 3', 'pages': '\n          295-302\n        ', 'name': 'Developmental science'}",60.0,"When the social mirror breaks: deficits in automatic, but not voluntary, mimicry of emotional facial expressions in autism.",2006.0
827,436b2f2a5c299224671aee48c0bcfc66351c9ee1,"We investigate the usability of humanlike agent-based interfaces for interactive advice-giving systems. In an experiment with a travel advisory system, we manipulate the “humanlikeness” of the agent interface. We demonstrate that users of the more humanlike agents try to exploit capabilities that were not signaled by the system. This severely reduces the usability of systems that look human but lack humanlikehumanlike capabilities (overestimation effect). We explain this effect by showing that users of humanlike agents form anthropomorphic beliefs (a user's “mental model”) about the system: They act humanlike towards the system and try to exploit typical humanlike capabilities they believe the system possesses. Furthermore, we demonstrate that the mental model users form of an agent-based system is inherently integrated (as opposed to the compositional mental model they form of conventional interfaces): Cues provided by the system do not instill user responses in a one-to-one matter but are instead integrated into a single mental model.","[{'authorId': '2477993', 'name': 'Bart P. Knijnenburg'}, {'authorId': '1918235', 'name': 'M. Willemsen'}]",66.0,"{'bibtex': '@Article{Knijnenburg2016InferringCO,\n author = {Bart P. Knijnenburg and M. Willemsen},\n journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},\n pages = {1 - 25},\n title = {Inferring Capabilities of Intelligent Agents from Their External Traits},\n volume = {6},\n year = {2016}\n}\n'}",,"{'volume': '6', 'pages': '1 - 25', 'name': 'ACM Transactions on Interactive Intelligent Systems (TiiS)'}",86.0,Inferring Capabilities of Intelligent Agents from Their External Traits,2016.0
828,438219194cedac00974ad28604b63a66e0b6f436,"We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.","[{'authorId': '1751126', 'name': 'F. Eyben'}, {'authorId': '2103575', 'name': 'M. Wöllmer'}, {'authorId': '145411696', 'name': 'Björn Schuller'}]",2365.0,"{'bibtex': '@Article{Eyben2010OpensmileTM,\n author = {F. Eyben and M. Wöllmer and Björn Schuller},\n journal = {Proceedings of the 18th ACM international conference on Multimedia},\n title = {Opensmile: the munich versatile and fast open-source audio feature extractor},\n year = {2010}\n}\n'}",,{'name': 'Proceedings of the 18th ACM international conference on Multimedia'},17.0,Opensmile: the munich versatile and fast open-source audio feature extractor,2010.0
829,43b51cd58a9c49b3da2c936e71496e817a4c59a7,,"[{'authorId': '40515590', 'name': 'F. N. Willis'}]",188.0,"{'bibtex': '@Article{Willis1966InitialSD,\n author = {F. N. Willis},\n journal = {Psychonomic Science},\n pages = {221-222},\n title = {Initial speaking distance as a function of the speakers’ relationship},\n volume = {5},\n year = {1966}\n}\n'}",,"{'volume': '5', 'pages': '221-222', 'name': 'Psychonomic Science'}",6.0,Initial speaking distance as a function of the speakers’ relationship,1966.0
830,43d1bd95f891fb226e7f517ee2a46eebeded103d,"Today, when computing is pervasive and deployed over a range of devices by a multiplicity of users, we need to develop computer software to interact with both the ever-increasing complexity of the technical world and the growing fluidity of social organizations. The Art of Agent-Oriented Modeling presents a new conceptual model for developing software systems that are open, intelligent, and adaptive. It describes an approach for modeling complex systems that consist of people, devices, and software agents in a changing environment (sometimes known as distributed sociotechnical systems). The authors take an agent-oriented view, as opposed to the more common object-oriented approach. Thinking in terms of agents (which they define as the human and man-made components of a system), they argue, can change the way people think of software and the tasks it can perform. The book offers an integrated and coherent set of concepts and models, presenting the models at three levels of abstraction corresponding to a motivation layer (where the purpose, goals, and requirements of the system are described), a design layer, and an implementation layer. It compares platforms by implementing the same models in four different languages; compares methodologies by using a common example; includes extensive case studies; and offers exercises suitable for either class use or independent study. Intelligent Robotics and Autonomous Agents series","[{'authorId': '145977411', 'name': 'L. Sterling'}, {'authorId': '2478538', 'name': 'K. Taveter'}]",297.0,"{'bibtex': '@Inproceedings{Sterling2009TheAO,\n author = {L. Sterling and K. Taveter},\n title = {The Art of Agent-Oriented Modeling},\n year = {2009}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Art of Agent-Oriented Modeling,2009.0
831,43d6dd39bef33e3298f74933aa1d1a3cb00b363a,"Embodied conversational agents are required to be able to express themselves convincingly and autonomously. Based on an empirial study on spatial descriptions of landmarks in direction-giving, we present a model that allows virtual agents to automatically generate, i.e., select the content and derive the form of coordinated language and iconic gestures. Our model simulates the interplay between these two modes of expressiveness on two levels. First, two kinds of knowledge representation (propositional and imagistic) are utilized to capture the modality-specific contents and processes of content planning. Second, specific planners are integrated to carry out the formulation of concrete verbal and gestural behavior. A probabilistic approach to gesture formulation is presented that incorporates multiple contextual factors as well as idiosyncratic patterns in the mapping of visuo-spatial referent properties onto gesture morphology. Results from a prototype implementation are described.","[{'authorId': '2025591', 'name': 'K. Bergmann'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",61.0,"{'bibtex': '@Inproceedings{Bergmann2009IncreasingTE,\n author = {K. Bergmann and S. Kopp},\n pages = {361-368},\n title = {Increasing the expressiveness of virtual agents: autonomous generation of speech and gesture for spatial description tasks},\n year = {2009}\n}\n'}",,{'pages': '361-368'},22.0,Increasing the expressiveness of virtual agents: autonomous generation of speech and gesture for spatial description tasks,2009.0
833,43f772b01549ab4b6b4a6359be033b9c5ce0fd0c,"The proposed interactive crowd-behavior learning algorithms can be used to analyze crowd videos for surveillance and training applications. The authors' formulation combines online tracking algorithms from computer vision, nonlinear pedestrian motion models from computer graphics, and machine learning techniques to automatically compute trajectory-level pedestrian behaviors for each agent in the video. These learned behaviors are used to automatically detect anomalous behaviors, perform motion segmentation, and generate realistic behaviors for virtual reality training applications.","[{'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",13.0,"{'bibtex': '@Article{Bera2016InteractiveCL,\n author = {Aniket Bera and Sujeong Kim and Dinesh Manocha},\n journal = {IEEE Computer Graphics and Applications},\n pages = {37-45},\n title = {Interactive Crowd-Behavior Learning for Surveillance and Training},\n volume = {36},\n year = {2016}\n}\n'}",,"{'volume': '36', 'pages': '37-45', 'name': 'IEEE Computer Graphics and Applications'}",19.0,Interactive Crowd-Behavior Learning for Surveillance and Training,2016.0
834,43fcd0ca9ddb52ac666c9881e72b020659ebfca0,,"[{'authorId': '51068804', 'name': 'Laura Loucks'}, {'authorId': '3871408', 'name': 'Carly W Yasinski'}, {'authorId': '4786216', 'name': 'S. Norrholm'}, {'authorId': '1402540811', 'name': 'J. Maples-Keller'}, {'authorId': '5687182', 'name': 'L. Post'}, {'authorId': '7393790', 'name': 'Liza C Zwiebach'}, {'authorId': '14106480', 'name': 'Devika Fiorillo'}, {'authorId': '51068246', 'name': 'Megan Goodlin'}, {'authorId': '2770002', 'name': 'T. Jovanović'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '1831766', 'name': 'B. Rothbaum'}]",74.0,"{'bibtex': '@Article{Loucks2019YouCD,\n author = {Laura Loucks and Carly W Yasinski and S. Norrholm and J. Maples-Keller and L. Post and Liza C Zwiebach and Devika Fiorillo and Megan Goodlin and T. Jovanović and A. Rizzo and B. Rothbaum},\n journal = {Journal of anxiety disorders},\n pages = {\n          55-63\n        },\n title = {You can do that?!: Feasibility of virtual reality exposure therapy in the treatment of PTSD due to military sexual trauma.},\n volume = {61},\n year = {2019}\n}\n'}",,"{'volume': '61', 'pages': '\n          55-63\n        ', 'name': 'Journal of anxiety disorders'}",52.0,You can do that?!: Feasibility of virtual reality exposure therapy in the treatment of PTSD due to military sexual trauma.,2019.0
836,440c81e2159149c33cf82b843cfd79d323c92de5,"Human intelligence is being increasingly redefined to include the all-encompassing effect of emotions upon what used to be considered 'pure reason'. With the recent progress of research in computer vision, speech/prosody recognition, and bio-feedback, real-time recognition of affect will enhance human-computer interaction considerably, as well as assist further progress in the development of new emotion theories.In this article, we describe how affect, moods and emotions closely interact with cognition and how affect and emotion are the quintessential multimodal processes in humans. We then propose an adaptive system architecture designed to sense the user's emotional and affective states via three multimodal subsystems (V, K, A): namely (1) the Visual (from facial images and videos), (2) Kinesthetic (from autonomic nervous system (ANS) signals), and (3) Auditory (from speech). The results of the system sensing are then integrated into the multimodal perceived multimodal anthropomorphic interface agent then adapts its interface by responding most appropriately to the current emotional states of its user, and provides intelligent multi-modal feedback to the user.","[{'authorId': '143607713', 'name': 'Christine L. Lisetti'}, {'authorId': '2107729', 'name': 'Fatma Nasoz'}]",123.0,"{'bibtex': '@Inproceedings{Lisetti2002MAUIAM,\n author = {Christine L. Lisetti and Fatma Nasoz},\n pages = {161-170},\n title = {MAUI: a multimodal affective user interface},\n year = {2002}\n}\n'}",,{'pages': '161-170'},51.0,MAUI: a multimodal affective user interface,2002.0
837,442175fbd2692ee3923f6ea6c0cac00c1562a79d,"In this paper we present an experiment on the use of face expression recognition in interactive performance, conduced by using a platform for interactive drama that augments the performer-audience interaction with an intelligent prompt. Step after step, the emotional response of the audience is automatically detected through a face expression recognition service and the performer decides whether to encourage the audience's response or not. We describe the data collected during the performance, discussing the potential of the approach and the redesign issues emerged from the experience gained.","[{'authorId': '144411873', 'name': 'R. Damiano'}, {'authorId': '152862473', 'name': 'V. Lombardo'}, {'authorId': '1419987119', 'name': 'Giulia Monticone'}, {'authorId': '1944743919', 'name': 'Antonio Pizzo'}]",2.0,"{'bibtex': '@Article{Damiano2019AllAF,\n author = {R. Damiano and V. Lombardo and Giulia Monticone and Antonio Pizzo},\n journal = {2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)},\n pages = {1-7},\n title = {All about face. An experiment in face emotion recognition in interactive dramatic performance},\n year = {2019}\n}\n'}",,"{'pages': '1-7', 'name': '2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)'}",19.0,All about face. An experiment in face emotion recognition in interactive dramatic performance,2019.0
838,442ff8e7b2b354bb20ba6bb83f71310b3de51534,"Copyright It is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s) and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an open content licence (like Creative Commons). UvA-DARE is a service provided by the library of the University of Amsterdam (http://dare.uva.nl) & Schuster, 1996] motivated the direct route in his dual-pathway model by arguing that the ability to switch rapidly between different modes of behavior is highly adaptive. A computational study into the evolution of dual-route dynamics for affective processing. in which foraging agents, controlled by simple inheritable neural networks, navigated an artificial world while avoiding predation. After many generations, a dual-processing architecture evolved that enabled a rapid switch to avoidance behavior when a predator appeared. We added recurrent connections to a new "" context "" layer in the indirect pathway to provide the agents with a working memory of previous input (i.e., a "" context ""). Unexpectedly, agents with oscillating networks emerged that had a much higher fitness than agents without oscillations. Oscillations seemed to have similar effects on switching speed as the dual-processing architecture, but they enhanced switching efficacy to a much larger degree. There has been much neurobiological speculation on the function, if any, of neural oscillations. These findings suggest that the facilitation of switching behavior is a likely candidate. Moreover, the strongly improved adaptation in the simulations contradicts the position that neural oscillations are merely a by-product of cell firing and have no functional value [Pareti, G., & De Palma, A. Does the brain oscillate? The dispute on neuronal synchronization.","[{'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2545816', 'name': 'R. Phaf'}, {'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2229542920', 'name': 'R. Hans'}, {'authorId': '2229563644', 'name': 'Phaf Abstract'}, {'authorId': '2096216265', 'name': 'Ledoux'}, {'authorId': '2332694', 'name': 'Joseph E LeDoux'}]",11.0,"{'bibtex': '@Misc{None,\n author = {B. T. Heerebout and R. Phaf and B. T. Heerebout and R. Hans and Phaf Abstract and Ledoux and Joseph E LeDoux},\n title = {Source (or Part of the following Source): Type Article Title Emergent Oscillations in Evolutionary Simulations: Oscillating Networks Increase Switching Efficacy Author(s) Emergent Oscillations in Evolutionary Simulations: Oscillating Networks Increase Switching Efficacy}\n}\n'}",,,46.0,Source (or Part of the following Source): Type Article Title Emergent Oscillations in Evolutionary Simulations: Oscillating Networks Increase Switching Efficacy Author(s) Emergent Oscillations in Evolutionary Simulations: Oscillating Networks Increase Switching Efficacy,
839,443136202ddf2b5a8fc95ba8eacfa2dc1274c22d,"BACKGROUND
Although emotion dysregulation is not a defining feature of Autism Spectrum Disorder (ASD), there is a growing consensus that emotional problems play a prominent role in this disorder.


METHODS
The present study examined a wide range of emotion regulation (ER) strategies in 32 individuals with ASD compared to 31 group-matched typically developing (TD) participants in three emotional domains (anger, anxiety, and amusement). Parents of individuals with ASD and TD individuals were interviewed about their child's emotional experience and the use and efficacy of 10 ER strategies. In addition, participants filled out daily diaries on experience and regulation in the same emotional domains.


RESULTS
Compared to TD individuals, parents reported that individuals with ASD experienced more anger and anxiety and less amusement, made less frequent use of a variety of adaptive ER strategies (e.g. problem solving, cognitive reappraisal), and made more frequent use of maladaptive strategies (e.g. repetitive behavior). Moreover, individuals with ASD were less effective at utilizing adaptive ER strategies. Self-reports showed differences in experience of amusement and in ER strategies for anger and anxiety, but not in experience of anger and anxiety.


CONCLUSIONS
This study provides evidence that individuals with ASD less frequently use adaptive - but more frequently use maladaptive - ER strategies. Implications for ASD treatments that focus on increasing the use of adaptive strategies are discussed.","[{'authorId': '38707445', 'name': 'Andrea C. Samson'}, {'authorId': '33448033', 'name': 'W. Wells'}, {'authorId': '2536136', 'name': 'Jennifer M. Phillips'}, {'authorId': '6760287', 'name': 'A. Hardan'}, {'authorId': '1775321', 'name': 'J. Gross'}]",84.0,"{'bibtex': ""@Article{Samson2015EmotionRI,\n author = {Andrea C. Samson and W. Wells and Jennifer M. Phillips and A. Hardan and J. Gross},\n journal = {Journal of child psychology and psychiatry, and allied disciplines},\n pages = {\n          903-13\n        },\n title = {Emotion regulation in autism spectrum disorder: evidence from parent interviews and children's daily diaries.},\n volume = {56 8},\n year = {2015}\n}\n""}",,"{'volume': '56 8', 'pages': '\n          903-13\n        ', 'name': 'Journal of child psychology and psychiatry, and allied disciplines'}",45.0,Emotion regulation in autism spectrum disorder: evidence from parent interviews and children's daily diaries.,2015.0
840,443f871d8c4b4569032337c48d4d4a34bcc33762,,"[{'authorId': '32982720', 'name': 'Rena A. Menke'}]",11.0,"{'bibtex': '@Inproceedings{Menke2011ExaminingNS,\n author = {Rena A. Menke},\n title = {Examining nonverbal shame markers among post-pregnancy women with maltreatment histories},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Examining nonverbal shame markers among post-pregnancy women with maltreatment histories,2011.0
841,44509e21527ef0e70c7d4d0603a631ae319e2451,,"[{'authorId': '82128751', 'name': 'M. Hoffman'}]",76.0,"{'bibtex': '@Inproceedings{Hoffman1978TowardAT,\n author = {M. Hoffman},\n pages = {227-256},\n title = {Toward a Theory of Empathic Arousal and Development},\n year = {1978}\n}\n'}",,"{'volume': '', 'pages': '227-256', 'name': ''}",52.0,Toward a Theory of Empathic Arousal and Development,1978.0
842,4463302e706e8547a549ac6a401de0cf1cf0cb20,"The author discusses 3 variables that assess different aspects of social relationships—social support, social integration, and negative interaction. The author argues that all 3 are associated with health outcomes, that these variables each influence health through different mechanisms, and that associations between these variables and health are not spurious findings attributable to our personalities. This argument suggests a broader view of how to intervene in social networks to improve health. This includes facilitating both social integration and social support by creating and nurturing both close (strong) and peripheral (weak) ties within natural social networks and reducing opportunities for negative social interaction. Finally, the author emphasizes the necessity to understand more about who benefits most and least from socialconnectedness interventions.","[{'authorId': '145708972', 'name': 'Sheldon Cohen'}]",2369.0,"{'bibtex': '@Article{Cohen2004SocialRA,\n author = {Sheldon Cohen},\n journal = {The American psychologist},\n pages = {\n          676-684\n        },\n title = {Social relationships and health.},\n volume = {59 8},\n year = {2004}\n}\n'}",,"{'volume': '59 8', 'pages': '\n          676-684\n        ', 'name': 'The American psychologist'}",39.0,Social relationships and health.,2004.0
843,446d4b669654ab35dea35cf885d10e580e9d2e48,,"[{'authorId': '145580293', 'name': 'S. Belleville'}, {'authorId': '4862602', 'name': 'A. LeBlanc'}, {'authorId': '144549746', 'name': 'M. Kergoat'}, {'authorId': '3922173', 'name': 'F. Calon'}, {'authorId': '144992181', 'name': 'P. Gaudreau'}, {'authorId': '5540255', 'name': 'Sébastien S. Hébert'}, {'authorId': '9164412', 'name': 'C. Hudon'}, {'authorId': '2090038348', 'name': 'N. Leclerc'}, {'authorId': '4984276', 'name': 'N. Mechawar'}, {'authorId': '2873793', 'name': 'S. Duchesne'}, {'authorId': '2059389436', 'name': 'S. Gauthier'}, {'authorId': None, 'name': 'Pierre Sylvie Christian Frédéric Howard Louis Stephen Sim Bellec Belleville Bocti Calon Chertkow Collins Cun'}, {'authorId': '2943011', 'name': 'Pierre Bellec'}, {'authorId': '145580293', 'name': 'S. Belleville'}, {'authorId': '47456475', 'name': 'C. Bocti'}, {'authorId': '3922173', 'name': 'F. Calon'}, {'authorId': '114187679', 'name': 'H. Chertkow'}, {'authorId': '123435565', 'name': 'L. Collins'}, {'authorId': '2193740444', 'name': 'S. Cunnane'}, {'authorId': '2873793', 'name': 'S. Duchesne'}, {'authorId': '144992181', 'name': 'P. Gaudreau'}, {'authorId': '2059389436', 'name': 'S. Gauthier'}, {'authorId': '5540255', 'name': 'Sébastien S. Hébert'}, {'authorId': '2185658429', 'name': 'Carol Hudon Marie-Jeanne-Kergoat'}, {'authorId': '4862602', 'name': 'A. LeBlanc'}, {'authorId': '2090038348', 'name': 'N. Leclerc'}, {'authorId': '4984276', 'name': 'N. Mechawar'}, {'authorId': '1392498620', 'name': 'Natalie Philips'}, {'authorId': '145318694', 'name': 'J. Soucy'}, {'authorId': '1737522062', 'name': 'T. T. Dang Vu'}, {'authorId': '39110343', 'name': 'L. Verret'}, {'authorId': '7580946', 'name': 'J. Villalpando'}]",22.0,"{'bibtex': ""@Article{Belleville2019TheCF,\n author = {S. Belleville and A. LeBlanc and M. Kergoat and F. Calon and P. Gaudreau and Sébastien S. Hébert and C. Hudon and N. Leclerc and N. Mechawar and S. Duchesne and S. Gauthier and Pierre Sylvie Christian Frédéric Howard Louis Stephen Sim Bellec Belleville Bocti Calon Chertkow Collins Cun and Pierre Bellec and S. Belleville and C. Bocti and F. Calon and H. Chertkow and L. Collins and S. Cunnane and S. Duchesne and P. Gaudreau and S. Gauthier and Sébastien S. Hébert and Carol Hudon Marie-Jeanne-Kergoat and A. LeBlanc and N. Leclerc and N. Mechawar and Natalie Philips and J. Soucy and T. T. Dang Vu and L. Verret and J. Villalpando},\n journal = {Alzheimer's & Dementia : Diagnosis, Assessment & Disease Monitoring},\n pages = {787 - 796},\n title = {The Consortium for the early identification of Alzheimer's disease–Quebec (CIMA-Q)},\n volume = {11},\n year = {2019}\n}\n""}",,"{'volume': '11', 'pages': '787 - 796', 'name': ""Alzheimer's & Dementia : Diagnosis, Assessment & Disease Monitoring""}",48.0,The Consortium for the early identification of Alzheimer's disease–Quebec (CIMA-Q),2019.0
844,44752d423353ca5fcaf9123a9e71f98921ada266,"- Some machine should interpret the emotional state of humans beings or human and adapt its behaviour to them, giving an appropriate response for the emotions. Affective computing is the study and development of systems and devices which could or can recognize, interpret, process, and simulate human affects. Affective computing is human-computer interaction where the device has the ability to detect and appropriately respond to its user's emotions and other stimuli. It is an interdisciplinary field spanning computer science, psychology, and cognitive science. A motivation is the ability to simulate empathy.","[{'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",4806.0,"{'bibtex': '@Inproceedings{Picard2015AFFECTIVEC,\n author = {Rosalind W. Picard},\n title = {AFFECTIVE COMPUTING},\n year = {2015}\n}\n'}",,,30.0,AFFECTIVE COMPUTING,2015.0
849,4477d12f07e538738766dc90102c74ceb89d291d,,"[{'authorId': '143917088', 'name': 'J. Michael'}]",56.0,"{'bibtex': '@Article{Michael2011SharedEA,\n author = {J. Michael},\n journal = {Review of Philosophy and Psychology},\n pages = {355-373},\n title = {Shared Emotions and Joint Action},\n volume = {2},\n year = {2011}\n}\n'}",,"{'volume': '2', 'pages': '355-373', 'name': 'Review of Philosophy and Psychology'}",65.0,Shared Emotions and Joint Action,2011.0
850,4478d467ea7ccca581e29ec51ced90373e90e364,,"[{'authorId': '2427954', 'name': 'Liwei Zhao'}, {'authorId': '2135004', 'name': 'K. Schuler'}, {'authorId': '1747648', 'name': 'William Schuler'}, {'authorId': '2467082', 'name': 'Christian Vogler'}, {'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '145755155', 'name': 'Martha Palmer'}]",189.0,"{'bibtex': '@Inproceedings{Zhao2000AMT,\n author = {Liwei Zhao and K. Schuler and William Schuler and Christian Vogler and N. Badler and Martha Palmer},\n pages = {54-67},\n title = {A machine translation system from English to American Sign Language},\n year = {2000}\n}\n'}",,{'pages': '54-67'},34.0,A machine translation system from English to American Sign Language,2000.0
851,44b35318ded38fba380de265aa66e0ac307d19ac,"Essays written by currently-depressed, formerly-depressed, and never-depressed college students were examined for differences in language that might shed light on the cognitive operations associated with depression and depression-vulnerability. A text analysis program computed the incidence of words in predesignated categories. Consistent with Beck's cognitive model and with Pyczsinski and Greenberg's self-focus model of depression, depressed participants used more negatively valenced words and used the word, ""I"" more than did never-depressed participants. Formerly-depressed (presumably depression-vulnerable) participants did not differ from never-depressed participants on these indices of depressive processing. However, consistent with prediction, formerly-depressed participants' use of the word ""I"" increased across the essays and was significantly greater than that of never-depressed writers in the final portion of the essays.","[{'authorId': '2244916451', 'name': 'Stephanie Rude'}, {'authorId': '48294156', 'name': 'Eva-Maria Gortner'}, {'authorId': '2244916477', 'name': 'James Pennebaker'}]",878.0,"{'bibtex': '@Article{Rude2004LanguageUO,\n author = {Stephanie Rude and Eva-Maria Gortner and James Pennebaker},\n journal = {Cognition and Emotion},\n pages = {1121 - 1133},\n title = {Language use of depressed and depression-vulnerable college students},\n volume = {18},\n year = {2004}\n}\n'}",,"{'volume': '18', 'pages': '1121 - 1133', 'name': 'Cognition and Emotion'}",33.0,Language use of depressed and depression-vulnerable college students,2004.0
852,45012e6cc91a62c69c80a5e375117feb4f381df5,"Animated pedagogical agents (APAs) are known to possess great potential in supporting learning because of their ability to simulate a real classroom learning environment. But research in thisareahasproducedmixedresults.Thereasonforthisremainspuzzling.Thispaperiswritten with two purposes: (1) to examine some recent research and organize the findings in terms of classroom characteristics, and (2) to discuss and reveal any uncovered issues pertaining to the findings and provide input whenever possible.Aframework formed by usingAPAcharacteristics, APA presentation, and learners’ characteristics is used to analyse past research findings. ThefindingsfromtheanalysisrevealthatbecauseAPAsareregardedassocialmemberssimilar to humans, they are more effective in engaging learners in environments that require social communication and interactions. They therefore produce more definitive results in terms of affective gain and group learning. But such conditions also impose greater demand on designers to create more complex learning environments that can provide interactions with several agents and yet maintain a mode of communication that is pedagogically effective. The challengesforcreatingsuchenvironmentincludeusinganagent’sgesturestoduplicateitsspeechin instruction,whichisusuallyuncommoninhumanpractice,overcomingtheneedstouseinput‐ output interface for communication and taking into consideration the possible influence of the learners’characteristicssuchastheirsensorypreference.ThispapersuggeststhatAPAs’application in instruction should be seen in the light of affordances and be designed within its own practical limits.jca_299 203..218","[{'authorId': '2592204', 'name': 'H. Woo'}]",75.0,"{'bibtex': '@Article{Woo2009DesigningML,\n author = {H. Woo},\n journal = {J. Comput. Assist. Learn.},\n pages = {203-218},\n title = {Designing multimedia learning environments using animated pedagogical agents: factors and issues},\n volume = {25},\n year = {2009}\n}\n'}",,"{'volume': '25', 'pages': '203-218', 'name': 'J. Comput. Assist. Learn.'}",79.0,Designing multimedia learning environments using animated pedagogical agents: factors and issues,2009.0
853,45018ea1121bee5bb7c6e236edc687701220b2c7,"To make the intelligent virtual agent become more natural and believable, giving her emotion ability is very important. According to Basic Emotion Theory and Cognitive Evaluation Theory in Psychology, it proposes an emotion model based on Fuzzy Rules. Firstly, emotion factor is generated according to the emotion elicited rules based on Fuzzy IF-THEN rules. Then a nonlinear function restricted by personality, emotion factor and emotion state at the previous moment was used to compute emotion strength. The simulation result shows, this model can simulate the fuzzy and nonlinear characteristic of human emotion to a certain extent.","[{'authorId': '2108833763', 'name': 'Shi Lin'}, {'authorId': '46644542', 'name': 'Li Zhigang'}, {'authorId': '31161618', 'name': 'Ding Aihua'}]",2.0,"{'bibtex': '@Conference{Lin2010ResearchOE,\n author = {Shi Lin and Li Zhigang and Ding Aihua},\n booktitle = {International Conference on Computer and Automation Engineering},\n journal = {2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)},\n pages = {286-289},\n title = {Research of emotion modeling for intelligent virtual agent},\n volume = {2},\n year = {2010}\n}\n'}","[{'paperId': 'b2eb41b929239eac491016debd325b4f05d36447', 'title': 'The emotions effect on a virtual characters design–A student perspective analysis'}, {'paperId': 'f3a2cc37e0a56aac38ca2d720707fcf2c8b56a2b', 'title': 'Fuzzy Controlled PAD Emotional State of a NAO Robot'}]","{'name': '2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)', 'pages': '286-289', 'volume': '2'}",12.0,Research of emotion modeling for intelligent virtual agent,2010.0
854,451d6f9b4be8a478cf177b228dd06f65cabbd530,"Convincing conversational agents require a coherent set of behavioral responses that can be interpreted by a human observer as indicative of a personality. This paper discusses the continued development and subsequent evaluation of virtual agents based on sound psychological principles. We use Eysenck's theoretical basis to explain aspects of the characterization of our agents, and we describe an architecture where personality affects the agent's global behavior quality as well as their back-channel productions. Drawing on psychological research, we evaluate perception of our agents' personalities and credibility by human viewers (N = 187). Our results suggest that we succeeded in validating theoretically grounded indicators of personality in our virtual agents, and that it is feasible to place our characters on Eysenck's scales. A key finding is that the presence of behavioral characteristics reinforces the prescribed personality profiles that are already emerging from the still images. Our long-term goal is to enhance agents' ability to sustain realistic interaction with human users, and we discuss how this preliminary work may be further developed to include more systematic variation of Eysenck's personality scales.","[{'authorId': '2314861', 'name': 'Margaret McRorie'}, {'authorId': '145688200', 'name': 'I. Sneddon'}, {'authorId': '2228246', 'name': 'G. McKeown'}, {'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '1761859', 'name': 'E. D. Sevin'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",58.0,"{'bibtex': '@Article{McRorie2012EvaluationOF,\n author = {Margaret McRorie and I. Sneddon and G. McKeown and Elisabetta Bevacqua and E. D. Sevin and C. Pelachaud},\n journal = {IEEE Transactions on Affective Computing},\n pages = {311-322},\n title = {Evaluation of Four Designed Virtual Agent Personalities},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'pages': '311-322', 'name': 'IEEE Transactions on Affective Computing'}",64.0,Evaluation of Four Designed Virtual Agent Personalities,2012.0
855,4550e5fb37a6b0198be6c4f266e3e9b9aecc8a89,,"[{'authorId': '3350062', 'name': 'Emmanuel Ayedoun'}, {'authorId': '145494830', 'name': 'Yuki Hayashi'}, {'authorId': '35658486', 'name': 'Kazuhisa Seta'}]",41.0,"{'bibtex': '@Article{Ayedoun2018AddingCA,\n author = {Emmanuel Ayedoun and Yuki Hayashi and Kazuhisa Seta},\n journal = {International Journal of Artificial Intelligence in Education},\n pages = {29-57},\n title = {Adding Communicative and Affective Strategies to an Embodied Conversational Agent to Enhance Second Language Learners’ Willingness to Communicate},\n volume = {29},\n year = {2018}\n}\n'}",,"{'volume': '29', 'pages': '29-57', 'name': 'International Journal of Artificial Intelligence in Education'}",73.0,Adding Communicative and Affective Strategies to an Embodied Conversational Agent to Enhance Second Language Learners’ Willingness to Communicate,2018.0
856,45a37d1ead355ec931be6f4f26913214c67df39b,,"[{'authorId': '51966382', 'name': 'Martin H. Levinson'}]",61.0,"{'bibtex': '@Inproceedings{Levinson2000TheAH,\n author = {Martin H. Levinson},\n pages = {500-500},\n title = {The American Heritage Dictionary of the English Language},\n volume = {57},\n year = {2000}\n}\n'}",,"{'volume': '57', 'pages': '500-500', 'name': ''}",0.0,The American Heritage Dictionary of the English Language,2000.0
857,45b9156f980c51aa2bfaead20effed6b02ff5272,"In this paper, we examine whether affect influences higher level cognitive processes. We review research on the effect of emotion on interpretation, judgement, decision making, and reasoning. In all cases, we ask first whether there is evidence that emotion affects each of these processes, and second what mechanisms might underlie these effects. Our review highlights the fact that interpretive biases are primarily linked with anxiety, while more general mood-congruent effects may be seen in judgement. Risk perception is also affected by negative and positive affect. Research shows complex effects of emotion on decision making and reasoning, with emotion sometimes hindering normatively correct thinking and sometimes promoting it. There are also important effects of emotion on reasoning style. We discuss key differences between the effects of incidental affect (feeling states not related to the semantic contents of the cognitive task) and integral affect (where the feeling state is caused by or linked to the contents of the cognitive task). In the conclusion, we suggest that focusing on some of the constituent mechanisms involved in interpretation, judgement, decision making and reasoning provides a way to link some of the diverse findings in the field. We also highlight important areas for future research.","[{'authorId': '5691792', 'name': 'I. Blanchette'}, {'authorId': '145141666', 'name': 'A. Richards'}]",606.0,"{'bibtex': '@Article{Blanchette2010TheIO,\n author = {I. Blanchette and A. Richards},\n journal = {Cognition and Emotion},\n pages = {561 - 595},\n title = {The influence of affect on higher level cognition: A review of research on interpretation, judgement, decision making and reasoning},\n volume = {24},\n year = {2010}\n}\n'}",,"{'volume': '24', 'pages': '561 - 595', 'name': 'Cognition and Emotion'}",188.0,"The influence of affect on higher level cognition: A review of research on interpretation, judgement, decision making and reasoning",2010.0
858,45c842da3c9115f38c9cd5243bb43279ae6dd6e8,"The authors examined precisely when and how listeners insert their responses into a speaker's narrative. A collaborative theory would predict a relationship between the speaker's acts and the listener's responses, and the authors proposed that speaker gaze coordinated this collaboration. The listener typically looks more at the speaker than the reverse, but at key points while speaking the speaker seeks a response by looking at the listener, creating a brief period of mutual gaze called here a gaze window. The listener was very likely to respond with “mhm,” a nod, or other reaction during this period, after which the speaker quickly looked away and continued speaking. This model was tested with 9 dyads in which 1 person was telling a close-call story to the other. The results confirmed the model for each dyad, demonstrating both collaboration in dialogue at the microlevel and a high degree of integration and coordination of audible and visible acts, in this case, speech and gaze.","[{'authorId': '2816755', 'name': 'J. Bavelas'}, {'authorId': '143892343', 'name': 'Linda Coates'}, {'authorId': '2114152350', 'name': 'Trudy Johnson'}]",425.0,"{'bibtex': '@Article{Bavelas2002ListenerRA,\n author = {J. Bavelas and Linda Coates and Trudy Johnson},\n journal = {Journal of Communication},\n pages = {566-580},\n title = {Listener Responses as a Collaborative Process: The Role of Gaze},\n volume = {52},\n year = {2002}\n}\n'}",,"{'volume': '52', 'pages': '566-580', 'name': 'Journal of Communication'}",27.0,Listener Responses as a Collaborative Process: The Role of Gaze,2002.0
859,45d5eb95969bd3965b3dac7c92dbba12a5e14659,,"[{'authorId': '1721062', 'name': 'Nick Koudas'}, {'authorId': '2440762', 'name': 'A. Marathe'}, {'authorId': '145860176', 'name': 'D. Srivastava'}]",123.0,"{'bibtex': '@Inproceedings{Koudas2004FlexibleSM,\n author = {Nick Koudas and A. Marathe and D. Srivastava},\n pages = {1078-1086},\n title = {Flexible String Matching Against Large Databases in Practice},\n year = {2004}\n}\n'}",,{'pages': '1078-1086'},4.0,Flexible String Matching Against Large Databases in Practice,2004.0
860,4607cc32250a1a7f638b2d6e25e8026b32054906,,"[{'authorId': '40641768', 'name': 'Oh-Woog Kwon'}, {'authorId': '2152542430', 'name': 'Ki-Young Lee'}, {'authorId': '3019507', 'name': 'Yoon-Hyung Roh'}, {'authorId': '2926314', 'name': 'Jin-Xia Huang'}, {'authorId': '2621870', 'name': 'Sung-Kwon Choi'}, {'authorId': '3030175', 'name': 'Y. K. Kim'}, {'authorId': '2067116', 'name': 'Hyung-Bae Jeon'}, {'authorId': '2299444', 'name': 'Y. Oh'}, {'authorId': '2146337673', 'name': 'Yun-Kyung Lee'}, {'authorId': '2094201', 'name': 'B. Kang'}, {'authorId': '2801379', 'name': 'Euisok Chung'}, {'authorId': '1923028', 'name': 'J. Park'}, {'authorId': '2146337673', 'name': 'Yun-Kyung Lee'}]",15.0,"{'bibtex': '@Inproceedings{Kwon2015GenieTutorAC,\n author = {Oh-Woog Kwon and Ki-Young Lee and Yoon-Hyung Roh and Jin-Xia Huang and Sung-Kwon Choi and Y. K. Kim and Hyung-Bae Jeon and Y. Oh and Yun-Kyung Lee and B. Kang and Euisok Chung and J. Park and Yun-Kyung Lee},\n pages = {257-262},\n title = {GenieTutor: A Computer-Assisted Second-Language Learning System Based on Spoken Language Understanding},\n year = {2015}\n}\n'}",,{'pages': '257-262'},5.0,GenieTutor: A Computer-Assisted Second-Language Learning System Based on Spoken Language Understanding,2015.0
861,460fd4fdf1ebe60dcbf57060dc19904635869290,"In an e-commerce environment, user-oriented question-answering (QA) text pair could carry rich sentiment information. In this study, we propose a novel task/method to address QA sentiment analysis. In particular, we create a high-quality annotated corpus with specially-designed annotation guidelines for QA-style sentiment classification. On the basis, we propose a three-stage hierarchical matching network to explore deep sentiment information in a QA text pair. First, we segment both the question and answer text into sentences and construct a number of [Q-sentence, A-sentence] units in each QA text pair. Then, by leveraging a QA bidirectional matching layer, the proposed approach can learn the matching vectors of each [Q-sentence, A-sentence] unit. Finally, we characterize the importance of the generated matching vectors via a self-matching attention layer. Experimental results, comparing with a number of state-of-the-art baselines, demonstrate the impressive effectiveness of the proposed approach for QA-style sentiment classification.","[{'authorId': '50413860', 'name': 'Chenlin Shen'}, {'authorId': '2060934', 'name': 'Changlong Sun'}, {'authorId': '2109839506', 'name': 'Jingjing Wang'}, {'authorId': '38753454', 'name': 'Yangyang Kang'}, {'authorId': '2109167274', 'name': 'Shoushan Li'}, {'authorId': '1713802', 'name': 'Xiaozhong Liu'}, {'authorId': '2059080424', 'name': 'Luo Si'}, {'authorId': None, 'name': 'Min Zhang'}, {'authorId': '143740945', 'name': 'Guodong Zhou'}]",27.0,"{'bibtex': '@Inproceedings{Shen2018SentimentCT,\n author = {Chenlin Shen and Changlong Sun and Jingjing Wang and Yangyang Kang and Shoushan Li and Xiaozhong Liu and Luo Si and Min Zhang and Guodong Zhou},\n pages = {3654-3663},\n title = {Sentiment Classification towards Question-Answering with Hierarchical Matching Network},\n year = {2018}\n}\n'}",,{'pages': '3654-3663'},36.0,Sentiment Classification towards Question-Answering with Hierarchical Matching Network,2018.0
862,4631b75b09c6289ab706e741b41c095981c5dd5f,"This study examines the question of gender-equivalent outcomes of mental health and social behavior in the context of crowding stress. It tests the hypothesis that gender will influence the exhibition of stress outcomes resulting from exposure to high-density living environments, with women displaying internalized responses and men responding with externalized styles. Expanding on the types of gender-appropriate disorders examined in this area of research, I selected depression, aggression, and withdrawal as gender-specific disorders based on theory and prior research. Multilevel analyses of data from a survey of Toronto residents indicate that, while the effects of household density are conditioned by gender, support for the existence of gender-equivalent outcomes is mixed. While women living in crowded homes are more likely to be depressed, men exposed to high-density living environments do not report increased aggression. However, men report higher levels of withdrawal, and some males respond with both aggression and withdrawal.","[{'authorId': '11033590', 'name': 'Wendy C. Regoeczi'}]",75.0,"{'bibtex': '@Article{Regoeczi2008CrowdingIC,\n author = {Wendy C. Regoeczi},\n journal = {Journal of Health and Social Behavior},\n pages = {254 - 268},\n title = {Crowding in Context: An Examination of the Differential Responses of Men and Women to High-Density Living Environments∗},\n volume = {49},\n year = {2008}\n}\n'}",,"{'volume': '49', 'pages': '254 - 268', 'name': 'Journal of Health and Social Behavior'}",92.0,Crowding in Context: An Examination of the Differential Responses of Men and Women to High-Density Living Environments∗,2008.0
864,46443fcfc5d651b8ae96c74d55bbe9d7f56107dd,"We have developed an agent's architecture towards the goal of building efficient, robust and safe multi-robot systems considered as cooperating distributed reactive agents. This architecture is based on satisfaction and altruism allowing the agents to amend their low-level behavior like goal seeking and collision avoidance in order to solve more complex problems. We demonstrate in particular that local conflicting and locking situations are automatically avoided or made repulsive. Computer simulations of tasks in complex environments confirm it. The designed mini-robots, the implementation of their architecture, and the communication protocol are described. The same hardware is shared between communication, collision avoidance, and task achievement. Experiments using two mobile robots and a test bed confirm the theoretical and simulation results.","[{'authorId': '2472443', 'name': 'P. Lucidarme'}, {'authorId': '1807441', 'name': 'Olivier Simonin'}, {'authorId': '2904376', 'name': 'A. Liégeois'}]",50.0,"{'bibtex': '@Article{Lucidarme2002ImplementationAE,\n author = {P. Lucidarme and Olivier Simonin and A. Liégeois},\n journal = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},\n pages = {1007-1012 vol.1},\n title = {Implementation and evaluation of a satisfaction/altruism based architecture for multi-robot systems},\n volume = {1},\n year = {2002}\n}\n'}",,"{'volume': '1', 'pages': '1007-1012 vol.1', 'name': 'Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)'}",15.0,Implementation and evaluation of a satisfaction/altruism based architecture for multi-robot systems,2002.0
865,46465a97ab883cc72d9f601263a208afae6fb31a,,"[{'authorId': '6408438', 'name': 'Beatrice Rammstedt'}, {'authorId': '2254103', 'name': 'O. John'}]",3252.0,"{'bibtex': '@Article{Rammstedt2007MeasuringPI,\n author = {Beatrice Rammstedt and O. John},\n journal = {Journal of Research in Personality},\n pages = {203-212},\n title = {Measuring personality in one minute or less: A 10-item short version of the Big Five Inventory in English and German},\n volume = {41},\n year = {2007}\n}\n'}",,"{'volume': '41', 'pages': '203-212', 'name': 'Journal of Research in Personality'}",20.0,Measuring personality in one minute or less: A 10-item short version of the Big Five Inventory in English and German,2007.0
866,46571652648f95aa23bc1d8f4697905db292fce3,"Social robotics is a thriving field in building artificial agents. The possibility to construct agents that can engage in meaningful social interaction with humans presents new challenges for engineers. In general, social robotics has been inspired primarily by psychologists with the aim of building human-like robots. Only a small subcategory of “companion robots” (also referred to as robotic pets) was built to mimic animals. In this opinion essay we argue that all social robots should be seen as companions and more conceptual emphasis should be put on the inter-specific interaction between humans and social robots. This view is underlined by the means of an ethological analysis and critical evaluation of present day companion robots. We suggest that human–animal interaction provides a rich source of knowledge for designing social robots that are able to interact with humans under a wide range of conditions.","[{'authorId': '52191123', 'name': 'Á. Miklósi'}, {'authorId': '3131165', 'name': 'M. Gácsi'}]",83.0,"{'bibtex': '@Article{Miklósi2012OnTU,\n author = {Á. Miklósi and M. Gácsi},\n journal = {Frontiers in Psychology},\n title = {On the Utilization of Social Animals as a Model for Social Robotics},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Psychology'}",67.0,On the Utilization of Social Animals as a Model for Social Robotics,2012.0
867,469c5d279c5e0625f730f98dc07d2d8b875a2e82,1. Introducing the Problem: Individual and Group 2. Rediscovering the Social Group 3. A Self-Categorization Theory 4. The Analysis of Social Influence 5. Social Identity 6. The Salience of Social Categories 7. Social Identity and Group Polarization 8. Crowd Behaviour as Social Action 9. Conclusion.,"[{'authorId': '144385080', 'name': 'J. Turner'}, {'authorId': '152475749', 'name': 'M. Hogg'}, {'authorId': '48266513', 'name': 'P. Oakes'}, {'authorId': '6078899', 'name': 'S. Reicher'}, {'authorId': '32919858', 'name': 'M. Wetherell'}]",9144.0,"{'bibtex': '@Article{Turner1989RediscoveringTS,\n author = {J. Turner and M. Hogg and P. Oakes and S. Reicher and M. Wetherell},\n journal = {Contemporary Sociology},\n pages = {645},\n title = {Rediscovering the social group: A self-categorization theory.},\n volume = {18},\n year = {1989}\n}\n'}",,"{'volume': '18', 'pages': '645', 'name': 'Contemporary Sociology'}",0.0,Rediscovering the social group: A self-categorization theory.,1989.0
868,46b9fbc467c4865985a426e79e1b1e276e13e17c,"To improve the performance and wellbeing of humans in complex human-computer interaction settings, ambient (or pervasive) systems need the capability to recognize the emotions of humans, but also the ability to reason about their emotion regulation processes. To this end, this paper introduces a computational model to estimate and reason about emotion regulation. The model has been implemented and tested using the high-level modeling language LEADSTO. A first evaluation indicates that the model is successful in estimating a person's emotion regulation dynamics, and is robust to different parameter settings.","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '38590917', 'name': 'F. J. Lange'}]",5.0,"{'bibtex': '@Inproceedings{Bosse2008EstimatingER,\n author = {T. Bosse and F. J. Lange},\n pages = {93},\n title = {Estimating emotion regulation capabilities},\n year = {2008}\n}\n'}",,{'pages': '93'},33.0,Estimating emotion regulation capabilities,2008.0
869,46d01e53a0531e659048000e273ae0e194332204,"Trust is an important aspect of decision making for Internet applications and particularly influences the specification of security policy, i.e., who is authorized to perform actions as well as the techniques needed to manage and implement security to and for the applications. This survey examines the various definitions of trust in the literature and provides a working definition of trust for Internet applications. The properties of trust relationships are explained and classes of different types of trust identified in the literature are discussed with examples. Some influential examples of trust management systems are described.","[{'authorId': '47346594', 'name': 'T. Grandison'}, {'authorId': '1738169', 'name': 'M. Sloman'}]",1264.0,"{'bibtex': '@Article{Grandison2000ASO,\n author = {T. Grandison and M. Sloman},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {2-16},\n title = {A survey of trust in internet applications},\n volume = {3},\n year = {2000}\n}\n'}",,"{'volume': '3', 'pages': '2-16', 'name': 'IEEE Communications Surveys & Tutorials'}",74.0,A survey of trust in internet applications,2000.0
870,46d800dd01e9711deb61687334cf7bf4ec8c53ad,,"[{'authorId': '2025591', 'name': 'K. Bergmann'}, {'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",108.0,"{'bibtex': '@Inproceedings{Bergmann2012ASC,\n author = {K. Bergmann and F. Eyssel and S. Kopp},\n pages = {126-138},\n title = {A Second Chance to Make a First Impression? How Appearance and Nonverbal Behavior Affect Perceived Warmth and Competence of Virtual Agents over Time},\n year = {2012}\n}\n'}",,{'pages': '126-138'},34.0,A Second Chance to Make a First Impression? How Appearance and Nonverbal Behavior Affect Perceived Warmth and Competence of Virtual Agents over Time,2012.0
871,46dbf46178bdc6475e5bd68cc712a066ca222f49,,"[{'authorId': '145951543', 'name': 'A. Mitrovic'}, {'authorId': '2659276', 'name': 'P. Suraweera'}]",64.0,"{'bibtex': '@Inproceedings{Mitrovic2000EvaluatingAA,\n author = {A. Mitrovic and P. Suraweera},\n pages = {73-82},\n title = {Evaluating an Animated Pedagogical Agent},\n year = {2000}\n}\n'}",,{'pages': '73-82'},13.0,Evaluating an Animated Pedagogical Agent,2000.0
872,46e3485f18ff334b4fc3c15a71ba61a6c9ea2a0e,"Due to safety and ethical issues, traditional experimental approaches to modelling underground risk behaviours can be costly, dangerous and even impossible to realize. Based on multi-agent technology, a virtual coalmine platform for risk behaviour simulation is presented to model and simulate the human-machineenvironment related risk factors in underground coalmines. To reveal mine workers’ risk behaviours, a fuzzy emotional behaviour model is proposed to simulate underground miners’ responding behaviours to potential hazardous events based on cognitive appraisal theories and fuzzy logic techniques. The proposed emotion model can generate more believable behaviours for virtual miners according to personalized emotion states, internal motivation needs and behaviour selection thresholds. Finally, typical accident cases of underground hazard spotting and locomotive transport were implemented. The behaviour believability of virtual miners was evaluated with a user assessment method. Experimental results show that the proposed models can create more realistic and reasonable behaviours in virtual coalmine environments, which can improve miners’ risk awareness and further train miners’ emergent decision-making ability when facing unexpected underground situations.","[{'authorId': '2524401', 'name': 'Linqin Cai'}, {'authorId': '2109506237', 'name': 'Zhuo Yang'}, {'authorId': '98726631', 'name': 'Simon X. Yang'}, {'authorId': '2133747', 'name': 'Hongchun Qu'}]",0.0,"{'bibtex': '@Inproceedings{Cai2017InternationalJO,\n author = {Linqin Cai and Zhuo Yang and Simon X. Yang and Hongchun Qu},\n title = {International Journal of Advanced Robotic Systems Modelling and Simulating of Risk Behaviours in Virtual Environments Based on Multi-Agent and Fuzzy Logic Regular Paper},\n year = {2017}\n}\n'}",[],,32.0,International Journal of Advanced Robotic Systems Modelling and Simulating of Risk Behaviours in Virtual Environments Based on Multi-Agent and Fuzzy Logic Regular Paper,2017.0
873,470c41daed60c733d422280ce1c568b4548a6f38,"In this study, we create a 3D interactive virtual character based on multi-modal emotional recognition and rule based emotional synthesize techniques. This agent estimates users' emotional state by combining the information from the audio and facial expression with CART and boosting. For the output module of the agent, the voice is generated by TTS (Text-to-Speech)system by freely given text. The synchronous visual information of agent, including facial expression, head motion, gesture and body animation, are generated by multi-modal mapping from motion capture database. A kind of high level behavior markerup language(hBML) which contains five keywords is used to drive the animation of virtual agent for emotional expression. Experiments show that this virtual character is considered natural and realistic in multimodal interaction environments.","[{'authorId': '2740129', 'name': 'Minghao Yang'}, {'authorId': '37670752', 'name': 'J. Tao'}, {'authorId': '1706574', 'name': 'Hao Li'}, {'authorId': '3295988', 'name': 'Kaihui Mu'}]",0.0,"{'bibtex': '@Article{Yang2012MultimodalEE,\n author = {Minghao Yang and J. Tao and Hao Li and Kaihui Mu},\n booktitle = {2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems},\n journal = {2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems},\n pages = {191-196},\n title = {Multimodal emotion estimation and emotional synthesize for interaction virtual agent},\n volume = {01},\n year = {2012}\n}\n'}",[],"{'name': '2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems', 'pages': '191-196', 'volume': '01'}",41.0,Multimodal emotion estimation and emotional synthesize for interaction virtual agent,2012.0
874,4712389d29a2aa864f2deaf88325e2c9ad814b0f,,"[{'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2545816', 'name': 'R. Phaf'}]",16.0,"{'bibtex': '@Article{Heerebout2010GoodVS,\n author = {B. T. Heerebout and R. Phaf},\n journal = {Cognitive, Affective, & Behavioral Neuroscience},\n pages = {217-229},\n title = {Good vibrations switch attention: An affective function for network oscillations in evolutionary simulations},\n volume = {10},\n year = {2010}\n}\n'}",,"{'volume': '10', 'pages': '217-229', 'name': 'Cognitive, Affective, & Behavioral Neuroscience'}",53.0,Good vibrations switch attention: An affective function for network oscillations in evolutionary simulations,2010.0
875,472ba8dd4ec72b34e85e733bccebb115811fd726,,"[{'authorId': '2110579514', 'name': 'Hieu V. Nguyen'}, {'authorId': '143642695', 'name': 'L. Bai'}]",668.0,"{'bibtex': '@Inproceedings{Nguyen2010CosineSM,\n author = {Hieu V. Nguyen and L. Bai},\n pages = {709-720},\n title = {Cosine Similarity Metric Learning for Face Verification},\n year = {2010}\n}\n'}",,{'pages': '709-720'},25.0,Cosine Similarity Metric Learning for Face Verification,2010.0
876,473908b1e211530710969c4117f517f7c900af55,A long-standing question within the robotics community is about the degree of human-likeness robots ought to have when interacting with humans. We explore an unexamined aspect of this problem: how people empathize with robots along the anthropomorphic spectrum. We conducted an experiment that measured how people empathized with robots shown to be experiencing mistreatment by humans. Our results indicate that people empathize more strongly with more human-looking robots and less with mechanical-looking robots.,"[{'authorId': '144786679', 'name': 'L. Riek'}, {'authorId': '2640789', 'name': 'Tal-Chen Rabinowitch'}, {'authorId': '3102450', 'name': 'B. Chakrabarti'}, {'authorId': '2149814967', 'name': 'P. Robinson'}]",243.0,"{'bibtex': '@Article{Riek2009HowAA,\n author = {L. Riek and Tal-Chen Rabinowitch and B. Chakrabarti and P. Robinson},\n journal = {2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {245-246},\n title = {How anthropomorphism affects empathy toward robots},\n year = {2009}\n}\n'}",,"{'pages': '245-246', 'name': '2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",8.0,How anthropomorphism affects empathy toward robots,2009.0
879,4751bf540930c77876dabaa2aee20c00c58eef66,,"[{'authorId': '123583022', 'name': 'C. Bailey'}, {'authorId': '2102392', 'name': 'Jiaming You'}, {'authorId': '2073644828', 'name': 'G. Acton'}, {'authorId': '2073890045', 'name': 'A. Rankin'}, {'authorId': '1793961', 'name': 'M. Katchabaw'}]",12.0,"{'bibtex': '@Inproceedings{Bailey2012BelievabilityTP,\n author = {C. Bailey and Jiaming You and G. Acton and A. Rankin and M. Katchabaw},\n pages = {29-68},\n title = {Believability Through Psychosocial Behaviour: Creating Bots That Are More Engaging and Entertaining},\n year = {2012}\n}\n'}",,{'pages': '29-68'},80.0,Believability Through Psychosocial Behaviour: Creating Bots That Are More Engaging and Entertaining,2012.0
880,47524f71f2e22307a6bd2e86dabd9aabe3a140bd,"What kinds of social relationships can people have with computers? Are there activities that computers can engage in that actively draw people into relationships with them? What are the potential benefits to the people who participate in these human-computer relationships? To address these questions this work introduces a theory of Relational Agents, which are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. These can be purely software humanoid animated agents--as developed in this work--but they can also be non-humanoid or embodied in various physical forms, from robots, to pets, to jewelry, clothing, hand-helds, and other interactive devices. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions; thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Finally, relationships are fundamentally social and emotional, and detailed knowledge of human social psychology--with a particular emphasis on the role of affect--must be incorporated into these agents if they are to effectively leverage the mechanisms of human social cognition in order to build relationships in the most natural manner possible. People build relationships primarily through the use of language, and primarily within the context of face-to-face conversation. Embodied Conversational Agents--anthropomorphic computer characters that emulate the experience of face-to-face conversation--thus provide the substrate for this work, and so the relational activities provided by the theory will primarily be specific types of verbal and nonverbal conversational behaviors used by people to negotiate and maintain relationships. This work also provides an analysis of the types of applications in which having a human-computer relationship is advantageous to the human participant. In addition to applications in which the relationship is an end in itself (e.g., in entertainment systems), human-computer relationships are important in tasks in which the human is attempting to undergo some change in behavior or cognitive or emotional state. One such application is explored here: a system for assisting the user through a month-long health behavior change program in the area of exercise adoption. This application involves the research, design and implementation of relational agents as well as empirical evaluation of their ability to build relationships and effect change over a series of interactions with users. Thesis Supervisors: Rosalind W. Picard, Associate Professor of Media Arts and Sciences Justine Cassell, Associate Professor of Media Arts and Sciences","[{'authorId': '1690448', 'name': 'T. Bickmore'}]",321.0,"{'bibtex': '@Inproceedings{Bickmore2003RelationalA,\n author = {T. Bickmore},\n title = {Relational agents : effecting change through human-computer relationships},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",246.0,Relational agents : effecting change through human-computer relationships,2003.0
881,475a9b7b23da93c35da9aeb5e4fbfef01bef9385,"Empathy is a multifaceted emotional and mental faculty that is often found to be affected in a great number of psychopathologies, such as schizophrenia, yet it remains very difficult to measure in an ecological context. The challenge stems partly from the complexity and fluidity of this social process, but also from its covert nature. One powerful tool to enhance experimental control over such dynamic social interactions has been the use of avatars in virtual reality (VR); information about an individual in such an interaction can be collected through the analysis of his or her neurophysiological and behavioral responses. We have developed a unique platform, the Empathy-Enhancing Virtual Evolving Environment (EEVEE), which is built around three main components: (1) different avatars capable of expressing feelings and emotions at various levels based on the Facial Action Coding System (FACS); (2) systems for measuring the physiological responses of the observer (heart and respiration rate, skin conductance, gaze and eye movements, facial expression); and (3) a multimodal interface linking the avatar's behavior to the observer's neurophysiological response. In this article, we provide a detailed description of the components of this innovative platform and validation data from the first phases of development. Our data show that healthy adults can discriminate different negative emotions, including pain, expressed by avatars at varying intensities. We also provide evidence that masking part of an avatar's face (top or bottom half) does not prevent the detection of different levels of pain. This innovative and flexible platform provides a unique tool to study and even modulate empathy in a comprehensive and ecological manner in various populations, notably individuals suffering from neurological or psychiatric disorders.","[{'authorId': '6079112', 'name': 'P. Jackson'}, {'authorId': '145500372', 'name': 'P. Michon'}, {'authorId': '34861774', 'name': 'Erik Geslin'}, {'authorId': '32406041', 'name': 'Maxime Carignan'}, {'authorId': '33470496', 'name': 'Danny Beaudoin'}]",28.0,"{'bibtex': '@Article{Jackson2015EEVEETE,\n author = {P. Jackson and P. Michon and Erik Geslin and Maxime Carignan and Danny Beaudoin},\n journal = {Frontiers in Human Neuroscience},\n title = {EEVEE: the Empathy-Enhancing Virtual Evolving Environment},\n volume = {9},\n year = {2015}\n}\n'}",,"{'volume': '9', 'name': 'Frontiers in Human Neuroscience'}",69.0,EEVEE: the Empathy-Enhancing Virtual Evolving Environment,2015.0
882,476029ac9be26bf7f121a388f5c1e45d204efe52,"Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.","[{'authorId': '47261124', 'name': 'Qian Chen'}, {'authorId': '1500648476', 'name': 'Zhu Zhuo'}, {'authorId': '2117827836', 'name': 'Wen Wang'}]",426.0,"{'bibtex': '@Article{Chen2019BERTFJ,\n author = {Qian Chen and Zhu Zhuo and Wen Wang},\n journal = {ArXiv},\n title = {BERT for Joint Intent Classification and Slot Filling},\n volume = {abs/1902.10909},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1902.10909', 'name': 'ArXiv'}",26.0,BERT for Joint Intent Classification and Slot Filling,2019.0
883,476ee56bf99d8b848815a61a77fdc79325e2d5b3,"This paper proposes a new method, minimum risk approach, to address the local path planning to escape from local minimum during goal-oriented robot navigation in unknown environments. This approach is theoretically proved to guarantee global convergence even in the long-wall, unstructured, cluttered, maze-like, and modified environments. The approach adopts a strategy of multi-behavior coordination, in which a novel path-searching behavior is developed to recommend the regional direction with minimum risk. The paper provides a fuzzy logic framework to implement the behavior design and coordination. It is verified by the simulated and real world tests.","[{'authorId': '39872583', 'name': 'M. Wang'}, {'authorId': '2116031967', 'name': 'J.N.K. Liu'}]",83.0,"{'bibtex': '@Article{Wang2005FuzzyLB,\n author = {M. Wang and J.N.K. Liu},\n journal = {2005 International Conference on Machine Learning and Cybernetics},\n pages = {813-818 Vol. 2},\n title = {Fuzzy logic based robot path planning in unknown environment},\n volume = {2},\n year = {2005}\n}\n'}",,"{'volume': '2', 'pages': '813-818 Vol. 2', 'name': '2005 International Conference on Machine Learning and Cybernetics'}",12.0,Fuzzy logic based robot path planning in unknown environment,2005.0
884,478d05393388f47179d55df8c791156b88697919,"In order to improve the social capabilities of embodied conversational agents, we propose a computational model to enable agents to automatically select and display appropriate smiling behavior during human--machine interaction. A smile may convey different communicative intentions depending on subtle characteristics of the facial expression and contextual cues. To construct such a model, as a first step, we explore the morphological and dynamic characteristics of different types of smiles (polite, amused, and embarrassed smiles) that an embodied conversational agent may display. The resulting lexicon of smiles is based on a corpus of virtual agents’ smiles directly created by users and analyzed through a machine-learning technique. Moreover, during an interaction, a smiling expression impacts on the observer’s perception of the interpersonal stance of the speaker. As a second step, we propose a probabilistic model to automatically compute the user’s potential perception of the embodied conversational agent’s social stance depending on its smiling behavior and on its physical appearance. This model, based on a corpus of users’ perceptions of smiling and nonsmiling virtual agents, enables a virtual agent to determine the appropriate smiling behavior to adopt given the interpersonal stance it wants to express. An experiment using real human--virtual agent interaction provided some validation of the proposed model.","[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '2228246', 'name': 'G. McKeown'}]",34.0,"{'bibtex': '@Article{Ochs2017AUP,\n author = {M. Ochs and C. Pelachaud and G. McKeown},\n journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},\n pages = {1 - 33},\n title = {A User Perception--Based Approach to Create Smiling Embodied Conversational Agents},\n volume = {7},\n year = {2017}\n}\n'}",,"{'volume': '7', 'pages': '1 - 33', 'name': 'ACM Transactions on Interactive Intelligent Systems (TiiS)'}",97.0,A User Perception--Based Approach to Create Smiling Embodied Conversational Agents,2017.0
885,479106770ee21a67247396f558828ac557024abf,,"[{'authorId': '144116292', 'name': 'A. Bernardo'}]",195.0,"{'bibtex': '@Article{Bernardo2017VirtualRA,\n author = {A. Bernardo},\n journal = {World neurosurgery},\n pages = {\n          1015-1029\n        },\n title = {Virtual Reality and Simulation in Neurosurgical Training.},\n volume = {106},\n year = {2017}\n}\n'}",,"{'volume': '106', 'pages': '\n          1015-1029\n        ', 'name': 'World neurosurgery'}",57.0,Virtual Reality and Simulation in Neurosurgical Training.,2017.0
886,47b4460a4920cc42ad3473610130a2aeb3d6577e,"Over 10 years' experience with VR displays, visualization applications, and informal feedback from scientists using these applications has convinced RWTH Aachen University researchers that the combination of full immersion, high image quality, and advanced interaction metaphors makes immersive visualization valuable as an analysis tool in simulation science.","[{'authorId': '144483066', 'name': 'T. Kuhlen'}, {'authorId': '38784313', 'name': 'B. Hentschel'}]",39.0,"{'bibtex': '@Article{Kuhlen2014QuoVC,\n author = {T. Kuhlen and B. Hentschel},\n journal = {IEEE Computer Graphics and Applications},\n pages = {14-21},\n title = {Quo Vadis CAVE: Does Immersive Visualization Still Matter?},\n volume = {34},\n year = {2014}\n}\n'}",,"{'volume': '34', 'pages': '14-21', 'name': 'IEEE Computer Graphics and Applications'}",14.0,Quo Vadis CAVE: Does Immersive Visualization Still Matter?,2014.0
887,47cc3c785ded8f0ab82d36949da78724425b8551,"Computer modelling of personality and behaviour is becoming increasingly important in many fields of computer science and psychology. Personality and emotion-driven Believable Agents are needed in areas like human–machine interfaces, electronic advertising and, most notably, electronic entertainment. Computer models of personality can help explain personality by illustrating its underlying structure and dynamics. This work presents a neural network model of personality and personality change. The goals are to help understand personality and create more realistic and believable characters for interactive video games. The model is based largely on trait theories of personality. Behaviour in the model results from the interaction of three components: (1) personality-based predispositions for behaviour, (2) moods/emotions and (3) environmental situations. Personality develops gradually over time depending on the situations encountered. Modelling personality change produces interesting and believable virtual characters whose behaviours change in psychologically plausible ways.","[{'authorId': '2071944563', 'name': 'M. Poznanski'}, {'authorId': '1756123', 'name': 'Paul Thagard'}]",39.0,"{'bibtex': '@Article{Poznanski2005ChangingPT,\n author = {M. Poznanski and Paul Thagard},\n journal = {Journal of Experimental & Theoretical Artificial Intelligence},\n pages = {221 - 241},\n title = {Changing personalities: towards realistic virtual characters},\n volume = {17},\n year = {2005}\n}\n'}",,"{'volume': '17', 'pages': '221 - 241', 'name': 'Journal of Experimental & Theoretical Artificial Intelligence'}",37.0,Changing personalities: towards realistic virtual characters,2005.0
888,47fc921add1421ff8adb730df7aa9e7f865bfdeb,"Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.","[{'authorId': '143973061', 'name': 'J. Whitehill'}, {'authorId': '2724380', 'name': 'G. Littlewort'}, {'authorId': '2039025', 'name': 'Ian R. Fasel'}, {'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '1741200', 'name': 'J. Movellan'}]",325.0,"{'bibtex': '@Article{Whitehill2009TowardPS,\n author = {J. Whitehill and G. Littlewort and Ian R. Fasel and M. Bartlett and J. Movellan},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2106-2111},\n title = {Toward Practical Smile Detection},\n volume = {31},\n year = {2009}\n}\n'}",,"{'volume': '31', 'pages': '2106-2111', 'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence'}",43.0,Toward Practical Smile Detection,2009.0
889,480aab717bd4551bedb25949080888b6731bce90,,"[{'authorId': '3196314', 'name': 'Judith H. Danovitch'}, {'authorId': '7560415', 'name': 'Candice M. Mills'}]",26.0,"{'bibtex': ""@Article{Danovitch2014HowFC,\n author = {Judith H. Danovitch and Candice M. Mills},\n journal = {Journal of experimental child psychology},\n pages = {\n          1-20\n        },\n title = {How familiar characters influence children's judgments about information and products.},\n volume = {128},\n year = {2014}\n}\n""}",,"{'volume': '128', 'pages': '\n          1-20\n        ', 'name': 'Journal of experimental child psychology'}",30.0,How familiar characters influence children's judgments about information and products.,2014.0
890,480fe6a1313998446fe445bf32fcac84c2191143,,"[{'authorId': '2731533', 'name': 'B. V. Fraassen'}]",272.0,"{'bibtex': ""@Article{Fraassen1973ValuesAT,\n author = {B. V. Fraassen},\n journal = {The Journal of Philosophy},\n pages = {5-19},\n title = {Values and the Heart's Command},\n volume = {70},\n year = {1973}\n}\n""}",,"{'volume': '70', 'pages': '5-19', 'name': 'The Journal of Philosophy'}",0.0,Values and the Heart's Command,1973.0
891,48596a00247b68200568ac511f2fa78773947a68,"Recently interest has grown in applying activity theory, the leading theoretical approach in Russian psychology, to issues of human-computer interaction. This chapter analyzes why experts in the field are looking for an alternative to the currently dominant cognitive approach. The basic principles of activity theory are presented and their implications for human-computer interaction are discussed. The chapter concludes with an outline of the potential impact of activity theory on studies and design of computer use in real-life settings. It is generally accepted that the lack of an adequate theory of human-computer interaction (HCI) is one of the most important reasons that progress in the field of HCI is relatively modest, compared with the rate of technological development. People coming to the field of HCI from different disciplines—psychology, computer science, graphics design, and others—have serious problems in coordinating and combining their efforts. For example, typical HCI curricula for undergraduate and graduate students present a mixture of knowledge from various disciplines rather than an integrated perspective. Traditional conceptual approaches cannot provide an appropriate basis for addressing many important aspects of HCI, including computer-supported cooperative work (CSCW) and cross-cultural aspects of computer use. Consequently the impact of HCI studies on current design practice is limited, with user interface design being based mainly on intuition and expensive trial and error. The form of a suitable HCI theory has been subjected to much debate recently (Carroll, Kellogg, and Rosson 1991). A major trend in the debate has been the growing dissatisfaction with the dominant cognitive approach (Bannon 1991; Wood 1992; Monk et al. 1993). In contrast to the general agreement that current attempts to apply cognitive psychology to HCI are not very successful, there is little agreement on the most promising theoretical alternatives. Proposals vary from an enrichment of the traditional cognitive scheme (Barnard 1991) to a radical shift in paradigms, for example, from scientific experimental studies to ethnographic methodology (see Monk et al. 1993). In this period of theoretical uncertainty there has been a growing interest in activity theory, greatly stimulated by Bødker's works (1989, 1991). She was the first Western researcher who presented the basic ideas and potential benefits of activity theory to the HCI community. Recently, a number of papers discussing the activity theory approach to HCI have appeared in major international journals and The aim of the present chapter is to summarize current work in activity theory and its implications for …","[{'authorId': '1751148', 'name': 'V. Kaptelinin'}]",426.0,"{'bibtex': '@Misc{None,\n author = {V. Kaptelinin},\n title = {Activity Theory: Implications for Human-computer Interaction the Need for a Theory of Human-computer Interaction Basic Principles of Activity Theory}\n}\n'}",,,31.0,Activity Theory: Implications for Human-computer Interaction the Need for a Theory of Human-computer Interaction Basic Principles of Activity Theory,
892,48669a13c5c30da39eb439c03e4e4c7d7ab5576e,"Emotions are action dispositions--states of vigilant readiness that vary widely in reported affect, physiology, and behavior. They are driven, however, by only 2 opponent motivational systems, appetitive and aversive--subcortical circuits that mediate reactions to primary reinforcers. Using a large emotional picture library, reliable affective psychophysiologies are shown, defined by the judged valence (appetitive/pleasant or aversive/unpleasant) and arousal of picture percepts. Picture-evoked affects also modulate responses to independently presented startle probe stimuli. In other words, they potentiate startle reflexes during unpleasant pictures and inhibit them during pleasant pictures, and both effects are augmented by high picture arousal. Implications are elucidated for research in basic emotions, psychopathology, and theories of orienting and defense. Conclusions highlight both the approach's constraints and promising paths for future study.","[{'authorId': '143853826', 'name': 'P. Lang'}]",2654.0,"{'bibtex': '@Article{Lang1995TheEP,\n author = {P. Lang},\n journal = {The American psychologist},\n pages = {\n          372-85\n        },\n title = {The emotion probe. Studies of motivation and attention.},\n volume = {50 5},\n year = {1995}\n}\n'}",,"{'volume': '50 5', 'pages': '\n          372-85\n        ', 'name': 'The American psychologist'}",76.0,The emotion probe. Studies of motivation and attention.,1995.0
894,4880434b8efc9a73db45b47b89f04df533420b51,"The virtual character’s expressions of emotions may significantly enhance human-machine interaction. To give the capability to virtual characters to display emotions, the latter should be endowed with a repertoire of facial expressions that convey emotional meanings in conversational settings. In this chapter, we explore research works highlighting different methodologies both to identify the morphological and dynamic characteristics of emotional facial expressions and to measure the effects of the emotional expressions on the user’s perception during human-machine interaction.","[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",19.0,"{'bibtex': '@Inproceedings{Ochs2015FacialEO,\n author = {M. Ochs and Radoslaw Niewiadomski and C. Pelachaud},\n title = {Facial Expressions of Emotions for Virtual Characters},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",59.0,Facial Expressions of Emotions for Virtual Characters,2015.0
896,48930c5afaeca22751c59ee5f16c92d473985a65,"Today’s video games are striving to maintain high levels of fidelity. The realistic graphical representation of virtual worlds is only betrayed by the lack of believability that ingame characters present. To maintain the immersion created by exquisite graphics, characters must be able to create the illusion of life, which requires them to possess basic human traits like social awareness, reactivity, and active goal pursuit. Some game genres, like role playing games, have seen this problem being addressed by using agency based characters. However, survival games have not been subject to the same attention. In this work, we address this issue by proposing a framework that allows developers to create characters based on agency models for survival games. By making use of FAtiMA Toolkit, a fully fledged model for agency, and Don’t Starve Together, a popular survival game, we’ve implemented and published such a framework with an example character, Walter. Walter has been run and tested against a behaviour tree based agent.","[{'authorId': '2053384133', 'name': 'Fábio Vieira de Almeida'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '145813496', 'name': 'C. Martinho'}]",2.0,"{'bibtex': '@Inproceedings{Almeida2018CreatingAA,\n author = {Fábio Vieira de Almeida and R. Prada and C. Martinho},\n title = {Creating an Agent-Based Framework for Don’t Starve Together},\n year = {2018}\n}\n'}",,,51.0,Creating an Agent-Based Framework for Don’t Starve Together,2018.0
897,48af5a77e5c4d32d90fe0601170552673530a125,"A major goal of research on virtual humans is the animation of expressive characters that display distinct psychological attributes. Body motion is an effective way of portraying different personalities and differentiating characters. The purpose and contribution of this work is to describe a formal, broadly applicable, procedural, and empirically grounded association between personality and body motion and apply this association to modify a given virtual human body animation that can be represented by these formal concepts. Because the body movement of virtual characters may involve different choices of parameter sets depending on the context, situation, or application, formulating a link from personality to body motion requires an intermediate step to assist generalization. For this intermediate step, we refer to Laban Movement Analysis, which is a movement analysis technique for systematically describing and evaluating human motion. We have developed an expressive human motion generation system with the help of movement experts and conducted a user study to explore how the psychologically validated OCEAN personality factors were perceived in motions with various Laban parameters. We have then applied our findings to procedurally animate expressive characters with personality, and validated the generalizability of our approach across different models and animations via another perception study.","[{'authorId': '2643744', 'name': 'Funda Durupinar'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '2047954359', 'name': 'Susan Deutsch'}, {'authorId': '143687087', 'name': 'Michael Neff'}, {'authorId': '1699200', 'name': 'N. Badler'}]",59.0,"{'bibtex': '@Article{Durupinar2016PERFORM,\n author = {Funda Durupinar and Mubbasir Kapadia and Susan Deutsch and Michael Neff and N. Badler},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 16},\n title = {PERFORM},\n volume = {36},\n year = {2016}\n}\n'}",,"{'volume': '36', 'pages': '1 - 16', 'name': 'ACM Transactions on Graphics (TOG)'}",0.0,PERFORM,2016.0
898,48c92013bd1fd410210399f5521bc6adf1998e72,"This paper describes an approach using wearables to demonstrate the viability of measuring physiometric arousal indicators such as heart rate in assessing how urban built environments can induce physiometric arousal indicators in a subject. In addition, a machine learning methodology is developed to classify sensor inputs based on annotated arousal output as a target. The results are then used as a foundation for designing and implementing an affective intelligent systems framework for arousal state detection via supervised learning and classification.","[{'authorId': '38005318', 'name': 'H. Yates'}, {'authorId': '36583823', 'name': 'Brent C. Chamberlain'}, {'authorId': '81017759', 'name': 'G. Norman'}, {'authorId': '145591174', 'name': 'W. Hsu'}]",15.0,"{'bibtex': '@Inproceedings{Yates2017ArousalDF,\n author = {H. Yates and Brent C. Chamberlain and G. Norman and W. Hsu},\n pages = {58-72},\n title = {Arousal Detection for Biometric Data in Built Environments using Machine Learning},\n year = {2017}\n}\n'}",,{'pages': '58-72'},31.0,Arousal Detection for Biometric Data in Built Environments using Machine Learning,2017.0
900,48cafa65a23cfa30fa30adcb60786b22e6a34b7a,,"[{'authorId': '1950943', 'name': 'Nick Halper'}, {'authorId': '48930261', 'name': 'M. Mellin'}, {'authorId': '1765445', 'name': 'C. Herrmann'}, {'authorId': '103357331', 'name': 'V. Linneweber'}, {'authorId': '1697367', 'name': 'T. Strothotte'}]",40.0,"{'bibtex': '@Inproceedings{Halper2003PsychologyAN,\n author = {Nick Halper and M. Mellin and C. Herrmann and V. Linneweber and T. Strothotte},\n pages = {277-286},\n title = {Psychology and Non-Photorealistic Rendering: The Beginning of a Beautiful Relationship},\n year = {2003}\n}\n'}",,{'pages': '277-286'},33.0,Psychology and Non-Photorealistic Rendering: The Beginning of a Beautiful Relationship,2003.0
901,48faa791ae8b5b174a161a4b1465379e51338ec5,,"[{'authorId': '40415515', 'name': 'A. Pais'}, {'authorId': '1836885', 'name': 'B. Argall'}, {'authorId': '1807928', 'name': 'A. Billard'}]",13.0,"{'bibtex': '@Article{Pais2013AssessingID,\n author = {A. Pais and B. Argall and A. Billard},\n journal = {International Journal of Social Robotics},\n pages = {477-490},\n title = {Assessing Interaction Dynamics in the Context of Robot Programming by Demonstration},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '477-490', 'name': 'International Journal of Social Robotics'}",32.0,Assessing Interaction Dynamics in the Context of Robot Programming by Demonstration,2013.0
902,49006327810b5009c4524cf87a7a1ca37d356970,"The eighth Audio-Visual Emotion Challenge and workshop AVEC 2018 was held in conjunction with ACM Multimedia'18. This year, the AVEC series addressed major novelties with three distinct sub-challenges: bipolar disorder classification, cross-cultural dimensional emotion recognition, and emotional label generation from individual ratings. The Bipolar Disorder Sub-challenge was based on a novel dataset of structured interviews of patients suffering from bipolar disorder (BD corpus), the Cross-cultural Emotion Sub-challenge relied on an extension of the SEWA dataset, which includes human-human interactions recorded 'in-the-wild' for the German and the Hungarian cultures, and the Gold-standard Emotion Sub-challenge was based on the RECOLA dataset, which was previously used in the AVEC series for emotion recognition. In this summary, we mainly describe participation and conditions of the AVEC Challenge.","[{'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",6.0,"{'bibtex': '@Article{Ringeval2018SummaryFA,\n author = {F. Ringeval and Björn Schuller and M. Valstar and R. Cowie and M. Pantic},\n journal = {Proceedings of the 26th ACM international conference on Multimedia},\n title = {Summary for AVEC 2018: Bipolar Disorder and Cross-Cultural Affect Recognition},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 26th ACM international conference on Multimedia'},14.0,Summary for AVEC 2018: Bipolar Disorder and Cross-Cultural Affect Recognition,2018.0
903,491bbc1ab6ae58f4e4fc1fc2169bba85bb697199,"Societies rely on trustworthy communication in order to function, and the need for trust clearly extends to human-machine communication. Therefore, it is essential to design machines to elicit trust, so as to make interactions with them acceptable and successful. However, while there is a substantial literature on first impressions of trustworthiness based on various characteristics, including voice, not much is known about the trust development process. Are first impressions maintained over time? Or are they influenced by the experience of an agent's behaviour? We addressed these questions in three experiments using the ""iterated investment game"", a methodology derived from game theory that allows implicit measures of trust to be collected over time. Participants played the game with various agents having different voices: in the first experiment, participants played with a computer agent that had either a Standard Southern British English accent or a Liverpool accent; in the second experiment, they played with a computer agent that had either an SSBE or a Birmingham accent; in the third experiment, they played with a robot that had either a natural or a synthetic voice. All these agents behaved either trustworthily or untrustworthily. In all three experiments, participants trusted the agent with one voice more when it was trustworthy, and the agent with the other voice more when it was untrustworthy. This suggests that participants might change their trusting behaviour based on the congruency of the agent's behaviour with the participant's first impression. Implications for human-machine interaction design are discussed.","[{'authorId': '1996210170', 'name': 'Ilaria Torre'}, {'authorId': '2826399', 'name': 'Jeremy Goslin'}, {'authorId': '8375293', 'name': 'Laurence White'}, {'authorId': '3364118', 'name': 'Debora Zanatto'}]",51.0,"{'bibtex': '@Article{Torre2018TrustIA,\n author = {Ilaria Torre and Jeremy Goslin and Laurence White and Debora Zanatto},\n journal = {Proceedings of the Technology, Mind, and Society},\n title = {Trust in artificial voices: A ""congruency effect"" of first impressions and behavioural experience},\n year = {2018}\n}\n'}",,"{'name': 'Proceedings of the Technology, Mind, and Society'}",28.0,"Trust in artificial voices: A ""congruency effect"" of first impressions and behavioural experience",2018.0
904,491e2c09243dab781cd616c2b60aecc9e018e6e5,"Numerous research efforts have been conducted to simulate the crowd movements, while relatively few of them are specifically focused on multihazard situations. In this paper, we propose a novel crowd simulation method by modeling the generation and contagion of panic emotion under multihazard circumstances. In order to depict the effect from hazards and other agents to crowd movement, we first classify hazards into different types (transient and persistent, concurrent and nonconcurrent, and static and dynamic) based on their inherent characteristics. Second, we introduce the concept of perilous field for each hazard and further transform the critical level of the field to its invoked-panic emotion. After that, we propose an emotional contagion model to simulate the evolving process of panic emotion caused by multiple hazards. Finally, we introduce an emotional reciprocal velocity obstacles (RVOs) model to simulate the crowd behaviors by augmenting the traditional RVO model with emotional contagion, which for the first time combines the emotional impact and local avoidance together. Our experimental results demonstrate that the overall approach is robust, can better generate realistic crowds and the panic emotion dynamics in a crowd. Furthermore, it is recommended that our method can be applied to various complex multihazard environments.","[{'authorId': '2285442', 'name': 'Mingliang Xu'}, {'authorId': '2016679', 'name': 'Xiaozhen Xie'}, {'authorId': '144470801', 'name': 'Pei Lv'}, {'authorId': '35667840', 'name': 'Jiangwei Niu'}, {'authorId': '2113254290', 'name': 'Hua Wang'}, {'authorId': '7431398', 'name': 'Chaochao Li'}, {'authorId': '2070270237', 'name': 'Ruijie Zhu'}, {'authorId': '145140508', 'name': 'Z. Deng'}, {'authorId': '2118869098', 'name': 'Bing Zhou'}]",88.0,"{'bibtex': '@Article{Xu2018CrowdBS,\n author = {Mingliang Xu and Xiaozhen Xie and Pei Lv and Jiangwei Niu and Hua Wang and Chaochao Li and Ruijie Zhu and Z. Deng and Bing Zhou},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n pages = {1567-1581},\n title = {Crowd Behavior Simulation With Emotional Contagion in Unexpected Multihazard Situations},\n volume = {51},\n year = {2018}\n}\n'}",,"{'volume': '51', 'pages': '1567-1581', 'name': 'IEEE Transactions on Systems, Man, and Cybernetics: Systems'}",57.0,Crowd Behavior Simulation With Emotional Contagion in Unexpected Multihazard Situations,2018.0
905,4921d2f5eaace341f8597db48803b0a34daad347,"In long-term human robot interaction, memory of experience is important. This paper suggests episodic memory system for an affective robot which contains emotion. Storage, retention and retrieval of episodic memory are modeled with psychological bases. This modeled memory system will be implemented to pet-like virtual agent with reactive emotion generation model[2].","[{'authorId': '51226875', 'name': 'Hansoul Kim'}, {'authorId': '47988042', 'name': 'Jeong-Yean Yang'}, {'authorId': '145079887', 'name': 'D. Kwon'}]",4.0,"{'bibtex': '@Article{Kim2013EpisodicMS,\n author = {Hansoul Kim and Jeong-Yean Yang and D. Kwon},\n booktitle = {International Conference on Ubiquitous Robots and Ambient Intelligence},\n journal = {2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},\n pages = {720-722},\n title = {Episodic memory system of affective agent with emotion for long-term human-robot interaction},\n year = {2013}\n}\n'}","[{'paperId': '43971c87b79383923f37e41f0bece6be5e6f254e', 'title': 'Harnessing Long-term Memory for Personalized Human-Robot Interactions'}, {'paperId': '3d01d7646b7270347a4ad48bcfc9823b78a23697', 'title': 'A novel multimodal communication framework using robot partner for aging population'}, {'paperId': '4ea336562801cad257a183d70db86802d2c2398d', 'title': 'Emotional Intelligence for Artificial Intelligence : A Review'}, {'paperId': 'ac6ec6a266df0d8245959319d55caa1c133adf7e', 'title': 'Emotional agents - state of the art and applications'}]","{'name': '2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)', 'pages': '720-722'}",8.0,Episodic memory system of affective agent with emotion for long-term human-robot interaction,2013.0
906,494ea49753b90fc8eac4daf0a2e7197c128af197,"Based on the development of several different HRI scenarios using different robots, we have been establishing the SERA ecosystem. SERA is composed of both a model and tools for integrating an AI agent with a robotic embodiment, in human-robot interaction scenarios. We present the model, and several of the reusable tools that were developed, namely Thalamus, Skene and Nutty Tracks. Finally we exemplify how such tools and model have been used and integrated in five different HRI scenarios using the NAO, Keepon and EMYS robots.","[{'authorId': '145856842', 'name': 'T. Ribeiro'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '3188237', 'name': 'E. D. Tullio'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",29.0,"{'bibtex': '@Inproceedings{Ribeiro2016TheSE,\n author = {T. Ribeiro and André Pereira and E. D. Tullio and Ana Paiva},\n title = {The SERA Ecosystem: Socially Expressive Robotics Architecture for Autonomous Human-Robot Interaction},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",15.0,The SERA Ecosystem: Socially Expressive Robotics Architecture for Autonomous Human-Robot Interaction,2016.0
907,4973d4ab05a3289d5b3a12052471f3660f048d36,"ION. A. ABSTRACTION, 226. Actions, reflex, 35 ; coughing, sneezing, &c., 85; muscular action of decapitated frog, 36; closing the eyelids, 38 : starting, 3841; contraction of the iris, 41. Admiration, 289. Affirmation, signs of. 272. Albinos, blushing in, 312, 326. Alison, Professor, 31. Ambition, 261. Anatomical drawin,s by HeDle, 5. Anatomy and Philosophy of Expression, 2. Anderson, Dr., 106, n. 26. Anger, as a stimulant, 79; expreqsion, 244; in monkeys, 136. See also Rage. Animals, special expressions of, 115. See al8o Expression. -7 habitual associated movements in the lower, 42-49; dogs, 43; wolves and Jackals, 44;","[{'authorId': '121452695', 'name': 'C. Darwin'}]",161.0,"{'bibtex': '@Inproceedings{Darwin2020TheEO,\n author = {C. Darwin},\n title = {The Expression of Emotion in Man and Animals},\n year = {2020}\n}\n'}",,"{'volume': '', 'name': ''}",9.0,The Expression of Emotion in Man and Animals,2020.0
908,49820a5500b7e6fbb8d77d642c6360857da09cef,"Conversational agents (CAs) are becoming an increasingly common component in many information systems. The ubiquity of CAs in cell phones, entertainment systems, and messaging applications has led to a growing need to understand how design choices made when developing CAs influence user interactions. In this study, we explore the use case of CAs that gather potentially sensitive information from people—for example, in a medical interview. Using a laboratory experiment, we examine the influence of CA responsiveness and embodiment on the answers people give in response to sensitive and non-sensitive questions. The results show that for sensitive questions, the responsiveness of the CA increased the social desirability of the responses given by participants.","[{'authorId': '50818348', 'name': 'Ryan M. Schuetzler'}, {'authorId': '2436820', 'name': 'J. Giboney'}, {'authorId': '145026915', 'name': 'G. M. Grimes'}, {'authorId': '1752841', 'name': 'J. Nunamaker'}]",30.0,"{'bibtex': '@Inproceedings{Schuetzler2018TheIO,\n author = {Ryan M. Schuetzler and J. Giboney and G. M. Grimes and J. Nunamaker},\n pages = {1-10},\n title = {The Influence of Conversational Agents on Socially Desirable Responding},\n year = {2018}\n}\n'}",,{'pages': '1-10'},47.0,The Influence of Conversational Agents on Socially Desirable Responding,2018.0
909,499a32b0753128e0fcdb09e95e9789c5c6262a19,The Feeling Wheel is designed to aid people in learning to recognize and communicate about their feelings. It consists of an inner circle with 5 sectors and two outer concentric circles. The sector...,"[{'authorId': '40462593', 'name': 'G. Willcox'}]",48.0,"{'bibtex': '@Article{Willcox1982TheFW,\n author = {G. Willcox},\n journal = {Transactional Analysis Journal},\n pages = {274-276},\n title = {The Feeling Wheel A Tool for Expanding Awareness of Emotions and Increasing Spontaneity and Intimacy},\n volume = {12},\n year = {1982}\n}\n'}",,"{'volume': '12', 'pages': '274-276', 'name': 'Transactional Analysis Journal'}",1.0,The Feeling Wheel A Tool for Expanding Awareness of Emotions and Increasing Spontaneity and Intimacy,1982.0
910,49d0166c97abdeda04637c1c911921e2c71c58b8,"Due to safety and ethical issues, traditional experimental approaches to modelling underground risk behaviours can be costly, dangerous and even impossible to realize. Based on multi-agent technology, a virtual coalmine platform for risk behaviour simulation is presented to model and simulate the human-machine-environment related risk factors in underground coalmines. To reveal mine workers' risk behaviours, a fuzzy emotional behaviour model is proposed to simulate underground miners' responding behaviours to potential hazardous events based on cognitive appraisal theories and fuzzy logic techniques. The proposed emotion model can generate more believable behaviours for virtual miners according to personalized emotion states, internal motivation needs and behaviour selection thresholds. Finally, typical accident cases of underground hazard spotting and locomotive transport were implemented. The behaviour believability of virtual miners was evaluated with a user assessment method. Experimental results show that the proposed models can create more realistic and reasonable behaviours in virtual coalmine environments, which can improve miners' risk awareness and further train miners' emergent decision-making ability when facing unexpected underground situations.","[{'authorId': '2524401', 'name': 'Linqin Cai'}, {'authorId': '2109506237', 'name': 'Zhuo Yang'}, {'authorId': '98726631', 'name': 'Simon X. Yang'}, {'authorId': '2133747', 'name': 'Hongchun Qu'}]",10.0,"{'bibtex': '@Article{Cai2013ModellingAS,\n author = {Linqin Cai and Zhuo Yang and Simon X. Yang and Hongchun Qu},\n journal = {International Journal of Advanced Robotic Systems},\n title = {Modelling and Simulating of Risk Behaviours in Virtual Environments Based on Multi-Agent and Fuzzy Logic},\n volume = {10},\n year = {2013}\n}\n'}","[{'paperId': 'fe4b09375e5c3393e4ab629b32da2e3722f63689', 'title': 'A Comprehensive Model of the Relationship between Miners’ Work Commitment, Cultural Emotion and Unemployment Risk Perception'}, {'paperId': '321bdcc323ec2dc5717aad580776fbc28f82d277', 'title': 'Simulation Research on the Effectiveness of a Multiagent Mine Safety Supervision System and Its Verification'}, {'paperId': 'aafecd2d0df38bddc2c58f9ac352bd98eff5497e', 'title': 'Studying Emotions at Work Using Agent-Based Modeling and Simulation'}, {'paperId': '102d813ebde772da2c66a9509162f2891258ec5c', 'title': 'Fuzzy Logic-Based Model That Incorporates Personality Traits for Heterogeneous Pedestrians'}, {'paperId': 'c990fdc28b2f2ee744503c4888e2e4c6be33721a', 'title': 'Design and characteristic evaluation of a novel amphibious spherical robot'}, {'paperId': 'f4e716e08fc0afbbfd5940e42171883a45dc95df', 'title': 'A Novel Tracking Controller for Autonomous Underwater Vehicles with Thruster Fault Accommodation'}, {'paperId': '97764545bc2250c02a33c03cfb0717c14104b2cc', 'title': 'The Evolution of Health and Safety Training Needs of the Mining Sector in Greece and EU †'}, {'paperId': '62ffab00fe37dc3c07d0ddad81ee9314127162d9', 'title': 'Virtual Reality Applications in the Extractive Industry—A Short Review'}, {'paperId': 'fb0eba02244832fc15a3fa05420b5b74b05afb0e', 'title': 'A Review on Role of Fuzzy Logic in Psychology'}, {'paperId': 'a840527310cfa6de7eb367bc77f6b4708982430b', 'title': 'The Mystery of Job Performance : A System Dynamics Model of Human Behavior'}]","{'name': 'International Journal of Advanced Robotic Systems', 'volume': '10'}",35.0,Modelling and Simulating of Risk Behaviours in Virtual Environments Based on Multi-Agent and Fuzzy Logic,2013.0
911,49d07082a104a22bd66b3ae5133175ee2e49e82b,"* About the Author* Foreword by Tim Schafer* Preface* About the DVD*I First Impressions* What Is Covered and Why* Who Will Find Part I Most Useful* Overview of Key Concepts* Take-Aways from Part I*1 Social Surface*1.1 What Is Covered and Why*1.2 The Psychological Principles*1.3 Design Pointers*1.4 Interview: Gonzalo Frasca*1.5 Summary and What Is Next*1.6 Exercises*1.7 Further Reading*2 Practical Questions - Dominance, Friendliness, and Personality*2.1 What Is Covered and Why*2.2 The Psychological Principles*2.3 Design Pointers*2.4 Summary and What Is Next*2.5 Exercises*2.6 Further Reading*II Focus on the Player* What Is Covered and Why* Who Will Find Part II Most Useful* Overview of Key Concepts* Take-Aways from Part II*3 Culture*3.1 What Is Covered and Why*3.2 The Psychological Principles*3.3 Design Pointers*3.4 Interview: Ryoichi Hasegawa and Roppyaku Tsurumi of Sony*3.5 Interview: Lewis Johnson*3.6 Summary and What Is Next*3.7 Exercises*3.8 Further Reading*4 Gender*4.1 What Is Covered and Why*4.2 The Psychological Principles*4.3 Design Pointers*4.4 Interviews with Gamers - Personal Perspectives*4.5 Summary and What Is Next*4.6 Exercises*4.7 Further Reading*III Using a Character's Social Equipment* What Is Covered and Why* Who Will Find Part III Most Useful* Overview of Key Concepts* Take-Aways from Part III*5 The Face*5.1 What Is Covered and Why*5.2 The Psychological Principles*5.3 Design Pointers*5.4 Summary and What Is Next*5.5 Exercises*5.6 Further Reading*6 The Body*6.1 What Is Covered and Why*6.2 The Psychological Principles*6.3 Design Pointers*6.4 Interview: Chuck Clanton*6.5 Summary and What Is Next*6.6 Exercise*6.7 Further Reading*7 The Voice*7.1 What Is Covered and Why*7.2 The Psychological Principles*7.3 Design Pointers*7.4 Further Directions - Emotion Detection*7.5 Interview: MIT Media Lab's Zeynep Inanoglu and Ron Caneel*7.6 Summary and What Is Next*7.7 Exercise*7.8 Further Reading*7.9 Answers to Exercises*IV Characters in Action* What Is Covered and Why* Who Will Find Part IV Most Useful* Overview of Key Concepts* Take-Aways from Part IV*8 Player-Characters*8.1 What Is Covered and Why*8.2 The Psychological Principles*8.3 Design Pointers*8.4 Interview: Marc Laidlaw*8.5 Summary and What Is Next*8.6 Exercises*8.7 Further Reading*8.8 Acknowledgments*9 Nonplayer-Characters*9.1 What Is Covered and Why*9.2 The Psychological Principles*9.3 Dimensions of Social Roles and NPCs*9.4 Common Social Roles in Games*9.5 Design Guidelines*9.6 Summary and What Is Next*9.7 Exercises*9.8 Further Reading*V Putting It All Together* What Is Covered and Why* Who Will Find Part V Most Useful* Overview of Key Concepts* Take-Aways from Part V*10 Process*10.1 What Is Covered and Why*10.2 Arguments for Bringing a Social-Psychological Approach to Game Development*10.3 The Development Time Line*10.4 Building in the Social-Psychological Approach*10.5 Interview: Tim Schafer*10.6 Summary and What Is Next*10.7 Further Reading*11 Evaluation*11.1 What Is Covered and Why*11.2 The Psychological Principles*11.3 Current Evaluation Practice in Game Design: Market Research and Play Testing*11.4 Taking Design to the Next Level with Preproduction Evaluation*11.5 A Note on Postproduction Evaluation*11.6 Evaluation Checklist*11.7 Games Usability Perspectives*11.8 Interview: Randy Pagulayan*11.9 Interview: Nicole Lazzaro*11.10 Affective Sensing: An Evaluation Method for the Future?*11.11 Summary*11.12 Exercises*11.13 Further Reading * Appendix* Index","[{'authorId': '1740889', 'name': 'K. Isbister'}]",68.0,"{'bibtex': '@Inproceedings{Isbister2006BetterGC,\n author = {K. Isbister},\n title = {Better game characters by design : a psychological approach},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Better game characters by design : a psychological approach,2006.0
912,49f9ba2a1699e105ec651b480f91c59af73d7e8f,,"[{'authorId': '48949057', 'name': 'Starkey Duncan'}, {'authorId': '5255944', 'name': 'G. Niederehe'}]",198.0,"{'bibtex': ""@Article{Duncan1974OnST,\n author = {Starkey Duncan and G. Niederehe},\n journal = {Journal of Experimental Social Psychology},\n pages = {234-247},\n title = {On signalling that it's your turn to speak☆},\n volume = {10},\n year = {1974}\n}\n""}",,"{'volume': '10', 'pages': '234-247', 'name': 'Journal of Experimental Social Psychology'}",16.0,On signalling that it's your turn to speak☆,1974.0
913,49fc6a156e6c0c7f00ab4309f133adb99f1c3530,"Abstract Researchers interested in emotion have long struggled with the problem of how to elicit emotional responses in the laboratory. In this article, we summarise five years of work to develop a set of films that reliably elicit each of eight emotional states (amusement, anger, contentment, disgust, fear, neutral, sadness, and surprise). After evaluating over 250 films, we showed selected film clips to an ethnically diverse sample of 494 English-speaking subjects. We then chose the two best films for each of the eight target emotions based on the intensity and discreteness of subjects' responses to each film. We found that our set of 16 films successfully elicited amusement, anger, contentment. disgust, sadness, surprise, a relatively neutral state, and, to a lesser extent, fear. We compare this set of films with another set recently described by Philippot (1993), and indicate that detailed instructions for creating our set of film stimuli will be provided on request.","[{'authorId': '1775321', 'name': 'J. Gross'}, {'authorId': '2001910', 'name': 'R. Levenson'}]",2481.0,"{'bibtex': '@Article{Gross1995EmotionEU,\n author = {J. Gross and R. Levenson},\n journal = {Cognition & Emotion},\n pages = {87-108},\n title = {Emotion elicitation using films},\n volume = {9},\n year = {1995}\n}\n'}",,"{'volume': '9', 'pages': '87-108', 'name': 'Cognition & Emotion'}",78.0,Emotion elicitation using films,1995.0
914,4a3207d19b45552332b084d0dc39ce5134c6f82e,"This paper presents the possibility of using pattern recognition algorithms of infant gaze patterns at six months of age among children at high risk for an autism spectrum disorder (ASD). ASDs, which must be diagnosed by 3 years of age, are characterized by communication and interaction impairments which frequently involve disturbances of visual attention and gaze patterning. We used video cameras to record the face-to-face interactions of 32 infant subjects with their parents. The video was manually coded to determine the eye gaze pattern of infants by marking where the infant was looking in each frame (either at their parent's face or away from their parent's face). In order to identify infants ASD diagnosis at three years, we analyzed infant eye gaze patterns at six months. Variable-order Markov Models (VMM) were used to create models for typically developing comparison children as well as children with an ASD. The models correctly classified infants who did and did not develop an ASD diagnosis with an accuracy rate of 93.75 percent. Employing an assessment tool at a very young age offers the hope of early intervention, potentially mitigating the effects of the disorder throughout the rest of the child's life.","[{'authorId': '1849265', 'name': 'David Alie'}, {'authorId': '145531712', 'name': 'M. Mahoor'}, {'authorId': '144009444', 'name': 'W. Mattson'}, {'authorId': '47965400', 'name': 'Daniel R. Anderson'}, {'authorId': '1874236', 'name': 'D. Messinger'}]",19.0,"{'bibtex': '@Article{Alie2011AnalysisOE,\n author = {David Alie and M. Mahoor and W. Mattson and Daniel R. Anderson and D. Messinger},\n journal = {2011 IEEE Workshop on Applications of Computer Vision (WACV)},\n pages = {282-287},\n title = {Analysis of eye gaze pattern of infants at risk of autism spectrum disorder using Markov models},\n year = {2011}\n}\n'}",,"{'pages': '282-287', 'name': '2011 IEEE Workshop on Applications of Computer Vision (WACV)'}",16.0,Analysis of eye gaze pattern of infants at risk of autism spectrum disorder using Markov models,2011.0
915,4a393ea8f6d50cabf24e017e59a88a8f7845ccd7,,"[{'authorId': '2145238780', 'name': 'L. V. Minh'}, {'authorId': '2236335', 'name': 'C. Adam'}, {'authorId': '2281462', 'name': 'Richard Canal'}, {'authorId': '1735938', 'name': 'B. Gaudou'}, {'authorId': '2186675', 'name': 'Hô Tuòng Vinh'}, {'authorId': '1788938', 'name': 'P. Taillandier'}]",47.0,"{'bibtex': '@Inproceedings{Minh2010SimulationOT,\n author = {L. V. Minh and C. Adam and Richard Canal and B. Gaudou and Hô Tuòng Vinh and P. Taillandier},\n pages = {604-619},\n title = {Simulation of the Emotion Dynamics in a Group of Agents in an Evacuation Situation},\n year = {2010}\n}\n'}",,{'pages': '604-619'},28.0,Simulation of the Emotion Dynamics in a Group of Agents in an Evacuation Situation,2010.0
916,4a4dc23d9e6a37c443e04d928c0ef29740b6e158,,"[{'authorId': '46412398', 'name': 'K. L. Bell'}]",1121.0,"{'bibtex': '@Article{Bell2001HandbookOA,\n author = {K. L. Bell},\n journal = {Journal of Marriage and Family},\n pages = {588},\n title = {Handbook of Attachment: Theory, Research, and Clinical Applications},\n volume = {63},\n year = {2001}\n}\n'}",,"{'volume': '63', 'pages': '588', 'name': 'Journal of Marriage and Family'}",0.0,"Handbook of Attachment: Theory, Research, and Clinical Applications",2001.0
917,4a554da55fd9ff76c99e25d2ce937b225dc1100c,"This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.","[{'authorId': '40421028', 'name': 'David Nadeau'}, {'authorId': '1714612', 'name': 'S. Sekine'}]",2617.0,"{'bibtex': '@Article{Nadeau2007ASO,\n author = {David Nadeau and S. Sekine},\n journal = {Lingvisticae Investigationes},\n pages = {3-26},\n title = {A survey of named entity recognition and classification},\n volume = {30},\n year = {2007}\n}\n'}",,"{'volume': '30', 'pages': '3-26', 'name': 'Lingvisticae Investigationes'}",84.0,A survey of named entity recognition and classification,2007.0
918,4a56cb6c5cb7c60caf5e585d27a8e5f2c7a689ee,"This paper captures almost 110 years of history of underground coal mine disasters in the United States. The deadly disasters of the first ten years of the twentieth century led to the U.S. Congress founding the U.S. Bureau of Mines (USBM) in 1910. The authors examine the changing trends in mine disasters including the frequency of fatalities, causal types, the responses to those disasters and most importantly, the growing body of research on human behavior in mine emergencies. Emphasis is on the future - integrating the research on human behavior in disasters into the mining industry. This research includes the integration of the judgment decision- making process, communication, leadership in escape, expectations training, incident command center issues including fatigue, shifts and leadership, plus issues concerning the introduction of refuge chambers into U.S. mines. The authors suggest that a key factor in meeting the goal of increasing successful mine escape and rescue while decreasing fatalities and injuries lies in the field of social-psychological research and human behavior interventions.","[{'authorId': '13530702', 'name': 'M. Brnich'}, {'authorId': '1448466172', 'name': 'Kathleen M. Kowalski-Trakofker'}]",29.0,"{'bibtex': '@Inproceedings{Brnich2010UndergroundCM,\n author = {M. Brnich and Kathleen M. Kowalski-Trakofker},\n title = {Underground Coal Mine Disasters 1900 - 2010: Events, Responses, and a Look to the Future},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",19.0,"Underground Coal Mine Disasters 1900 - 2010: Events, Responses, and a Look to the Future",2010.0
921,4a5f0c5c7d1b404cbb3a717e5581d86ff29025de,"In the last few years, reinforcement learning (RL), also called adaptive (or approximate) dynamic programming, has emerged as a powerful tool for solving complex sequential decision-making problems in control theory. Although seminal research in this area was performed in the artificial intelligence (AI) community, more recently it has attracted the attention of optimization theorists because of several noteworthy success stories from operations management. It is on large-scale and complex problems of dynamic optimization, in particular the Markov decision problem (MDP) and its variants, that the power of RL becomes more obvious. It has been known for many years that on large-scale MDPs, the curse of dimensionality and the curse of modeling render classical dynamic programming (DP) ineffective. The excitement in RL stems from its direct attack on these curses, which allows it to solve problems that were considered intractable via classical DP in the past. The success of RL is due to its strong mathematical roots in the principles of DP, Monte Carlo simulation, function approximation, and AI. Topics treated in some detail in this survey are temporal differences, Q-learning, semi-MDPs, and stochastic games. Several recent advances in RL, e.g., policy gradients and hierarchical RL, are covered along with references. Pointers to numerous examples of applications are provided. This overview is aimed at uncovering the mathematical roots of this science so that readers gain a clear understanding of the core concepts and are able to use them in their own research. The survey points to more than 100 references from the literature.","[{'authorId': '2536655', 'name': 'A. Gosavi'}]",294.0,"{'bibtex': '@Article{Gosavi2009ReinforcementLA,\n author = {A. Gosavi},\n journal = {INFORMS J. Comput.},\n pages = {178-192},\n title = {Reinforcement Learning: A Tutorial Survey and Recent Advances},\n volume = {21},\n year = {2009}\n}\n'}",,"{'volume': '21', 'pages': '178-192', 'name': 'INFORMS J. Comput.'}",140.0,Reinforcement Learning: A Tutorial Survey and Recent Advances,2009.0
922,4a62a517a45897be4291dc75d825c5ab2ae7e1da,"Language-based interfaces for children hold great promise in education, therapy, and entertainment. An important subset of these interfaces includes those with a virtual agent that mediates the interaction. When participants are groups of children, the agent will need to exert a certain amount of turn-taking control to ensure that all group members participate and benefit from the experience, but must do so without being so overtly directive as to undermine the children's enjoyment of and engagement in the task. We present a hierarchy of nonverbal and verbal behaviors that a virtual agent can employ flexibly when passing the conversational turn. When used effectively, these behaviors can equalize participation, and potentially decrease the amount of overlapping speech among participants, improving automatic speech recognition in turn. We evaluated the behaviors by having children play a language-based game twice, once with a flexible host and once with an inflexible host that did not have access to the behaviors. Post-game opinion cards revealed no difference between the conditions with respect to fun or likability of the host, despite the flexible agent eliciting more evenly distributed play.","[{'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '2142730', 'name': 'J. Lehman'}]",17.0,"{'bibtex': '@Article{Andrist2013FunAF,\n author = {Sean Andrist and Iolanda Leite and J. Lehman},\n journal = {Proceedings of the 12th International Conference on Interaction Design and Children},\n title = {Fun and fair: influencing turn-taking in a multi-party game with a virtual agent},\n year = {2013}\n}\n'}",,{'name': 'Proceedings of the 12th International Conference on Interaction Design and Children'},13.0,Fun and fair: influencing turn-taking in a multi-party game with a virtual agent,2013.0
923,4a6e6ac6103002030da08b723be0cca025686d37,"Background The objective of this review was to obtain an overview of the technologies that have been explored with older adults with mild cognitive impairment and dementia (MCI/D), current knowledge on the usability and acceptability of such technologies, and how people with MCI/D and their family carers (FCs) were involved in these studies. Materials and methods Primary studies published between 2007 and 2017 that explored the use of technologies for community-dwelling people with MCI/D were identified through five databases: MEDLINE, PsycINFO, Embase, AMED, and CINAHL. Twenty-nine out of 359 papers met the criteria for eligibility. We used the Mixed Methods Appraisal Tool for quality assessment. Results A wide range of technologies was presented in the 29 studies, sorted into four domains: 1) safe walking indoors and outdoors; 2) safe living; 3) independent living; and 4) entertainment and social communication. The current state of knowledge regarding usability and acceptability reveals that even if researchers are aware of these concepts and intend to measure usability and acceptability, they seem difficult to assess. Terms such as “user friendliness” and “acceptance” were used frequently. User participation in the 29 studies was high. Persons with MCI/D, FCs, and staff/other older adults were involved in focus groups, workshops, and interviews as part of the preimplementation process. Conclusion Research regarding technologies to support people with MCI/D seems optimistic, and a wide range of technologies has been evaluated in homes with people with MCI/D and their FCs. A major finding was the importance of including people with MCI/D and their FCs in research, in order to learn about required design features to enhance usability and acceptability. Surprisingly, very few studies reported on the consequences of technology use with regard to quality of life, occupational performance, or human dignity.","[{'authorId': '8298739', 'name': 'Torhild Holthe'}, {'authorId': '4262176', 'name': 'Liv Halvorsrud'}, {'authorId': '14561772', 'name': 'D. Karterud'}, {'authorId': '2051567307', 'name': 'Kari-Anne Hoel'}, {'authorId': '144388483', 'name': 'A. Lund'}]",124.0,"{'bibtex': '@Article{Holthe2018UsabilityAA,\n author = {Torhild Holthe and Liv Halvorsrud and D. Karterud and Kari-Anne Hoel and A. Lund},\n journal = {Clinical Interventions in Aging},\n pages = {863 - 886},\n title = {Usability and acceptability of technology for community-dwelling older adults with mild cognitive impairment and dementia: a systematic literature review},\n volume = {13},\n year = {2018}\n}\n'}",,"{'volume': '13', 'pages': '863 - 886', 'name': 'Clinical Interventions in Aging'}",70.0,Usability and acceptability of technology for community-dwelling older adults with mild cognitive impairment and dementia: a systematic literature review,2018.0
924,4a76e941dbac0585cd355cd6e6af00b6164461ac,,"[{'authorId': '145514522', 'name': 'B. Anderson'}]",147.0,"{'bibtex': '@Inproceedings{Anderson2013AffectAE,\n author = {B. Anderson},\n pages = {452-464},\n title = {Affect and Emotion},\n year = {2013}\n}\n'}",,"{'volume': '', 'pages': '452-464', 'name': ''}",39.0,Affect and Emotion,2013.0
925,4a7b8fb8cb69f53c6c2ac3ae518a974773240b94,,"[{'authorId': '1717616', 'name': 'S. Eickhoff'}, {'authorId': '1715046', 'name': 'K. Stephan'}, {'authorId': '2766350', 'name': 'H. Mohlberg'}, {'authorId': '1824186', 'name': 'C. Grefkes'}, {'authorId': '38644159', 'name': 'G. Fink'}, {'authorId': '1751796', 'name': 'K. Amunts'}, {'authorId': '144897358', 'name': 'K. Zilles'}]",3847.0,"{'bibtex': '@Article{Eickhoff2005ANS,\n author = {S. Eickhoff and K. Stephan and H. Mohlberg and C. Grefkes and G. Fink and K. Amunts and K. Zilles},\n journal = {NeuroImage},\n pages = {1325-1335},\n title = {A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data},\n volume = {25},\n year = {2005}\n}\n'}",,"{'volume': '25', 'pages': '1325-1335', 'name': 'NeuroImage'}",43.0,A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data,2005.0
926,4aa9b1c9078b596c5daaf4d51e729c16163d5dac,"In social interactions, humans are expected to regulate interpersonal distance in response to the emotion displayed by others. Yet, the neural mechanisms implicated in approach‐avoidance tendencies to distinct emotional expressions have not been fully described. Here, we investigated the neural systems implicated in regulating the distance to different emotions, and how they vary as a function of empathy. Twenty‐three healthy participants assessed for psychopathic traits underwent fMRI scanning while they viewed approaching and withdrawing angry, fearful, happy, sad and neutral faces. Participants were also asked to set the distance to those faces on a computer screen, and to adjust the physical distance from the experimenter outside the scanner. Participants kept the greatest distances from angry faces, and shortest from happy expressions. This was accompanied by increased activation in the dorsomedial prefrontal and orbitofrontal cortices, inferior frontal gyrus, and temporoparietal junction for angry and happy expressions relative to the other emotions. Irrespective of emotion, longer distances were kept from approaching faces, which was associated with increased activation in the amygdala and insula, as well as parietal and prefrontal regions. Amygdala activation was positively correlated with greater preferred distances to angry, fearful and sad expressions. Moreover, participants scoring higher on coldhearted psychopathic traits (lower empathy) showed reduced amygdala activation to sad expressions. These findings elucidate the neural mechanisms underlying social approach‐avoidance, and how they are related to variations in empathy. Hum Brain Mapp 38:1492–1506, 2017. © 2016 Wiley Periodicals, Inc.","[{'authorId': '80682861', 'name': 'Joana B. Vieira'}, {'authorId': '3500182', 'name': 'T. Tavares'}, {'authorId': '22358598', 'name': 'A. Marsh'}, {'authorId': '2928107', 'name': 'D. Mitchell'}]",27.0,"{'bibtex': '@Article{Vieira2017EmotionAP,\n author = {Joana B. Vieira and T. Tavares and A. Marsh and D. Mitchell},\n journal = {Human Brain Mapping},\n title = {Emotion and personal space: Neural correlates of approach‐avoidance tendencies to different facial expressions as a function of coldhearted psychopathic traits},\n volume = {38},\n year = {2017}\n}\n'}",,"{'volume': '38', 'name': 'Human Brain Mapping'}",88.0,Emotion and personal space: Neural correlates of approach‐avoidance tendencies to different facial expressions as a function of coldhearted psychopathic traits,2017.0
928,4abedd09e2410921a4174883b89dc8d020e7f929,"In complex simulations, multi-agents systems allow to model virtual humans with an explicit cognitive process representation. However, this cognitive process is hard to model and is therefore generally simplified in an application-dependant way. In order to improve the realism of individual and collective behavior of these agents, we propose to integrate the perception of events and the computation of agents emotions in a fuzzy framework. The modeling of the perception and its effect on emotions through fuzzy rules enables the agents to consider properly the virtual environment. We show how different kinds of fuzzy rules can help in the calculus of emotions. Computation of emotions is based on the evaluation of events’ occurrence. Once the events are perceived by the agents, our method uses the desirability of these events to compute emotions relevant to crisis situations. We illustrate this model with a traffic simulation example.","[{'authorId': '31600786', 'name': 'H. Jones'}, {'authorId': '1708997', 'name': 'Julien Saunier'}, {'authorId': '1790872', 'name': 'D. Lourdeaux'}]",6.0,"{'bibtex': '@Inproceedings{Jones2011FuzzyRF,\n author = {H. Jones and Julien Saunier and D. Lourdeaux},\n pages = {657-664},\n title = {Fuzzy Rules for Events Perception and Emotions in an Agent Architecture},\n year = {2011}\n}\n'}",,{'pages': '657-664'},32.0,Fuzzy Rules for Events Perception and Emotions in an Agent Architecture,2011.0
929,4ac16699f2562446ed77b0b15cce8fc021de5df6,,"[{'authorId': '2262729', 'name': 'M. Posner'}]",2185.0,"{'bibtex': '@Inproceedings{Posner1978ChronometricEO,\n author = {M. Posner},\n title = {Chronometric explorations of mind},\n year = {1978}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Chronometric explorations of mind,1978.0
930,4ad6f75115b02de4f1bc674322ab9f1ead4e70d7,,"[{'authorId': '1561332263', 'name': 'A. Singh'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '144231505', 'name': 'Mohammed Hasanuzzaman'}, {'authorId': '144710196', 'name': 'K. Dey'}]",19.0,"{'bibtex': '@Article{Singh2021MultitaskLF,\n author = {A. Singh and S. Saha and Mohammed Hasanuzzaman and K. Dey},\n journal = {Cognitive Computation},\n pages = {212-227},\n title = {Multitask Learning for Complaint Identification and Sentiment Analysis},\n volume = {14},\n year = {2021}\n}\n'}",,"{'volume': '14', 'pages': '212-227', 'name': 'Cognitive Computation'}",58.0,Multitask Learning for Complaint Identification and Sentiment Analysis,2021.0
931,4ae06b3d9fe2a93c15d3830efa9076408d9ae365,"Objectives: This study examined the current assessment practices of clinicians working with people with social cognition impairment following traumatic brain injury. Method: Two hundred and sixty clinicians completed an on-line survey that was disseminated through professional brain injury organisations. Of respondents around 90% were allied health clinicians, with the remainder comprising medical, nursing and academia. Main outcomes: The four areas of social cognition that were routinely assessed across the disciplines were insight, disinhibition, anger and social adjustment. The least routinely assessed areas were theory of mind and alexithymia. The test suggested most likely to identify social cognition impairments was The Awareness of Social Inference Test, although only 8% of clinicians responded to this question. Clinicians preferred informal assessment methods over standardised assessment methods for identifying social cognition rehabilitation goals. Higher levels of education were associated with greater use of standardised assessment modalities. Whilst there was paucity of responses overall, TBI Express was most commonly used for social cognition rehabilitation. Conclusions: Considering the high prevalence of social cognition impairments in this population, formal assessment is extremely limited. The under-utilisation of assessment tools is problematic for the assessment and rehabilitation initiatives offered to people with TBI. These results have implications for the training of clinicians working in brain injury rehabilitation.","[{'authorId': '8060857', 'name': 'M. Kelly'}, {'authorId': '143982280', 'name': 'S. McDonald'}, {'authorId': '40044706', 'name': 'Matthew Frith'}]",24.0,"{'bibtex': '@Article{Kelly2017AssessmentAR,\n author = {M. Kelly and S. McDonald and Matthew Frith},\n journal = {Brain Impairment},\n pages = {11 - 35},\n title = {Assessment and Rehabilitation of Social Cognition Impairment after Brain Injury: Surveying Practices of Clinicians},\n volume = {18},\n year = {2017}\n}\n'}",,"{'volume': '18', 'pages': '11 - 35', 'name': 'Brain Impairment'}",131.0,Assessment and Rehabilitation of Social Cognition Impairment after Brain Injury: Surveying Practices of Clinicians,2017.0
932,4aeaaccce086158ce55393cb3778c553563110a4,"Computer simulated avatars and humanoid robots have an increasingly prominent place in today's world. Acceptance of these synthetic characters depends on their ability to properly and recognizably convey basic emotion states to a user population. This study presents an analysis of the interaction between emotional audio (human voice) and video (simple animation) cues. The emotional relevance of the channels is analyzed with respect to their effect on human perception and through the study of the extracted audio-visual features that contribute most prominently to human perception. As a result of the unequal level of expressivity across the two channels, the audio was shown to bias the perception of the evaluators. However, even in the presence of a strong audio bias, the video data were shown to affect human perception. The feature sets extracted from emotionally matched audio-visual displays contained both audio and video features while feature sets resulting from emotionally mismatched audio-visual displays contained only audio information. This result indicates that observers integrate natural audio cues and synthetic video cues only when the information expressed is in congruence. It is therefore important to properly design the presentation of audio-visual cues as incorrect design may cause observers to ignore the information conveyed in one of the channels.","[{'authorId': '2523983', 'name': 'E. Provost'}, {'authorId': '1742183', 'name': 'M. Matarić'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",62.0,"{'bibtex': '@Article{Provost2009HumanPO,\n author = {E. Provost and M. Matarić and Shrikanth S. Narayanan},\n journal = {IEEE Transactions on Multimedia},\n pages = {843-855},\n title = {Human Perception of Audio-Visual Synthetic Character Emotion Expression in the Presence of Ambiguous and Conflicting Information},\n volume = {11},\n year = {2009}\n}\n'}",,"{'volume': '11', 'pages': '843-855', 'name': 'IEEE Transactions on Multimedia'}",46.0,Human Perception of Audio-Visual Synthetic Character Emotion Expression in the Presence of Ambiguous and Conflicting Information,2009.0
933,4aede0d0759ca6c6eef53ffa357cbc326af60814,"We empirically examined the impact of virtual human animation on the emotional responses of participants in a medical virtual reality system for education in the signs and symptoms of patient deterioration. Participants were presented with one of two virtual human conditions in a between-subjects experiment, static (non-animated) and dynamic (animated). Our objective measures included the use of psycho-physical Electro Dermal Activity (EDA) sensors, and subjective measures inspired by social psychology research included the Differential Emotions Survey (DES IV) and Positive and Negative Affect Survey (PANAS). We analyzed the quantitative and qualitative measures associated with participants' emotional state at four distinct time-steps in the simulated interpersonal experience as the virtual patient's medical condition deteriorated. Results suggest that participants in the dynamic condition with animations exhibited a higher sense of co-presence and greater emotional response as compared to participants in the static condition, corresponding to the deterioration in the medical condition of the virtual patient. Negative affect of participants in the dynamic condition increased at a higher rate than for participants in the static condition. The virtual human animations elicited a stronger response in negative emotions such as anguish, fear, and anger as the virtual patient's medical condition worsened.","[{'authorId': '2134149709', 'name': 'Yanxiang Wu'}, {'authorId': '144403504', 'name': 'Sabarish V. Babu'}, {'authorId': '143923500', 'name': 'R. Armstrong'}, {'authorId': '33296377', 'name': 'Jeffrey W. Bertrand'}, {'authorId': '2116813593', 'name': 'Jun Luo'}, {'authorId': '144455263', 'name': 'Tania Roy'}, {'authorId': '1959041', 'name': 'S. Daily'}, {'authorId': '2725134', 'name': 'Lauren Cairco'}, {'authorId': '1710833', 'name': 'L. Hodges'}, {'authorId': '3043236', 'name': 'Tracy Fasolino'}]",48.0,"{'bibtex': '@Article{Wu2014EffectsOV,\n author = {Yanxiang Wu and Sabarish V. Babu and R. Armstrong and Jeffrey W. Bertrand and Jun Luo and Tania Roy and S. Daily and Lauren Cairco and L. Hodges and Tracy Fasolino},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {626-635},\n title = {Effects of Virtual Human Animation on Emotion Contagion in Simulated Inter-Personal Experiences},\n volume = {20},\n year = {2014}\n}\n'}",,"{'volume': '20', 'pages': '626-635', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",29.0,Effects of Virtual Human Animation on Emotion Contagion in Simulated Inter-Personal Experiences,2014.0
934,4afd3665d73fcd852cf9176e44d6131dcdf89868,,"[{'authorId': '1779970', 'name': 'H. Thimbleby'}]",705.0,"{'bibtex': '@Inproceedings{Thimbleby1990UserID,\n author = {H. Thimbleby},\n pages = {I-XXIII, 1-470},\n title = {User interface design},\n year = {1990}\n}\n'}",,"{'pages': 'I-XXIII, 1-470'}",12.0,User interface design,1990.0
935,4b2126d5f0584788b67514b3e509e1acb1ef92a8,"A real-time speech-driven synthetic talking face provides an effective multimodal communication interface in distributed collaboration environments. Nonverbal gestures such as facial expressions are important to human communication and should be considered by speech-driven face animation systems. In this paper, we present a framework that systematically addresses facial deformation modeling, automatic facial motion analysis, and real-time speech-driven face animation with expression using neural networks. Based on this framework, we learn a quantitative visual representation of the facial deformations, called the motion units (MUs). A facial deformation can be approximated by a linear combination of the MUs weighted by MU parameters (MUPs). We develop an MU-based facial motion tracking algorithm which is used to collect an audio-visual training database. Then, we construct a real-time audio-to-MUP mapping by training a set of neural networks using the collected audio-visual training database. The quantitative evaluation of the mapping shows the effectiveness of the proposed approach. Using the proposed method, we develop the functionality of real-time speech-driven face animation with expressions for the iFACE system. Experimental results show that the synthetic expressive talking face of the iFACE system is comparable with a real face in terms of the effectiveness of their influences on bimodal human emotion perception.","[{'authorId': '8824068', 'name': 'P. Hong'}, {'authorId': '2067930817', 'name': 'Zhen Wen'}, {'authorId': '153652752', 'name': 'Thomas S. Huang'}]",107.0,"{'bibtex': '@Article{Hong2002RealtimeSF,\n author = {P. Hong and Zhen Wen and Thomas S. Huang},\n journal = {IEEE transactions on neural networks},\n pages = {\n          916-27\n        },\n title = {Real-time speech-driven face animation with expressions using neural networks},\n volume = {13 4},\n year = {2002}\n}\n'}",,"{'volume': '13 4', 'pages': '\n          916-27\n        ', 'name': 'IEEE transactions on neural networks'}",57.0,Real-time speech-driven face animation with expressions using neural networks,2002.0
936,4b5e889135b0f52d02089be7e3251fa397b83829,,"[{'authorId': '145718274', 'name': 'L. Zadeh'}]",34895.0,"{'bibtex': '@Inproceedings{Zadeh1996FuzzyS,\n author = {L. Zadeh},\n pages = {394},\n title = {Fuzzy sets},\n year = {1996}\n}\n'}",,"{'volume': '', 'pages': '394', 'name': ''}",0.0,Fuzzy sets,1996.0
937,4b80501bc072c12e4e51cb8db0b626c4974e6ca0,"Nonconscious behavioral mimicry occurs when a person unwittingly imitates the behaviors of another person. This mimicry has been attributed to a direct link between perceiving a behavior and performing that same behavior. The current experiments explored whether having a goal to affiliate augments the tendency to mimic the behaviors of interaction partners. Experiment 1 demonstrated that having an affiliation goal increases nonconscious mimicry, and Experiment 2 further supported this proposition by demonstrating that people who have unsuccessfully attempted to affiliate in an interaction subsequently exhibit more mimicry than those who have not experienced such a failure. Results suggest that behavioral mimicry may be part of a person's repertoire of behaviors, used nonconsciously, when there is a desire to create rapport.","[{'authorId': '5622454', 'name': 'Jessica L. Lakin'}, {'authorId': '6026289', 'name': 'T. Chartrand'}]",1202.0,"{'bibtex': '@Article{Lakin2003UsingNB,\n author = {Jessica L. Lakin and T. Chartrand},\n journal = {Psychological Science},\n pages = {334 - 339},\n title = {Using Nonconscious Behavioral Mimicry to Create Affiliation and Rapport},\n volume = {14},\n year = {2003}\n}\n'}",,"{'volume': '14', 'pages': '334 - 339', 'name': 'Psychological Science'}",25.0,Using Nonconscious Behavioral Mimicry to Create Affiliation and Rapport,2003.0
938,4b89810485efdf71e0eae4174a687a1fc6871814,"When constructing a formal model of emotions for intelligent agents, two types of aspects have to be taken into account. First, qualitative aspects pertain to the conditions that elicit emotions. Second, quantitative aspects pertain to the actual experience and intensity of elicited emotions. In this presentation, we show how the qualitative aspects of a well-known psychological model of human emotions can be formalized in an agent specification language and how its quantitative aspects can be integrated into this model. Furthermore, we discuss several unspecified details and implicit assumptions in the psychological model that are explicated by this effort.","[{'authorId': '1714059', 'name': 'Bas R. Steunebrink'}, {'authorId': '1707738', 'name': 'M. Dastani'}, {'authorId': '1691228', 'name': 'J. Meyer'}]",57.0,"{'bibtex': '@Inproceedings{Steunebrink2008AFM,\n author = {Bas R. Steunebrink and M. Dastani and J. Meyer},\n pages = {256-260},\n title = {A Formal Model of Emotions: Integrating Qualitative and Quantitative Aspects},\n year = {2008}\n}\n'}",,{'pages': '256-260'},7.0,A Formal Model of Emotions: Integrating Qualitative and Quantitative Aspects,2008.0
939,4b90006f2ba4c0e1a9be1a6a6eb19f921a0748ba,"Building intelligent agents that can believably interact with humans is a difficult yet important task in a host of applications, including therapy, education, and entertainment. We submit that in order to enhance believability, the agent's affective state should be accurately modeled and should realistically influence the agent's behavior. We propose a computational model of affect which incorporates an empirically-based interplay between its various affective components - personality, motivation, emotion, and mood. Further, our model captures a number of salient mechanisms that are observable in humans and that influence the agent's behavior. We are therefore hopeful that our model will facilitate more engaging and meaningful human-agent interactions. We evaluate our model and illustrate its efficacy, as well as the importance of the different components in the model and their interplay.","[{'authorId': '40959392', 'name': 'Maayan Shvo'}, {'authorId': '51147349', 'name': 'Jakob Buhmann'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",17.0,"{'bibtex': '@Article{Shvo2019AnIM,\n author = {Maayan Shvo and Jakob Buhmann and Mubbasir Kapadia},\n booktitle = {International Conference on Intelligent Virtual Agents},\n journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},\n title = {An Interdependent Model of Personality, Motivation, Emotion, and Mood for Intelligent Virtual Agents},\n year = {2019}\n}\n'}","[{'paperId': '1e61bc7abe5fd33102c5bc4e21ab8a1627cbcdfc', 'title': 'The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents'}, {'paperId': '2fdb2aaaeb0e11a36715c8cc4ee41f90478aeb95', 'title': 'Towards the construction of computational models of emotions from the perspective of a software system'}, {'paperId': '870c74ebca01388697fdcaaf2c7725e8d21326d1', 'title': 'Multimodal Embodied Conversational Agents: A discussion of architectures, frameworks and modules for commercial applications'}, {'paperId': '14624f972b1c14ecec3a3b1cebae8147943c0fa1', 'title': 'Comprehensive guidelines for emotion annotation'}, {'paperId': 'db68d4fdfcca8142c0b9a92ccd0c9e78ba306d95', 'title': 'Cognitive motivations and foundations for building intelligent decision-making systems'}, {'paperId': '512df5eb736411c4941ab54f2f987f6993a0159a', 'title': 'Investigating how speech and animation realism influence the perceived personality of virtual characters and agents'}, {'paperId': 'd6d242b7a561c7729ee4b9ce144aa82e35ed1cd9', 'title': 'What Lies Beneath—A Survey of Affective Theory Use in Computational Models of Emotion'}, {'paperId': 'c12ccb4701dcbbe6046970a129bd6db55eec2364', 'title': 'Expressing Personality Through Non-verbal Behaviour in Real-Time Interaction'}, {'paperId': '88c5d878bb1cc2be6329023f12d207a29647f2d4', 'title': 'Reactive Virtual Agents: A Viewpoint-Driven Approach for Bodily Nonverbal Communication'}, {'paperId': 'fed2ae5acc2fd9d0795621de3438ee1f08365220', 'title': 'A Conversational Agent Framework with Multi-modal Personality Expression'}, {'paperId': '7c4a4b065cf18f7b13e6648ef12c42fd8bb79735', 'title': 'Dynamic Emotional Language Adaptation in Multiparty Interactions with Agents'}, {'paperId': '788744fd40fcef45949a78be7f2925db484295cb', 'title': 'An Autonomous Emotional Virtual Character: An Approach with Deep and Goal-Parameterized Reinforcement Learning'}, {'paperId': '40484e1b49f62a6788ae80fd2ecc9dfe45e8875b', 'title': 'A Lasting Impact: Using Second-Order Dynamics to Customize the Continuous Emotional Expression of a Social Robot'}, {'paperId': 'fd4bba73ab9737035d296e690ac7905bcf8fe9d3', 'title': 'COPALZ: A Computational Model of Pathological Appraisal Biases for an Interactive Virtual Alzheimer Patient'}, {'paperId': 'fc632427b9a69bad70f4a81df720d7df058e0a9a', 'title': 'Personality and Emotion in Strong-Story Narrative Planning'}, {'paperId': '6c6ab5f852ca5db5a17a69cd4c63ff9db601ebf2', 'title': 'Human Digital Twin in Industry 4.0: Concept and Preliminary Model'}, {'paperId': '05a2a43edfff98334bdceeff94b26a296be9f681', 'title': 'Camelot: A Modular Customizable Sandbox for Visualizing Interactive Narratives'}]",{'name': 'Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents'},37.0,"An Interdependent Model of Personality, Motivation, Emotion, and Mood for Intelligent Virtual Agents",2019.0
942,4bec2b45dab7a48e8b30de09fc417fca5f9637ba,"Research on the use of software programs and tools such as pedagogical agents has peaked over the last decade. Pedagogical agents are on-screen characters that facilitate instruction. This meta-analysis examined the effect of using pedagogical agents on learning by reviewing 43 studies involving 3,088 participants. Analysis of the results indicated that pedagogical agents produced a small but significant effect on learning. The overall mean effect size was moderated by the contextual and methodological features of the studies. The findings revealed that the use of pedagogical agents were more beneficial for K-12 students than post-secondary students. Pedagogical agents that communicated with students using on-screen text facilitated learning more effectively than agents that communicated using narration. The findings of this study have implications for advancing theory and practice, as well as highlighting productive future directions for research.","[{'authorId': '2854999', 'name': 'Noah L. Schroeder'}, {'authorId': '2819909', 'name': 'Olusola O. Adesope'}, {'authorId': '1399335114', 'name': 'R. Gilbert'}]",189.0,"{'bibtex': '@Article{Schroeder2013HowEA,\n author = {Noah L. Schroeder and Olusola O. Adesope and R. Gilbert},\n journal = {Journal of Educational Computing Research},\n pages = {1 - 39},\n title = {How Effective are Pedagogical Agents for Learning? A Meta-Analytic Review},\n volume = {49},\n year = {2013}\n}\n'}",,"{'volume': '49', 'pages': '1 - 39', 'name': 'Journal of Educational Computing Research'}",80.0,How Effective are Pedagogical Agents for Learning? A Meta-Analytic Review,2013.0
943,4bf061f85d1495448f83b5e3bb6ec7e30bfe818d,"Animated pedagogical agents that inhabit interactive learning environments can exhibit strikingly lifelike behaviors. In addition to providing problem-solving advice in response to students’ activities in the learning environment, these agents may also be able to play a powerful motivational role. To design the most effective agent-based learning environment software, it is essential to understand how students perceive an animated pedagogical agent with regard to affective dimensions such as encouragement, utility, credibility, and clarity. This paper describes a study of the affective impact of animated pedagogical agents on students’ learning experiences. One hundred middle school students interacted with animated pedagogical agents to assess their perception of agents’ affective characteristics. The study revealed the persona eflecr, which is that the presence of a lifelike character in an interactive learning environment~ven one that is not expressive— can have a strong positive effect on student’s perception of their learning experience. The study also demonstrates the interesting effect of multiple types of explanatory behaviors on both affective perception and learning performance.","[{'authorId': '2249310517', 'name': 'James C. Lester'}, {'authorId': '3047047', 'name': 'S. Converse'}, {'authorId': '2249314519', 'name': 'Susan H. Kahler'}, {'authorId': '2249325955', 'name': 'S. T. Barlow'}, {'authorId': '143980642', 'name': 'Brian A. Stone'}, {'authorId': '2687019', 'name': 'Ravinder S. Bhogal'}]",956.0,"{'bibtex': '@Article{Lester1997ThePE,\n author = {James C. Lester and S. Converse and Susan H. Kahler and S. T. Barlow and Brian A. Stone and Ravinder S. Bhogal},\n journal = {Proceedings of the ACM SIGCHI Conference on Human factors in computing systems},\n title = {The persona effect: affective impact of animated pedagogical agents},\n year = {1997}\n}\n'}",,{'name': 'Proceedings of the ACM SIGCHI Conference on Human factors in computing systems'},23.0,The persona effect: affective impact of animated pedagogical agents,1997.0
946,4c0c696d2dc6a12f148fb8a5d1f94c3ab0a97207,"Abstract : We describe a new architecture to integrate a psychological model into a crowd simulation system in order to obtain believable emergent behaviors. Our existing crowd simulation system (MACES) performs high level wayfinding to explore unknown environments and obtain a cognitive map for navigation purposes, in addition to dealing with low level motion within each room based on social forces. Communication and roles are added to achieve individualistic behaviors and a realistic way to spread information about the environment. To expand the range of realistic human behaviors, we use a system (PMFserv) that implements human behavior models from a range of ability, stress, emotion, decision theoretic and motivation sources. An architecture is proposed that combines and integrates MACES and PMFserv to add validated agent behaviors to crowd simulations.","[{'authorId': '1746484', 'name': 'N. Pelechano'}, {'authorId': '2059261082', 'name': ""Kevin O'Brien""}, {'authorId': '1714492', 'name': 'B. Silverman'}, {'authorId': '1699200', 'name': 'N. Badler'}]",241.0,"{'bibtex': ""@Inproceedings{Pelechano2005CrowdSI,\n author = {N. Pelechano and Kevin O'Brien and B. Silverman and N. Badler},\n title = {Crowd simulation incorporating agent psychological models, roles and communication},\n year = {2005}\n}\n""}",,"{'volume': '', 'name': ''}",35.0,"Crowd simulation incorporating agent psychological models, roles and communication",2005.0
947,4c1a0720b0f85eba213bf5b2a29232daed434280,"The limits of human multitasking capabilities in intense conditions are well understood. However, little is known about how increasing and continuous multitasking impacts innovation in virtual teams. During this investigation, we developed a construct called virtual distance to understand how both perceived and physical distance impacts innovation on virtual teams and applied it to 223 individuals and mangers at seventeen organizations. We then explored virtual distance as a moderator of the relationship between multitasking and innovation. Our results showed that virtual distance has a significant and negative relationship to innovation. We farther found that virtual distance significantly moderated the relationship between multitasking and innovation. When virtual distance is low, there is a positive relationship and when virtual distance is high, there is a significant curvilinear relationship. The results have implications for the selection of virtual team members and virtual project management activities when critical project outcomes include innovation","[{'authorId': '2131535', 'name': 'Karen Sobel Lojeski'}, {'authorId': '1933702', 'name': 'R. Reilly'}, {'authorId': '2180615', 'name': 'P. Dominick'}]",20.0,"{'bibtex': ""@Article{Lojeski2007MultitaskingAI,\n author = {Karen Sobel Lojeski and R. Reilly and P. Dominick},\n journal = {2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)},\n pages = {44-44},\n title = {Multitasking and Innovation in Virtual Teams},\n year = {2007}\n}\n""}",,"{'pages': '44-44', 'name': ""2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)""}",57.0,Multitasking and Innovation in Virtual Teams,2007.0
948,4c1f0185446e631f760a6a435a98c915554f9dcb,,"[{'authorId': '21451088', 'name': 'P. Ekman'}]",4420.0,"{'bibtex': '@Inproceedings{Ekman1976PicturesOF,\n author = {P. Ekman},\n title = {Pictures of Facial Affect},\n year = {1976}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Pictures of Facial Affect,1976.0
949,4c2ad60b8aa29f4aa7d16f5953fb57b8bf816b8e,"Emotions are discrete, automatic responses to universally shared, culture-specific and individual-specific events. The emotion terms, such as anger, fear, etcetera, denote a family of related states sharing at least 12 characteristics, which distinguish one emotion family from another, as well as from other affective states. These affective responses are preprogrammed and involuntary, but are also shaped by life experiences.","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '4548664', 'name': 'Daniel T. Cordaro'}]",902.0,"{'bibtex': '@Article{Ekman2011WhatIM,\n author = {P. Ekman and Daniel T. Cordaro},\n journal = {Emotion Review},\n pages = {364 - 370},\n title = {What is Meant by Calling Emotions Basic},\n volume = {3},\n year = {2011}\n}\n'}",,"{'volume': '3', 'pages': '364 - 370', 'name': 'Emotion Review'}",17.0,What is Meant by Calling Emotions Basic,2011.0
950,4c35ee9ae51c4cf00dc3183d462e43b6f0685f86,"Previous research demonstrated social influence resulting from mimicry (the chameleon effect); a confederate who mimicked participants was more highly regarded than a confederate who did not, despite the fact that participants did not explicitly notice the mimicry. In the current study, participants interacted with an embodied artificial intelligence agent in immersive virtual reality. The agent either mimicked a participant's head movements at a 4-s delay or utilized prerecorded movements of another participant as it verbally presented an argument. Mimicking agents were more persuasive and received more positive trait ratings than nonmimickers, despite participants' inability to explicitly detect the mimicry. These data are uniquely powerful because they demonstrate the ability to use automatic, indiscriminate mimicking (i.e., a computer algorithm blindly applied to all movements) to gain social influence. Furthermore, this is the first study to demonstrate social influence effects with a nonhuman, nonverbal mimicker.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '38811484', 'name': 'N. Yee'}]",327.0,"{'bibtex': '@Article{Bailenson2005DigitalC,\n author = {J. Bailenson and N. Yee},\n journal = {Psychological Science},\n pages = {814 - 819},\n title = {Digital Chameleons},\n volume = {16},\n year = {2005}\n}\n'}",,"{'volume': '16', 'pages': '814 - 819', 'name': 'Psychological Science'}",29.0,Digital Chameleons,2005.0
951,4c3ca19de3a47f4fec7428c437fca9a198e453ab,,"[{'authorId': '145500740', 'name': 'Juan Garzón'}, {'authorId': '145879358', 'name': 'J. Pavón'}, {'authorId': '1728461', 'name': 'S. Baldiris'}]",27.0,"{'bibtex': '@Inproceedings{Garzón2017AugmentedRA,\n author = {Juan Garzón and J. Pavón and S. Baldiris},\n pages = {402-414},\n title = {Augmented Reality Applications for Education: Five Directions for Future Research},\n year = {2017}\n}\n'}",,{'pages': '402-414'},23.0,Augmented Reality Applications for Education: Five Directions for Future Research,2017.0
952,4c52ba78db5f29132d7f77db4f6c3d4f4ff0d276,,"[{'authorId': '4528719', 'name': 'Kristopher J Preacher'}, {'authorId': '23662196', 'name': 'A. Hayes'}]",26769.0,"{'bibtex': '@Article{Preacher2008AsymptoticAR,\n author = {Kristopher J Preacher and A. Hayes},\n journal = {Behavior Research Methods},\n pages = {879-891},\n title = {Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models},\n volume = {40},\n year = {2008}\n}\n'}",,"{'volume': '40', 'pages': '879-891', 'name': 'Behavior Research Methods'}",80.0,Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models,2008.0
953,4c592c049c552e60c965558782c9847427d11eb6,"This paper introduces a formal BDI-based agent model for Theory of Mind. The model uses BDI-concepts to describe the reasoning process of an agent that reasons about the reasoning process of another agent, which is also based on BDI-concepts. A case study illustrates how the model can be used for social manipulation. This case study addresses the scenario of a manager that reasons about the task avoiding behaviour of his employee. For this scenario, a number of simulation experiments have been performed, and some of their results are discussed.","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '2468373', 'name': 'Z. A. Memon'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",59.0,"{'bibtex': '@Inproceedings{Bosse2007ATB,\n author = {T. Bosse and Z. A. Memon and Jan Treur},\n pages = {335-342},\n title = {A Two-Level BDI-Agent Model for Theory of Mind and its Use in Social Manipulation},\n year = {2007}\n}\n'}",,"{'volume': '', 'pages': '335-342', 'name': ''}",18.0,A Two-Level BDI-Agent Model for Theory of Mind and its Use in Social Manipulation,2007.0
954,4c62f32673410a1ade44aae78bd28c3ebdd1d590,"Previous longitudinal studies of personality in adulthood have been limited in the range of traits examined, have chiefly made use of self-reports, and have frequently included only men. In this study, self-reports (N = 983) and spouse ratings (N = 167) were gathered on the NEO Personality Inventory (Costa & McCrae, 1985b), which measures all five of the major dimensions of normal personality. Cross-sectional and longitudinal analyses on data from men and women aged 21 to 96 years showed evidence of small declines in Activity, Positive Emotions, and openness to Actions that might be attributed to maturation, but none of these effects was replicated in sequential analyses. The 20 other scales examined showed no consistent pattern of maturational effects. In contrast, retest stability was quite high for all five dimensions in self-reports and for the three dimensions measured at both times in spouse ratings. Comparable levels of stability were seen for men and women and for younger and older subjects. The data support the position that personality is stable after age 30.","[{'authorId': '2281038', 'name': 'P. Costa'}, {'authorId': '6206591', 'name': 'R. McCrae'}]",1710.0,"{'bibtex': '@Article{Costa1988PersonalityIA,\n author = {P. Costa and R. McCrae},\n journal = {Journal of personality and social psychology},\n pages = {\n          853-63\n        },\n title = {Personality in adulthood: a six-year longitudinal study of self-reports and spouse ratings on the NEO Personality Inventory.},\n volume = {54 5},\n year = {1988}\n}\n'}",,"{'volume': '54 5', 'pages': '\n          853-63\n        ', 'name': 'Journal of personality and social psychology'}",36.0,Personality in adulthood: a six-year longitudinal study of self-reports and spouse ratings on the NEO Personality Inventory.,1988.0
955,4c790c71219f6be248a3d426347bf7c4e3a0a6c4,"Objectives: To develop a 10‐minute cognitive screening tool (Montreal Cognitive Assessment, MoCA) to assist first‐line physicians in detection of mild cognitive impairment (MCI), a clinical state that often progresses to dementia.","[{'authorId': '4789895', 'name': 'Z. Nasreddine'}, {'authorId': '2227151', 'name': 'N. Phillips'}, {'authorId': '49488473', 'name': 'Valérie Bédirian'}, {'authorId': '46626405', 'name': 'S. Charbonneau'}, {'authorId': '51356427', 'name': 'V. Whitehead'}, {'authorId': '2073409937', 'name': 'Isabelle Collin'}, {'authorId': '2084085', 'name': 'J. Cummings'}, {'authorId': '114187679', 'name': 'H. Chertkow'}]",16369.0,"{'bibtex': '@Article{Nasreddine2005TheMC,\n author = {Z. Nasreddine and N. Phillips and Valérie Bédirian and S. Charbonneau and V. Whitehead and Isabelle Collin and J. Cummings and H. Chertkow},\n journal = {Journal of the American Geriatrics Society},\n title = {The Montreal Cognitive Assessment, MoCA: A Brief Screening Tool For Mild Cognitive Impairment},\n volume = {53},\n year = {2005}\n}\n'}",,"{'volume': '53', 'name': 'Journal of the American Geriatrics Society'}",23.0,"The Montreal Cognitive Assessment, MoCA: A Brief Screening Tool For Mild Cognitive Impairment",2005.0
956,4c915c1eecb217c123a36dc6d3ce52d12c742614,,"[{'authorId': '2116648700', 'name': 'Ronald J. Williams'}]",7550.0,"{'bibtex': '@Article{Williams1992SimpleSG,\n author = {Ronald J. Williams},\n journal = {Machine Learning},\n pages = {229-256},\n title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},\n volume = {8},\n year = {1992}\n}\n'}",,"{'volume': '8', 'pages': '229-256', 'name': 'Machine Learning'}",37.0,Simple statistical gradient-following algorithms for connectionist reinforcement learning,1992.0
957,4c940c7726ddd513e95d808442a77396d71e9f07,,"[{'authorId': '1885803', 'name': 'S. Fiske'}, {'authorId': '3513501', 'name': 'Amy J. C. Cuddy'}, {'authorId': '48151160', 'name': 'P. Glick'}]",3246.0,"{'bibtex': '@Article{Fiske2007UniversalDO,\n author = {S. Fiske and Amy J. C. Cuddy and P. Glick},\n journal = {Trends in Cognitive Sciences},\n pages = {77-83},\n title = {Universal dimensions of social cognition: warmth and competence},\n volume = {11},\n year = {2007}\n}\n'}",,"{'volume': '11', 'pages': '77-83', 'name': 'Trends in Cognitive Sciences'}",69.0,Universal dimensions of social cognition: warmth and competence,2007.0
958,4c97102fe8d31dc506af642ae0902251b1406a59,"The paper presents the TUCSON coordination model for Internet applications based on network-aware (possibly mobile) agents. The model is based on the notion of tuple centre, an enhanced tuple space whose behaviour can be extended according to the application needs. Everv node of a TUCSON environment provides its local communication space, made up of a multiplicity of independently-programmable tuple centres. This makes it possible to embed global system properties into the space of components’ interaction, thus enabling flexible cooperation over space and time between agents, and permitting to easily face many issues critical to Internet applications, such as heterogeneity and dynamicity of the execution environments.","[{'authorId': '3119182', 'name': 'Andrea Omicini'}, {'authorId': '1684412', 'name': 'F. Zambonelli'}]",78.0,"{'bibtex': '@Inproceedings{Omicini1999TupleCF,\n author = {Andrea Omicini and F. Zambonelli},\n pages = {183-190},\n title = {Tuple centres for the coordination of Internet agents},\n year = {1999}\n}\n'}",,{'pages': '183-190'},36.0,Tuple centres for the coordination of Internet agents,1999.0
959,4ca84d9ab54ac8efabc6c52e320292852a16adee,"In the first of two studies, 52 subjects were required to judge the appropriateness of IS behaviors in each of IS situations in a behavior-situation matrix. Differences among behaviors, situations, and their interaction contributed substantial proportions of the total variance in judgments. The concepts of behavioral appropriateness and situational constraint were offered to account for the differences obtained among behaviors and situations, respectively. A second study using a new sample of 42 subjects and different methods of measurement provided initial construct validity evidence for the concepts. Implications of these results for the construction of situational response hierarchies, the development of behavior and situation taxonomies, and causal attribution were discussed.","[{'authorId': '143809316', 'name': 'R. Price'}, {'authorId': '113907614', 'name': 'D. L. Bouffard'}]",208.0,"{'bibtex': '@Article{Price1974BehavioralAA,\n author = {R. Price and D. L. Bouffard},\n journal = {Journal of Personality and Social Psychology},\n pages = {579-586},\n title = {Behavioral appropriateness and situational constraint as dimensions of social behavior.},\n volume = {30},\n year = {1974}\n}\n'}",,"{'volume': '30', 'pages': '579-586', 'name': 'Journal of Personality and Social Psychology'}",31.0,Behavioral appropriateness and situational constraint as dimensions of social behavior.,1974.0
960,4ca85470edf8a5499921ac35209887e013b64a9b,,"[{'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '40645851', 'name': 'Junichiro Mori'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}]",172.0,"{'bibtex': '@Article{Prendinger2005UsingHP,\n author = {H. Prendinger and Junichiro Mori and M. Ishizuka},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {231-245},\n title = {Using human physiology to evaluate subtle expressivity of a virtual quizmaster in a mathematical game},\n volume = {62},\n year = {2005}\n}\n'}",,"{'volume': '62', 'pages': '231-245', 'name': 'Int. J. Hum. Comput. Stud.'}",39.0,Using human physiology to evaluate subtle expressivity of a virtual quizmaster in a mathematical game,2005.0
961,4cb462b2d7a161908556bf09eadebebf9aae8122,,"[{'authorId': '2068695177', 'name': 'Christian Becker'}, {'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",71.0,"{'bibtex': '@Inproceedings{Becker2005EvaluatingAF,\n author = {Christian Becker and H. Prendinger and M. Ishizuka and I. Wachsmuth},\n pages = {466-473},\n title = {Evaluating Affective Feedback of the 3D Agent Max in a Competitive Cards Game},\n year = {2005}\n}\n'}",,{'pages': '466-473'},19.0,Evaluating Affective Feedback of the 3D Agent Max in a Competitive Cards Game,2005.0
963,4cc7d53edbf76313a65f32b4cb0347d528868d98,,"[{'authorId': '3501849', 'name': 'C. Mazefsky'}, {'authorId': '2197231', 'name': 'S. White'}]",926.0,"{'bibtex': '@Misc{None,\n author = {C. Mazefsky and S. White},\n title = {Emotion Regulation}\n}\n'}",,,59.0,Emotion Regulation,
965,4cc8c4b33d4af568b2fbe5fd42766b9b4b1d1f18,,"[{'authorId': '3292250', 'name': 'M. Curumsing'}]",14.0,"{'bibtex': '@Inproceedings{Curumsing2017EmotionorientedRE,\n author = {M. Curumsing},\n title = {Emotion-oriented requirements engineering},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotion-oriented requirements engineering,2017.0
966,4cfd967688cb5efe7489ef9ecff51630d55db528,,"[{'authorId': '1802126', 'name': 'I. Poggi'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1807752', 'name': 'F. D. Rosis'}, {'authorId': '1694255', 'name': 'V. Carofiglio'}, {'authorId': '1739256', 'name': 'B. D. Carolis'}]",150.0,"{'bibtex': '@Inproceedings{Poggi2005GRETAAB,\n author = {I. Poggi and C. Pelachaud and F. D. Rosis and V. Carofiglio and B. D. Carolis},\n pages = {3-25},\n title = {GRETA. A BELIEVABLE EMBODIED CONVERSATIONAL AGENT},\n volume = {27},\n year = {2005}\n}\n'}",,"{'volume': '27', 'pages': '3-25', 'name': ''}",53.0,GRETA. A BELIEVABLE EMBODIED CONVERSATIONAL AGENT,2005.0
969,4d193b61afb62301652f8fb41fafaa2a85898062,,"[{'authorId': '2061081005', 'name': 'Jaime C. Acosta'}, {'authorId': '32987878', 'name': 'Nigel G. Ward'}]",98.0,"{'bibtex': '@Article{Acosta2011AchievingRW,\n author = {Jaime C. Acosta and Nigel G. Ward},\n journal = {Speech Commun.},\n pages = {1137-1148},\n title = {Achieving rapport with turn-by-turn, user-responsive emotional coloring},\n volume = {53},\n year = {2011}\n}\n'}",,"{'volume': '53', 'pages': '1137-1148', 'name': 'Speech Commun.'}",53.0,"Achieving rapport with turn-by-turn, user-responsive emotional coloring",2011.0
970,4d1c5ed6d0255f3f6e520f4eef786ce578685bc9,,"[{'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}]",138.0,"{'bibtex': '@Article{Cassell1999FullyEC,\n author = {Justine Cassell and H. Vilhjálmsson},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {45-64},\n title = {Fully Embodied Conversational Avatars: Making Communicative Behaviors Autonomous},\n volume = {2},\n year = {1999}\n}\n'}",,"{'volume': '2', 'pages': '45-64', 'name': 'Autonomous Agents and Multi-Agent Systems'}",30.0,Fully Embodied Conversational Avatars: Making Communicative Behaviors Autonomous,1999.0
971,4d3bca2565bba38799c41bf8cdf995a072123e6e,"During social interactions, people's eyes convey a wealth of information about their direction of attention and their emotional and mental states. This review aims to provide a comprehensive overview of past and current research into the perception of gaze behavior and its effect on the observer. This encompasses the perception of gaze direction and its influence on perception of the other person, as well as gaze-following behavior such as joint attention, in infant, adult, and clinical populations. Particular focus is given to the gaze-cueing paradigm that has been used to investigate the mechanisms of joint attention. The contribution of this paradigm has been significant and will likely continue to advance knowledge across diverse fields within psychology and neuroscience.","[{'authorId': '47039010', 'name': 'A. Frischen'}, {'authorId': '40471972', 'name': 'A. Bayliss'}, {'authorId': '6674999', 'name': 'S. Tipper'}]",1211.0,"{'bibtex': '@Article{Frischen2007GazeCO,\n author = {A. Frischen and A. Bayliss and S. Tipper},\n journal = {Psychological bulletin},\n pages = {\n          694-724\n        },\n title = {Gaze cueing of attention: visual attention, social cognition, and individual differences.},\n volume = {133 4},\n year = {2007}\n}\n'}",,"{'volume': '133 4', 'pages': '\n          694-724\n        ', 'name': 'Psychological bulletin'}",367.0,"Gaze cueing of attention: visual attention, social cognition, and individual differences.",2007.0
972,4d49dc7078e9a69eddfbd9f81e13fbc97db3827f,"When interacting and communicating with virtual agents in immersive environments, the agents’ behavior should be believable and authentic. Thereby, one important aspect is a convincing auralization of their speech. In this work-in-progress paper a study design to evaluate the effect of adding directivity to speech sound source on the perceived social presence of a virtual agent is presented. Therefore, we describe the study design and discuss first results of a prestudy as well as consequential improvements of the design.","[{'authorId': '39812907', 'name': 'J. Wendt'}, {'authorId': '2824434', 'name': 'Tom Vierjahn'}, {'authorId': '144483066', 'name': 'T. Kuhlen'}, {'authorId': '1753088713', 'name': 'Andrea Bonsch'}, {'authorId': '52111617', 'name': 'J. Stienen'}, {'authorId': '2638784', 'name': 'B. Weyers'}, {'authorId': '2979397', 'name': 'M. Vorländer'}]",6.0,"{'bibtex': ""@Inproceedings{Wendt2018DoesTD,\n author = {J. Wendt and Tom Vierjahn and T. Kuhlen and Andrea Bonsch and J. Stienen and B. Weyers and M. Vorländer},\n title = {Does the Directivity of a Virtual Agent's Speech Influence the Perceived Social Presence?},\n year = {2018}\n}\n""}",,"{'volume': '', 'name': ''}",11.0,Does the Directivity of a Virtual Agent's Speech Influence the Perceived Social Presence?,2018.0
973,4d5111410d212e004395ab7cc27ac4d3a0046c1d,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '2790338', 'name': 'R. Duell'}, {'authorId': '2468373', 'name': 'Z. A. Memon'}, {'authorId': '1726343', 'name': 'Jan Treur'}, {'authorId': '1881843', 'name': 'C. N. V. D. Wal'}]",95.0,"{'bibtex': '@Article{Bosse2015AgentBasedMO,\n author = {T. Bosse and R. Duell and Z. A. Memon and Jan Treur and C. N. V. D. Wal},\n journal = {Cognitive Computation},\n pages = {111-136},\n title = {Agent-Based Modeling of Emotion Contagion in Groups},\n volume = {7},\n year = {2015}\n}\n'}",,"{'volume': '7', 'pages': '111-136', 'name': 'Cognitive Computation'}",41.0,Agent-Based Modeling of Emotion Contagion in Groups,2015.0
974,4d934d15b5636263946ec48db3653201c18044a5,"A growing body of evidence suggests emotion boosts memory accuracy to an extent but affects the subjective sense of recollection even more. The result is vivid memories for emotional events that are held with confidence but that may be surprisingly inaccurate in their details. We examine the neural circuitry underlying emotion's impact on memory and the subjective sense of recollection to provide insight into this puzzling phenomenon. This research suggests that for emotional stimuli the quality and strength of memory for a few details may mediate judgments of recollection, whereas for neutral stimuli the quantity of contextual details may be more important. Finally, we speculate that the enhanced subjective sense of recollection with emotion, in the absence of absolute veridicality, may have evolved to enable fast and unambiguous decision making in emotional situations.","[{'authorId': '2471431', 'name': 'E. Phelps'}, {'authorId': '5586879', 'name': 'T. Sharot'}]",200.0,"{'bibtex': '@Article{Phelps2008HowW,\n author = {E. Phelps and T. Sharot},\n journal = {Current Directions in Psychological Science},\n pages = {147 - 152},\n title = {How (and Why) Emotion Enhances the Subjective Sense of Recollection},\n volume = {17},\n year = {2008}\n}\n'}",,"{'volume': '17', 'pages': '147 - 152', 'name': 'Current Directions in Psychological Science'}",21.0,How (and Why) Emotion Enhances the Subjective Sense of Recollection,2008.0
975,4d9a20882eaecdc8189050e93d3a60ecfb2c6854,"In this article, we describe an Affective Knowledge Representation (AKR) scheme to represent emotion schemata to be used in the design a variety of socially intelligent artificial agents. Our approach in this article and in the applications of our AKR scheme, focusses on the notion of “social expertise” of socially intelligent agents in terms of their 1) external behavior and 2) internal motivational goal-based abilities. AKR is derived from combining multiple emotion theories in a useful hierarchical model of affective phenomena and includes a taxonomy of affect, mood, emotion, and personality, as well as a framework for emotional state dynamics. Our model is being applied to design and implement two systems: 1) EBA, an Emotion-Based Architecture for two autonomous robots, 2) MAUI, a Multimodal Affective User Interface agent.","[{'authorId': '143607713', 'name': 'Christine L. Lisetti'}]",41.0,"{'bibtex': '@Inproceedings{Lisetti2002PersonalityAA,\n author = {Christine L. Lisetti},\n pages = {397-401},\n title = {Personality, Affect and Emotion Taxonomy for Socially Intelligent Agents},\n year = {2002}\n}\n'}",,{'pages': '397-401'},15.0,"Personality, Affect and Emotion Taxonomy for Socially Intelligent Agents",2002.0
976,4db1ab1b3b826c926ab617c32ab3d4cfc6f908e0,"This article reports on an exploratory study of the relationship between grounding and problem solving in multimodal computer-mediated collaboration. This article examines two different media, a shared whiteboard and a MOO environment that includes a text chat facility. A study was done on how the acknowledgment rate (how often partners give feedback of having perceived, understood, and accepted partner's contributions) varies according to the media and the content of interactions. It was expected that the whiteboard would serve to draw schemata that disambiguate chat utterances. Instead, results show that the whiteboard is primarily used to represent the state of problem solving and the chat is used for grounding information created on the whiteboard. These results are interpreted in terms of persistence: More persistent information is exchanged through the more persistent medium. The whiteboard was used as a shared memory rather than a grounding tool.","[{'authorId': '1799133', 'name': 'P. Dillenbourg'}, {'authorId': '144518646', 'name': 'D. Traum'}]",252.0,"{'bibtex': '@Article{Dillenbourg2006SharingSP,\n author = {P. Dillenbourg and D. Traum},\n journal = {Journal of the Learning Sciences},\n pages = {121 - 151},\n title = {Sharing Solutions: Persistence and Grounding in Multimodal Collaborative Problem Solving},\n volume = {15},\n year = {2006}\n}\n'}",,"{'volume': '15', 'pages': '121 - 151', 'name': 'Journal of the Learning Sciences'}",23.0,Sharing Solutions: Persistence and Grounding in Multimodal Collaborative Problem Solving,2006.0
977,4dd5d605ff615e9be0362eb29d39029a62d4c134,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}]",1279.0,"{'bibtex': '@Article{Mehrabian1996PleasurearousaldominanceAG,\n author = {A. Mehrabian},\n journal = {Current Psychology},\n pages = {261-292},\n title = {Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament},\n volume = {14},\n year = {1996}\n}\n'}",,"{'volume': '14', 'pages': '261-292', 'name': 'Current Psychology'}",62.0,Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament,1996.0
987,4ddf4d220e9b0366328b401f3ed83b4fa9db6a5f,,"[{'authorId': '2662596', 'name': 'M. Nagamachi'}]",41.0,"{'bibtex': '@Inproceedings{Nagamachi2003KanseiEA,\n author = {M. Nagamachi},\n pages = {25-1-25-14},\n title = {Kansei Engineering: A New Consumer-Oriented Technology for Product Development},\n year = {2003}\n}\n'}",,"{'volume': '', 'pages': '25-1-25-14', 'name': ''}",0.0,Kansei Engineering: A New Consumer-Oriented Technology for Product Development,2003.0
988,4df2f94782cd9c96214094977a639968727e8e72,,"[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",446.0,"{'bibtex': '@Article{Marsella2009EMAAP,\n author = {S. Marsella and J. Gratch},\n journal = {Cognitive Systems Research},\n pages = {70-90},\n title = {EMA: A process model of appraisal dynamics},\n volume = {10},\n year = {2009}\n}\n'}",,"{'volume': '10', 'pages': '70-90', 'name': 'Cognitive Systems Research'}",63.0,EMA: A process model of appraisal dynamics,2009.0
999,4e39951c2f8b4600239dec7e10b7ee1ba3a000dd,"Emotional signals are perceived whether or not we are aware of it. The evidence so far mostly came from studies with facial expressions. Here, we investigated whether the pattern of non-conscious face expression perception is found for whole body expressions. Continuous flash suppression (CFS) was used to measure the time for neutral, fearful, and angry facial or bodily expressions to break from suppression. We observed different suppression time patterns for emotions depending on whether the stimuli were faces or bodies. The suppression time for anger was shortest for bodily expressions, but longest for the facial expressions. This pattern indicates different processing and detection mechanisms for faces and bodies outside awareness, and suggests that awareness mechanisms associated with dorsal structures might play a role in becoming conscious of angry bodily expressions.","[{'authorId': '14162527', 'name': 'M. Zhan'}, {'authorId': '1974324', 'name': 'R. Hortensius'}, {'authorId': '4628064', 'name': 'B. de Gelder'}]",26.0,"{'bibtex': '@Article{Zhan2015TheBA,\n author = {M. Zhan and R. Hortensius and B. de Gelder},\n journal = {PLoS ONE},\n title = {The Body as a Tool for Anger Awareness—Differential Effects of Angry Facial and Bodily Expressions on Suppression from Awareness},\n volume = {10},\n year = {2015}\n}\n'}",,"{'volume': '10', 'name': 'PLoS ONE'}",54.0,The Body as a Tool for Anger Awareness—Differential Effects of Angry Facial and Bodily Expressions on Suppression from Awareness,2015.0
1000,4e3fbf9d06210b556b27b6ae720266113dac4092,"Two studies established the psychometric properties of two friendship questionnaires; one taps respondents' feelings for a friend and satisfaction with the friendship, the other, respondents' assessments of the degree to which a friend fulfills six friendship functions (stimulating companionship, help, intimacy, reliable alliance, self-validation, and emotional security). Factor analysis confirmed the subscale structure of each questionnaire. The subscales showed high internal consistency, distinguished best from casual friends, and did not covary with social desirability. They did covary with the duration of being a best friend and with a self-esteem subscale regarding close friends, but not with other self-esteem measures. Women reported higher positive feelings for their friend than did men, and evaluated the friend higher on friendship functions. Finally, positive feelings and satisfaction covaried with each friendship function subscale. The research here concerns the development and validation of two multi-scale friendship questionnaires-one concerning a respondent's feelings for a friend and friendship, the other concerning the respondent's assessment of the degree to which the friend fulfills six friendship functions. The studies grew out of work on friendship quality in children and young adolescents (Aboud & Mendelson, 1996; Mendelson, Aboud, & Lanthier, 1994). The goal here was to design measures, based on a similar model of friendship, suitable for late adolescents and young adults. Friendships, like other relationships, vary in quality. Although it is difficult to judge friendship quality in behavioral terms, length of the relationship and reciprocated versus nonreciprocated nominations are gross criterion measures of friendship quality. Furthermore, individuals can specify types of friendships, distinguishing, for example, between best friends, good friends, casual friends, and acquaintances (i.e., nonfriends); and such distinctions are also gross criterion measures of quality. Gender also provides a criterion for validating friendship measures, because there is ample evidence that gender differences do exist, with women's friendships characterized by better overall quality, closeness, enjoyment, intimacy, and nurturance (e.g., Bell, 1991; Jones, 1991; Sapadin, 1988; Wright & Scanlon, 1991). Thus, any friendship measure should be sensitive enough to differentiate women's and men's friendships. At the most general level, relationships can be assessed as positive or negative. In these terms, friendship scales have been developed to assess attachment to the friend and conflict.2 Attachment refers to the special 1This paper was completed in 1997; we subsequently published a brief version (Mendelson & Aboud, 1999). Part of the research was presented as a poster (Measuring Friendship Quality in Late Adolescents and Young Adults) at the American Psychological Association, Toronto, ON, August, 1996. We heartily thank the following: Jocelyne Andrews, Sophie Beugnot, Lisa Seidel, and Tsafrir Vanounou collected data reported here as part of their undergraduate theses. Rhonda Amsel offered excellent statistical advice. Finally, Barry Corenblum, Richard Koestner, and Debbie Moskowitz constructively criticized a draft of the manuscript. The research was supported by grants from the Social Sciences and Humanities Research Council of Canada and from the Social Sciences Research Grants Subcommittee of the Faculty of Graduate Studies and Research, McGill University. Send correspondence to Morton J. Mendelson (morton.mendelson@mcgill.ca). 2Conflict, which might be considered the opposite of attachment, can certainly be an important aspect of any relationship. Indeed, friends often have conflicts, but may nonetheless be able to resolve them equitably and without bad feelings (Hartup, Laursen, Stewart & Eastenson, 1988). Thus, numerous friendship measures have subscales related to conflict or conflict resolution (Bukowski et al., 1994; Furman & Adler, 1982; Furman McGill Friendship Questionnaires 2 feelings that individuals have for a friend. Mutual liking has often been used as a criterion to identify a friend, but a separate subscale may be used to assess liking in greater depth (cf. Bukowski, Hoza, & Boivin, 1994; Sharabany, 1974; Wright, 1991). One of the measures reported here, taps positive feelings for the friend and satisfaction with the friendship, which will be viewed as criterion measures of undifferentiated friendship quality. Although the two measures are conceptually distinct, they should covary highly because they are both assumed to reflect overall friendship quality. However, an important assumption guiding research on friendship is that it is possible to assess specific qualities of friendships. Consideration of the theoretical foundations of various scales suggested that a framework based on friendship functions (Furman & Buhrmester, 1985; Parker & Asher, 1989)--rather than specific behaviors (Bukowski, et al. 1994; Sharabany, Gershoni, & Hofman, 1981) or motives (Wright, 1991)-is preferable for a number of reasons: It provides a broader scope, yields a better ideal for a mature relationship, and makes it possible to develop analogous, if not identical, measures for different developmental stages. Within a functional approach, a friend is seen as a source of certain social, emotional and instrumental resources that a person seeks (Asher & Parker, 1989; Weiss, 1974). In a review of existing measures (Aboud & Mendelson, 1992), we sought to define friendship functions that were theoretically distinct, that distinguished between friends and nonfriends, and that were associated with affection/satisfaction. We identified six relevant functions (stimulating companionship, help, intimacy, reliable alliance, self-validation and, emotional security). It is assumed that individual friends fulfill some, if not all, of these functions, so measures of the different functions should covary. Nonetheless, the following definitions describe what are clearly six conceptually distinguishable functions of friendship: Stimulating Companionship refers to doing things together that arouse enjoyment, amusement, and excitement. This quality seems to be an important expectation of friends at all ages. Some measures have focused mainly on doing things together (Buhrmester, 1990; Bukowski et al., 1994; Parker & Asher, 1989; Sharabany, 1974), but it seems important to stress the fun and excitement in common activities (Jones, 1991; Wright, 1991). Help refers to providing guidance, assistance, information, advice, and other forms of tangible aid necessary to meet needs or goals. Thus, it need not be reciprocal (Jones, 1991). Help has been assessed in specific subscales (Bukowski et al., 1994; Parker & Asher, 1989; Wright, 1991) and it has also been combined with support (Bukowski et al., 1994; Sharabany, 1974). However, the instrumental aspect of support tapped by help is distinguishable from other aspects of support tapped by Emotional Security and Self-Validation. Intimacy refers to sensitivity to the other's needs and states, providing an accepting context in which personal thoughts and feelings can be openly and honestly expressed, and openly and honestly disclosing personal information about oneself. A number of researchers have Intimacy subscales (e.g., Buhrmester & Furman, 1987), although Mannarino (1976) and Buhrmester (1990) assess it as a composite along with companionship. Sharabany's (1974) Sensitivity and Knowing subscale stresses the importance of knowing without any explicit disclosure. Wright (1991) does not include such a subscale except as it pertains to selfaffirmation or the expression of true feelings. Reliable Alliance refers to being able to count on the continuing availability and loyalty of the friend. This was an important dimension underlying Selman's (1980) distinction between a fair-weather friend who would end the relationship if conflict or strains arose and a more durable friend. It is assessed in a specific Conflict and Betrayal subscale (Parker & Asher, 1989) and in a Trust and Loyalty subscale (Sharabany, 1974). Bukowski and colleagues' (1994) Reliable Alliance subscale concerns self-disclosure, which is referred to here & Buhrmester, 1985; Parker and Asher, 1989; Wright, 1991). However, these constructs are not theoretically analogous to the six friendship functions considered here. Therefore, we are currently developing separate instruments to tap negative feelings for a friend and the incidence of conflict and conflict resolution in a friendship. McGill Friendship Questionnaires 3 as Intimacy; but they combine it with a Transcending Problems subscale that is closer to the definition of Reliable Alliance. Self-Validation refers to perceiving the other as reassuring, agreeing, encouraging, listening, and otherwise helping to maintain one's self-image as a competent and worthwhile person. This is often achieved through social comparison and consensual validation of one's attributes and beliefs. Similar items have been referred to as Attachment (Sharabany, 1974), as Ego Support and Self-Affirmation (Wright, 1991), and as Reflected Appraisal (Bukowski et al., 1994), although that was combined with an Affective Bond subscale as part of Closeness. Emotional Security refers to the comfort and confidence provided by the friend in novel or threatening situations. Although the emotional support provided by a friend is considered to be important, only Wright (1991) includes items in a Security subscale to assess perception of the friend as safe and unthreatening because he or she does not betray one's trust or draw attention to one's weaknesses. Besides assessing the respondent's feelings for the friend and satisfaction with the friendship, it seems important to choose between assessing the functions that the friend is perceived to fulfill and the functions that the respondent reportedly fulfills. Most,","[{'authorId': '34212255', 'name': 'M. Mendelson'}, {'authorId': '5913573', 'name': 'F. Aboud'}]",275.0,"{'bibtex': '@Article{Mendelson1999MeasuringFQ,\n author = {M. Mendelson and F. Aboud},\n journal = {Canadian Journal of Behavioural Science},\n pages = {130-132},\n title = {Measuring friendship quality in late adolescents and young adults: McGill Friendship Questionnaires.},\n volume = {31},\n year = {1999}\n}\n'}",,"{'volume': '31', 'pages': '130-132', 'name': 'Canadian Journal of Behavioural Science'}",11.0,Measuring friendship quality in late adolescents and young adults: McGill Friendship Questionnaires.,1999.0
1001,4e547c5e3b6fa9b8861e0801ea354b4150d330cd,"Many psychological scientists and behavioral neuroscientists affirm that “emotion” influences thinking, decision-making, actions, social relationships, well-being, and physical and mental health. Yet there is no consensus on a definition of the word “emotion,” and the present data suggest that it cannot be defined as a unitary concept. Theorists and researchers attribute quite different yet heuristic meanings to “emotion.” They show considerable agreement about emotion activation, functions, and regulation. The central goal of this article is to alert researchers, students, and other consumers of “emotion” research to the multiple meanings or aspects that distinguished scientists attribute to ”emotion,” increase appreciation of its interesting and challenging complexity, and sharpen perspectives on “emotion” and the associated body of literature that is of critical significance to science and society.","[{'authorId': '38430881', 'name': 'C. Izard'}]",458.0,"{'bibtex': '@Article{Izard2010TheMM,\n author = {C. Izard},\n journal = {Emotion Review},\n pages = {363 - 370},\n title = {The Many Meanings/Aspects of Emotion: Definitions, Functions, Activation, and Regulation},\n volume = {2},\n year = {2010}\n}\n'}",,"{'volume': '2', 'pages': '363 - 370', 'name': 'Emotion Review'}",32.0,"The Many Meanings/Aspects of Emotion: Definitions, Functions, Activation, and Regulation",2010.0
1002,4e57c5a19ede727045a5d6664ca1be9b8d298e95,"ABSTRACT This research addressed three questions concerning facial mimicry: (a) Does the relationship between mimicry and liking characterize all facial expressions, or is it limited to specific expressions? (b) Is the relationship between facial mimicry and liking symmetrical for the mimicker and the mimickee? (c) Does conscious mimicry have consequences for emotion recognition? A paradigm is introduced in which participants interact over a computer setup with a confederate whose prerecorded facial displays of emotion are synchronized with participants’ behavior to create the illusion of social interaction. In Experiment 1, the confederate did or did not mimic participants’ facial displays of various subsets of basic emotions. Mimicry promoted greater liking for the confederate regardless of which emotions were mimicked. Experiment 2 reversed these roles: participants were instructed to mimic or not to mimic the confederate’s facial displays. Mimicry did not affect liking for the confederate but it did impair emotion recognition.","[{'authorId': '145213848', 'name': 'Wojciech Kulesza'}, {'authorId': '15744706', 'name': 'A. Cislak'}, {'authorId': '3038804', 'name': 'Robin R. Vallacher'}, {'authorId': '49683665', 'name': 'Andrzej Nowak'}, {'authorId': '5195231', 'name': 'Martyna Czekiel'}, {'authorId': '6410603', 'name': 'S. Bedyńska'}]",33.0,"{'bibtex': '@Article{Kulesza2015TheFO,\n author = {Wojciech Kulesza and A. Cislak and Robin R. Vallacher and Andrzej Nowak and Martyna Czekiel and S. Bedyńska},\n journal = {The Journal of Social Psychology},\n pages = {590 - 604},\n title = {The Face of the Chameleon: The Experience of Facial Mimicry for the Mimicker and the Mimickee},\n volume = {155},\n year = {2015}\n}\n'}",,"{'volume': '155', 'pages': '590 - 604', 'name': 'The Journal of Social Psychology'}",72.0,The Face of the Chameleon: The Experience of Facial Mimicry for the Mimicker and the Mimickee,2015.0
1004,4e6ac44ad26ae8767bbe73e40d768b8d02382560,,"[{'authorId': '48369504', 'name': 'Alessandra Rossi'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}, {'authorId': '1749179', 'name': 'K. Koay'}, {'authorId': '1847981', 'name': 'M. Walters'}]",48.0,"{'bibtex': ""@Inproceedings{Rossi2017HowTT,\n author = {Alessandra Rossi and K. Dautenhahn and K. Koay and M. Walters},\n pages = {42-52},\n title = {How the Timing and Magnitude of Robot Errors Influence Peoples' Trust of Robots in an Emergency Scenario},\n year = {2017}\n}\n""}",,{'pages': '42-52'},33.0,How the Timing and Magnitude of Robot Errors Influence Peoples' Trust of Robots in an Emergency Scenario,2017.0
1005,4f0abe8b1a974885996e8938d9a395c2353894e3,"The present study examined the effects of four types of intergroup orientation on interpersonal postural mirroring both within and between groups. One hundred and four female subjects were assigned to quartets, each made up of two dyads in one of four conditions: (1) Control; (2) coacting; (3) cooperating; and (4) competing. As predicted, results showed greater intergroup relative to intragroup mirroring for cooperating dyads than for competing dyads. Unexpectedly, subjects in the coacting condition showed a significantly higher level of intergroup mirroring than any other condition. Both results are interpreted as evidence that postural mirroring is an obvious yet unobtrusive indicator of openness to interpersonal involvement.","[{'authorId': '37544705', 'name': 'M. LaFrance'}]",96.0,"{'bibtex': '@Article{LaFrance1985PosturalMA,\n author = {M. LaFrance},\n journal = {Personality and Social Psychology Bulletin},\n pages = {207 - 217},\n title = {Postural Mirroring and Intergroup Relations},\n volume = {11},\n year = {1985}\n}\n'}",,"{'volume': '11', 'pages': '207 - 217', 'name': 'Personality and Social Psychology Bulletin'}",8.0,Postural Mirroring and Intergroup Relations,1985.0
1006,4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e,"We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.","[{'authorId': '2265067', 'name': 'Sainbayar Sukhbaatar'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '145183709', 'name': 'J. Weston'}, {'authorId': '2276554', 'name': 'R. Fergus'}]",2352.0,"{'bibtex': '@Inproceedings{Sukhbaatar2015EndToEndMN,\n author = {Sainbayar Sukhbaatar and Arthur Szlam and J. Weston and R. Fergus},\n pages = {2440-2448},\n title = {End-To-End Memory Networks},\n year = {2015}\n}\n'}",,{'pages': '2440-2448'},26.0,End-To-End Memory Networks,2015.0
1007,4f118d8cfd9e4eb3df249e97844af338fc171b4f,"Background Stanley Milgram's 1960s experimental findings that people would administer apparently lethal electric shocks to a stranger at the behest of an authority figure remain critical for understanding obedience. Yet, due to the ethical controversy that his experiments ignited, it is nowadays impossible to carry out direct experimental studies in this area. In the study reported in this paper, we have used a similar paradigm to the one used by Milgram within an immersive virtual environment. Our objective has not been the study of obedience in itself, but of the extent to which participants would respond to such an extreme social situation as if it were real in spite of their knowledge that no real events were taking place. Methodology Following the style of the original experiments, the participants were invited to administer a series of word association memory tests to the (female) virtual human representing the stranger. When she gave an incorrect answer, the participants were instructed to administer an ‘electric shock’ to her, increasing the voltage each time. She responded with increasing discomfort and protests, eventually demanding termination of the experiment. Of the 34 participants, 23 saw and heard the virtual human, and 11 communicated with her only through a text interface. Conclusions Our results show that in spite of the fact that all participants knew for sure that neither the stranger nor the shocks were real, the participants who saw and heard her tended to respond to the situation at the subjective, behavioural and physiological levels as if it were real. This result reopens the door to direct empirical studies of obedience and related extreme social situations, an area of research that is otherwise not open to experimental study for ethical reasons, through the employment of virtual environments.","[{'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '1705895', 'name': 'Angus Antley'}, {'authorId': '2052135863', 'name': 'Adam Davison'}, {'authorId': '2339529', 'name': 'David Swapp'}, {'authorId': '145896814', 'name': 'C. Guger'}, {'authorId': '143798288', 'name': 'C. Barker'}, {'authorId': '2464111', 'name': 'N. Pistrang'}, {'authorId': '1384107200', 'name': 'Maria V. Sanchez-Vives'}]",571.0,"{'bibtex': '@Article{Slater2006AVR,\n author = {M. Slater and Angus Antley and Adam Davison and David Swapp and C. Guger and C. Barker and N. Pistrang and Maria V. Sanchez-Vives},\n journal = {PLoS ONE},\n title = {A Virtual Reprise of the Stanley Milgram Obedience Experiments},\n volume = {1},\n year = {2006}\n}\n'}",,"{'volume': '1', 'name': 'PLoS ONE'}",34.0,A Virtual Reprise of the Stanley Milgram Obedience Experiments,2006.0
1008,4f1450fc7a37a5c09abc2b23bff3aee31f25d2a3,,"[{'authorId': '115055132', 'name': 'Ali Abbas'}, {'authorId': '122228036', 'name': 'M. Choi'}, {'authorId': '3039883', 'name': 'Joonoh Seo'}, {'authorId': '38821821', 'name': 'S. Cha'}, {'authorId': '49404672', 'name': 'Heng Li'}]",31.0,"{'bibtex': '@Article{Abbas2019EffectivenessOI,\n author = {Ali Abbas and M. Choi and Joonoh Seo and S. Cha and Heng Li},\n journal = {KSCE Journal of Civil Engineering},\n pages = {4972 - 4983},\n title = {Effectiveness of Immersive Virtual Reality-based Communication for Construction Projects},\n volume = {23},\n year = {2019}\n}\n'}",,"{'volume': '23', 'pages': '4972 - 4983', 'name': 'KSCE Journal of Civil Engineering'}",48.0,Effectiveness of Immersive Virtual Reality-based Communication for Construction Projects,2019.0
1009,4f59f676086cfc45cea8ba9a6239a16d3a652b6c,,"[{'authorId': '1711777', 'name': 'C. Breazeal'}]",1172.0,"{'bibtex': '@Article{Breazeal2003EmotionAS,\n author = {C. Breazeal},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {119-155},\n title = {Emotion and sociable humanoid robots},\n volume = {59},\n year = {2003}\n}\n'}",,"{'volume': '59', 'pages': '119-155', 'name': 'Int. J. Hum. Comput. Stud.'}",77.0,Emotion and sociable humanoid robots,2003.0
1012,4f744f09a575471059a7dafb142b44e5305e5e3f,"Humans have a natural tendency to unconsciously treat computers as though they are social actors. Many of the human-human social rules that have been well researched and defined in social psychology also seem to apply in human-computer interaction (HCI). This presents an opportunity for computers to use many of the principles and rules from human-human interaction and utilise them for enhancing HCI. One such set of principles that have been extensively investigated and examined in social psychology are influence strategies. A number of different approaches and techniques can be used to influence another’s attitudes and behaviour. These persuasive techniques could potentially be used by computers to influence or change user attitudes and behaviour for beneficial purposes (e.g. smoking cessation or improving exercise behaviour). In this project I will investigate how we respond to interface agents which make use of influence strategies when attempting to change our attitudes and behaviour toward eating. The manipulation of emotions is an essential aspect of attitude and behaviour change and it is proposed that the role of emotion in HCI will also be investigated. Interface designers often incorporate emotional expressions and statements into interface agents, through the use of textual content, synthetic or recorded speech, embodied characters, or multimedia video, without an understanding of the psychological impact on users. Moreover, it is largely unknown how strong user responses are to the synthetic emotional expressions made by computers. I will attempt to address these issues through three different investigations. Firstly, an examination of how users respond to the emotional expressions of different types of interface agents will be undertaken. Secondly, an investigation will be conducted to help understand whether agents that make use of influence strategies are able to influence or change user attitudes and behaviour more effectively than agents which do not. Finally, I will test whether emotional interface agents (which employ influence tactics) are able to influence or change user attitudes and behaviour more effectively than unemotional agents.","[{'authorId': '3134697', 'name': 'C. Creed'}, {'authorId': '144189909', 'name': 'R. Beale'}]",12.0,"{'bibtex': '@Inproceedings{Creed2005UsingES,\n author = {C. Creed and R. Beale},\n title = {Using Emotion Simulation to Influence User Attitudes and Behaviour},\n year = {2005}\n}\n'}",,,60.0,Using Emotion Simulation to Influence User Attitudes and Behaviour,2005.0
1013,4f758c91bb56d9be1280de1aa79ec90365b592f9,"Believable virtual characters enhance the experience of interactive virtual worlds. BDI-based state-space narrative planners make their characters more compelling by granting them specific goals to pursue and different (possibly wrong) beliefs about the state of the world. However, such systems do not account for character individuality. We extend work on narrative planning by proposing a personality model based on the Big Five. This paper discusses how each factor of the Big Five is represented by narrative planning features. We also present a study to evaluate whether human readers can perceive the modeled personality traits through simple action patterns in short stories.","[{'authorId': '144650313', 'name': 'A. Shirvani'}, {'authorId': '34810994', 'name': 'Stephen G. Ware'}]",9.0,"{'bibtex': '@Inproceedings{Shirvani2019APP,\n author = {A. Shirvani and Stephen G. Ware},\n pages = {188-194},\n title = {A Plan-Based Personality Model for Story Characters},\n year = {2019}\n}\n'}",,{'pages': '188-194'},20.0,A Plan-Based Personality Model for Story Characters,2019.0
1014,4f7bf03bfe27341a16ea605597d31a5925e31ef4,"This paper presents the emotional interaction model that receives classified inputs from user's response and decides what emotional response the robot should generate. Cognitive emotion modeling requires profound understanding about human's cognitive processes and ideas on how to implement each constitutional components of the model into the robot. The proposed model is composed of two layers: reactive and deliberative layers. Reactive layer is in charge of immediate emotional response of the robot. It works with pre-defined rules which relate input to its corresponding emotional expressions. This layer enables immediate display of the robot's emotional state to the user so that lifelike characteristics of the robot can be achieved. The deliberative layer is in charge of the appraisal-based emotion expression and carries out the function of ""action-coloring,"" which adds flavor to the actions being taken. Emotional interaction scenarios which are considered to be possible with the proposed emotion model are also introduced.","[{'authorId': '2031350', 'name': 'Hyoung-Rock Kim'}, {'authorId': '2118245627', 'name': 'Kang-Woo Lee'}, {'authorId': '145079887', 'name': 'D. Kwon'}]",25.0,"{'bibtex': '@Article{Kim2005EmotionalIM,\n author = {Hyoung-Rock Kim and Kang-Woo Lee and D. Kwon},\n journal = {ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.},\n pages = {672-678},\n title = {Emotional interaction model for a service robot},\n year = {2005}\n}\n'}",,"{'pages': '672-678', 'name': 'ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.'}",14.0,Emotional interaction model for a service robot,2005.0
1015,4f81f418fe3efbf5ff956d5248288a8c7d188ebd,,"[{'authorId': '119400961', 'name': 'Mourad Ali Eissa Saad'}]",17.0,"{'bibtex': '@Inproceedings{Saad2016TheEO,\n author = {Mourad Ali Eissa Saad},\n pages = {51-60},\n title = {The Effectiveness of Social Stories among Children and Adolescents with Autism Spectrum Disorders: Meta-Analysis.},\n volume = {5},\n year = {2016}\n}\n'}",,"{'volume': '5', 'pages': '51-60', 'name': ''}",0.0,The Effectiveness of Social Stories among Children and Adolescents with Autism Spectrum Disorders: Meta-Analysis.,2016.0
1016,4f8291709f45d0408ba8816b6e650d40ba46a62a,"Contents: Preface. Introduction. The Structure of American Sign Language: Linguistic Universals and Modality Effects. The Confluence of Language and Space. Psycholinguistic Studies of Sign Perception, Online Processing, and Production. Sign Language Acquisition. The Critical Period Hypothesis and the Effects of Late Language Acquisition. Memory for Sign Language: Implications for the Structure of Working Memory. The Impact of Sign Language Use on Visuospatial Cognition. Sign Language and the Brain. Appendices: Handshapes in American Sign Language. Linguistic Distinctions Among Communication Forms in Nicaragua.","[{'authorId': '2094807072', 'name': 'K. Emmorey'}]",608.0,"{'bibtex': '@Inproceedings{Emmorey2001LanguageCA,\n author = {K. Emmorey},\n title = {Language, Cognition, and the Brain: Insights From Sign Language Research},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Language, Cognition, and the Brain: Insights From Sign Language Research",2001.0
1017,4f89397812c37d95407771abaf50bccf8feae8e0,,"[{'authorId': '1691036', 'name': 'Lina Zhou'}, {'authorId': '144217185', 'name': 'Ammar Mohammed'}, {'authorId': '33006924', 'name': 'Dongsong Zhang'}]",44.0,"{'bibtex': '@Article{Zhou2012MobilePI,\n author = {Lina Zhou and Ammar Mohammed and Dongsong Zhang},\n journal = {Inf. Process. Manag.},\n pages = {23-31},\n title = {Mobile personal information management agent: Supporting natural language interface and application integration},\n volume = {48},\n year = {2012}\n}\n'}",,"{'volume': '48', 'pages': '23-31', 'name': 'Inf. Process. Manag.'}",40.0,Mobile personal information management agent: Supporting natural language interface and application integration,2012.0
1018,4f95af1f0d23c58f9316dbebaf5651c1d0bf85c6,"Humans use gestures as a means of non-verbal communication. Often accompanying speech, these gestures have several purposes but in general, aim to convey an intended message to the receiver. Researchers have tried to develop systems to allow embodied agents to be better communicators when interacting with humans via using gestures. In this article, we present a scoping literature review of the methods and the metrics used to generate and evaluate co-speech gestures. After collecting a set of papers using a term search on the Scopus database, we analysed the content of these papers based on methodology (i.e., model, the dataset used), evaluation measures (i.e., objective and subjective) and limitations. The results indicate that data-driven approaches are used more frequently. In terms of evaluation measures, we found a trend of combining objective and subjective metrics, while no standards exist for either. This literature review provides an overview of the research in the area and, more specifically insights the trends and the challenges to be met in building a system to automatically generate gestures for embodied agents.","[{'authorId': '2155143593', 'name': 'Yu Liu'}, {'authorId': '4911295', 'name': 'Gelareh Mohammadi'}, {'authorId': '2157994950', 'name': 'Yang Song'}, {'authorId': '2963096', 'name': 'W. Johal'}]",14.0,"{'bibtex': '@Article{Liu2021SpeechbasedGG,\n author = {Yu Liu and Gelareh Mohammadi and Yang Song and W. Johal},\n journal = {Proceedings of the 9th International Conference on Human-Agent Interaction},\n title = {Speech-based Gesture Generation for Robots and Embodied Agents: A Scoping Review},\n year = {2021}\n}\n'}",,{'name': 'Proceedings of the 9th International Conference on Human-Agent Interaction'},44.0,Speech-based Gesture Generation for Robots and Embodied Agents: A Scoping Review,2021.0
1019,4f9958946ad9fc71c2299847e9ff16741401c591,"This paper presents a complete system for automatic facial expression recognition. The Candide-3 face model is used in conjunction with a learned objective function for face model fitting. The resulting sequence of model parameters is then presented to a recurrent neural network for classification. The advantage of using a recurrent network is that the temporal dependencies present in the image sequences can be taken into account during the classification. Since the entire process is automatic, and the recurrent networks can be used to make online predictions, the system would be ideal for real-time recognition. This would make it suitable for the CoTeSys ‘coffee break’ scenario, where guests must be recognised and served by robot waiters. Promising experimental results are presented on the Cohn-Kanade database.","[{'authorId': '1753223', 'name': 'Alex Graves'}, {'authorId': '50565622', 'name': 'C. Mayer'}, {'authorId': '32131501', 'name': 'M. Wimmer'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}, {'authorId': '1699132', 'name': 'B. Radig'}]",36.0,"{'bibtex': '@Inproceedings{Graves2008FacialER,\n author = {Alex Graves and C. Mayer and M. Wimmer and J. Schmidhuber and B. Radig},\n title = {Facial Expression Recognition with Recurrent Neural Networks},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': ''}",21.0,Facial Expression Recognition with Recurrent Neural Networks,2008.0
1020,4f9dd3ba172b13f35152fd68477b9f57809e5789,"In an analysis of the nature and origins of predictability in social behavior, two propositions are considered: (1) There exist categories of individuals whose social behavior is readily predictable from measures of personal attributes such as attitudes, traits, and dispositions as well as categories of individuals whose social behavior is readily predictable from situational and interpersonal specifications of behavioral appropriateness; (2) underlying these differences in predictability are systematic choices to enter and to spend time in social settings and interpersonal contexts that promote and facilitate one or other of these characteristic behavioral orientations. The implications of these propositions for the study of personality and social behavior are considered in the specific case of the psychological construct of self-monitoring and in the general case of understanding the reciprocal influences of individuals and their social worlds.","[{'authorId': '143958550', 'name': 'M. Snyder'}]",137.0,"{'bibtex': '@Article{Snyder1983TheIO,\n author = {M. Snyder},\n journal = {Journal of personality},\n pages = {\n          497-516\n        },\n title = {The influence of individuals on situations: Implications for understanding the links between personality and social behavior.},\n volume = {51 3},\n year = {1983}\n}\n'}",,"{'volume': '51 3', 'pages': '\n          497-516\n        ', 'name': 'Journal of personality'}",55.0,The influence of individuals on situations: Implications for understanding the links between personality and social behavior.,1983.0
1021,4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563,"Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.","[{'authorId': '2155954521', 'name': 'Jing Xu'}, {'authorId': '3092435', 'name': 'Da Ju'}, {'authorId': '6649233', 'name': 'Margaret Li'}, {'authorId': '90841478', 'name': 'Y-Lan Boureau'}, {'authorId': '145183709', 'name': 'J. Weston'}, {'authorId': '31461304', 'name': 'Emily Dinan'}]",163.0,"{'bibtex': '@Article{Xu2020RecipesFS,\n author = {Jing Xu and Da Ju and Margaret Li and Y-Lan Boureau and J. Weston and Emily Dinan},\n journal = {ArXiv},\n title = {Recipes for Safety in Open-domain Chatbots},\n volume = {abs/2010.07079},\n year = {2020}\n}\n'}",,"{'volume': 'abs/2010.07079', 'name': 'ArXiv'}",75.0,Recipes for Safety in Open-domain Chatbots,2020.0
1022,4fa57b5c223f9fde3a4e19b40f46d07635b41b94,"This paper presents the research challenges for modeling crowd dynamics modeling for evacuations purposes. The paper presents the current achievements and the research challenges of such systems. Then, it proposes an architecture which can solve these problems. The architecture comprises of a synthesis of computer vision algorithms and complex science. Using such synthesis we are not able to observe current observations but also to predict the outcome of a process resulting in high adaptation schemes.","[{'authorId': '120205775', 'name': 'N. Doulamis'}]",9.0,"{'bibtex': '@Article{Doulamis2009EvacuationPT,\n author = {N. Doulamis},\n journal = {2009 16th International Conference on Systems, Signals and Image Processing},\n pages = {1-4},\n title = {Evacuation Planning through Cognitive Crowd Tracking},\n year = {2009}\n}\n'}",,"{'pages': '1-4', 'name': '2009 16th International Conference on Systems, Signals and Image Processing'}",12.0,Evacuation Planning through Cognitive Crowd Tracking,2009.0
1023,4fa737263241cbcea713504452f23fedcf06e968,,"[{'authorId': '2786681', 'name': 'Joan Marc Llargues Asensio'}, {'authorId': '144734360', 'name': 'Juan Peralta'}, {'authorId': '2187165', 'name': 'Raúl Arrabales'}, {'authorId': '2960222', 'name': 'M. G. Bedia'}, {'authorId': '145286037', 'name': 'P. Cortez'}, {'authorId': '2110296033', 'name': 'Antonio M. López'}]",43.0,"{'bibtex': '@Article{Asensio2014ArtificialIA,\n author = {Joan Marc Llargues Asensio and Juan Peralta and Raúl Arrabales and M. G. Bedia and P. Cortez and Antonio M. López},\n journal = {Expert Syst. Appl.},\n pages = {7281-7290},\n title = {Artificial Intelligence approaches for the generation and assessment of believable human-like behaviour in virtual characters},\n volume = {41},\n year = {2014}\n}\n'}",,"{'volume': '41', 'pages': '7281-7290', 'name': 'Expert Syst. Appl.'}",33.0,Artificial Intelligence approaches for the generation and assessment of believable human-like behaviour in virtual characters,2014.0
1024,4fd89ba6cd608a927e9cbbaabf67f724ba406625,"The paper focuses on the development of a human-computer communication method which utilizes the user's typing skills to control the facial expression of a computer-generated 3D face. The method is based on the realization that the human face is a movable and deformable system with 26 degrees of freedom (the same number as the letters of the English alphabet). Therefore it is possible to create a parameterized graphical facial model confined to a set of 26 parameters each one controlled by a letter key. The method is an extension of the KUI technique (Adamo-Villani and Beni, 2004; Adamo-Villani and Beni, 2003) recently developed to encode hand gestures.","[{'authorId': '1403309968', 'name': 'N. Adamo-Villani'}, {'authorId': '145297778', 'name': 'G. Beni'}]",11.0,"{'bibtex': '@Article{Adamo-Villani2004KeyboardEO,\n author = {N. Adamo-Villani and G. Beni},\n journal = {Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.},\n pages = {324-328},\n title = {Keyboard encoding of facial expressions},\n year = {2004}\n}\n'}",,"{'pages': '324-328', 'name': 'Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.'}",25.0,Keyboard encoding of facial expressions,2004.0
1025,4fdc075d1cc14d64c11c3c860e86a082b761f4a0,"This paper surveys the field of augmented reality (AR), in which 3D virtual objects are integrated into a 3D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment, and military applications that have been explored. This paper describes the characteristics of augmented reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective augmented reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using augmented reality.","[{'authorId': '34679537', 'name': 'Ronald T. Azuma'}]",8697.0,"{'bibtex': '@Article{Azuma1997ASO,\n author = {Ronald T. Azuma},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {355-385},\n title = {A Survey of Augmented Reality},\n volume = {6},\n year = {1997}\n}\n'}",,"{'volume': '6', 'pages': '355-385', 'name': 'Presence: Teleoperators & Virtual Environments'}",156.0,A Survey of Augmented Reality,1997.0
1026,4fdc24cbc96e50e059b758ce0180219f6a9cb226,"Natural multimodal interaction with realistic virtual characters provides rich opportunities for entertainment and education. In this paper we present the current VIRTUALHUMAN demonstrator system. It provides a knowledge-based framework to create interactive applications in a multi-user, multi-agent setting. The behavior of the virtual humans and objects in the 3D environment is controlled by interacting affective conversational dialogue engines. An elaborate model of affective behavior adds natural emotional reactions and presence of the virtual humans. Actions are defined in a XML-based markup language that supports the incremental specification of synchronized multimodal output. The system was successfully demonstrated during CeBIT 2006.","[{'authorId': '1731353', 'name': 'Norbert Reithinger'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '1700512', 'name': 'Markus Löckelt'}, {'authorId': '2706336', 'name': 'A. Ndiaye'}, {'authorId': '1705387', 'name': 'Norbert Pfleger'}, {'authorId': '2922093', 'name': 'Martin Klesen'}]",32.0,"{'bibtex': '@Inproceedings{Reithinger2006VirtualHumanDA,\n author = {Norbert Reithinger and Patrick Gebhard and Markus Löckelt and A. Ndiaye and Norbert Pfleger and Martin Klesen},\n pages = {51-58},\n title = {VirtualHuman: dialogic and affective interaction with virtual characters},\n year = {2006}\n}\n'}",,{'pages': '51-58'},28.0,VirtualHuman: dialogic and affective interaction with virtual characters,2006.0
1027,4ff562c142a46c11757c002cd0a2b111fadb4410,"Through evolutionary computation, affective models may emerge autonomously in unanticipated ways. We explored whether core affect would be leveraged through communication with conspecifics (e.g. signalling danger or foraging opportunities). Genetic algorithms served to evolve recurrent neural networks controlling virtual agents in an environment with fitness-increasing food and fitness-reducing predators. Previously, neural oscillations emerged serendipitously, with higher frequencies for positive than negative stimuli, which we replicated here in the fittest agent. The setup was extended so that oscillations could be exapted for the communication between two agents. An adaptive communicative function evolved, as shown by fitness benefits relative to (1) a non-communicative reference simulation and (2) lesioning of the connections used for communication. An exaptation of neural oscillations for communication was not observed but a simpler type of communication developed than was initially expected. The agents approached each other in a periodic fashion and slightly modified these movements to approach food or avoid predators. The coupled agents, though controlled by separate networks, appeared to self-assemble into a single vibrating organism. The simulations (a) strengthen an account of core affect as an oscillatory modulation of neural-network competition, and (b) encourage further work on the exaptation of core affect for communicative purposes.","[{'authorId': '29859415', 'name': 'C. Hesp'}, {'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2545816', 'name': 'R. Phaf'}]",2.0,"{'bibtex': '@Article{Hesp2020EvolutionaryCF,\n author = {C. Hesp and B. T. Heerebout and R. Phaf},\n booktitle = {Connection science},\n journal = {Connection Science},\n pages = {296 - 320},\n title = {Evolutionary computation for bottom-up hypothesis generation on emotion and communication},\n volume = {33},\n year = {2020}\n}\n'}","[{'paperId': '90bf72c7ca64a49c8bf56554553a5e2edcefa78e', 'title': 'Matching heterogeneous ontologies with adaptive evolutionary algorithm'}, {'paperId': '24e712409cc1daf9e2fd3a11294247466b978467', 'title': 'Spontaneous modular NeuroEvolution arising from a life/dinner paradox'}]","{'name': 'Connection Science', 'pages': '296 - 320', 'volume': '33'}",38.0,Evolutionary computation for bottom-up hypothesis generation on emotion and communication,2020.0
1028,500d6e335c8490234511a74b3165fac606fc543d,"Many studies showed the ability of movies and imagery techniques to elicit emotions. Nevertheless, it is less clear how to manipulate the content of interactive media to induce specific emotional responses. In particular, this is true for the emerging medium virtual reality (VR), whose main feature is the ability to induce a feeling of ""presence"" in the computer-generated world experienced by the user. The main goal of this study was to analyze the possible use of VR as an affective medium. Within this general goal, the study also analyzed the relationship between presence and emotions. The results confirmed the efficacy of VR as affective medium: the interaction with ""anxious"" and ""relaxing"" virtual environments produced anxiety and relaxation. The data also showed a circular interaction between presence and emotions: on one side, the feeling of presence was greater in the ""emotional"" environments; on the other side, the emotional state was influenced by the level of presence. The significance of these results for the assessment of affective interaction is discussed.","[{'authorId': '144059813', 'name': 'G. Riva'}, {'authorId': '2274674', 'name': 'F. Mantovani'}, {'authorId': '2208482', 'name': 'Claret S. Capideville'}, {'authorId': '3234117', 'name': 'A. Preziosa'}, {'authorId': '1745080', 'name': 'F. Morganti'}, {'authorId': '103238720', 'name': 'D. Villani'}, {'authorId': '1700503', 'name': 'A. Gaggioli'}, {'authorId': '145945543', 'name': 'C. Botella'}, {'authorId': '7604530', 'name': 'M. A. Raya'}]",817.0,"{'bibtex': '@Article{Riva2007AffectiveIU,\n author = {G. Riva and F. Mantovani and Claret S. Capideville and A. Preziosa and F. Morganti and D. Villani and A. Gaggioli and C. Botella and M. A. Raya},\n journal = {Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society},\n pages = {\n          45-56\n        },\n title = {Affective Interactions Using Virtual Reality: The Link between Presence and Emotions},\n volume = {10 1},\n year = {2007}\n}\n'}",,"{'volume': '10 1', 'pages': '\n          45-56\n        ', 'name': 'Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society'}",45.0,Affective Interactions Using Virtual Reality: The Link between Presence and Emotions,2007.0
1029,50135b37e5be956a317a39589178cf54b724f00d,,"[{'authorId': '2223622395', 'name': 'D. Thalmann'}]",119.0,"{'bibtex': '@Inproceedings{Thalmann2019CrowdS,\n author = {D. Thalmann},\n title = {Crowd Simulation},\n year = {2019}\n}\n'}",,,40.0,Crowd Simulation,2019.0
1030,504eff078bd0a2b9c4bcf58730daa5a42845f5dd,"This paper describes a new multi-player computer game, Colored Trails (CT), which may be played by people, computers and heterogeneous groups. CT was designed to enable investigation of properties of decision-making strategies in multi-agent situations of varying complexity. The paper presents the results of an initial series of experiments of CT games in which agentsý choices affected not only their own outcomes but also the outcomes of other agents. It compares the behavior of people with that of computer agents deploying a variety of decision-making strategies. The results align with behavioral economics studies in showing that people cooperate when they play and that factors of social dependency influence their levels of cooperation. Preliminary results indicate that people design agents to play strategies closer to game-theory predictions, yielding lower utility. Additional experiments show that such agents perform worse than agents designed to make choices that resemble human cooperative behavior. The paper describes challenges raised by these results for designers of agents, especially agents that need to operate in heterogeneous groups that include people.","[{'authorId': '1692242', 'name': 'B. Grosz'}, {'authorId': '1691597', 'name': 'Sarit Kraus'}, {'authorId': '2939375', 'name': 'Shavit Talman'}, {'authorId': '40446340', 'name': 'B. Stossel'}, {'authorId': '2425853', 'name': 'Moti Havlin'}]",138.0,"{'bibtex': '@Article{Grosz2004TheIO,\n author = {B. Grosz and Sarit Kraus and Shavit Talman and B. Stossel and Moti Havlin},\n journal = {Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.},\n pages = {782-789},\n title = {The influence of social dependencies on decision-making: initial investigations with a new game},\n year = {2004}\n}\n'}",,"{'pages': '782-789', 'name': 'Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.'}",16.0,The influence of social dependencies on decision-making: initial investigations with a new game,2004.0
1031,5069dceb2ddc553790f09b9550a1e84c8bc51f83,,"[{'authorId': '37585849', 'name': 'A. Weiss'}, {'authorId': '2411054', 'name': 'D. Wurhofer'}, {'authorId': '1751253', 'name': 'M. Tscheligi'}]",85.0,"{'bibtex': '@Article{Weiss2009ILT,\n author = {A. Weiss and D. Wurhofer and M. Tscheligi},\n journal = {International Journal of Social Robotics},\n pages = {243-248},\n title = {“I Love This Dog”—Children’s Emotional Attachment to\xa0the\xa0Robotic Dog AIBO},\n volume = {1},\n year = {2009}\n}\n'}",,"{'volume': '1', 'pages': '243-248', 'name': 'International Journal of Social Robotics'}",16.0,“I Love This Dog”—Children’s Emotional Attachment to the Robotic Dog AIBO,2009.0
1032,50981e47da7a9d1eb620b96db6bf820485eaa31f,,"[{'authorId': '2524401', 'name': 'Linqin Cai'}, {'authorId': '2109444810', 'name': 'Binbin Liu'}, {'authorId': '2115987564', 'name': 'Jimin Yu'}, {'authorId': '2108128657', 'name': 'Jianrong Zhang'}]",15.0,"{'bibtex': '@Article{Cai2017HumanBM,\n author = {Linqin Cai and Binbin Liu and Jimin Yu and Jianrong Zhang},\n booktitle = {Multimedia tools and applications},\n journal = {Multimedia Tools and Applications},\n pages = {5851-5871},\n title = {Human behaviors modeling in multi-agent virtual environment},\n volume = {76},\n year = {2017}\n}\n'}","[{'paperId': '57ab1ac3a3c18b9e2be66c9d4f375950ecf2d9d2', 'title': 'Correction to: Modeling and simulation of virtual learning environment for automatic control principle'}, {'paperId': '3636c903cd1165a5aedbbc5e208308393d01158f', 'title': 'Shaping the Behavior of Reinforcement Learning Agents'}, {'paperId': '4830644f34e8ed835a4b49a926c559bf92b74efd', 'title': 'A behavior model for improving realism in a VR team sport'}, {'paperId': 'a7715c2dcd434d7bcf835a51ba17297d6c918fde', 'title': 'On the Synergy between Virtual Reality and Multi-Agent Systems'}, {'paperId': '4c98e7b09c737c40741ee47cec83631f7f688929', 'title': 'Learner Modeling in Educational Games Based on Fuzzy Logic and Gameplay Data'}, {'paperId': 'fe4b09375e5c3393e4ab629b32da2e3722f63689', 'title': 'A Comprehensive Model of the Relationship between Miners’ Work Commitment, Cultural Emotion and Unemployment Risk Perception'}, {'paperId': '17f67cb0406caa088b9c5d0bfb6d5f8adcfd4e00', 'title': 'Affective interaction: Using emotions as a user interface in games'}, {'paperId': 'b0ecfdeadee6e51cc59906d3ab755d4bce3382f6', 'title': ""Research on Visual Comfort of Underground Commercial Streets' Pavement in China on the Basis of Virtual Simulation""}, {'paperId': '34458880f02232a3a77dd8a48ad88484f775f05f', 'title': 'Modeling Group Structures With Emotion in Crowd Evacuation'}, {'paperId': '5d0f19862eca95f1fd803c8d699afe6aba324b79', 'title': 'Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment'}, {'paperId': '6c280c23a7d3a24afb49f4c0b57c73c98b9f8f74', 'title': 'Emotion Model for Artificial Intelligence and their Applications'}, {'paperId': 'de892b92ba53f213151786be15d803e9b0369965', 'title': 'Multi-agents based virtual environments for cultural heritage'}, {'paperId': '6177633a55c424308e27688a9500c305c7a5eca1', 'title': 'A Review on Multi-agent Systems and Virtual Reality'}, {'paperId': '218ebb081422fd1796f480dbd9fef53f4ea9dac2', 'title': 'Realistic Simulation of Cultural Heritage'}, {'paperId': '62ffab00fe37dc3c07d0ddad81ee9314127162d9', 'title': 'Virtual Reality Applications in the Extractive Industry—A Short Review'}]","{'name': 'Multimedia Tools and Applications', 'pages': '5851-5871', 'volume': '76'}",34.0,Human behaviors modeling in multi-agent virtual environment,2017.0
1033,50a42ed2f81b9fe150883a6c89194c88a9647106,The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.,"[{'authorId': '2737945', 'name': 'H. Akaike'}]",46599.0,"{'bibtex': '@Article{Akaike1974ANL,\n author = {H. Akaike},\n journal = {IEEE Transactions on Automatic Control},\n pages = {716-723},\n title = {A new look at the statistical model identification},\n volume = {19},\n year = {1974}\n}\n'}",,"{'volume': '19', 'pages': '716-723', 'name': 'IEEE Transactions on Automatic Control'}",45.0,A new look at the statistical model identification,1974.0
1034,50aa2fb1a359d2ad618ccbdee943d6f90742a2b5,,"[{'authorId': '21451088', 'name': 'P. Ekman'}]",2592.0,"{'bibtex': '@Inproceedings{Ekman1972UniversalsAC,\n author = {P. Ekman},\n pages = {207-282},\n title = {Universals and cultural differences in facial expressions of emotion.},\n volume = {1971},\n year = {1972}\n}\n'}",,"{'volume': '1971', 'pages': '207-282', 'name': ''}",0.0,Universals and cultural differences in facial expressions of emotion.,1972.0
1035,50dbf733e1bf8bb210a9c4ddd1d687dade014182,"We examined the effect of rendering style and the interplay between attention and emotion in users during interaction with a virtual patient in a medical training simulator. The virtual simulation was rendered representing a sample from the photo-realistic to the non-photorealistic continuum, namely Near-Realistic, Cartoon or Pencil-Shader. In a mixed design study, we collected 45 participants’ emotional responses and gaze behavior using surveys and an eye tracker while interacting with a virtual patient who was medically deteriorating over time. We used a cross-lagged panel analysis of attention and emotion to understand their reciprocal relationship over time. We also performed a mediation analysis to compare the extent to which the virtual agent’s appearance and his affective behavior impacted users’ emotional and attentional responses. Results showed the interplay between participants’ visual attention and emotion over time and also showed that attention was a stronger variable than emotion during the interaction with the virtual human.","[{'authorId': '51250937', 'name': 'Matias Volonte'}, {'authorId': '121111647', 'name': 'Reza Ghaiumy Anaraky'}, {'authorId': '2477993', 'name': 'Bart P. Knijnenburg'}, {'authorId': '2245673', 'name': 'A. Duchowski'}, {'authorId': '144403504', 'name': 'Sabarish V. Babu'}]",5.0,"{'bibtex': '@Article{Volonte2019EmpiricalEO,\n author = {Matias Volonte and Reza Ghaiumy Anaraky and Bart P. Knijnenburg and A. Duchowski and Sabarish V. Babu},\n booktitle = {ACM Symposium on Applied Perception},\n journal = {ACM Symposium on Applied Perception 2019},\n title = {Empirical Evaluation of the Interplay of Emotion and Visual Attention in Human-Virtual Human Interaction},\n year = {2019}\n}\n'}","[{'paperId': '694bb351897fdeab6a9d830883f7194c8c41e3d6', 'title': 'Investigating the effect of visual realism on empathic responses to emotionally expressive virtual humans'}, {'paperId': '4bc213194416266ad9cfb46eba23901142d46319', 'title': 'An Emotionally Responsive Virtual Parent for Pediatric Nursing Education: A Framework for Multimodal Momentary and Accumulated Interventions'}, {'paperId': '669c3f3e9aaf55cbe1a1e4727e251fe2d89f98c0', 'title': ""Effects of Small Talk With a Crowd of Virtual Humans on Users' Emotional and Behavioral Responses""}, {'paperId': 'fb60f426c8fa7e4fa885c369863a226a3103dcc5', 'title': 'Effects of Language Familiarity in Simulated Natural Dialogue with a Virtual Crowd of Digital Humans on Emotion Contagion in Virtual Reality'}, {'paperId': '813b697c9857e41b5c3c47fa97ac060cb7381e94', 'title': 'Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum'}]",{'name': 'ACM Symposium on Applied Perception 2019'},44.0,Empirical Evaluation of the Interplay of Emotion and Visual Attention in Human-Virtual Human Interaction,2019.0
1036,50fbcfc18723ccebb0c4e1a6e931a341929d4ac8,,"[{'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '34654263', 'name': 'W. C. Ho'}, {'authorId': '30169286', 'name': 'Thurid Vogt'}, {'authorId': '2620317', 'name': 'Nathalie Beeckman'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '1742930', 'name': 'E. André'}]",42.0,"{'bibtex': '@Inproceedings{Dias2007IKW,\n author = {João Dias and W. C. Ho and Thurid Vogt and Nathalie Beeckman and Ana Paiva and E. André},\n pages = {606-617},\n title = {I Know What I Did Last Summer: Autobiographic Memory in Synthetic Characters},\n year = {2007}\n}\n'}",,{'pages': '606-617'},19.0,I Know What I Did Last Summer: Autobiographic Memory in Synthetic Characters,2007.0
1038,51045c5de3f17d4fb4baa9458b10077e9d520d06,,"[{'authorId': '39824100', 'name': 'M. C. Goldberg'}, {'authorId': '39824100', 'name': 'M. C. Goldberg'}, {'authorId': '7517252', 'name': 'S. Mostofsky'}, {'authorId': '7517252', 'name': 'S. Mostofsky'}, {'authorId': '2576395', 'name': 'L. Cutting'}, {'authorId': '2576395', 'name': 'L. Cutting'}, {'authorId': '144783784', 'name': 'E. Mahone'}, {'authorId': '144783784', 'name': 'E. Mahone'}, {'authorId': '4519538', 'name': 'B. Astor'}, {'authorId': '3485134', 'name': 'M. Denckla'}, {'authorId': '3485134', 'name': 'M. Denckla'}, {'authorId': '2074773491', 'name': 'Rebecca Landa'}, {'authorId': '3231968', 'name': 'R. Landa'}]",370.0,"{'bibtex': '@Article{Goldberg2005SubtleEI,\n author = {M. C. Goldberg and M. C. Goldberg and S. Mostofsky and S. Mostofsky and L. Cutting and L. Cutting and E. Mahone and E. Mahone and B. Astor and M. Denckla and M. Denckla and Rebecca Landa and R. Landa},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {279-293},\n title = {Subtle Executive Impairment in Children with Autism and Children with ADHD},\n volume = {35},\n year = {2005}\n}\n'}",,"{'volume': '35', 'pages': '279-293', 'name': 'Journal of Autism and Developmental Disorders'}",59.0,Subtle Executive Impairment in Children with Autism and Children with ADHD,2005.0
1039,5132181bc1a35587f6db6643ae3241ce433d6527,"We used fMRI to study the organization of brain responses to different types of complex visual motion. In a rapid eventrelated design, subjects viewed video clips of humans performing different whole-body motions, video clips of manmade manipulable objects (tools) moving with their characteristic natural motion, point-light displays of human whole-body motion, and point-light displays of manipulable objects. The lateral temporal cortex showed strong responses to both moving videos and moving point-light displays, supporting the hypothesis that the lateral temporal cortex is the cortical locus for processing complex visual motion. Within the lateral temporal cortex, we observed segregated responses to different types of motion. The superior temporal sulcus (STS) responded strongly to human videos and human point-light displays, while the middle temporal gyrus (MTG) and the inferior temporal sulcus responded strongly to tool videos and tool point-light displays. In the ventral temporal cortex, the lateral fusiform responded more to human videos than to any other stimulus category while the medial fusiform preferred tool videos. The relatively weak responses observed to point-light displays in the ventral temporal cortex suggests that form, color, and texture (present in video but not point-light displays) are the main contributors to ventral temporal activity. In contrast, in the lateral temporal cortex, the MTG responded as strongly to point-light displays as to videos, suggesting that motion is the key determinant of response in the MTG. Whereas the STS responded strongly to point-light displays, it showed an even larger response to video displays, suggesting that the STS integrates form, color, and motion information.","[{'authorId': '1928392', 'name': 'M. Beauchamp'}, {'authorId': '49564320', 'name': 'Kathryn E. Lee'}, {'authorId': '2327323', 'name': 'J. Haxby'}, {'authorId': '144404253', 'name': 'Alex Martin'}]",462.0,"{'bibtex': '@Article{Beauchamp2003fMRIRT,\n author = {M. Beauchamp and Kathryn E. Lee and J. Haxby and Alex Martin},\n journal = {Journal of Cognitive Neuroscience},\n pages = {991-1001},\n title = {fMRI Responses to Video and Point-Light Displays of Moving Humans and Manipulable Objects},\n volume = {15},\n year = {2003}\n}\n'}",,"{'volume': '15', 'pages': '991-1001', 'name': 'Journal of Cognitive Neuroscience'}",68.0,fMRI Responses to Video and Point-Light Displays of Moving Humans and Manipulable Objects,2003.0
1040,5132b0463e8d2d7c6e2d775b839bab6282495bae,"Joy or Pain? Face recognition and processing are so completely central to human social interactions that these functions are supported by specialized regions in the brain. One of the fundamental aspects being processed is emotion, particularly whether the emotion being expressed is positive or negative. Nevertheless, neuroimaging studies have documented that perceiving opposite emotions often activates the same or overlapping regions. Aviezer et al. (p. 1225) report that the recognition of positive versus negative emotions actually relies on information communicated by the body—the extent to which perceivers identified joy versus grief in composite figures was driven by whether the body came from a joyous (versus grievous) image rather than the face. The body reveals what the face conceals. The distinction between positive and negative emotions is fundamental in emotion models. Intriguingly, neurobiological work suggests shared mechanisms across positive and negative emotions. We tested whether similar overlap occurs in real-life facial expressions. During peak intensities of emotion, positive and negative situations were successfully discriminated from isolated bodies but not faces. Nevertheless, viewers perceived illusory positivity or negativity in the nondiagnostic faces when seen with bodies. To reveal the underlying mechanisms, we created compounds of intense negative faces combined with positive bodies, and vice versa. Perceived affect and mimicry of the faces shifted systematically as a function of their contextual body emotion. These findings challenge standard models of emotion expression and highlight the role of the body in expressing and perceiving emotions.","[{'authorId': '4387567', 'name': 'Hillel Aviezer'}, {'authorId': '5169679', 'name': 'Y. Trope'}, {'authorId': '145441940', 'name': 'A. Todorov'}]",627.0,"{'bibtex': '@Article{Aviezer2012BodyCN,\n author = {Hillel Aviezer and Y. Trope and A. Todorov},\n journal = {Science},\n pages = {1225 - 1229},\n title = {Body Cues, Not Facial Expressions, Discriminate Between Intense Positive and Negative Emotions},\n volume = {338},\n year = {2012}\n}\n'}",,"{'volume': '338', 'pages': '1225 - 1229', 'name': 'Science'}",34.0,"Body Cues, Not Facial Expressions, Discriminate Between Intense Positive and Negative Emotions",2012.0
1043,513b211c568add795b38f9092a11aa19c582c7f1,"Human eyes convey a remarkable variety of complex social and emotional information. However, it is unknown which physical eye features convey mental states and how that came about. In the current experiments, we tested the hypothesis that the receiver’s perception of mental states is grounded in expressive eye appearance that serves an optical function for the sender. Specifically, opposing features of eye widening versus eye narrowing that regulate sensitivity versus discrimination not only conveyed their associated basic emotions (e.g., fear vs. disgust, respectively) but also conveyed opposing clusters of complex mental states that communicate sensitivity versus discrimination (e.g., awe vs. suspicion). This sensitivity-discrimination dimension accounted for the majority of variance in perceived mental states (61.7%). Further, these eye features remained diagnostic of these complex mental states even in the context of competing information from the lower face. These results demonstrate that how humans read complex mental states may be derived from a basic optical principle of how people see.","[{'authorId': '143876403', 'name': 'D. H. Lee'}, {'authorId': '5040426', 'name': 'A. Anderson'}]",54.0,"{'bibtex': '@Article{Lee2017ReadingWT,\n author = {D. H. Lee and A. Anderson},\n journal = {Psychological Science},\n pages = {494 - 503},\n title = {Reading What the Mind Thinks From How the Eye Sees},\n volume = {28},\n year = {2017}\n}\n'}",,"{'volume': '28', 'pages': '494 - 503', 'name': 'Psychological Science'}",42.0,Reading What the Mind Thinks From How the Eye Sees,2017.0
1044,5176b29039e467eeadbaf3bbf76488a5000c791e,,"[{'authorId': '48521575', 'name': 'YingLiang Ma'}, {'authorId': '47877778', 'name': 'H. Paterson'}, {'authorId': '2819854', 'name': 'F. Pollick'}]",189.0,"{'bibtex': '@Article{Ma2006AMC,\n author = {YingLiang Ma and H. Paterson and F. Pollick},\n journal = {Behavior Research Methods},\n pages = {134-141},\n title = {A motion capture library for the study of identity, gender, and emotion perception from biological motion},\n volume = {38},\n year = {2006}\n}\n'}",,"{'volume': '38', 'pages': '134-141', 'name': 'Behavior Research Methods'}",24.0,"A motion capture library for the study of identity, gender, and emotion perception from biological motion",2006.0
1046,51a9be2996068c689fd5b8cdd487acbbe3fd2b8e,"Progress in computer graaphics over the last decade has ren- dered the creation of believable anthropomorphic graphical avatars pos- sible. Issues in rendering these animated graphical avatars believable and engaging during Human-Computer Interaction still remain. In this arti- cle, we focus on the animation of avatar's facial expressions. We explain how we created our animation on Scherer's theory of emotion gener- ation associated with facial expressions to create five facial expression animations (happiness, disgust, sadness, fear and anger) that are con- gruent with Scherer's theory. We discuss the specific steps and issues we followed as well as the evaluation results of a user study we conducted.","[{'authorId': '2912518', 'name': 'Marco Paleari'}]",26.0,"{'bibtex': '@Inproceedings{Paleari2006PsychologicallyGA,\n author = {Marco Paleari},\n title = {Psychologically grounded avatars expressions},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",10.0,Psychologically grounded avatars expressions,2006.0
1047,51d1b5c2e56711add0cd453e8191ba42c8165ef1,"In the context of the very dynamic and challenging domain of affective computing, we adopt a software engineering point of view on emotion recognition in interactive systems. Our goal is threefold: first, developing an architecture model for emotion recognition. This architecture model emphasizes multimodality and reusability. Second, developing a prototype based on this architecture model. For this prototype we focus on gesturebased emotion recognition. And third, using this prototype for augmenting a ballet dance show.","[{'authorId': '7237380', 'name': 'Alexis Clay'}, {'authorId': '2742240', 'name': 'N. Couture'}, {'authorId': '1807947', 'name': 'L. Nigay'}]",8.0,"{'bibtex': '@Inproceedings{Clay2009TowardsAA,\n author = {Alexis Clay and N. Couture and L. Nigay},\n pages = {19-24},\n title = {Towards an Architecture Model for Emotion Recognition in Interactive Systems: Application to a Ballet Dance Show},\n year = {2009}\n}\n'}",,"{'volume': '', 'pages': '19-24', 'name': ''}",26.0,Towards an Architecture Model for Emotion Recognition in Interactive Systems: Application to a Ballet Dance Show,2009.0
1048,520737ed72f1880747f4a4f0cae535b2be17239d,"This study is to propose an analytical technique and evaluation measures effective for investigations of emotion estimation by using biological information. For conducting investigations to analyze atmosphere and emotions by corresponding biological information with a Russell's circumplex model, any of analytical techniques or evaluation measures are not established and therefore are used with uncertain effectiveness. In this study, we propose a novel technique using vector decomposition, while a reported study proposed a technique using the ratio of distances analysis, for emotion estimation on the basis of 8 emotion classifications by using 2 different types of biological information including an electroencephalogram and pulse rate, in which a point of emotion is classified on a circumplex model corresponding to a two-dimensional coordinate. Also, to validate the effectiveness, we analyzed audience's emotions responded to a show initially motivating us to investigate. We demonstrated that the evaluation measures could be more effective by validating the evaluation.","[{'authorId': '145094033', 'name': 'Midori Sugaya'}, {'authorId': '2071056968', 'name': 'Takuya Hiramatsu'}, {'authorId': '46784763', 'name': 'Reiji Yoshida'}, {'authorId': '144180429', 'name': 'F. Chen'}]",6.0,"{'bibtex': '@Article{Sugaya2018PreliminaryRA,\n author = {Midori Sugaya and Takuya Hiramatsu and Reiji Yoshida and F. Chen},\n journal = {2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},\n pages = {601-605},\n title = {Preliminary Reaction Analysis of Audience with Bio-Emotion Estimation Method},\n volume = {02},\n year = {2018}\n}\n'}",,"{'volume': '02', 'pages': '601-605', 'name': '2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)'}",11.0,Preliminary Reaction Analysis of Audience with Bio-Emotion Estimation Method,2018.0
1049,52464b7c13d33375996971c829d942a615f8e094,,"[{'authorId': '1931731', 'name': 'G. Fagiolo'}, {'authorId': '1782759', 'name': 'C. Birchenhall'}, {'authorId': '1766251', 'name': 'P. Windrum'}]",77.0,"{'bibtex': '@Article{Fagiolo2007EmpiricalVI,\n author = {G. Fagiolo and C. Birchenhall and P. Windrum},\n journal = {Computational Economics},\n pages = {189-194},\n title = {Empirical Validation in Agent-based Models: Introduction to the Special Issue},\n volume = {30},\n year = {2007}\n}\n'}",,"{'volume': '30', 'pages': '189-194', 'name': 'Computational Economics'}",27.0,Empirical Validation in Agent-based Models: Introduction to the Special Issue,2007.0
1050,52876c0d67d7876b4e902d7355b2eadf657e63bc,,"[{'authorId': '1691444', 'name': 'H. Hastie'}, {'authorId': '2508321', 'name': 'K. Lohan'}, {'authorId': '31980290', 'name': 'A. Deshmukh'}, {'authorId': '1954098', 'name': 'F. Broz'}, {'authorId': '1732377', 'name': 'R. Aylett'}]",8.0,"{'bibtex': '@Inproceedings{Hastie2017TheIB,\n author = {H. Hastie and K. Lohan and A. Deshmukh and F. Broz and R. Aylett},\n pages = {64-74},\n title = {The Interaction Between Voice and Appearance in the Embodiment of a Robot Tutor},\n year = {2017}\n}\n'}",,{'pages': '64-74'},28.0,The Interaction Between Voice and Appearance in the Embodiment of a Robot Tutor,2017.0
1051,52dfba1732dbc7644ff50c85bdfa144309cf916c,"E-learning is one of the emerging needs of the information age. Therefore a lot of potential is seen in distance learning development. Virtual environment interface to E-learning systems have recently appeared on the Internet. Using virtual reality environment, the applications appear to be promising to E-learning tasks more nature and interactive. Also using this technology, it is possible to get a sense of three dimensional environments and level of user immersion. Extensible 3D (X3D) is the most common tool for building 3D viewing and browsing of E-learning systems. In this paper the benefits of virtual reality environment using X3D in E-learning applications are demonstrated. Those will be shown via implementing two Web enabled virtual environment E-learning systems. The first one is an on-line virtual chemistry lab system. This application gives the student the ability to perform all experiments in a certain crucial. The second application is an on-line English language education system. This application gives the students the ability to learn the language audile and visual via on line interactive system. X3D is used as the main implementation tool which give the systems users the full visualization and interactivity of all learning steps.","[{'authorId': '150119659', 'name': 'H. Abdul-Kader'}]",41.0,"{'bibtex': '@Article{Abdul-Kader2008ELearningSI,\n author = {H. Abdul-Kader},\n journal = {2008 ITI 6th International Conference on Information & Communications Technology},\n pages = {71-76},\n title = {E-Learning Systems in Virtual Environment},\n year = {2008}\n}\n'}",,"{'pages': '71-76', 'name': '2008 ITI 6th International Conference on Information & Communications Technology'}",18.0,E-Learning Systems in Virtual Environment,2008.0
1052,5300fe7b113721377ae10e68da9cc8b59936f44e,,[],237.0,"{'bibtex': '@Inproceedings{None,\n title = {Cerebral Cortex doi:10.1093/cercor/bhq064 A Role for REM Sleep in Recalibrating the Sensitivity of the Human Brain to Specific Emotions},\n year = {2010}\n}\n'}",,,0.0,Cerebral Cortex doi:10.1093/cercor/bhq064 A Role for REM Sleep in Recalibrating the Sensitivity of the Human Brain to Specific Emotions,2010.0
1053,53020f0473fd052c8e1307907410da878c256756,"THE ACQUISITION OF LEXICAL AND GRAMMATICAL ASPECT. Ping Li and Yasuhiro Shirai. Berlin: Mouton de Gruyter, 2000. Pp. v + 261. $106.00 cloth. This book covers everything related to aspect in the field of language acquisition; I was amazed by how the authors managed to do this in such a clear manner in only 261 pages. The book discusses numerous topics related to the acquisition of aspect: from tense to grammatical to lexical aspect, from nativist to functionalist approaches, from a connectionist model to Universal Grammar, and from first language (L1) to second language (L2). It also covers relevant previous research in the acquisition of tense and aspect in Chinese, English, and Japanese. Although the aim of the book is not an extensive survey of the research done on aspect in all possible languages, given the importance of the work done on aspect in Slavic languages, one might have expected to see more discussion of this area.","[{'authorId': '2264303363', 'name': 'Barbara Seidlhofer'}]",508.0,"{'bibtex': '@Article{Seidlhofer2004THEAO,\n author = {Barbara Seidlhofer},\n journal = {Studies in Second Language Acquisition},\n pages = {469 - 470},\n title = {THE ACQUISITION OF LEXICAL AND GRAMMATICAL ASPECT},\n volume = {26},\n year = {2004}\n}\n'}",,"{'volume': '26', 'pages': '469 - 470', 'name': 'Studies in Second Language Acquisition'}",37.0,THE ACQUISITION OF LEXICAL AND GRAMMATICAL ASPECT,2004.0
1054,5317e548dfbb35fdbecd14779bd85822efc75d75,"Research on interactional feedback has typically focused on feedback learners receive from native speakers (i.e., NS-learner contexts). However, for many second language (L2) learners, the majority of their opportunities to engage in interaction occur with other learners (i.e., learner-learner contexts). The literature has suggested that feedback in learner-learner interaction contexts differs from that found in NS-learner contexts in the quantity of feedback moves (e.g., Mackey, Oliver, & Leeman, 2003), types of feedback used (Pica, Lincoln-Porter, Paninos, & Linnell, 1996), and narrowness of linguistic foci (Toth, 2008). The current study examines how learners provide each other with two types of input-providing feedback, recasts (implicit feedback), and explicit corrections (explicit feedback), in order to investigate how different types of feedback and responses to feedback promote learning of English past tense and locatives. Findings suggest a limited evidence for a relationship between implicit feedback, modified output, and L2 learning, and evidence for a negative effect of explicit corrections from peers. These findings indicate that the role of feedback and modified output in learning may be different in learner-learner interactions than has been found in NS-learner interactions. [ABSTRACT FROM AUTHOR]","[{'authorId': '48460302', 'name': 'R. Adams'}, {'authorId': '1409367152', 'name': 'Ana-Maria Nuevo'}, {'authorId': '92221073', 'name': 'T. Egi'}]",94.0,"{'bibtex': '@Article{Adams2011ExplicitAI,\n author = {R. Adams and Ana-Maria Nuevo and T. Egi},\n journal = {The Modern Language Journal},\n pages = {42-63},\n title = {Explicit and Implicit Feedback, Modified Output, and SLA: Does Explicit and Implicit Feedback Promote Learning and Learner–Learner Interactions?},\n volume = {95},\n year = {2011}\n}\n'}",,"{'volume': '95', 'pages': '42-63', 'name': 'The Modern Language Journal'}",49.0,"Explicit and Implicit Feedback, Modified Output, and SLA: Does Explicit and Implicit Feedback Promote Learning and Learner–Learner Interactions?",2011.0
1055,532ca98132aa867d3429a57e0e223527ca606b21,,"[{'authorId': '1680103', 'name': 'Danny Weyns'}, {'authorId': '3119182', 'name': 'Andrea Omicini'}, {'authorId': '144687425', 'name': 'J. Odell'}]",465.0,"{'bibtex': '@Article{Weyns2007EnvironmentAA,\n author = {Danny Weyns and Andrea Omicini and J. Odell},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {5-30},\n title = {Environment as a first class abstraction in multiagent systems},\n volume = {14},\n year = {2007}\n}\n'}",,"{'volume': '14', 'pages': '5-30', 'name': 'Autonomous Agents and Multi-Agent Systems'}",100.0,Environment as a first class abstraction in multiagent systems,2007.0
1056,5331fc3019f9400f7aba4d2719671d20157e9bd6,,"[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '38811484', 'name': 'N. Yee'}, {'authorId': '39699737', 'name': 'Kayur Patel'}, {'authorId': '40458739', 'name': 'A. Beall'}]",73.0,"{'bibtex': '@Article{Bailenson2008DetectingDC,\n author = {J. Bailenson and N. Yee and Kayur Patel and A. Beall},\n journal = {Comput. Hum. Behav.},\n pages = {66-87},\n title = {Detecting digital chameleons},\n volume = {24},\n year = {2008}\n}\n'}",,"{'volume': '24', 'pages': '66-87', 'name': 'Comput. Hum. Behav.'}",54.0,Detecting digital chameleons,2008.0
1057,536b6852f110559b899639f13f4d17b10e20fe0c,,"[{'authorId': '2000386602', 'name': 'Mei-Ju Chen'}, {'authorId': '2956666', 'name': 'C. Farn'}]",46.0,"{'bibtex': '@Article{Chen2020ExaminingTI,\n author = {Mei-Ju Chen and C. Farn},\n journal = {Inf. Process. Manag.},\n pages = {102266},\n title = {Examining the Influence of Emotional Expressions in Online Consumer Reviews on Perceived Helpfulness},\n volume = {57},\n year = {2020}\n}\n'}",,"{'volume': '57', 'pages': '102266', 'name': 'Inf. Process. Manag.'}",60.0,Examining the Influence of Emotional Expressions in Online Consumer Reviews on Perceived Helpfulness,2020.0
1058,5386be85c00ed356dd149489309686ddd68f307b,"This chapter examines the effects of task complexity [± reasoning demands] on modified output and the relationship between output modifications and L2 development. Seventy-nine adult English as a Second Language learners were divided into two groups: (1) low reasoning demands; and (2) high reasoning demands; and engaged in two sets of tasks which targeted English past tense and locative prepositions. While learners modified their output using a variety of modification moves, learners who completed high complexity tasks produced more self-repair than those who completed the low complexity ones. Self-repair was related to learning locatives for the high complexity group only as measured by delayed grammaticality judgment and oral post-tests. Pushed output, as well as the total amount of modified output, was related to learning past tense for the low complexity group only as measured by delayed grammaticality judgment post-tests. These findings are discussed in light of the Cognition Hypothesis and show an intricate pattern among level of task complexity, type of target structure, type of modified output, and learning.","[{'authorId': '1409367152', 'name': 'Ana-Maria Nuevo'}, {'authorId': '48460302', 'name': 'R. Adams'}, {'authorId': '1410826438', 'name': 'Lauren Ross-Feldman'}]",15.0,"{'bibtex': '@Inproceedings{Nuevo2011Chapter7T,\n author = {Ana-Maria Nuevo and R. Adams and Lauren Ross-Feldman},\n pages = {175-202},\n title = {Chapter 7. Task complexity, modified output, and L2 development in learner–learner interaction},\n year = {2011}\n}\n'}",,"{'volume': '', 'pages': '175-202', 'name': ''}",0.0,"Chapter 7. Task complexity, modified output, and L2 development in learner–learner interaction",2011.0
1059,538f7c7fe733e80741e8588934feb1837b31587a,"Autism spectrum disorders are characterized by core deficits with regard to three domains, i.e. social interaction, communication and repetitive or stereotypic behaviour. It is crucial to develop intervention strategies helping individuals with autism, their caregivers and educators in daily life. For this purpose, virtual reality (VR), i.e. a simulation of the real world based on computer graphics, can be useful as it allows instructors and therapists to offer a safe, repeatable and diversifiable environment during learning. This mini review examines studies that have investigated the use of VR in autism.","[{'authorId': '2069094', 'name': 'M. Bellani'}, {'authorId': '20890019', 'name': 'L. Fornasari'}, {'authorId': '1692716', 'name': 'L. Chittaro'}, {'authorId': '3110755', 'name': 'P. Brambilla'}]",198.0,"{'bibtex': '@Article{Bellani2011VirtualRI,\n author = {M. Bellani and L. Fornasari and L. Chittaro and P. Brambilla},\n journal = {Epidemiology and Psychiatric Sciences},\n pages = {235 - 238},\n title = {Virtual reality in autism: state of the art},\n volume = {20},\n year = {2011}\n}\n'}",,"{'volume': '20', 'pages': '235 - 238', 'name': 'Epidemiology and Psychiatric Sciences'}",26.0,Virtual reality in autism: state of the art,2011.0
1060,539bb58cd1060c1fa222b77ad2c168e1ff78aa83,"Simulating the motion of realistic, large, dense crowds of autonomous agents is still a challenge for the computer graphics community. Typical approaches either resemble particle simulations (where agents lack orientation controls) or are conservative in the range of human motion possible (agents lack psychological state and aren't allowed to 'push' each other). Our HiDAC system (for High-Density Autonomous Crowds) focuses on the problem of simulating the local motion and global wayfinding behaviors of crowds moving in a natural manner within dynamically changing virtual environments. By applying a combination of psychological and geometrical rules with a social and physical forces model, HiDAC exhibits a wide variety of emergent behaviors from agent line formation to pushing behavior and its consequences; relative to the current situation, personalities of the individuals and perceived social density.","[{'authorId': '48351473', 'name': 'Dimitris N. Metaxas'}, {'authorId': '145492783', 'name': 'J. Popović'}, {'authorId': '1746484', 'name': 'N. Pelechano'}, {'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '2196059848', 'name': 'N. I. Badler'}]",594.0,"{'bibtex': '@Inproceedings{Metaxas2007ControllingIA,\n author = {Dimitris N. Metaxas and J. Popović and N. Pelechano and J. Allbeck and N. I. Badler},\n pages = {99-108},\n title = {Controlling individual agents in high-density crowd simulation},\n year = {2007}\n}\n'}",,{'pages': '99-108'},35.0,Controlling individual agents in high-density crowd simulation,2007.0
1061,539ec2b760c656a153b252225cbe5920f6ca8a50,"Human perceptual studies have shown that facial characteristics affect judgments about the personality of a person. For example, larger facial width has been associated with judgments of aggressiveness, dominance, and untrustworthiness. Previous studies of virtual faces have not been able to reflect the same perceptual rules, but have used characters with unrealistic feature sizes or highly abstract characters. For this study, we created virtual characters with realistic feature dimensions and investigated the effects of facial width and eye size on personality perception. Our results indicate that virtual characters may indeed follow different perceptual rules for facial width, and care must be taken when manipulating eye size. These findings are useful for effective character design for video games, movies, and embodied virtual agents.","[{'authorId': '3430725', 'name': 'Ylva Ferstl'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",19.0,"{'bibtex': '@Article{Ferstl2018APS,\n author = {Ylva Ferstl and R. Mcdonnell},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {A perceptual study on the manipulation of facial features for trait portrayal in virtual agents},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},27.0,A perceptual study on the manipulation of facial features for trait portrayal in virtual agents,2018.0
1062,53c27bdbd7d6c13684e2a1e20a4b13c1a1afd283,,"[{'authorId': '31450532', 'name': 'B. Greenspan'}, {'authorId': '114946010', 'name': 'J. Bowlby'}]",170.0,"{'bibtex': '@Article{Greenspan1974SeparationAA,\n author = {B. Greenspan and J. Bowlby},\n journal = {The Family Coordinator},\n pages = {428},\n title = {Separation: Anxiety and Anger (Attachment and Loss--Volume II)},\n volume = {23},\n year = {1974}\n}\n'}",,"{'volume': '23', 'pages': '428', 'name': 'The Family Coordinator'}",0.0,Separation: Anxiety and Anger (Attachment and Loss--Volume II),1974.0
1063,53ca6eb4330cfd2cbda76febef3e82864d2250f1,"Theorists have proposed that men and women and those in various occupational groups should differ in their susceptibility to primitive emotional contagion. Study 1 was designed to explore the extent to which gender and occupation affected respondents’ self-reports of emotional contagion, as measured by the Emotional Contagion (EC) scale. As predicted, women in a variety of occupations secured higher total EC scores than did men. Study 2 was designed to determine the extent to which gender affected self-reports of emotional contagion (again as measured by the EC scale) and actual responsiveness to others’ emotions. As predicted, women received higher EC scores, reported sharing the targets’ emotions to a greater extent, and were rated by judges as displaying more emotional contagion than did men.","[{'authorId': '2071426439', 'name': 'William Doherty'}, {'authorId': '2260244028', 'name': 'Lisa Orirnoto'}]",1598.0,"{'bibtex': '@Article{Doherty1995EmotionalC,\n author = {William Doherty and Lisa Orirnoto},\n journal = {Psychology of Women Quarterly},\n pages = {355 - 371},\n title = {Emotional Contagion},\n volume = {19},\n year = {1995}\n}\n'}",,"{'volume': '19', 'pages': '355 - 371', 'name': 'Psychology of Women Quarterly'}",23.0,Emotional Contagion,1995.0
1065,53e66b6934516a9859573f4866f81f04bce977ae,,"[{'authorId': '1723344', 'name': 'M. Corbetta'}, {'authorId': '39269549', 'name': 'G. Shulman'}]",11074.0,"{'bibtex': '@Article{Corbetta2002ControlOG,\n author = {M. Corbetta and G. Shulman},\n journal = {Nature Reviews Neuroscience},\n pages = {201-215},\n title = {Control of goal-directed and stimulus-driven attention in the brain},\n volume = {3},\n year = {2002}\n}\n'}",,"{'volume': '3', 'pages': '201-215', 'name': 'Nature Reviews Neuroscience'}",129.0,Control of goal-directed and stimulus-driven attention in the brain,2002.0
1066,53ff0f5d9f2ecf1654cafb2ec204521489ca808b,,"[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",4.0,"{'bibtex': '@Article{Hudlicka2018ModelingCI,\n author = {E. Hudlicka},\n booktitle = {Intelligent Systems, Control and Automation: Science and Engineering},\n journal = {Intelligent Systems, Control and Automation: Science and Engineering},\n title = {Modeling Cognition–Emotion Interactions in Symbolic Agent Architectures: Examples of Research and Applied Models},\n year = {2018}\n}\n'}","[{'paperId': '2fdb2aaaeb0e11a36715c8cc4ee41f90478aeb95', 'title': 'Towards the construction of computational models of emotions from the perspective of a software system'}, {'paperId': '0bef65aafa4310e14704273470b12b79249ca9d7', 'title': 'Metacognitive Agents for Ethical Decision Support: Conceptual Model and Research Roadmap'}, {'paperId': 'd6d242b7a561c7729ee4b9ce144aa82e35ed1cd9', 'title': 'What Lies Beneath—A Survey of Affective Theory Use in Computational Models of Emotion'}, {'paperId': 'a7b8fc312e836cab2f690c1d3736ff610234516d', 'title': 'Was ist KI und was nicht'}]","{'name': 'Intelligent Systems, Control and Automation: Science and Engineering'}",24.0,Modeling Cognition–Emotion Interactions in Symbolic Agent Architectures: Examples of Research and Applied Models,2018.0
1067,5408534d9996f0a867d8ad03a45891dd73d80833,"It was hypothesized that females respond more negatively than males to sideby-side invasions of personal space, while males respond more negatively than females to face-to-face invasions. In an experimental field study, 62 males and 63 females were ""invaded"" by a male or female confederate in one of three spatial positions. Regardless of the invader's sex, a consistent pattern of interactions was observed such that affect (p ==.05), environmental perceptions (p < .004), and attributions of intent (p < .005) were negative for males when a stranger sat across from them and negative for females when a stranger sat adjacent to them. In an observational field study of 66 students, solitary males in a library were observed to erect barriers against face-to-face invasions, and females were found to erect barriers against adjacent invasions (p < .02).","[{'authorId': '47015634', 'name': 'J. Fisher'}, {'authorId': '65874924', 'name': 'D. Byrne'}]",115.0,"{'bibtex': '@Article{Fisher1975TooCF,\n author = {J. Fisher and D. Byrne},\n journal = {Journal of Personality and Social Psychology},\n pages = {15-21},\n title = {Too close for comfort : Sex differences in response to invasions of personal space},\n volume = {32},\n year = {1975}\n}\n'}",,"{'volume': '32', 'pages': '15-21', 'name': 'Journal of Personality and Social Psychology'}",27.0,Too close for comfort : Sex differences in response to invasions of personal space,1975.0
1068,541da799bdaeef4a0ee2b8f0e4e7e2ce74f0d762,"I present an overview of emotion theories, organised around the question of emotion causation. I argue that theories of emotion causation should ideally address the problems of elicitation, intensity, and differentiation. Each of these problems can be divided into a subquestion that asks about the relation between stimuli and emotions (i.e., the functional level of process description, cf. Marr, 1982) and a subquestion that asks about the mechanism and representations that intervene (i.e., the algorithmic level of process description). The overview reveals that theories of emotion causation sometimes differ with regard to the kind of process that they hold responsible for emotion causation. More precisely, they hold different assumptions regarding the conditions under which the process is supposed to operate (optimal versus suboptimal), the format of the representations involved (propositional versus perceptual), and the object or input of the central process (stimulus versus responses/experience). Further, the overview reveals that theories of emotion causation sometimes differ with regard to the level of process description that they focus on. Finally, the overview brings to light several similarities among the theories discussed.","[{'authorId': '2064945', 'name': 'A. Moors'}]",316.0,"{'bibtex': '@Article{Moors2009TheoriesOE,\n author = {A. Moors},\n journal = {Cognition and Emotion},\n pages = {625 - 662},\n title = {Theories of emotion causation: A review},\n volume = {23},\n year = {2009}\n}\n'}",,"{'volume': '23', 'pages': '625 - 662', 'name': 'Cognition and Emotion'}",133.0,Theories of emotion causation: A review,2009.0
1069,5424c6b055b923b7b617e0d1a3faa1bb4fdb20fd,"Copyright It is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s) and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an open content licence (like Creative Commons). UvA-DARE is a service provided by the library of the University of Amsterdam (http://dare.uva.nl) In this paper we sketch a new framework for affect elicitation, which is based on previous evolutionary and connectionist modeling and experimental work from our group. Affective monitoring is considered a local match–mismatch process within a module of the neural network. Negative affect is raised instantly by mismatches, incongruency, disfluency, novelty, incoherence, and dissonance, whereas positive affect follows from matches, con-gruency, fluency, familiarity, coherence, and resonance, at least when an initial mismatch can be solved quickly. Affective monitoring is considered an evolutionary-early conflict and change detection process operating at the same level as, for instance, attentional selection. It runs in parallel and imparts affective flavor to emotional behavior systems, which involve evolutionary-prepared stimuli and action tendencies related to for instance defensive, exploratory, attachment, or appetitive behavior. Positive affect is represented in the networks by high-frequency oscillations, presumably in the gamma band. Negative affect corresponds to more incoherent lower-frequency oscillations, presumably in the theta band. For affect to become conscious, large-scale synchronization of the oscillations over the network and the construction of emotional experiences are required. These constructions involve perceptions of bodily states and action tendencies, but also appraisals as well as efforts to regulate the emotion. Importantly, affective monitoring accompanies every kind of information processing, but conscious emotions, which result from the later integration of affect in a cognitive context, are much rarer events.","[{'authorId': '2545816', 'name': 'R. Phaf'}, {'authorId': '2241940018', 'name': 'R. Hans Phaf'}, {'authorId': '4837582', 'name': 'M. Rotteveel'}, {'authorId': '2241939820', 'name': 'Eddie Harmon-Jones'}]",19.0,"{'bibtex': '@Misc{None,\n author = {R. Phaf and R. Hans Phaf and M. Rotteveel and Eddie Harmon-Jones},\n title = {Source (or Part of the following Source): Type Article Title Affective Monitoring: a Generic Mechanism for Affect Elicitation Author(s) Hypothesis and Theory Article Affective Monitoring: a Generic Mechanism for Affect Elicitation}\n}\n'}",,,124.0,Source (or Part of the following Source): Type Article Title Affective Monitoring: a Generic Mechanism for Affect Elicitation Author(s) Hypothesis and Theory Article Affective Monitoring: a Generic Mechanism for Affect Elicitation,
1070,5437c4dc3ce6e571817481cdd6cb8d2c7afb785e,"Attending where others gaze is one of the most fundamental mechanisms of social cognition. The present study is the first to examine the impact of the attribution of mind to others on gaze-guided attentional orienting and its ERP correlates. Using a paradigm in which attention was guided to a location by the gaze of a centrally presented face, we manipulated participants' beliefs about the gazer: gaze behavior was believed to result either from operations of a mind or from a machine. In Experiment 1, beliefs were manipulated by cue identity (human or robot), while in Experiment 2, cue identity (robot) remained identical across conditions and beliefs were manipulated solely via instruction, which was irrelevant to the task. ERP results and behavior showed that participants' attention was guided by gaze only when gaze was believed to be controlled by a human. Specifically, the P1 was more enhanced for validly, relative to invalidly, cued targets only when participants believed the gaze behavior was the result of a mind, rather than of a machine. This shows that sensory gain control can be influenced by higher-order (task-irrelevant) beliefs about the observed scene. We propose a new interdisciplinary model of social attention, which integrates ideas from cognitive and social neuroscience, as well as philosophy in order to provide a framework for understanding a crucial aspect of how humans' beliefs about the observed scene influence sensory processing.","[{'authorId': '2351204', 'name': 'A. Wykowska'}, {'authorId': '2658383', 'name': 'E. Wiese'}, {'authorId': '2079343109', 'name': 'A. Prosser'}, {'authorId': '2093435673', 'name': 'H. Müller'}]",128.0,"{'bibtex': '@Article{Wykowska2014BeliefsAT,\n author = {A. Wykowska and E. Wiese and A. Prosser and H. Müller},\n journal = {PLoS ONE},\n title = {Beliefs about the Minds of Others Influence How We Process Sensory Information},\n volume = {9},\n year = {2014}\n}\n'}",,"{'volume': '9', 'name': 'PLoS ONE'}",53.0,Beliefs about the Minds of Others Influence How We Process Sensory Information,2014.0
1071,54413f5265f7b6e8dc19858549e7d4d96f1a7eb1,,"[{'authorId': '1684753', 'name': 'Hung-Hsuan Huang'}, {'authorId': '50166441', 'name': 'Y. Ida'}, {'authorId': '2116781460', 'name': 'Kohei Yamaguchi'}, {'authorId': '2865341', 'name': 'K. Kawagoe'}]",5.0,"{'bibtex': '@Inproceedings{Huang2016DevelopmentOA,\n author = {Hung-Hsuan Huang and Y. Ida and Kohei Yamaguchi and K. Kawagoe},\n pages = {489-493},\n title = {Development of a Virtual Classroom for High School Teacher Training},\n year = {2016}\n}\n'}",,{'pages': '489-493'},5.0,Development of a Virtual Classroom for High School Teacher Training,2016.0
1072,54787e19899972f47f154f5c63254144e20edee8,,"[{'authorId': '144978737', 'name': 'A. Dix'}]",2341.0,"{'bibtex': '@Inproceedings{Dix1993HumanComputerI,\n author = {A. Dix},\n pages = {1327-1331},\n title = {Human-Computer Interaction},\n year = {1993}\n}\n'}",,{'pages': '1327-1331'},325.0,Human-Computer Interaction,1993.0
1073,547d72c2ba516a4219ed941a1e40ce5f7ec600d8,Simulating mood within a decision making process has been shown to allow cooperation to occur within the Prisoner’s Dilemma. In this paper we propose how to integrate a mood model into the classica...,"[{'authorId': '3418996', 'name': 'Joe Collenette'}, {'authorId': '144689159', 'name': 'Katie Atkinson'}, {'authorId': '2539968', 'name': 'D. Bloembergen'}, {'authorId': '2274623', 'name': 'K. Tuyls'}]",4.0,"{'bibtex': '@Inproceedings{Collenette2017MoodMW,\n author = {Joe Collenette and Katie Atkinson and D. Bloembergen and K. Tuyls},\n pages = {106-113},\n title = {Mood modelling within reinforcement learning},\n year = {2017}\n}\n'}",,{'pages': '106-113'},32.0,Mood modelling within reinforcement learning,2017.0
1074,548c2fae5e76d5f99f8432a4f9a2990a4237de00,,"[{'authorId': '36136195', 'name': 'J. Laird'}, {'authorId': '118401445', 'name': 'C. Bresler'}]",109.0,"{'bibtex': '@Inproceedings{Laird1992ThePO,\n author = {J. Laird and C. Bresler},\n title = {The process of emotional experience: A self-perception theory.},\n year = {1992}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The process of emotional experience: A self-perception theory.,1992.0
1075,54a5fc0cdf5d252f53a05e3ec07477a4774c41f5,,"[{'authorId': '80264274', 'name': 'Ö. Yalçın'}]",33.0,"{'bibtex': '@Article{Yalçın2020EmpathyFF,\n author = {Ö. Yalçın},\n journal = {Cognitive Systems Research},\n pages = {123-132},\n title = {Empathy framework for embodied conversational agents},\n volume = {59},\n year = {2020}\n}\n'}",,"{'volume': '59', 'pages': '123-132', 'name': 'Cognitive Systems Research'}",49.0,Empathy framework for embodied conversational agents,2020.0
1076,54cc1f2e86d1913521b466cef19d72ed02b6c800,,"[{'authorId': '2258460322', 'name': 'Priyanka Shahi'}, {'authorId': '51951956', 'name': 'Serguei Loukianiouk'}, {'authorId': '2258458990', 'name': 'Andreas Bohne-Lang'}, {'authorId': '34263049', 'name': 'M. Kenzelmann'}, {'authorId': '2258458988', 'name': 'Stefan Küffer'}, {'authorId': '2258459103', 'name': 'S. Maertens'}, {'authorId': '2257284464', 'name': 'Roland Eils'}, {'authorId': '2244235521', 'name': 'Hermann-Josef Gröne'}, {'authorId': '2244740436', 'name': 'Norbert Gretz'}, {'authorId': '2254975286', 'name': 'Benedikt Brors'}]",50229.0,"{'bibtex': '@Article{Shahi2005ArgonauteaDF,\n author = {Priyanka Shahi and Serguei Loukianiouk and Andreas Bohne-Lang and M. Kenzelmann and Stefan Küffer and S. Maertens and Roland Eils and Hermann-Josef Gröne and Norbert Gretz and Benedikt Brors},\n journal = {Nucleic Acids Research},\n pages = {D115 - D118},\n title = {Argonaute—a database for gene regulation by mammalian microRNAs},\n volume = {34},\n year = {2005}\n}\n'}",,"{'volume': '34', 'pages': 'D115 - D118', 'name': 'Nucleic Acids Research'}",55.0,Argonaute—a database for gene regulation by mammalian microRNAs,2005.0
1077,55216d13302c27514e987ca657e30bd3e5fcc841,"We present a preliminary definition and theory of artificial emotion viewed as a sequential process comprising the appraisal of the agent global state, the generation of an emotion-signal, and an emotion-response. This theory distinguishes cognitive from affective appraisal on an architecture-grounded basis. Affective appraisal is performed by the affective component of the architecture; cognitive appraisal is performed by its cognitive component. A scheme for emotion classification with seven dimensions is presented. Among them, we emphasize the roles played by emotions and the way these roles are fulfilled. It is shown how emotions are generated, represented, and used in the Salt & Pepper architecture for autonomous agents (Botelho, 1997). Salt & Pepper is a specific architecture comprising an affective engine, a cognitive and behavioral engine, and an interruption manager. Most properties of the cognitive and behavioral engine rely upon a hybrid associative, schema-based long-term memory. In Salt & Pep...","[{'authorId': '2509141', 'name': 'L. Botelho'}, {'authorId': '144739158', 'name': 'H. Coelho'}]",36.0,"{'bibtex': '@Article{Botelho2001MachineryFA,\n author = {L. Botelho and H. Coelho},\n journal = {Cybern. Syst.},\n pages = {465-506},\n title = {Machinery for Artificial Emotions},\n volume = {32},\n year = {2001}\n}\n'}",,"{'volume': '32', 'pages': '465-506', 'name': 'Cybern. Syst.'}",40.0,Machinery for Artificial Emotions,2001.0
1079,552e996936492cb482cddf47441b6beee0942df4,"From the Publisher: 
Emotions have been much studied and discussed in recent years. Most books, however, treat only one aspect of emotions, such as emotions and the brain, emotions and well-being, or emotions and computer agents. This interdisciplinary book presents recent work on emotions in neuroscience, cognitive science, philosophy, computer science, artificial intelligence, and software and game development. The book discusses the components of human emotion and how they might be incorporated into machines, whether artificial agents should convey emotional responses to human users and how such responses could be made believable, and whether agents should accept and interpret the emotions of users without displaying emotions of their own. It also covers the evolution and brain architecture of emotions, offers vocabularies and classifications for defining emotions, and examines emotions in relation to machines, games, virtual worlds, and music.","[{'authorId': '1691580', 'name': 'R. Trappl'}, {'authorId': '2117794', 'name': 'Sabine Payr'}, {'authorId': '1764052', 'name': 'P. Petta'}]",208.0,"{'bibtex': '@Inproceedings{Trappl2003EmotionsIH,\n author = {R. Trappl and Sabine Payr and P. Petta},\n title = {Emotions in Humans and Artifacts},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotions in Humans and Artifacts,2003.0
1080,554745c7792d5d34dc86a7556a3de61fe338091d,"Most academics agree that emotions and moods are related but distinct phenomena. The present study assessed emotion-mood distinctions among a non-academic population and compared these views with distinctions proposed in the literature. Content analysis of responses from 106 participants identified 16 themes, with cause (65% of respondents), duration (40%), control (25%), experience (15%), and consequences (14%) the most frequently cited distinctions. Among 65 contributions to the academic literature, eight themes were proposed, with duration (62% of authors), intentionality (41%), cause (31percnt;), consequences (31%), and function (18%) the most frequently cited. When the eight themes cited by both academics and non-academics were rank ordered, approximately 60% overlap in opinion was evident. A data-derived summary of emotion-mood distinctions is provided. These data should prove useful to investigators interested in developing a clearer scientific distinction between emotion and mood than is currently available.","[{'authorId': '40390681', 'name': 'Chris Beedie'}, {'authorId': '3341881', 'name': 'P. Terry'}, {'authorId': '2676254', 'name': 'A. Lane'}]",480.0,"{'bibtex': '@Article{Beedie2005DistinctionsBE,\n author = {Chris Beedie and P. Terry and A. Lane},\n journal = {Cognition and Emotion},\n pages = {847 - 878},\n title = {Distinctions between emotion and mood},\n volume = {19},\n year = {2005}\n}\n'}",,"{'volume': '19', 'pages': '847 - 878', 'name': 'Cognition and Emotion'}",102.0,Distinctions between emotion and mood,2005.0
1081,554f93785da4640eb9583a92218253b0dc0de56f,,"[{'authorId': '145616714', 'name': 'Michael Kipp'}, {'authorId': '2812935', 'name': 'A. Héloir'}, {'authorId': '144951065', 'name': 'M. Schröder'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}]",30.0,"{'bibtex': '@Inproceedings{Kipp2010RealizingMB,\n author = {Michael Kipp and A. Héloir and M. Schröder and Patrick Gebhard},\n pages = {57-63},\n title = {Realizing Multimodal Behavior - Closing the Gap between Behavior Planning and Embodied Agent Presentation},\n year = {2010}\n}\n'}",,{'pages': '57-63'},11.0,Realizing Multimodal Behavior - Closing the Gap between Behavior Planning and Embodied Agent Presentation,2010.0
1082,557837599c2199a35457a5ab02e696f98ef03522,,"[{'authorId': '145056721', 'name': 'E. Moser'}, {'authorId': '2643854', 'name': 'B. Derntl'}, {'authorId': '145455683', 'name': 'S. Robinson'}, {'authorId': '4166802', 'name': 'B. Fink'}, {'authorId': '144762538', 'name': 'R. Gur'}, {'authorId': '1850299', 'name': 'K. Grammer'}]",139.0,"{'bibtex': '@Article{Moser2007AmygdalaAA,\n author = {E. Moser and B. Derntl and S. Robinson and B. Fink and R. Gur and K. Grammer},\n journal = {Journal of Neuroscience Methods},\n pages = {126-133},\n title = {Amygdala activation at 3T in response to human and avatar facial expressions of emotions},\n volume = {161},\n year = {2007}\n}\n'}",,"{'volume': '161', 'pages': '126-133', 'name': 'Journal of Neuroscience Methods'}",51.0,Amygdala activation at 3T in response to human and avatar facial expressions of emotions,2007.0
1083,557b0d6b7ae7f9d046bd8914b354628cd1c9c20b,"Appraisal processes provide an affective assessment of an agent’s current situation, in light of its needs and goals. This paper describes a computational model of the appraisal process, implemented within the broader context of a cognitive agent architecture. A particular focus here is on modeling the interacting influences of states and traits on perception and cognition, including their effects on the appraisal process itself. These effects are modeled by manipulating a series of architecture parameters, such as the speed and processing capacity of the individual modules. The paper presents results of an evaluation experiment modeling the behavior of three types of agents: ‘normal’, ‘anxious’, and ‘aggressive’. The appraisal model generated different affective appraisals of the same set of external circumstances for the different agent types, resulting in distinct emotions, and eventually leading to observable differences in behavior. The paper concludes with a brief discussion of some of the issues encountered during the appraisal model development.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",27.0,"{'bibtex': '@Inproceedings{Hudlicka2004TwoSO,\n author = {E. Hudlicka},\n title = {Two Sides of Appraisal: Implementing Appraisal and Its Consequences within a Cognitive Architecture},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",27.0,Two Sides of Appraisal: Implementing Appraisal and Its Consequences within a Cognitive Architecture,2004.0
1084,55c6ef565aa51e37eea2c536f104b962af54aa1c,An efficient method for rendering animated wrinkles on a human face is presented. This method allows an animator to independently blend multiple wrinkle maps across multiple regions of a textured mesh such as the female character shown in Figure 1. This method is both efficient in terms of computation as well as storage costs and is easily implemented in a real-time application using modern programmable graphics processors.,"[{'authorId': '2355579', 'name': 'Christopher Oat'}]",39.0,"{'bibtex': '@Article{Oat2007AnimatedWM,\n author = {Christopher Oat},\n journal = {ACM SIGGRAPH 2007 courses},\n title = {Animated wrinkle maps},\n year = {2007}\n}\n'}",,{'name': 'ACM SIGGRAPH 2007 courses'},2.0,Animated wrinkle maps,2007.0
1085,55d94bb8c8bb01169c3432ff1c1ffe680c0b3879,"It is important for robot designers to know how to make robots that interact effectively with humans. One key dimension is robot appearance and in particular how humanlike the robot should be. Uncanny Valley theory suggests that robots look uncanny when their appearance approaches, but is not absolutely, human. An underlying mechanism may be that appearance affects users’ perceptions of the robot’s personality and mind. This study aimed to investigate how robot facial appearance affected perceptions of the robot’s mind, personality and eeriness. A repeated measures experiment was conducted. 30 participants (14 females and 16 males, mean age 22.5 years) interacted with a Peoplebot healthcare robot under three conditions in a randomized order: the robot had either a humanlike face, silver face, or no-face on its display screen. Each time, the robot assisted the participant to take his/her blood pressure. Participants rated the robot’s mind, personality, and eeriness in each condition. The robot with the humanlike face display was most preferred, rated as having most mind, being most humanlike, alive, sociable and amiable. The robot with the silver face display was least preferred, rated most eerie, moderate in mind, humanlikeness and amiability. The robot with the no-face display was rated least sociable and amiable. There was no difference in blood pressure readings between the robots with different face displays. Higher ratings of eeriness were related to impressions of the robot with the humanlike face display being less amiable, less sociable and less trustworthy. These results suggest that the more humanlike a healthcare robot’s face display is, the more people attribute mind and positive personality characteristics to it. Eeriness was related to negative impressions of the robot’s personality. Designers should be aware that the face on a robot’s display screen can affect both the perceived mind and personality of the robot.","[{'authorId': '145095677', 'name': 'E. Broadbent'}, {'authorId': '2157710657', 'name': 'Vinayak Kumar'}, {'authorId': '2155446048', 'name': 'Xingyan Li'}, {'authorId': '2095702731', 'name': 'J. Sollers'}, {'authorId': '2150937', 'name': 'Rebecca Q. Stafford'}, {'authorId': '145285619', 'name': 'B. MacDonald'}, {'authorId': '1810430', 'name': 'D. Wegner'}]",177.0,"{'bibtex': '@Article{Broadbent2013RobotsWD,\n author = {E. Broadbent and Vinayak Kumar and Xingyan Li and J. Sollers and Rebecca Q. Stafford and B. MacDonald and D. Wegner},\n journal = {PLoS ONE},\n title = {Robots with Display Screens: A Robot with a More Humanlike Face Display Is Perceived To Have More Mind and a Better Personality},\n volume = {8},\n year = {2013}\n}\n'}",,"{'volume': '8', 'name': 'PLoS ONE'}",45.0,Robots with Display Screens: A Robot with a More Humanlike Face Display Is Perceived To Have More Mind and a Better Personality,2013.0
1086,55dfc09b17523d85c74c0206e604c837a3049698,"We propose a data-driven approach for tuning, validating and optimizing crowd simulations by learning parameters from real-life videos. We discuss the common traits of incidents and their video footages suitable for the learning step. We then demonstrate the learning process in three real-life incidents: a bombing attack, a panic situation on the subway and a Black Friday rush. We reanimate the incidents using an existing emotion contagion and crowd simulation framework and optimize the parameters that characterize agent behavior with respect to the data extracted from the video footages of the incidents. © 2018 Elsevier Ltd. All rights reserved.","[{'authorId': '2059121007', 'name': 'A. Eren'}, {'authorId': '2084226639', 'name': 'ğur Güdükbay'}, {'authorId': '2643744', 'name': 'Funda Durupinar'}]",6.0,"{'bibtex': '@Inproceedings{Eren2018UsingRL,\n author = {A. Eren and ğur Güdükbay and Funda Durupinar},\n title = {Using real life incidents for realistic virtual crowds with data-driven emotion contagion ✩},\n year = {2018}\n}\n'}","[{'paperId': '10a44bad05613353bcd03c8b6670591403945094', 'title': 'Crowd Evacuation Simulation Research Based on Improved Reciprocal Velocity Obstacles (RVO) Model with Path Planning and Emotion Contagion'}, {'paperId': '02ac271b76f5ce8a949163382b296ab5ce05dea5', 'title': 'A history of crowd simulation: the past, evolution, and new perspectives'}, {'paperId': 'a6077fa80467e741bdbbfdcc29003aeefe174179', 'title': 'Modeling crowd emotion from emergent event video'}, {'paperId': 'c24c1d970f8cddd51803ae030609ec4042dc80d5', 'title': 'A personalized traffic simulation integrating emotion using a driving simulator'}, {'paperId': 'cdcee1355981de99b93608ab61b09a249da728b6', 'title': 'A perception‐based emotion contagion model in crowd emergent evacuation simulation'}, {'paperId': '2e1ccf630e2c114c9ca3abe268c945a87b2936d3', 'title': 'A unified crowd simulation model revealing relationships among ""Physiology-Psychology-Physics"" factors'}]",,0.0,Using real life incidents for realistic virtual crowds with data-driven emotion contagion ✩,2018.0
1087,55f1773c1c806054dba0aa3de1b19e1621f6255e,,"[{'authorId': '51499868', 'name': 'Xiumin Shang'}, {'authorId': '1682684', 'name': 'Marcelo Kallmann'}, {'authorId': '145888782', 'name': 'A. Arif'}]",4.0,"{'bibtex': '@Article{Shang2019EffectsOV,\n author = {Xiumin Shang and Marcelo Kallmann and A. Arif},\n journal = {Lecture Notes in Networks and Systems},\n title = {Effects of Virtual Agent Gender on User Performance and Preference in a VR Training Program},\n year = {2019}\n}\n'}",,{'name': 'Lecture Notes in Networks and Systems'},27.0,Effects of Virtual Agent Gender on User Performance and Preference in a VR Training Program,2019.0
1088,563e821bb5ea825efb56b77484f5287f08cf3753,,"[{'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}]",5102.0,"{'bibtex': '@Inproceedings{LeCun1998ConvolutionalNF,\n author = {Yann LeCun and Yoshua Bengio},\n pages = {255-258},\n title = {Convolutional networks for images, speech, and time series},\n year = {1998}\n}\n'}",,"{'volume': '', 'pages': '255-258', 'name': ''}",18.0,"Convolutional networks for images, speech, and time series",1998.0
1089,5651a95e52669118b27463083755cb2437d8d8da,"Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years.","[{'authorId': '1700040', 'name': 'S. DiPaola'}, {'authorId': '80264274', 'name': 'Ö. Yalçın'}]",6.0,"{'bibtex': '@Article{DiPaola2019AMA,\n author = {S. DiPaola and Ö. Yalçın},\n journal = {2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)},\n pages = {91-92},\n title = {A multi-layer artificial intelligence and sensing based affective conversational embodied agent},\n year = {2019}\n}\n'}",,"{'pages': '91-92', 'name': '2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)'}",3.0,A multi-layer artificial intelligence and sensing based affective conversational embodied agent,2019.0
1090,565d85a75332607440e2cf8d67ab20650b8f83da,,"[{'authorId': '46270580', 'name': 'Z. Liu'}, {'authorId': '145086315', 'name': 'Zhigeng Pan'}]",45.0,"{'bibtex': '@Inproceedings{Liu2005AnEM,\n author = {Z. Liu and Zhigeng Pan},\n pages = {629-636},\n title = {An Emotion Model of 3D Virtual Characters in Intelligent Virtual Environment},\n year = {2005}\n}\n'}",,{'pages': '629-636'},14.0,An Emotion Model of 3D Virtual Characters in Intelligent Virtual Environment,2005.0
1091,565ec5cd68f81e5d3d1a9306335795622d1425e4,"The paper presents an investigation into the role of virtual reality and web technologies in the field of distance education. Within this frame, special emphasis is given on the building of web-based virtual learning environments so as to successfully fulfill their educational objectives. In particular, basic pedagogical methods are studied, focusing mainly on the efficient preparation, approach and presentation of learning content, and specific designing rules are presented considering the hypermedia, virtual and educational nature of this kind of applications. The paper also aims to highlight the educational benefits arising from the use of virtual reality technology in medicine and study the emerging area of web-based medical simulations. Finally, an innovative virtual reality environment for distance education in medicine is demonstrated. The proposed environment reproduces conditions of the real learning process and enhances learning through a real-time interactive simulator. Keywords—Distance education, medicine, virtual reality, web.","[{'authorId': '2296506', 'name': 'K. Dimitropoulos'}, {'authorId': '144028942', 'name': 'A. Manitsaris'}, {'authorId': '143612830', 'name': 'I. Mavridis'}]",36.0,"{'bibtex': '@Article{Dimitropoulos2007BuildingVR,\n author = {K. Dimitropoulos and A. Manitsaris and I. Mavridis},\n journal = {World Academy of Science, Engineering and Technology, International Journal of Social, Behavioral, Educational, Economic, Business and Industrial Engineering},\n pages = {645-653},\n title = {Building Virtual Reality Environments for Distance Education on the Web: A Case Study in Medical Education},\n volume = {1},\n year = {2007}\n}\n'}",,"{'volume': '1', 'pages': '645-653', 'name': 'World Academy of Science, Engineering and Technology, International Journal of Social, Behavioral, Educational, Economic, Business and Industrial Engineering'}",28.0,Building Virtual Reality Environments for Distance Education on the Web: A Case Study in Medical Education,2007.0
1092,567219290b6a01b55c63221f700245c9f8f529ee,"Embodied conversational agents have been proven to be powerful tools for engaging users in interactions and thus are suitable for training scenarios that rely on a role-playing metaphor. In the CUBE-G project we propose an approach for culture-adaptive behavior generation of such agents, which can be employed in edutainment applications for increasing cultural awareness and for learning some of the appropriate behavior routines. In this paper we present the methodological approach of a standardized collection of multimodal behavioral corpora for different cultures to inform a parametrized model of cultural behavior","[{'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '1790555', 'name': 'Nikolaus Bee'}, {'authorId': '2158172', 'name': 'Birgit Lugrin'}, {'authorId': '2754538', 'name': 'M. Wissner'}]",48.0,"{'bibtex': '@Inproceedings{Rehm2007TheCA,\n author = {M. Rehm and E. André and Nikolaus Bee and Birgit Lugrin and M. Wissner},\n title = {The CUBE-G approach – Coaching culture-specific nonverbal behavior by virtual agents},\n year = {2007}\n}\n'}",,,20.0,The CUBE-G approach – Coaching culture-specific nonverbal behavior by virtual agents,2007.0
1093,56a179f61072d0b1d554b5d9f2add15009f6c938,,"[{'authorId': '2118976734', 'name': 'Ying Zhao'}, {'authorId': '2055429479', 'name': 'Mengqi Yuan'}, {'authorId': '2881973', 'name': 'G. Su'}, {'authorId': '2118211827', 'name': 'Tao Chen'}]",14.0,"{'bibtex': '@Article{Zhao2015CrowdMS,\n author = {Ying Zhao and Mengqi Yuan and G. Su and Tao Chen},\n journal = {Physica A-statistical Mechanics and Its Applications},\n pages = {84-93},\n title = {Crowd macro state detection using entropy model},\n volume = {431},\n year = {2015}\n}\n'}",,"{'volume': '431', 'pages': '84-93', 'name': 'Physica A-statistical Mechanics and Its Applications'}",27.0,Crowd macro state detection using entropy model,2015.0
1094,56c63c2ee7480cccae09b76f590f7f057a58b9e8,,"[{'authorId': '1724799', 'name': 'C. Clavel'}, {'authorId': '2094801535', 'name': 'Justine Plessier'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}, {'authorId': '2305305', 'name': 'Laurent Ach'}, {'authorId': '32214926', 'name': 'Benoît Morel'}]",66.0,"{'bibtex': '@Inproceedings{Clavel2009CombiningFA,\n author = {C. Clavel and Justine Plessier and Jean-Claude Martin and Laurent Ach and Benoît Morel},\n pages = {287-300},\n title = {Combining Facial and Postural Expressions of Emotions in a Virtual Character},\n year = {2009}\n}\n'}",,{'pages': '287-300'},40.0,Combining Facial and Postural Expressions of Emotions in a Virtual Character,2009.0
1097,56cb8cd0cb348ab4da14e19918396a236235e08e,,"[{'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '34434164', 'name': 'N. Simons'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",57.0,"{'bibtex': ""@Inproceedings{Krämer2007TheEO,\n author = {N. Krämer and N. Simons and S. Kopp},\n pages = {238-251},\n title = {The Effects of an Embodied Conversational Agent's Nonverbal Behavior on User's Evaluation and Behavioral Mimicry},\n year = {2007}\n}\n""}",,{'pages': '238-251'},57.0,The Effects of an Embodied Conversational Agent's Nonverbal Behavior on User's Evaluation and Behavioral Mimicry,2007.0
1098,56ccf17dced2d3bb73f66a18afa20caf5a429c21,"Following Langer (1992), this article reviews a series of experimental studiesthat demonstrate that individuals mindlessly apply social rules and expecta-tions to computers. The first set of studies illustrates how individuals overusehuman social categories, applying gender stereotypes to computers and ethnicallyidentifying with computer agents. The second set demonstrates that people exhibitoverlearned social behaviors such as politeness and reciprocity toward comput-ers.Inthethirdsetofstudies,prematurecognitivecommitmentsaredemonstrated:Aspecialisttelevisionsetisperceivedasprovidingbettercontentthanageneralisttelevision set. A final series of studies demonstrates the depth of social responseswith respect to computer “personality.” Alternative explanations for these find -ings, such as anthropomorphism and intentional social responses, cannot explainthe results. We conclude with an agenda for future research.Computer users approach the personal computer in many different ways.Experienced word processors move smoothly from keyboard to mouse to menu,mixing prose and commands to the computer automatically; the distinctionbetween the hand and the tool blurs (Heidegger, 1977; Winograd & Flores, 1987).Novices cautiously strike each key, fearing that one false move will initiate anuncontrollable series of unwanted events. Game players view computers as","[{'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '33875827', 'name': 'Youngme Moon'}]",2359.0,"{'bibtex': '@Article{Nass2000MachinesAM,\n author = {C. Nass and Youngme Moon},\n journal = {Journal of Social Issues},\n pages = {81-103},\n title = {Machines and Mindlessness: Social Responses to Computers},\n volume = {56},\n year = {2000}\n}\n'}",,"{'volume': '56', 'pages': '81-103', 'name': 'Journal of Social Issues'}",64.0,Machines and Mindlessness: Social Responses to Computers,2000.0
1099,56db2ac44324d0649bbe643ee070d7ba5724f243,,"[{'authorId': '2056813503', 'name': 'Li Gong'}]",239.0,"{'bibtex': '@Article{Gong2008HowSI,\n author = {Li Gong},\n journal = {Comput. Hum. Behav.},\n pages = {1494-1509},\n title = {How social is social responses to computers? The function of the degree of anthropomorphism in computer representations},\n volume = {24},\n year = {2008}\n}\n'}",,"{'volume': '24', 'pages': '1494-1509', 'name': 'Comput. Hum. Behav.'}",29.0,How social is social responses to computers? The function of the degree of anthropomorphism in computer representations,2008.0
1100,56e3df3a807605151e18991e4298e4a8b00aee67,"The experiment reported investigated the phenomenological consequences of Easterners' and Westerners' perspectives on the self. Two findings are consistent with the notion that Asians are more likely than Westerners to experience the self from the perspective of the generalized other First, Eastern participants were more likely than Western participants to have third-person (as opposed to first-person) memories when they thought about situations in which they would be at the center of a scene. Second, Easterners and Westerners engaged in different sorts of projections when they read the emotional expressions of other people. Westerners were more biased than Easterners toward egocentric projection of their own emotions onto others, whereas Easterners were more biased than Westerners toward relational projection, in which they projected onto others the emotions that the generalized other would feel in relation to the participant. Implications for how phenomenological experiences could reinforce different Eastern and Western ideologies about the self and the group are discussed.","[{'authorId': '40634589', 'name': 'D. Cohen'}, {'authorId': '13321799', 'name': 'A. Gunz'}]",218.0,"{'bibtex': '@Article{Cohen2002AsSB,\n author = {D. Cohen and A. Gunz},\n journal = {Psychological Science},\n pages = {55 - 59},\n title = {As Seen by the Other … : Perspectives on the Self in the Memories and Emotional Perceptions of Easterners and Westerners},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '55 - 59', 'name': 'Psychological Science'}",24.0,As Seen by the Other … : Perspectives on the Self in the Memories and Emotional Perceptions of Easterners and Westerners,2002.0
1101,57080cd857d07ee2c3469ac06329c90deee6950c,"Continuous emotion recognition (CER) is a task which requires the prediction of time series emotional parameter outputs corresponding to query time series inputs given training data in the form of matched pairs of input and output time series. In order to address this task, it is important to be able to model not only relationships between points in the input and output spaces, but also temporal relationships between points within the output space. Gaussian process regression (GPR) is an inference technique which has desirable properties for CER, including its ability to produce predictive distributions over the outputs rather than only point estimates. However, GPR is generally applied to pointwise prediction or interpolation tasks, rather than to predictions of entire functional outputs. We propose a covariance structure that is able to incorporate both input-output and temporal information to produce predictions that take into account the functional nature of CER data. We demonstrate the application of this method to simulated data, and to the AVEC2016 CER task, showing that GPR with this covariance structure is able to make predictions of emotional arousal from audio with over twice the accuracy of a straightforward pointwise application of GPR in the input feature space, and is furthermore able to produce predictions with accuracy approaching that of a competitive CER system using only very general component covariance models.","[{'authorId': '26980547', 'name': 'M. Atcheson'}, {'authorId': '1721584', 'name': 'V. Sethu'}, {'authorId': '145815422', 'name': 'J. Epps'}]",11.0,"{'bibtex': '@Inproceedings{Atcheson2017GaussianPR,\n author = {M. Atcheson and V. Sethu and J. Epps},\n pages = {34-44},\n title = {Gaussian Process Regression for Continuous Emotion Recognition with Global Temporal Invariance},\n year = {2017}\n}\n'}",,{'pages': '34-44'},15.0,Gaussian Process Regression for Continuous Emotion Recognition with Global Temporal Invariance,2017.0
1102,5708c3096d01ae803945094372ee879bebf6669a,"Animation budget constraints during the development of a game often call for the use of a limited set of generic motions. Editing operations are thus generally required to animate virtual characters with a sufficient level of variety. Evaluating the perceptual plausibility of edited animations can therefore contribute greatly towards producing visually plausible animations. In this article, we study observers’ sensitivity to manipulations of overarm and underarm biological throwing animations. In the first experiment, we modified the release velocity of the ball while leaving the motion of the virtual thrower and the angle of release of the ball unchanged. In the second experiment, we evaluated the possibility of further modifying throwing animations by simultaneously editing the motion of the thrower and the release velocity of the ball, using dynamic time warping. In both experiments, we found that participants perceived shortened underarm throws to be particularly unnatural. We also found that modifying the thrower's motion in addition to modifying the release velocity of the ball does not significantly improve the perceptual plausibility of edited throwing animations. In the third experiment, we modified the angle of release of the ball while leaving the magnitude of release velocity and the motion of the thrower unchanged, and found that this editing operation is efficient for improving the perceptual plausibility of shortened underarm throws. Finally, in Experiment 4, we replaced the virtual human thrower with a mechanical throwing device (a ramp) and found the opposite pattern of sensitivity to modifications of the release velocity, indicating that biological and physical throws are subject to different perceptual rules. Our results provide valuable guidelines for developers of games and virtual reality applications by specifying thresholds for the perceptual plausibility of throwing manipulations while also providing several interesting insights for researchers in visual perception of biological motion.","[{'authorId': '2697278', 'name': 'Michele Vicovaro'}, {'authorId': '1869571', 'name': 'Ludovic Hoyet'}, {'authorId': '2720425', 'name': 'L. Burigana'}, {'authorId': '1404017833', 'name': ""C. O'Sullivan""}]",12.0,"{'bibtex': ""@Article{Vicovaro2014PerceptualEO,\n author = {Michele Vicovaro and Ludovic Hoyet and L. Burigana and C. O'Sullivan},\n journal = {ACM Trans. Appl. Percept.},\n pages = {10:1-10:23},\n title = {Perceptual Evaluation of Motion Editing for Realistic Throwing Animations},\n volume = {11},\n year = {2014}\n}\n""}",,"{'volume': '11', 'pages': '10:1-10:23', 'name': 'ACM Trans. Appl. Percept.'}",44.0,Perceptual Evaluation of Motion Editing for Realistic Throwing Animations,2014.0
1103,570d4a53b703af132f3b69e0f685c6feff0ebbc4,,"[{'authorId': '4840308', 'name': 'M. Cao'}, {'authorId': '3172102', 'name': 'Guijuan Zhang'}, {'authorId': '2145361200', 'name': 'Mengsi Wang'}, {'authorId': '7382513', 'name': 'Dianjie Lu'}, {'authorId': '2118902760', 'name': 'Hong Liu'}]",59.0,"{'bibtex': '@Article{Cao2017AMO,\n author = {M. Cao and Guijuan Zhang and Mengsi Wang and Dianjie Lu and Hong Liu},\n journal = {Physica A-statistical Mechanics and Its Applications},\n pages = {250-258},\n title = {A method of emotion contagion for crowd evacuation},\n volume = {483},\n year = {2017}\n}\n'}",,"{'volume': '483', 'pages': '250-258', 'name': 'Physica A-statistical Mechanics and Its Applications'}",24.0,A method of emotion contagion for crowd evacuation,2017.0
1105,570e196c99881312322f7e11264580a51ab6c133,"'Affective computing' is a branch of computing concerned with the theory and construction of machines which can detect, respond to, and simulate human emotional states. It is an interdisciplinary field spanning the computer sciences, psychology, and cognitive science. Affective computing is a rapidly developing field within industry and science. There is now a great drive to make technologies such as robotic systems, avatars in service-related human computer interaction, e-learning, game characters, or companion devices more marketable by endowing the 'soulless' robots or agents with the ability to recognize and adjust to the user's feelings as well as to be able to communicate appropriate emotional signals.A Blueprint for Affective Computing: A sourcebook and manual is the very first attempt to ground affective computing within the disciplines of psychology, affective neuroscience, and philosophy. This book illustrates the contributions of each of these disciplines to the development of the ever-growing field of affective computing. In addition, it demonstrates practical examples of cross-fertilization between disciplines in order to highlight the need for integration of computer science, engineering and the affective sciences. Focusing on a topic at the frontiers of human computer interaction research, this book will be of great interest to students and researchers in psychology, neuroscience, computational neuroscience, computer science, and artificial intelligence.","[{'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '2162242', 'name': 'T. Bänziger'}, {'authorId': '2489004', 'name': 'E. Roesch'}]",194.0,"{'bibtex': '@Inproceedings{Scherer2010ABF,\n author = {K. Scherer and T. Bänziger and E. Roesch},\n title = {A Blueprint for Affective Computing: A Sourcebook and Manual},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,A Blueprint for Affective Computing: A Sourcebook and Manual,2010.0
1106,57415c8c567d2ba3d92eaef6b5b974277d010094,"Scholars interested in emotion regulation have documented the different goals and strategies individuals have for regulating their emotions. However, little attention has been paid to the regulation of group-based emotions, which are based on individuals’ self-categorization as a group member and occur in response to situations perceived as relevant for that group. We propose a model for examining group-based emotion regulation that integrates intergroup emotions theory and the process model of emotion regulation. This synergy expands intergroup emotion theory by facilitating further investigation of different goals (i.e., hedonic or instrumental) and strategies (e.g., situation selection and modification strategies) used to regulate group-based emotions. It also expands emotion regulation research by emphasizing the role of self-categorization (e.g., as an individual or a group member) in the emotional process. Finally, we discuss the promise of this theoretical synergy and suggest several directions for future research on group-based emotion regulation.","[{'authorId': '40484649', 'name': 'Amit Goldenberg'}, {'authorId': '1722823', 'name': 'E. Halperin'}, {'authorId': '82305608', 'name': 'Martijn van Zomeren'}, {'authorId': '1775321', 'name': 'J. Gross'}]",140.0,"{'bibtex': '@Article{Goldenberg2016ThePM,\n author = {Amit Goldenberg and E. Halperin and Martijn van Zomeren and J. Gross},\n journal = {Personality and Social Psychology Review},\n pages = {118 - 141},\n title = {The Process Model of Group-Based Emotion},\n volume = {20},\n year = {2016}\n}\n'}",,"{'volume': '20', 'pages': '118 - 141', 'name': 'Personality and Social Psychology Review'}",220.0,The Process Model of Group-Based Emotion,2016.0
1107,5749b3590147259baeff87f91d250bf07938c60e,,"[{'authorId': '9267108', 'name': 'G. Abowd'}, {'authorId': '144021446', 'name': 'A. Dey'}, {'authorId': '144580566', 'name': 'P. Brown'}, {'authorId': '144396244', 'name': 'N. Davies'}, {'authorId': '2118906157', 'name': 'Mark T. Smith'}, {'authorId': '1771459', 'name': 'Pete Steggles'}]",5189.0,"{'bibtex': '@Inproceedings{Abowd1999TowardsAB,\n author = {G. Abowd and A. Dey and P. Brown and N. Davies and Mark T. Smith and Pete Steggles},\n pages = {304-307},\n title = {Towards a Better Understanding of Context and Context-Awareness},\n year = {1999}\n}\n'}",,{'pages': '304-307'},28.0,Towards a Better Understanding of Context and Context-Awareness,1999.0
1108,57624d99699f71f213b3a8d2f1905a56ac60f1c7,,"[{'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '2112486362', 'name': 'Chunling Ma'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}]",29.0,"{'bibtex': '@Article{Prendinger2007EyeMA,\n author = {H. Prendinger and Chunling Ma and M. Ishizuka},\n journal = {Interact. Comput.},\n pages = {281-292},\n title = {Eye movements as indices for the utility of life-like interface agents: A pilot study},\n volume = {19},\n year = {2007}\n}\n'}",,"{'volume': '19', 'pages': '281-292', 'name': 'Interact. Comput.'}",48.0,Eye movements as indices for the utility of life-like interface agents: A pilot study,2007.0
1109,576b1fdd25c1fbedd0b6e35e0d5a1f3b4dee51d5,"Many everyday decisions involve a social dilemma: cooperation can enhance joint gains, but also make one vulnerable to exploitation. Emotion and emotional signaling is an important element of how people resolve these dilemmas. With the rise of affective computing, emotion is also an important element of how people resolve these dilemmas with machines. In this article, we learn a predictive model of how people make decisions in an iterative social dilemma. We further show that model accuracy improves by incorporating a player's emotional displays as input to this model, and provide some insight into which emotions influence social decisions. Finally, we show how this model can be used to perform ""social planning"": i.e., to generate a sequence of actions and expressions that achieve social goals (such as maximizing individual rewards). These techniques can be used to enhance machine-understanding of human behavior, as social decision-aids, or to drive the actions of virtual and robotic agents.","[{'authorId': '2065815350', 'name': 'Rens Hoegen'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",13.0,"{'bibtex': '@Article{Hoegen2017IncorporatingEP,\n author = {Rens Hoegen and Giota Stratou and J. Gratch},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {801-809},\n title = {Incorporating Emotion Perception into Opponent Modeling for Social Dilemmas},\n year = {2017}\n}\n'}","[{'paperId': 'f82daa1a2c2be227667ca8b2865f6cf2533b9db4', 'title': 'The promise and peril of interactive embodied agents for studying non-verbal communication: a machine learning perspective'}, {'paperId': '6568743383872efcffa22f65c895014fd161a247', 'title': 'Predicting Strategic Decisions Based on Emotional Signals'}, {'paperId': '8351482e14f9218bbaeeb2c5d44e57cc6f0b6059', 'title': 'Dominance, reward, and affiliation smiles modulate the meaning of uncooperative or untrustworthy behaviour'}, {'paperId': 'a9f9fa599726a7f993ac33460bf96ffaff4d1da1', 'title': 'Multimodal Goal Recognition in Open-World Digital Games'}, {'paperId': 'a407b48a77b20e0cf24933c0912976bc16f8dc18', 'title': 'An Architecture for Emotional Facial Expressions as Social Signals'}, {'paperId': '981a1377b3d5c18490f95a4351b64adf27bf5a2c', 'title': 'Accumulating Risk Capital Through Investing in Cooperation'}, {'paperId': '1e10151430ddc4b38806a03547b9f229cd1731a5', 'title': 'Emotion or expressivity? An automated analysis of nonverbal perception in a social dilemma'}, {'paperId': '00c479139ad1022e7c23bd9db2e0221998675c63', 'title': 'Play With One’s Feelings: A Study on Emotion Awareness for Player Experience'}, {'paperId': '86def1585ff432a84c37c19937eabf8931fd36b8', 'title': 'Smiles Signal Surprise in a Social Dilemma'}, {'paperId': 'fb5ce13f484fccab4a640d7de6f3da8fd93eb013', 'title': 'The Interplay of Emotions and Norms in Multiagent Systems'}, {'paperId': '90c325bef1aa72a282e1f0e8297070ef8c08d10d', 'title': 'Beyond actions: Reparatory effects of regret in intergroup trust games'}, {'paperId': '660cbc814ddd64a977efb10c7b44ae81ffc73fdf', 'title': 'Predicting Folds in Poker Using Action Unit Detectors and Decision Trees'}, {'paperId': '4b18f028e84dc0e24bdfeed213de8419c6b20caf', 'title': 'Jonathan (2018) Predicting folds in poker using action unit detectors and decision trees. In: 13th IEEE International Conference Face and Gesture'}]",{'pages': '801-809'},31.0,Incorporating Emotion Perception into Opponent Modeling for Social Dilemmas,2017.0
1110,577b454ed08e4a48f873e90b651ab0b9d4cd440e,"To provide a means for recognition of affect from a distance, this paper analyzes the capability of gait to reveal a person's affective state. We address interindividual versus person-dependent recognition, recognition based on discrete affective states versus recognition based on affective dimensions, and efficient feature extraction with respect to affect. Principal component analysis (PCA), kernel PCA, linear discriminant analysis, and general discriminant analysis are compared to either reduce temporal information in gait or extract relevant features for classification. Although expression of affect in gait is covered by the primary task of locomotion, person-dependent recognition of motion capture data reaches 95% accuracy based on the observation of a single stride. In particular, different levels of arousal and dominance are suitable for being recognized in gait. It is concluded that gait can be used as an additional modality for the recognition of affect. Application scenarios include monitoring in high-security areas, human-robot interaction, and cognitive home environments.","[{'authorId': '2326840', 'name': 'M. Karg'}, {'authorId': '1708675', 'name': 'K. Kühnlenz'}, {'authorId': '20631844', 'name': 'M. Buss'}]",139.0,"{'bibtex': '@Article{Karg2010RecognitionOA,\n author = {M. Karg and K. Kühnlenz and M. Buss},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {1050-1061},\n title = {Recognition of Affect Based on Gait Patterns},\n volume = {40},\n year = {2010}\n}\n'}",,"{'volume': '40', 'pages': '1050-1061', 'name': 'IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)'}",49.0,Recognition of Affect Based on Gait Patterns,2010.0
1112,577b5f1f3787837d1d4641e0f373a6c1e7414e60,,"[{'authorId': '1726343', 'name': 'Jan Treur'}]",11.0,"{'bibtex': '@Inproceedings{Treur2011DreamingYF,\n author = {Jan Treur},\n pages = {197-209},\n title = {Dreaming Your Fear Away: A Computational Model for Fear Extinction Learning during Dreaming},\n year = {2011}\n}\n'}",,{'pages': '197-209'},33.0,Dreaming Your Fear Away: A Computational Model for Fear Extinction Learning during Dreaming,2011.0
1113,5793f23a5bf0a8fddbe6b6a3b891e043fdd9fa8f,,"[{'authorId': '21451088', 'name': 'P. Ekman'}]",157.0,"{'bibtex': '@Article{Ekman1981TheFO,\n author = {P. Ekman},\n journal = {Contemporary Sociology},\n pages = {521},\n title = {The face of man : expressions of universal emotions in a New Guinea village},\n volume = {10},\n year = {1981}\n}\n'}",,"{'volume': '10', 'pages': '521', 'name': 'Contemporary Sociology'}",0.0,The face of man : expressions of universal emotions in a New Guinea village,1981.0
1114,581018e542acb848bafa7983a226e68255e69ad2,The purpose of this article is to offer a conceptualization of rapport that has utility for identifying the nonverbal correlates associated with rapport. We describe the nature of rapport in terms ...,"[{'authorId': '1399872060', 'name': 'L. Tickle-Degnen'}, {'authorId': '1998366', 'name': 'R. Rosenthal'}]",855.0,"{'bibtex': '@Article{Tickle-Degnen1990TheNO,\n author = {L. Tickle-Degnen and R. Rosenthal},\n journal = {Psychological Inquiry},\n pages = {285-293},\n title = {The Nature of Rapport and Its Nonverbal Correlates},\n volume = {1},\n year = {1990}\n}\n'}",,"{'volume': '1', 'pages': '285-293', 'name': 'Psychological Inquiry'}",41.0,The Nature of Rapport and Its Nonverbal Correlates,1990.0
1117,581d3ebdb2eae794f1203d7fb927476f43925132,,"[{'authorId': '50377075', 'name': 'J. Neufeld'}, {'authorId': '3102450', 'name': 'B. Chakrabarti'}]",15.0,"{'bibtex': '@Article{Neufeld2016EmpathyMT,\n author = {J. Neufeld and B. Chakrabarti},\n journal = {Scientific Reports},\n title = {Empathy Modulates the Rewarding Effect of Mimicry},\n volume = {6},\n year = {2016}\n}\n'}",,"{'volume': '6', 'name': 'Scientific Reports'}",49.0,Empathy Modulates the Rewarding Effect of Mimicry,2016.0
1118,585639b260d8278208bb577ffc6bf2ef0ffa0312,"AbstractOBJECTIVE: While considerable attention has focused on improving the detection of depression, assessment of severity is also important in guiding treatment decisions. Therefore, we examined the validity of a brief, new measure of depression severity.
 MEASUREMENTS: The Patient Health Questionnaire (PHQ) is a self-administered version of the PRIME-MD diagnostic instrument for common mental disorders. The PHQ-9 is the depression module, which scores each of the 9 DSM-IV criteria as “0” (not at all) to “3” (nearly every day). The PHQ-9 was completed by 6,000 patients in 8 primary care clinics and 7 obstetrics-gynecology clinics. Construct validity was assessed using the 20-item Short-Form General Health Survey, self-reported sick days and clinic visits, and symptom-related difficulty. Criterion validity was assessed against an independent structured mental health professional (MHP) interview in a sample of 580 patients.
 RESULTS: As PHQ-9 depression severity increased, there was a substantial decrease in functional status on all 6 SF-20 subscales. Also, symptom-related difficulty, sick days, and health care utilization increased. Using the MHP reinterview as the criterion standard, a PHQ-9 score ≥10 had a sensitivity of 88% and a specificity of 88% for major depression. PHQ-9 scores of 5, 10, 15, and 20 represented mild, moderate, moderately severe, and severe depression, respectively. Results were similar in the primary care and obstetrics-gynecology samples.
 CONCLUSION: In addition to making criteria-based diagnoses of depressive disorders, the PHQ-9 is also a reliable and valid measure of depression severity. These characteristics plus its brevity make the PHQ-9 a useful clinical and research tool.","[{'authorId': '3737608', 'name': 'K. Kroenke'}, {'authorId': '2069763', 'name': 'R. Spitzer'}, {'authorId': '2111787096', 'name': 'Janet B W Williams'}]",19378.0,"{'bibtex': '@Article{Kroenke2001TheP,\n author = {K. Kroenke and R. Spitzer and Janet B W Williams},\n journal = {Journal of General Internal Medicine},\n pages = {606-613},\n title = {The PHQ-9},\n volume = {16},\n year = {2001}\n}\n'}",,"{'volume': '16', 'pages': '606-613', 'name': 'Journal of General Internal Medicine'}",27.0,The PHQ-9,2001.0
1119,586078656d5bee43e0cb7c57e3ececfb067cc451,"Co‐speech gestures are a vital ingredient in making virtual agents more human‐like and engaging. Automatically generated gestures based on speech‐input often lack realistic and defined gesture form. We present a database‐driven approach guaranteeing defined gesture form. We built a large corpus of over 23,000 motion‐captured co‐speech gestures and select individual gestures based on expressive gesture characteristics that can be estimated from speech audio. The expressive parameters are gesture velocity and acceleration, gesture size, arm swivel, and finger extension. Individual, parameter‐matched gestures are then combined into animated sequences. We evaluate our gesture generation system in two perceptual studies. The first study compares our method to the ground truth gestures as well as mismatched gestures. The second study compares our method to five current generative machine learning models. Our method outperformed mismatched gesture selection in the first study and showed competitive performance in the second.","[{'authorId': '3430725', 'name': 'Ylva Ferstl'}, {'authorId': '143687087', 'name': 'Michael Neff'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",30.0,"{'bibtex': '@Article{Ferstl2021ExpressGestureEG,\n author = {Ylva Ferstl and Michael Neff and R. Mcdonnell},\n journal = {Computer Animation and Virtual Worlds},\n title = {ExpressGesture: Expressive gesture generation from speech through database matching},\n volume = {32},\n year = {2021}\n}\n'}",,"{'volume': '32', 'name': 'Computer Animation and Virtual Worlds'}",34.0,ExpressGesture: Expressive gesture generation from speech through database matching,2021.0
1120,5871ba1359b95641dcadb6df23f52661acc4176f,"Video annotation is a vital part of research examining gestural and multimodal interaction as well as computer vision, machine learning, and interface design. However, annotation is a difficult, time-consuming task that requires high cognitive effort. Existing tools for labeling and annotation still require users to manually label most of the data, limiting the tools helpfulness. In this paper, we present the Easy Automatic Segmentation Event Labeler (EASEL), a tool supporting gesture analysis. EASEL streamlines the annotation process by introducing assisted annotation, using automatic gesture segmentation and recognition to automatically annotate gestures. To evaluate the efficacy of assisted annotation, we conducted a user study with 24 participants and found that assisted annotation decreased the time needed to annotate videos with no difference in accuracy compared with manual annotation. The results of our study demonstrate the benefit of adding computational intelligence to video and audio annotation tasks.","[{'authorId': '10693895', 'name': 'Isaac Wang'}, {'authorId': '145382418', 'name': 'P. Narayana'}, {'authorId': '2109847325', 'name': 'Jesse Smith'}, {'authorId': '1694404', 'name': 'B. Draper'}, {'authorId': '143905691', 'name': 'J. Beveridge'}, {'authorId': '151062472', 'name': 'J. Ruiz'}]",8.0,"{'bibtex': '@Article{Wang2018EASELEA,\n author = {Isaac Wang and P. Narayana and Jesse Smith and B. Draper and J. Beveridge and J. Ruiz},\n journal = {23rd International Conference on Intelligent User Interfaces},\n title = {EASEL: Easy Automatic Segmentation Event Labeler},\n year = {2018}\n}\n'}",,{'name': '23rd International Conference on Intelligent User Interfaces'},18.0,EASEL: Easy Automatic Segmentation Event Labeler,2018.0
1121,58751dabf6812284610e1123b3c10db852497a43,"Interpersonal human-human interaction is a dynamical exchange and coordination of social signals, feelings and emotions usually performed through and across multiple modalities such as facial expressions, gestures, and language. Developing machines able to engage humans in rich and natural interpersonal interactions requires capturing such dynamics. This paper addresses primitive emotional contagion during dyadic interactions in which roles are prefixed. Primitive emotional contagion was defined as the tendency people have to automatically mimic and synchronize their multimodal behavior during interactions and, consequently, to emotionally converge. To capture emotional contagion, a cross-recurrence based methodology that explicitly integrates short and long-term temporal dynamics through the analysis of both facial expressions and sentiment was developed. This approach is employed to assess emotional contagion at unimodal, multimodal and cross-modal levels and is evaluated on the Solid SAL-SEMAINE corpus. Interestingly, the approach is able to show the importance of the adoption of cross-modal strategies for addressing emotional contagion.","[{'authorId': '1958033', 'name': 'G. Varni'}, {'authorId': '2321433', 'name': 'I. Hupont'}, {'authorId': '2049106', 'name': 'C. Clavel'}, {'authorId': '1680828', 'name': 'M. Chetouani'}]",16.0,"{'bibtex': '@Article{Varni2020ComputationalSO,\n author = {G. Varni and I. Hupont and C. Clavel and M. Chetouani},\n journal = {IEEE Transactions on Affective Computing},\n pages = {258-271},\n title = {Computational Study of Primitive Emotional Contagion in Dyadic Interactions},\n volume = {11},\n year = {2020}\n}\n'}",,"{'volume': '11', 'pages': '258-271', 'name': 'IEEE Transactions on Affective Computing'}",77.0,Computational Study of Primitive Emotional Contagion in Dyadic Interactions,2020.0
1122,5899f3076301a1d40f23ff24391bd12cbeb82d79,"The paper describes some simulation models used to implement virtual reality applications, addressing the presentation of the architecture of VR systems, VR applications in different fields, including medicine, an introduction to simulation techniques and a set of mathematical models for creating virtual scenes. The material represents a significant development of the presentation given at the workshop VRRM 2007: Virtual Reality in Rehabilitation Medicine, with details on mathematical aspects.","[{'authorId': '9337392', 'name': 'G. Albeanu'}]",4.0,"{'bibtex': '@Inproceedings{Albeanu2008SimulationMF,\n author = {G. Albeanu},\n title = {Simulation Models for Virtual Reality Applications},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': ''}",26.0,Simulation Models for Virtual Reality Applications,2008.0
1123,58b32f05f840dd941bcaa04945c236326bdbabe4,,"[{'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '2144259493', 'name': 'Soyoung Kim'}]",30.0,"{'bibtex': '@Inproceedings{Baylor2008TheEO,\n author = {A. L. Baylor and Soyoung Kim},\n pages = {208-214},\n title = {The Effects of Agent Nonverbal Communication on Procedural and Attitudinal Learning Outcomes},\n year = {2008}\n}\n'}",,{'pages': '208-214'},25.0,The Effects of Agent Nonverbal Communication on Procedural and Attitudinal Learning Outcomes,2008.0
1124,58d73b1e2d218c265b145e1ac219113f93dc28b1,,"[{'authorId': '52093924', 'name': 'J. McHugh'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '1404017833', 'name': ""C. O'Sullivan""}, {'authorId': '1818330', 'name': 'F. Newell'}]",42.0,"{'bibtex': ""@Article{McHugh2010PerceivingEI,\n author = {J. McHugh and R. Mcdonnell and C. O'Sullivan and F. Newell},\n journal = {Experimental Brain Research},\n pages = {361-372},\n title = {Perceiving emotion in crowds: the role of dynamic body postures on the perception of emotion in crowded scenes},\n volume = {204},\n year = {2010}\n}\n""}",,"{'volume': '204', 'pages': '361-372', 'name': 'Experimental Brain Research'}",33.0,Perceiving emotion in crowds: the role of dynamic body postures on the perception of emotion in crowded scenes,2010.0
1125,58f72b53d576c6e4a42b4d8812e5542ffa2c03cc,,"[{'authorId': '1729154', 'name': 'Christian Bizer'}, {'authorId': '144568027', 'name': 'Jens Lehmann'}, {'authorId': '2051816', 'name': 'Georgi Kobilarov'}, {'authorId': '145044578', 'name': 'S. Auer'}, {'authorId': '2068696031', 'name': 'Christian Becker'}, {'authorId': '1702661', 'name': 'Richard Cyganiak'}, {'authorId': '2024066', 'name': 'Sebastian Hellmann'}]",2399.0,"{'bibtex': '@Article{Bizer2009DBpediaA,\n author = {Christian Bizer and Jens Lehmann and Georgi Kobilarov and S. Auer and Christian Becker and Richard Cyganiak and Sebastian Hellmann},\n journal = {J. Web Semant.},\n pages = {154-165},\n title = {DBpedia - A crystallization point for the Web of Data},\n volume = {7},\n year = {2009}\n}\n'}",,"{'volume': '7', 'pages': '154-165', 'name': 'J. Web Semant.'}",28.0,DBpedia - A crystallization point for the Web of Data,2009.0
1126,5940ee7d6bbf85bdb9c65777219ffef8477a09a0,,"[{'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '3364569', 'name': 'Bilge Karacora'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '145707560', 'name': 'Morteza Dehghani'}, {'authorId': '50714008', 'name': 'Gina Rüther'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",72.0,"{'bibtex': '@Article{Krämer2016ClosingTG,\n author = {N. Krämer and Bilge Karacora and Gale M. Lucas and Morteza Dehghani and Gina Rüther and J. Gratch},\n journal = {Comput. Educ.},\n pages = {1-13},\n title = {Closing the gender gap in STEM with friendly male instructors? On the effects of rapport behavior and gender of a virtual agent in an instructional interaction},\n volume = {99},\n year = {2016}\n}\n'}",,"{'volume': '99', 'pages': '1-13', 'name': 'Comput. Educ.'}",96.0,Closing the gender gap in STEM with friendly male instructors? On the effects of rapport behavior and gender of a virtual agent in an instructional interaction,2016.0
1127,5948fffabf53f4eec8333322c791d21c6d6280b4,"Abstract Emotions can be regarded as the manifestations of a system that realises multiple concerns and operates in an uncertain environment. Taking the concern realisation function as a starting point, it is argued that the major phenomena of emotion follow from considerations of what properties a subsystem implementing that function should have. The major phenomena are: the existence of the feelings of pleasure and pain, the importance of cognitive or appraisal variables, the presence of innate, pre-programmed behaviours as well as of complex constructed plans for achieving emotion goals, and the occurrence of behavioural interruption, disturbance and impulse-like priority of emotional goals. The system properties underlying these phenomena are facilities for relevance detection of events with regard to the multiple concerns, availability of relevance signals that can be recognised by the action system, and facilities for control precedence, or flexible goal priority ordering and shift. A computer progr...","[{'authorId': '49584958', 'name': 'N. Frijda'}, {'authorId': '116727475', 'name': 'Jaap Swagerman'}]",162.0,"{'bibtex': '@Article{Frijda1987CanCF,\n author = {N. Frijda and Jaap Swagerman},\n journal = {Cognition & Emotion},\n pages = {235-257},\n title = {Can computers feel? Theory and design of an emotional system},\n volume = {1},\n year = {1987}\n}\n'}",,"{'volume': '1', 'pages': '235-257', 'name': 'Cognition & Emotion'}",6.0,Can computers feel? Theory and design of an emotional system,1987.0
1128,5973adbdd599054cec8889c13f4fe6b6c6d5b27d,,"[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",20.0,"{'bibtex': '@Inproceedings{Hudlicka2014FromHT,\n author = {E. Hudlicka},\n pages = {3-23},\n title = {From Habits to Standards: Towards Systematic Design of Emotion Models and Affective Architectures},\n year = {2014}\n}\n'}",,{'pages': '3-23'},35.0,From Habits to Standards: Towards Systematic Design of Emotion Models and Affective Architectures,2014.0
1129,59813b9dac1acf83e317d0d992672040c6f6f378,"Humans automatically imitate other people's actions during social interactions, building rapport and social closeness in the process. Although the behavioral consequences and neural correlates of imitation have been studied extensively, little is known about the neural mechanisms that control imitative tendencies. For example, the degree to which an agent is perceived as human-like influences automatic imitation, but it is not known how perception of animacy influences brain circuits that control imitation. In the current fMRI study, we examined how the perception and belief of animacy influence the control of automatic imitation. Using an imitation–inhibition paradigm that involves suppressing the tendency to imitate an observed action, we manipulated both bottom–up (visual input) and top–down (belief) cues to animacy. Results show divergent patterns of behavioral and neural responses. Behavioral analyses show that automatic imitation is equivalent when one or both cues to animacy are present but reduces when both are absent. By contrast, right TPJ showed sensitivity to the presence of both animacy cues. Thus, we demonstrate that right TPJ is biologically tuned to control imitative tendencies when the observed agent both looks like and is believed to be human. The results suggest that right TPJ may be involved in a specialized capacity to control automatic imitation of human agents, rather than a universal process of conflict management, which would be more consistent with generalist theories of imitative control. Evidence for specialized neural circuitry that “controls” imitation offers new insight into developmental disorders that involve atypical processing of social information, such as autism spectrum disorders.","[{'authorId': '1864848', 'name': 'A. Klapper'}, {'authorId': '144772502', 'name': 'Richard Ramsey'}, {'authorId': '39854264', 'name': 'D. Wigboldus'}, {'authorId': '1850742', 'name': 'Emily S. Cross'}]",60.0,"{'bibtex': '@Article{Klapper2014TheCO,\n author = {A. Klapper and Richard Ramsey and D. Wigboldus and Emily S. Cross},\n journal = {Journal of Cognitive Neuroscience},\n pages = {2503-2513},\n title = {The Control of Automatic Imitation Based on Bottom–Up and Top–Down Cues to Animacy: Insights from Brain and Behavior},\n volume = {26},\n year = {2014}\n}\n'}",,"{'volume': '26', 'pages': '2503-2513', 'name': 'Journal of Cognitive Neuroscience'}",56.0,The Control of Automatic Imitation Based on Bottom–Up and Top–Down Cues to Animacy: Insights from Brain and Behavior,2014.0
1130,5998232f2dd056b19c30d20c61bf63c381f9c859,"Emotion communication research strongly focuses on the face and voice as expressive modalities, leaving the rest of the body relatively understudied. Contrary to the early assumption that body movement only indicates emotional intensity, recent studies have shown that body movement and posture also conveys emotion specific information. However, a deeper understanding of the underlying mechanisms is hampered by a lack of production studies informed by a theoretical framework. In this research we adopted the Body Action and Posture (BAP) coding system to examine the types and patterns of body movement that are employed by 10 professional actors to portray a set of 12 emotions. We investigated to what extent these expression patterns support explicit or implicit predictions from basic emotion theory, bidimensional theory, and componential appraisal theory. The overall results showed partial support for the different theoretical approaches. They revealed that several patterns of body movement systematically occur in portrayals of specific emotions, allowing emotion differentiation. Although a few emotions were prototypically expressed by one particular pattern, most emotions were variably expressed by multiple patterns, many of which can be explained as reflecting functional components of emotion such as modes of appraisal and action readiness. It is concluded that further work in this largely underdeveloped area should be guided by an appropriate theoretical framework to allow a more systematic design of experiments and clear hypothesis testing.","[{'authorId': '47658592', 'name': 'N. Dael'}, {'authorId': '37837552', 'name': 'M. Mortillaro'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",374.0,"{'bibtex': '@Article{Dael2012EmotionEI,\n author = {N. Dael and M. Mortillaro and K. Scherer},\n journal = {Emotion},\n pages = {\n          1085-101\n        },\n title = {Emotion expression in body action and posture.},\n volume = {12 5},\n year = {2012}\n}\n'}",,"{'volume': '12 5', 'pages': '\n          1085-101\n        ', 'name': 'Emotion'}",89.0,Emotion expression in body action and posture.,2012.0
1131,59a2918e95e633c0bf269b25a810629a9d8c8bf7,,"[{'authorId': '40611071', 'name': 'F. Riaz'}, {'authorId': '1795560', 'name': 'M. Niazi'}]",25.0,"{'bibtex': '@Article{Riaz2016RoadCA,\n author = {F. Riaz and M. Niazi},\n journal = {Complex Adaptive Systems Modeling},\n pages = {1-34},\n title = {Road collisions avoidance using vehicular cyber-physical systems: a taxonomy and review},\n volume = {4},\n year = {2016}\n}\n'}",,"{'volume': '4', 'pages': '1-34', 'name': 'Complex Adaptive Systems Modeling'}",176.0,Road collisions avoidance using vehicular cyber-physical systems: a taxonomy and review,2016.0
1132,59b0d670f75d3761a03e7298f8215ede2777cdc4,"We present a novel, real-time algorithm, EVA, for generating virtual agents with various perceived emotions. Our approach is based on using Expressive Features of gaze and gait to convey emotions corresponding to happy, sad, angry, or neutral. We precompute a data-driven mapping between gaits and their perceived emotions. EVA uses this gait emotion association at runtime to generate appropriate walking styles in terms of gaits and gaze. Using the EVA algorithm, we can simulate gaits and gazing behaviors of hundreds of virtual agents in real-time with known emotional characteristics. We have evaluated the benefits in different multi-agent VR simulation environments. Our studies suggest that the use of expressive features corresponding to gait and gaze can considerably increase the sense of presence in scenarios with multiple virtual agents.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '80905955', 'name': 'Kyra Kapsaskis'}, {'authorId': '2065900892', 'name': 'R. Sheth'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",29.0,"{'bibtex': '@Book{Randhavane2019EVAGE,\n author = {Tanmay Randhavane and Aniket Bera and Kyra Kapsaskis and R. Sheth and Kurt Gray and Dinesh Manocha},\n booktitle = {ACM Symposium on Applied Perception},\n journal = {ACM Symposium on Applied Perception 2019},\n title = {EVA: Generating Emotional Behavior of Virtual Agents using Expressive Features of Gait and Gaze},\n year = {2019}\n}\n'}","[{'paperId': 'd7b89862e2343303a31600b50275f461d045a755', 'title': 'Walk as you feel: Privacy preserving emotion recognition from gait patterns'}, {'paperId': '58f15ba2b7cf2fc22c07dff1331a8db6567c019c', 'title': 'AI Generated Content in the Metaverse: Risks and Mitigation Strategies'}, {'paperId': '95cdc47f5c0836cf14b3eb1229e78069cd48187e', 'title': 'Emotion Expression in Human Body Posture and Movement: A Survey on Intelligible Motion Factors, Quantification and Validation'}, {'paperId': '62d2c69930eeffc4dbd4562aa387bb5a9526d641', 'title': 'The Effects of Engaging and Affective Behaviors of Virtual Agents in Group Decision-Making'}, {'paperId': '149dc74d9a5d8802f73784d57ccdfd34e6375280', 'title': 'Fintech Agents: Technologies and Theories'}, {'paperId': '91a597a114682de2a79b913c9b410e61319c7f68', 'title': 'Empowering the Metaverse with Generative AI: Survey and Future Directions'}, {'paperId': '551c7ab29a1da4460714b07fdfdc914c6d64384e', 'title': 'When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence'}, {'paperId': '1e61bc7abe5fd33102c5bc4e21ab8a1627cbcdfc', 'title': 'The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents'}, {'paperId': '50a331fec83c0c889c6c071d57f68c2aa2e4a39b', 'title': 'Warping character animations using visual motion features'}, {'paperId': '177ce5a7bb7765d2cc653eae126103546ebf6d2c', 'title': 'MSA-GCN: Multiscale Adaptive Graph Convolution Network for Gait Emotion Recognition'}, {'paperId': 'fb1c85f2119bffb9264da5b5ca585ff47122625d', 'title': 'The Avatar’s Gist: How to Transfer Affective Components From Dynamic Walking to Static Body Postures'}, {'paperId': '9f5a3f4f9f8c539af172ef4d47a3b1e2e9d87a9b', 'title': 'Motion Capture Sensor-Based Emotion Recognition Using a Bi-Modular Sequential Neural Network'}, {'paperId': '53698697f28e9283fdfd2f773aa2dd4d6ede1ad5', 'title': 'A LSTM-based Approach for Gait Emotion Recognition'}, {'paperId': '88c5d878bb1cc2be6329023f12d207a29647f2d4', 'title': 'Reactive Virtual Agents: A Viewpoint-Driven Approach for Bodily Nonverbal Communication'}, {'paperId': '204cbf99a359f38599ed0ebd0d84c6bc50c21c80', 'title': 'Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning'}, {'paperId': '08bd72de6ab68f2db96929de1e2dfa920b999d85', 'title': 'PAVAL: Position-Aware Virtual Agent Locomotion for Assisted Virtual Reality Navigation'}, {'paperId': 'eb56ba5417c8b54785c97d9a87de41ba09d06788', 'title': 'Toward integrating cognitive components with computational models of emotion using software design patterns'}, {'paperId': '8159f3f1bc1155ff56bed7fea38d052c4524108b', 'title': 'Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents**This work has been supported in part by ARO Grants W911NF1910069 and W911NF1910315, and Intel. Code and additional materials available at: https://gamma.umd.edu/t2g'}, {'paperId': 'fed2ae5acc2fd9d0795621de3438ee1f08365220', 'title': 'A Conversational Agent Framework with Multi-modal Personality Expression'}, {'paperId': '658b6a3576bd45478f7271d985f6aad854a6525e', 'title': 'Exploration of Spatial and Temporal Modeling Alternatives for HOI'}, {'paperId': '7b754437bb03d6826495497b7af0fa2166a22a5d', 'title': 'Can a Robot Trust You? : A DRL-Based Approach to Trust-Driven Human-Guided Navigation'}, {'paperId': '7dbca475bfc609b6fd250219788bad5b37d37139', 'title': ""The Impact of a Virtual Agent's Non-Verbal Emotional Expression on a User's Personal Space Preferences""}, {'paperId': '44a7b10b7b1290b027d62747fbe9038df795863b', 'title': 'LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in videos'}, {'paperId': '146e0c3267716e33b603a6f641c971f0beadcc18', 'title': 'Generating Emotive Gaits for Virtual Agents Using Affect-Based Autoregression'}, {'paperId': '0e2a612527239dc8ec76a33681b45abb52dd824e', 'title': 'Development of computational models of emotions: A software engineering perspective'}, {'paperId': '3a718341f5db00071de408a4bd52e92541052e13', 'title': 'Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping'}, {'paperId': '70c4aca43371e620bf2ccd70d3a372b7780cf805', 'title': 'HADREB: Human Appraisals and (English) Descriptions of Robot Emotional Behaviors'}, {'paperId': '62549aabf6ca3bf3642501aed69f34fdebf33362', 'title': 'The Impact of Animations in the Perception of a Simulated Crowd'}, {'paperId': '428250afb5b5a6ef242f736e2cde333c56c38e9e', 'title': 'Generic Body Expression Recognition Based on Synthesis of Realistic Neutral Motion'}]",{'name': 'ACM Symposium on Applied Perception 2019'},68.0,EVA: Generating Emotional Behavior of Virtual Agents using Expressive Features of Gait and Gaze,2019.0
1133,59b8f17e92530c8462a6432f7d183656ed8d034a,,"[{'authorId': '2226641', 'name': 'C. Harley'}]",153.0,"{'bibtex': '@Article{Harley1987ARF,\n author = {C. Harley},\n journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},\n pages = {419-458},\n title = {A role for norepinephrine in arousal, emotion and learning?: Limbic modulation by norepinephrine and the kety hypothesis},\n volume = {11},\n year = {1987}\n}\n'}",,"{'volume': '11', 'pages': '419-458', 'name': 'Progress in Neuro-Psychopharmacology and Biological Psychiatry'}",100.0,"A role for norepinephrine in arousal, emotion and learning?: Limbic modulation by norepinephrine and the kety hypothesis",1987.0
1134,59c031abdc731e1fa6d7df300089b8c61a0ca1db,,"[{'authorId': '1422357475', 'name': 'E. Hall'}]",405.0,"{'bibtex': ""@Inproceedings{Hall1969TheHD,\n author = {E. Hall},\n title = {The hidden dimension: an anthropologist examines man's use of space in public and private},\n year = {1969}\n}\n""}",,"{'volume': '', 'name': ''}",0.0,The hidden dimension: an anthropologist examines man's use of space in public and private,1969.0
1136,59e1748506640a3ef8f69c0fe2f22bbe8db8c7ca,"Sentiment Analysis (SA) is a Natural Language Processing (NLP) and an Information Extraction (IE) task that primarily aims to obtain the writer’s feelings expressed in positive or negative by analyzing a large number of documents. SA is also widely studied in the fields of data mining, web mining, text mining, and information retrieval. The fundamental task in sentiment analysis is to classify the polarity of a given content as Positive, Negative, or Neutral. Although extensive research has been conducted in this area of computational linguistics, most of the research work has been carried out in the context of English language. However, Bengali sentiment expression has varying degree of sentiment labels, which can be plausibly distinct from English language. Therefore, sentiment assessment of Bengali language is undeniably important to be developed and executed properly. In sentiment analysis, the prediction potential of an automatic modeling is completely dependent on the quality of dataset annotation. Bengali sentiment annotation is a challenging task due to diversified structures (syntax) of the language and its different degrees of innate sentiments (i.e., weakly and strongly positive/negative sentiments). Thus, in this article, we propose a novel and precise guideline for the researchers, linguistic experts, and referees to annotate Bengali sentences immaculately with a view to building effective datasets for automatic sentiment prediction efficiently.","[{'authorId': '1806836', 'name': 'Md. Saddam Hossain Mukta'}, {'authorId': '7484275', 'name': 'Md. Adnanul Islam'}, {'authorId': '2086745422', 'name': 'Faisal Ahamed Khan'}, {'authorId': '152223089', 'name': 'Afjal Hossain'}, {'authorId': '49985445', 'name': 'Shuvanon Razik'}, {'authorId': '47867440', 'name': 'Shazzad Hossain'}, {'authorId': '145359935', 'name': 'J. Mahmud'}]",8.0,"{'bibtex': '@Article{Mukta2021ACG,\n author = {Md. Saddam Hossain Mukta and Md. Adnanul Islam and Faisal Ahamed Khan and Afjal Hossain and Shuvanon Razik and Shazzad Hossain and J. Mahmud},\n journal = {Transactions on Asian and Low-Resource Language Information Processing},\n pages = {1 - 19},\n title = {A Comprehensive Guideline for Bengali Sentiment Annotation},\n volume = {21},\n year = {2021}\n}\n'}",,"{'volume': '21', 'pages': '1 - 19', 'name': 'Transactions on Asian and Low-Resource Language Information Processing'}",73.0,A Comprehensive Guideline for Bengali Sentiment Annotation,2021.0
1137,59e56e29b9d82041d251e6dbdfc102a829790284,"This article reviews evidence for the roles that mood states and personality traits play in the processing of emotion-congruent information across different cognitive tasks. Evidence is reviewed for 3 emotion-congruency frameworks, each summarizing a different route to emotional processing: the traditional approach, a moderation approach, and a mediation approach. Most of the traditional literature includes studies that examine the effects of moods and traits on emotional processing separately; these studies have yielded some inconsistent findings. The moderation and mediation approaches offer potential solutions to the lack of consistency obtained in the traditional literature by allowing for the combined effects of personality traits and mood states on the processing of emotional information. The moderation approach suggests that mood states interact with individual differences in emotion-relevant personality traits to influence emotion-congruent processing. The mediation approach suggests that personality traits predispose individuals to certain mood states, which then influence emotional processing. These approaches provide a framework for understanding the literature and a starting point for future research on emotion-congruent processing.","[{'authorId': '5512375', 'name': 'Cheryl L. Rusting'}]",612.0,"{'bibtex': '@Article{Rusting1998PersonalityMA,\n author = {Cheryl L. Rusting},\n journal = {Psychological bulletin},\n pages = {\n          165-96\n        },\n title = {Personality, mood, and cognitive processing of emotional information: three conceptual frameworks.},\n volume = {124 2},\n year = {1998}\n}\n'}",,"{'volume': '124 2', 'pages': '\n          165-96\n        ', 'name': 'Psychological bulletin'}",254.0,"Personality, mood, and cognitive processing of emotional information: three conceptual frameworks.",1998.0
1138,59eebb07028cc7a964b132a0e1cc5f69f925a8af,,"[{'authorId': '1717955', 'name': 'James C. Lester'}, {'authorId': '143980642', 'name': 'Brian A. Stone'}, {'authorId': '2782142', 'name': 'Gary D. Stelling'}]",218.0,"{'bibtex': '@Article{Lester2004LifelikePA,\n author = {James C. Lester and Brian A. Stone and Gary D. Stelling},\n journal = {User Modeling and User-Adapted Interaction},\n pages = {1-44},\n title = {Lifelike Pedagogical Agents for Mixed-initiative Problem Solving in Constructivist Learning Environments},\n volume = {9},\n year = {2004}\n}\n'}",,"{'volume': '9', 'pages': '1-44', 'name': 'User Modeling and User-Adapted Interaction'}",69.0,Lifelike Pedagogical Agents for Mixed-initiative Problem Solving in Constructivist Learning Environments,2004.0
1139,5a26a2148fac588e4aa2ab5e77d36b6e503ff6de,"This paper presents a novel solution for realtime generation of stylistic human motion that automatically transforms unlabeled, heterogeneous motion data into new styles. The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models (MAR) to capture the complex relationships between styles of motion. We construct local MAR models on the fly by searching for the closest examples of each input pose in the database. Once the model parameters are estimated from the training data, the model adapts the current pose with simple linear transformations. In addition, we introduce an efficient local regression model to predict the timings of synthesized poses in the output style. We demonstrate the power of our approach by transferring stylistic human motion for a wide variety of actions, including walking, running, punching, kicking, jumping and transitions between those behaviors. Our method achieves superior performance in a comparison against alternative methods. We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system.","[{'authorId': '2314567', 'name': 'Shi-hong Xia'}, {'authorId': '2116638364', 'name': 'Congyi Wang'}, {'authorId': '1759700', 'name': 'Jinxiang Chai'}, {'authorId': '1788773', 'name': 'J. Hodgins'}]",150.0,"{'bibtex': '@Article{Xia2015RealtimeST,\n author = {Shi-hong Xia and Congyi Wang and Jinxiang Chai and J. Hodgins},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 10},\n title = {Realtime style transfer for unlabeled heterogeneous human motion},\n volume = {34},\n year = {2015}\n}\n'}",,"{'volume': '34', 'pages': '1 - 10', 'name': 'ACM Transactions on Graphics (TOG)'}",22.0,Realtime style transfer for unlabeled heterogeneous human motion,2015.0
1140,5a6e961426816d44953d7db02c75b51cbe7491df,"Collaborative Virtual Environments (CVEs) were designed as an expansion of the text-based chat room, rather than a novel application, exploiting the possibilities of online three dimensional graphical space. This initial design direction is observable at the interface level. We put forward the case that to achieve an efficient CVE system, one will have to design and implement a multi modal User Interface based on expressive Avatars as a representation of the different participants, also as an embodiment of software agents. We emphasise the expressiveness of the avatar as a crucial improvement to the efficiency of their communication capabilities, and we describe a vocabulary of expressions to be implemented. We put forward the case that to be more efficient, particularly during a dialogue, an avatar is required to play a role in the communication using non-verbal channels such as body postures, facial expressions and hand gestures. We also suggest conversation circles to facilitate the gathering of participants in a discussion. These circles will address navigation difficulties in CVEs and encourage social exchanges.","[{'authorId': '145909342', 'name': 'Benjamin Salem'}, {'authorId': '3461880', 'name': 'N. Earle'}]",72.0,"{'bibtex': '@Inproceedings{Salem2000DesigningAN,\n author = {Benjamin Salem and N. Earle},\n pages = {93-101},\n title = {Designing a non-verbal language for expressive avatars},\n year = {2000}\n}\n'}",,{'pages': '93-101'},15.0,Designing a non-verbal language for expressive avatars,2000.0
1141,5a7bbddf0d76cf3f0f575f2e2306456efba9dea4,,"[{'authorId': '47427643', 'name': 'J. Okuda'}, {'authorId': '15381401', 'name': 'T. Fujii'}, {'authorId': '32867660', 'name': 'H. Ohtake'}, {'authorId': '3111494', 'name': 'T. Tsukiura'}, {'authorId': '2754748', 'name': 'K. Tanji'}, {'authorId': '2109096190', 'name': 'Kyoko Suzuki'}, {'authorId': '144820330', 'name': 'R. Kawashima'}, {'authorId': '144596379', 'name': 'H. Fukuda'}, {'authorId': '46751096', 'name': 'M. Itoh'}, {'authorId': '3549564', 'name': 'A. Yamadori'}]",595.0,"{'bibtex': '@Article{Okuda2003ThinkingOT,\n author = {J. Okuda and T. Fujii and H. Ohtake and T. Tsukiura and K. Tanji and Kyoko Suzuki and R. Kawashima and H. Fukuda and M. Itoh and A. Yamadori},\n journal = {NeuroImage},\n pages = {1369-1380},\n title = {Thinking of the future and past: the roles of the frontal pole and the medial temporal lobes},\n volume = {19},\n year = {2003}\n}\n'}",,"{'volume': '19', 'pages': '1369-1380', 'name': 'NeuroImage'}",46.0,Thinking of the future and past: the roles of the frontal pole and the medial temporal lobes,2003.0
1142,5a7e59f69e9d1cdb10c46a366fe9d97855a1350c,"Reconstructing 3D face shape from a single 2D photograph as well as from video is an inherently ill-posed problem with many ambiguities. One way to solve some of the ambiguities is using a 3D face model to aid the task. 3D Morphable Face Models (3DMMs) are amongst the state of the art methods for 3D face reconstruction, or so called 3D model fitting. However, current existing methods have severe limitations, and most of them have not been trialled on in-the-wild data. Current analysis-by-synthesis methods form complex non-linear optimisation processes, and optimisers often get stuck in local optima. Further, most existing methods are slow, requiring in the order of minutes to process one photograph. 
This thesis presents an algorithm to reconstruct 3D face shape from a single image as well as from sets of images or video frames in real-time. We introduce a solution for linear fitting of a PCA shape identity model and expression blendshapes to 2D facial landmarks. To improve the accuracy of the shape, a fast face contour fitting algorithm is introduced. These different components of the algorithm are run in iteration, resulting in a fast, linear shape-to-landmarks fitting algorithm. The algorithm, specifically designed to fit to landmarks obtained from in-the-wild images, by tackling imaging conditions that occur in in-the-wild images like facial expressions and the mismatch of 2D–3D contour correspondences, achieves the shape reconstruction accuracy of much more complex, nonlinear state of the art methods, while being multiple orders of magnitudes faster. 
 Second, we address the problem of fitting to sets of multiple images of the same person, as well as monocular video sequences. We extend the proposed shape-tolandmarks fitting to multiple frames by using the knowledge that all images are from the same identity. To recover facial texture, the approach uses texture from the original images, instead of employing the often-used PCA albedo model of a 3DMM. We employ an algorithm that merges texture from multiple frames in real-time based on a weighting of each triangle of the reconstructed shape mesh. 
 Last, we make the proposed real-time 3D morphable face model fitting algorithm available as open-source software. In contrast to ubiquitous available 2D-based face models and code, there is a general lack of software for 3D morphable face model fitting, hindering a widespread adoption. The library thus constitutes a significant contribution to the community.","[{'authorId': '39976184', 'name': 'P. Huber'}]",5.0,"{'bibtex': '@Inproceedings{Huber2017Realtime3M,\n author = {P. Huber},\n title = {Real-time 3D morphable shape model fitting to monocular in-the-wild videos},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Real-time 3D morphable shape model fitting to monocular in-the-wild videos,2017.0
1143,5a842a3c66e77706d842b3a06ac3b13685171498,"The attachment behavioral system: basic concepts and principles -- A model of attachment-system functioning and dynamics in adulthood -- Normative attachment processes -- Measurement of attachment-related constructs in adulthood -- Individual differences in attachment-system functioning: development, stability, and change -- Attachment-related mental representations of self and others -- Attachment processes and emotion regulation -- Attachment orientations, behavioral self-regulation, and personal growth -- An attachment perspective on interpersonal regulation -- Attachment processes and couple functioning -- Relations between the attachment and caregiving systems -- Attachment and sex -- Attachment bases of psychopathology -- Implications of attachment theory and research for counseling and psychotherapy -- Applications of attachment theory and research in group and organizational settings -- Reflections on attachment security.","[{'authorId': '4021295', 'name': 'M. Mikulincer'}, {'authorId': '4509891', 'name': 'P. Shaver'}]",3560.0,"{'bibtex': '@Inproceedings{Mikulincer2007AttachmentIA,\n author = {M. Mikulincer and P. Shaver},\n title = {Attachment in Adulthood: Structure, Dynamics, and Change},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Attachment in Adulthood: Structure, Dynamics, and Change",2007.0
1144,5a9cac54de14e58697d0315fe3c01f3dbe69c186,"GROUNDING It takes two people working together to play a duet, shake hands, play chess, waltz, teach, or make love. To succeed, the two of them have to coordinate both the content and process of what they are doing. Alan and Barbara, on the piano, must come to play the same Mozart duet. This is coordination of content. They must also synchronize their entrances and exits, coordinate how loudly to play forte and pianissimo, and otherwise adjust to each other's tempo and dynamics. This is coordination of process. They cannot even begin to coordinate on content without assuming a vast amount of shared information or common ground-that is, mutual knowledge, mutual beliefs, and mutual assumptions And to coordinate on process, they need to update their common ground moment by moment. All collective actions are built on common ground and its accumulation. We thank many colleagues for discussion of the issues we take up here.","[{'authorId': '29224904', 'name': 'H. H. Clark'}, {'authorId': '71463834', 'name': 'S. Brennan'}]",4402.0,"{'bibtex': '@Inproceedings{Clark1991GroundingIC,\n author = {H. H. Clark and S. Brennan},\n pages = {127-149},\n title = {Grounding in communication},\n year = {1991}\n}\n'}",,{'pages': '127-149'},42.0,Grounding in communication,1991.0
1145,5aa693ba3415fe7353f106af33277491c14e8cb7,,"[{'authorId': '2287674', 'name': 'Kevin El Haddad'}, {'authorId': '1996210170', 'name': 'Ilaria Torre'}, {'authorId': '2977217', 'name': 'E. Gilmartin'}, {'authorId': '80951794', 'name': 'Hüseyin Çakmak'}, {'authorId': '144436412', 'name': 'S. Dupont'}, {'authorId': '49164810', 'name': 'T. Dutoit'}, {'authorId': '143731137', 'name': 'N. Campbell'}]",20.0,"{'bibtex': '@Inproceedings{Haddad2017IntroducingAT,\n author = {Kevin El Haddad and Ilaria Torre and E. Gilmartin and Hüseyin Çakmak and S. Dupont and T. Dutoit and N. Campbell},\n pages = {229-240},\n title = {Introducing AmuS: The Amused Speech Database},\n year = {2017}\n}\n'}",,{'pages': '229-240'},50.0,Introducing AmuS: The Amused Speech Database,2017.0
1146,5aae5913137cfb296db85c082c349af251e1152f,"Generating human-like behaviors for virtual agents has become increasingly important in many applications, such as crowd simulation, virtual training, digital entertainment, and safety planning. One of challenging issues in behavior modeling is how virtual agents make decisions given some time-critical and uncertain situations. In this paper, we present HumDPM, a decision process model for virtual agents, which incorporates two important factors of human decision making in time-critical situations: experience and emotion. In HumDPM, rather than relying on deliberate rational analysis, an agent makes its decisions by matching past experience cases to the current situation. We propose the detailed representation of experience case and investigate the mechanisms of situation assessment, experience matching and experience execution. To incorporate emotion into HumDPM, we introduce an emotion appraisal process in situation assessment for emotion elicitation. In HumDPM, the decision making process of an agent may be affected by its emotional states when: 1) deciding whether it is necessary to do a re-match of experience cases, 2) determining the situational context, and 3) selecting experience cases. We illustrate the effectiveness of HumDPM in crowd simulation. A case study for emergency evacuation in a subway station scenario is conducted, which shows how a varied crowd composition leads to different evacuation behaviors, due to the retrieval of different experiences and the variation of agents' emotional states.","[{'authorId': '49571827', 'name': 'Linbo Luo'}, {'authorId': '145038982', 'name': 'Suiping Zhou'}, {'authorId': '1688786', 'name': 'Wentong Cai'}, {'authorId': '143737543', 'name': 'M. Lees'}, {'authorId': '1696470', 'name': 'M. Low'}]",17.0,"{'bibtex': '@Article{Luo2010ModelingHD,\n author = {Linbo Luo and Suiping Zhou and Wentong Cai and M. Lees and M. Low},\n journal = {2010 International Conference on Cyberworlds},\n pages = {360-367},\n title = {Modeling Human-Like Decision Making for Virtual Agents in Time-Critical Situations},\n year = {2010}\n}\n'}",,"{'pages': '360-367', 'name': '2010 International Conference on Cyberworlds'}",14.0,Modeling Human-Like Decision Making for Virtual Agents in Time-Critical Situations,2010.0
1147,5af5095704d56f3611471a397692926dbc0a2642,,"[{'authorId': '3890791', 'name': 'T. Ruffman'}, {'authorId': '2149900272', 'name': 'J. Henry'}, {'authorId': '2089621685', 'name': 'Vicki Livingstone'}, {'authorId': '1994871', 'name': 'L. Phillips'}]",772.0,"{'bibtex': '@Article{Ruffman2008AMR,\n author = {T. Ruffman and J. Henry and Vicki Livingstone and L. Phillips},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {863-881},\n title = {A meta-analytic review of emotion recognition and aging: Implications for neuropsychological models of aging},\n volume = {32},\n year = {2008}\n}\n'}",,"{'volume': '32', 'pages': '863-881', 'name': 'Neuroscience & Biobehavioral Reviews'}",210.0,A meta-analytic review of emotion recognition and aging: Implications for neuropsychological models of aging,2008.0
1150,5b07c4016a6547e1211a172fe5ea95db5d463b16,"ion, Etc. Part II Of the Ideas of Space and Time, Part III Of Knowledge and Probability Part IV Of the Sceptical and Other Systems of","[{'authorId': '152806633', 'name': 'David Hume'}]",10442.0,"{'bibtex': '@Inproceedings{Hume1972ATO,\n author = {David Hume},\n title = {A Treatise of Human Nature: Being an Attempt to introduce the experimental Method of Reasoning into Moral Subjects},\n year = {1972}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,A Treatise of Human Nature: Being an Attempt to introduce the experimental Method of Reasoning into Moral Subjects,1972.0
1151,5b25edaf99383629ddda15e5fb5975ab3489205b,"Prior work shows that setting limits on young children's screen time is conducive to healthy development but can be a challenge for families. We investigate children's (age 1 - 5) transitions to and from screen-based activities to understand the boundaries families have set and their experiences living within them. We report on interviews with 27 parents and a diary study with a separate 28 families examining these transitions. These families turn on screens primarily to facilitate parents' independent activities. Parents feel this is appropriate but self-audit and express hesitation, as they feel they are benefiting from an activity that can be detrimental to their child's well-being. We found that families turn off screens when parents are ready to give their child their full attention and technology presents a natural stopping point. Transitioning away from screens is often painful, and predictive factors determine the pain of a transition. Technology-mediated transitions are significantly more successful than parent-mediated transitions, suggesting that the design community has the power to make this experience better for parents and children by creating technologies that facilitate boundary-setting and respect families' self-defined limits.","[{'authorId': '2749233', 'name': 'Alexis Hiniker'}, {'authorId': '83550719', 'name': 'Hye-Ji Suh'}, {'authorId': '3396372', 'name': 'Sabina Cao'}, {'authorId': '1738606', 'name': 'J. Kientz'}]",101.0,"{'bibtex': '@Article{Hiniker2016ScreenTT,\n author = {Alexis Hiniker and Hye-Ji Suh and Sabina Cao and J. Kientz},\n journal = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},\n title = {Screen Time Tantrums: How Families Manage Screen Media Experiences for Toddlers and Preschoolers},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems'},57.0,Screen Time Tantrums: How Families Manage Screen Media Experiences for Toddlers and Preschoolers,2016.0
1152,5b5a5c0c4a120da83895022bb2dc5ed2b6ac7a33,"Reverse appraisal: The importance of appraisals for the effect of emotion displays on people’s decision making in a social dilemma Celso M. de Melo (demelo@ict.usc.edu) Jonathan Gratch (gratch@ict.usc.edu) Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Building #4 Playa Vista, CA 90094-2536, USA Peter Carnevale (peter.carnevale@marshall.usc.edu) USC Marshall School of Business, Los Angeles, CA 90089-0808, USA Stephen Read (read@rcf.usc.edu) USC Department of Psychology, Los Angeles, CA 90089-1061, USA and maximize joint long-term reward (Kollock, 1998). Empirical evidence confirms that facial displays of emotion can impact cooperation (e.g., de Melo, Carnevale, & Gratch, 2012; Schug, Matsumoto, Horita, Yamagishi, & Bonnet, 2010). However, the mechanism by which such social effects of emotion are achieved is less well understood. In this paper we address this issue and look at appraisal theories of emotion to understand what is the information people retrieve from emotion displays and how is that accomplished. In appraisal theories, emotion displays arise from cognitive appraisal of events with respect to an agent’s goals, desires and beliefs (e.g., is this event congruent with my goals? Who is responsible for this event?). According to the pattern of appraisals that occurs, different emotions are experienced and displayed. Since displays reflect the agent’s intentions through the appraisal process, it is also plausible to ask whether people can infer from emotion displays the agent’s goals by reversing the appraisal mechanism. We refer to this proposal as reverse appraisal. The intuition is that if appraisal, abstractly, is a function that maps from to emotion, reverse appraisal is a function that maps from to mental state. Empirical evidence is still scarce but in a recent study Hareli and Hess (2010) showed that people could, from expressed emotion, make inferences about the character of the person displaying emotion. For instance, a person who reacted with anger to blame was perceived as being more aggressive, self-confident but also as less warm and gentle than a person who reacted with sadness. Moreover, the results showed that such inferences were mediated by appraisal variables. In our case, reverse appraisal predicts that people infer, from emotion displays, how the counterpart is appraising the social dilemma outcomes; then, from these perceptions of appraisal, people infer how likely the counterpart is to cooperate in the future, which we refer to as perceptions of cooperativeness. This causal model is shown in Figure 1. The goal of the paper is to establish this causal model. Abstract Two studies are presented that explore the interpersonal effect of emotion displays in decision making in a social dilemma. Experiment 1 (N=405) showed that facial displays of emotion (joy, sadness, anger and guilt) had an effect on perception of how the person was appraising the social dilemma outcomes (perception of appraisals) and on perception of how likely the person was to cooperate in the future (perception of cooperation). Experiment 1 also showed that perception of appraisals (partially and, in some cases, fully) mediated the effect of emotion displays on perception of cooperation. Experiment 2 (N=202) showed that manipulating perception of appraisals, by expressing them textually, produced an effect on perception of cooperation thus, providing evidence for a causal model where emotion displays cause perception of appraisals which, in turn, cause perception of cooperation. In line with Hareli and Hess’ (2010) findings and a social- functions view of emotion, we advance the reverse appraisal proposal that argues people can infer, from emotion displays, how others are appraising a situation which, in turn, support inferences that are relevant for decision making. We discuss implications of these results and proposal to decision and emotion theory. Keywords: Emotion Displays, Decision Making, Social Dilemma, Appraisal Theories, Reverse Appraisal Introduction Recent decades have seen growing interest on the interpersonal effect of emotion in decision making (e.g., Van Kleef, De Dreu, & Manstead, 2010). Complementing research on the impact of emotion in one’s own decision making (for a recent review see Blanchette & Richards, 2010), this research explores how one’s emotion displays impact another’s decision making and emphasizes that emotional expressions are not simple manifestations of internal experience; rather, expressions are other-directed and communicate intentions, desired courses of actions, expectations and behaviors (Frijda & Mesquita, 1994; Keltner & Kring, 1998). The expression of emotion has also been argued to play a significant role in emergence of cooperation in social dilemmas (Boone & Buck, 2003; Frank, 1988). Social dilemmas, such as the prisoner’s dilemma, are situations where people must choose between pursuing their own self-interest and collect a short-term reward or trust another person to reach mutual cooperation Emotion Display i Perceptions of Appraisals ii Perceptions of Cooperativeness Figure 1: Causal model for the effect of emotion displays in cooperation in a social dilemma.","[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '144940159', 'name': 'S. Read'}]",24.0,"{'bibtex': ""@Article{Melo2012ReverseAT,\n author = {C. D. Melo and J. Gratch and P. Carnevale and S. Read},\n journal = {Cognitive Science},\n title = {Reverse appraisal: The importance of appraisals for the effect of emotion displays on people's decision making in a social dilemma},\n volume = {34},\n year = {2012}\n}\n""}",,"{'volume': '34', 'name': 'Cognitive Science'}",20.0,Reverse appraisal: The importance of appraisals for the effect of emotion displays on people's decision making in a social dilemma,2012.0
1153,5b6730a1372792e71e026942fbd45b474ffd2ed2,"We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '80905955', 'name': 'Kyra Kapsaskis'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",19.0,"{'bibtex': '@Article{Randhavane2019FVAMP,\n author = {Tanmay Randhavane and Aniket Bera and Kyra Kapsaskis and Kurt Gray and Dinesh Manocha},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {3135-3145},\n title = {FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement Characteristics},\n volume = {25},\n year = {2019}\n}\n'}",,"{'volume': '25', 'pages': '3135-3145', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",64.0,FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement Characteristics,2019.0
1154,5b79ff699b21d2c9122e876bb93ff7dd0f5d7993,"A computational model of emotion must explain both the rapid dynamics of some emotional reactions as well as the slower responses that follow deliberation. This is often addressed by positing multiple appraisal processes such as fast pattern directed vs. slower deliberative appraisals. In our view, this confuses appraisal with inference. Rather, we argue for a single and automatic appraisal process that operates over a person’s interpretation of their relationship to the environment. Dynamics arise from perceptual and inferential processes operating on this interpretation (including deliberative and reactive processes). We illustrate this perspective through the computational modeling of a naturalistic emotional situation.","[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",68.0,"{'bibtex': '@Inproceedings{Marsella2006EMAAC,\n author = {S. Marsella and J. Gratch},\n title = {EMA: A computational model of appraisal dynamics},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",13.0,EMA: A computational model of appraisal dynamics,2006.0
1155,5b97856b564eb552777deca1bb794d4f91e18f46,,"[{'authorId': '1405594772', 'name': 'Jesse Spencer-Smith'}, {'authorId': '39512862', 'name': 'Heather A. Wild'}, {'authorId': '1402996412', 'name': 'Åse Innes-Ker'}, {'authorId': '1923072', 'name': 'J. Townsend'}, {'authorId': '1409054566', 'name': 'Christy Duffy'}, {'authorId': '39544460', 'name': 'Chad R. Edwards'}, {'authorId': '1409092922', 'name': 'Kristina Ervin'}, {'authorId': '23601231', 'name': 'Nicole Merritt'}, {'authorId': '2240740205', 'name': 'Jae Won Pair'}]",71.0,"{'bibtex': '@Article{Spencer-Smith2001MakingFC,\n author = {Jesse Spencer-Smith and Heather A. Wild and Åse Innes-Ker and J. Townsend and Christy Duffy and Chad R. Edwards and Kristina Ervin and Nicole Merritt and Jae Won Pair},\n journal = {Behavior Research Methods, Instruments, & Computers},\n pages = {115-123},\n title = {Making faces: Creating three-dimensional parameterized models of facial expression},\n volume = {33},\n year = {2001}\n}\n'}",,"{'volume': '33', 'pages': '115-123', 'name': 'Behavior Research Methods, Instruments, & Computers'}",17.0,Making faces: Creating three-dimensional parameterized models of facial expression,2001.0
1156,5bb63acee23a630fa242f193d5b9abfb6ffcd9ec,,"[{'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '1405505392', 'name': 'Laila Bukhari'}, {'authorId': '1787101', 'name': 'L. Vardoulakis'}, {'authorId': '1397166245', 'name': 'M. Paasche-Orlow'}, {'authorId': '2072882045', 'name': 'Christopher Shanahan'}]",22.0,"{'bibtex': '@Inproceedings{Bickmore2012HospitalBA,\n author = {T. Bickmore and Laila Bukhari and L. Vardoulakis and M. Paasche-Orlow and Christopher Shanahan},\n pages = {492-495},\n title = {Hospital Buddy: A Persistent Emotional Support Companion Agent for Hospital Patients},\n year = {2012}\n}\n'}",,{'pages': '492-495'},1.0,Hospital Buddy: A Persistent Emotional Support Companion Agent for Hospital Patients,2012.0
1158,5bcbcf866fccb34e052507c951d6012525523d09,"The statistic P rep estimates the probability of replicating an effect. It captures traditional publication criteria for signal-to-noise ratio, while avoiding parametric inference and the resulting Bayesian dilemma. In concert with effect size and replication intervals, P rep provides all of the information now used in evaluating research, while avoiding many of the pitfalls of traditional statistical inference.","[{'authorId': '2241098672', 'name': 'Peter R. Killeen'}]",460.0,"{'bibtex': '@Article{Killeen2005AnAT,\n author = {Peter R. Killeen},\n journal = {Psychological Science},\n pages = {345 - 353},\n title = {An Alternative to Null-Hypothesis Significance Tests},\n volume = {16},\n year = {2005}\n}\n'}",,"{'volume': '16', 'pages': '345 - 353', 'name': 'Psychological Science'}",47.0,An Alternative to Null-Hypothesis Significance Tests,2005.0
1159,5c4824f17261cd1be5ee212a390897a65754c466,"This paper describes the evaluation of the persona effect of a speech-driven anthropomorphic agent that has been embodied in the interface of an intelligent tutoring system (ITS). This agent is responsible for guiding the student in the environment and communicating the system's feedback messages. The agent was evaluated in terms of the effect that it could have on students' learning, behaviour and experience. The participants in the experiment were divided into two groups: half of them worked with a version of the ITS which embodied the agent and the rest worked with an agent-less version. The results from this study confirm the hypothesis that a pedagogical agent incorporated in an ITS can enhance students' learning experience. On the other hand, the hypothesis that the presence of the agent improves short-term learning effects was rejected.","[{'authorId': '1781216', 'name': 'Maria Moundridou'}, {'authorId': '1694669', 'name': 'M. Virvou'}]",145.0,"{'bibtex': '@Article{Moundridou2002EvaluatingTP,\n author = {Maria Moundridou and M. Virvou},\n journal = {J. Comput. Assist. Learn.},\n pages = {253-261},\n title = {Evaluating the persona effect of an interface agent in a tutoring system},\n volume = {18},\n year = {2002}\n}\n'}",,"{'volume': '18', 'pages': '253-261', 'name': 'J. Comput. Assist. Learn.'}",8.0,Evaluating the persona effect of an interface agent in a tutoring system,2002.0
1160,5c4a849694ba1ea569573ab7beb7c73460ae91ad,"This paper proposes a framework for animation that can achieve the intricacy of motion evident in certain natural ecosystems with minimal input from the animator. The realistic appearance, movement, and behavior of individual animals, as well as the patterns of behavior evident in groups of animals fall within the scope of the framework. Our approach to emulating this level of natural complexity is to model each animal holistically as an autonomous agent situated in its physical world. To demonstrate the approach, we develop a physics-based, virtual marine world. The world is inhabited by artificial fishes that can swim hydrodynamically in simulated water through the motor control of internal muscles that motivates fins. Their repertoire of behaviors relies on their perception of the dynamic environment. As in nature, the detailed motions of artificial fishes in their virtual habitat are not entirely predictable because they are not scripted.","[{'authorId': '40509745', 'name': 'Xiaoyuan Tu'}, {'authorId': '1750924', 'name': 'Demetri Terzopoulos'}]",835.0,"{'bibtex': '@Article{Tu1994ArtificialFP,\n author = {Xiaoyuan Tu and Demetri Terzopoulos},\n journal = {Proceedings of the 21st annual conference on Computer graphics and interactive techniques},\n title = {Artificial fishes: physics, locomotion, perception, behavior},\n year = {1994}\n}\n'}",,{'name': 'Proceedings of the 21st annual conference on Computer graphics and interactive techniques'},28.0,"Artificial fishes: physics, locomotion, perception, behavior",1994.0
1161,5c71a5bd507cf727ce838e76b6a0f877652187cf,"Extensive prior research has shown that the perception of an emotional facial expression automatically elicits a corresponding facial expression in the observer. Theories of embodied emotion, however, suggest that such reactions might also occur across expressive channels, because simulation is based on integrated motoric and affective representations of that emotion. In the present studies, we examined this idea by focusing on facial and experiential reactions to nonverbal emotion vocalizations. In Studies 1 and 2, we showed that both hearing and reproducing vocalizations of anger, disgust, happiness, and sadness resulted in specific facial behaviors, as well as congruent self-reported emotions (Study 2). In Studies 3 and 4, we showed that the inhibition of congruent facial actions impaired listeners' processing of emotion vocalizations (Study 3), as well as their experiences of a concordant subjective state (Study 4). Results support the idea that cross-channel simulations of others' states serve facilitative functions similar to more strict imitations of observed expressive behavior, suggesting flexibility in the motoric and affective systems involved in emotion processing and interpersonal emotion transfer. We discuss implications for embodiment research and the social consequences of expressing and matching emotions across nonverbal channels.","[{'authorId': '2390460', 'name': 'Skyler T. Hawk'}, {'authorId': '7444483', 'name': 'A. Fischer'}, {'authorId': '5980688', 'name': 'Gerben A. van Kleef'}]",123.0,"{'bibtex': '@Article{Hawk2012FaceTN,\n author = {Skyler T. Hawk and A. Fischer and Gerben A. van Kleef},\n journal = {Journal of personality and social psychology},\n pages = {\n          796-814\n        },\n title = {Face the noise: embodied responses to nonverbal vocalizations of discrete emotions.},\n volume = {102 4},\n year = {2012}\n}\n'}",,"{'volume': '102 4', 'pages': '\n          796-814\n        ', 'name': 'Journal of personality and social psychology'}",101.0,Face the noise: embodied responses to nonverbal vocalizations of discrete emotions.,2012.0
1162,5c8258a374b68af2f762f00d50cf7e9929f93431,,"[{'authorId': '9765768', 'name': 'K. Pelphrey'}, {'authorId': '4378420', 'name': 'N. Sasson'}, {'authorId': '2196228955', 'name': 'J. Reznick'}, {'authorId': '2057125948', 'name': 'Gregory Paul'}, {'authorId': '144387696', 'name': 'B. Goldman'}, {'authorId': '2718912', 'name': 'J. Piven'}]",1176.0,"{'bibtex': '@Article{Pelphrey2002VisualSO,\n author = {K. Pelphrey and N. Sasson and J. Reznick and Gregory Paul and B. Goldman and J. Piven},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {249-261},\n title = {Visual Scanning of Faces in Autism},\n volume = {32},\n year = {2002}\n}\n'}",,"{'volume': '32', 'pages': '249-261', 'name': 'Journal of Autism and Developmental Disorders'}",73.0,Visual Scanning of Faces in Autism,2002.0
1163,5ca1a4c07ff76c42c0e02df6dba13a58fa1bd95e,,"[{'authorId': '1963261', 'name': 'Cory-Ann Smarr'}]",0.0,"{'bibtex': '@Inproceedings{Smarr2011EmotionAM,\n author = {Cory-Ann Smarr},\n title = {Emotion and motion: age-related differences in recognizing virtual agent facial expressions},\n year = {2011}\n}\n'}",[],"{'name': '', 'volume': ''}",35.0,Emotion and motion: age-related differences in recognizing virtual agent facial expressions,2011.0
1164,5cd5c8bbfdf02c48105f2d515627d447e31a2bdb,"Emotion recognition is one of the most challenging tasks in the field of speech recognition and processing. Besides, it is deemed a significant branch of Human-Computer Interaction (HCI). Although there exist several techniques related to speech emotion recognition (SER), in this study, machine learning approaches for feature extraction is used. To be precise, the Mel-frequency Cepstrum Coefficient (MFCC) and Modulation Spectral (MS) is implemented to extract the relevant features from Bengali speech. Our study focuses on recognizing several states of emotion such as joy, sadness, anger, surprise, fear, and disgust from Bengali speech by exploring an emotion recognition model. Besides, in this study, a novel Bengali speech dataset is developed and used since no proper study, specifically for Bengali, on this topic is available in the literature yet. In this study, the recurrent neural network (RNN) technology is used to categorize the human voice (speech) into six particular states of emotion. Finally, our study shows that while the RNN classifier model achieves 47.66% accuracy with our Bengali speech dataset, achieved 51.33% accuracy when the dataset is first categorized using the RNN modulation technique.","[{'authorId': '2065954783', 'name': 'H. Hasan'}, {'authorId': '7484275', 'name': 'Md. Adnanul Islam'}]",14.0,"{'bibtex': '@Article{Hasan2020EmotionRF,\n author = {H. Hasan and Md. Adnanul Islam},\n journal = {2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT)},\n pages = {1131-1136},\n title = {Emotion Recognition from Bengali Speech using RNN Modulation-based Categorization},\n year = {2020}\n}\n'}",,"{'pages': '1131-1136', 'name': '2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT)'}",33.0,Emotion Recognition from Bengali Speech using RNN Modulation-based Categorization,2020.0
1165,5cf0d213f3253cd46673d955209f8463db73cc51,,"[{'authorId': '2106794', 'name': 'C. Busso'}, {'authorId': '38816202', 'name': 'M. Bulut'}, {'authorId': '2467369', 'name': 'Chi-Chun Lee'}, {'authorId': '1764265', 'name': 'Ebrahim (Abe) Kazemzadeh'}, {'authorId': '2523983', 'name': 'E. Provost'}, {'authorId': '2110026741', 'name': 'Samuel Kim'}, {'authorId': '2522842', 'name': 'Jeannette N. Chang'}, {'authorId': '2108057415', 'name': 'Sungbok Lee'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",2419.0,"{'bibtex': '@Article{Busso2008IEMOCAPIE,\n author = {C. Busso and M. Bulut and Chi-Chun Lee and Ebrahim (Abe) Kazemzadeh and E. Provost and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},\n journal = {Language Resources and Evaluation},\n pages = {335-359},\n title = {IEMOCAP: interactive emotional dyadic motion capture database},\n volume = {42},\n year = {2008}\n}\n'}",,"{'volume': '42', 'pages': '335-359', 'name': 'Language Resources and Evaluation'}",66.0,IEMOCAP: interactive emotional dyadic motion capture database,2008.0
1167,5d2a8502542f10aa6d132827467096b389a98ba4,"The core symptom of children with autism is social difficulties. According to the research, one of the main psychological factors supposed to underlie in these difficulties is the lack or low levels of joint attention with the interaction partners. The use of robots in autism spectrum disorder (ASD) interventions has received a lot of attention in the last years. Robots can achieve high levels of effectiveness in interacting with children with autism. This paper presents robots that play several important roles and benefits in the interaction of children with autism. In the absence of dialogue corpus, we collected and integrated conversation data for children with autism. We present to use a neural network to build a robot dialogue system that generates answers freely without restrictions, and design robot movements to attract attention from children with autism. Most importantly, the robot will interact smoothly with autistic children without human intervention.","[{'authorId': '153326943', 'name': 'Tianhao She'}, {'authorId': '144018134', 'name': 'Xin Kang'}, {'authorId': '1792084', 'name': 'S. Nishide'}, {'authorId': '145366409', 'name': 'F. Ren'}]",10.0,"{'bibtex': '@Article{She2018ImprovingLR,\n author = {Tianhao She and Xin Kang and S. Nishide and F. Ren},\n journal = {2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)},\n pages = {416-420},\n title = {Improving LEO Robot Conversational Ability via Deep Learning Algorithms for Children with Autism},\n year = {2018}\n}\n'}",,"{'pages': '416-420', 'name': '2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)'}",0.0,Improving LEO Robot Conversational Ability via Deep Learning Algorithms for Children with Autism,2018.0
1168,5d447ebed18b14b45cfbe31ff6673b6fbe4d7f58,"When game studies has tackled the player-character, it has tended to do so by means of an oppositon to the notion of the avatar, with the result that the ontological and semiotic nature of the character in itself has not been given due attention. This paper draws on understandings of character from the fields of narratology and literary theory to highlight the double-layered ontology of character as both a possible individual and as a semiotic construction. Uri Margolin’s narratological model of character signification is used as the basis for developing a semiotic-structural model of the player-character that addresses its specific medialities and formal nature – a task which is performed through illustrative close examinations of the player-characters in The Last of Us (Naughty Dog 2013) and Gone Home (The Fullbright Company 2013).","[{'authorId': '20777014', 'name': 'Daniel Vella'}]",12.0,"{'bibtex': '@Inproceedings{Vella2014ModelingTS,\n author = {Daniel Vella},\n title = {Modeling the Semiotic Structure of Game Characters},\n volume = {8},\n year = {2014}\n}\n'}",,"{'volume': '8', 'name': ''}",40.0,Modeling the Semiotic Structure of Game Characters,2014.0
1169,5d459e987b400e9a39bf18fd97543c6bf129c772,"Although global measures of social support demonstrate significant effects on psychological and physical well-being, the differential significance of various support sources is largely unknown. The present study examines differences in the effects of functional expressive support by source on depressive symptoms. This approach is contrasted with network interaction studies of elderly persons, which do not measure functional support but do suggest that friends are distinctly significant. Spouse, friends, and adult children were found to rank in descending order of importance; relatives show no effect. Low support may have stronger effects than unavailability of sources. Effects of supports and stressors are not conditioned by age, sex, or widowhood. Implications of findings and further research needs are discussed.","[{'authorId': '74778617', 'name': 'A. Dean'}, {'authorId': '5691548', 'name': 'B. Kolody'}, {'authorId': '153329127', 'name': 'P. Wood'}]",266.0,"{'bibtex': '@Article{Dean1990EffectsOS,\n author = {A. Dean and B. Kolody and P. Wood},\n journal = {Journal of health and social behavior},\n pages = {\n          148-61\n        },\n title = {Effects of social support from various sources on depression in elderly persons.},\n volume = {31 2},\n year = {1990}\n}\n'}",,"{'volume': '31 2', 'pages': '\n          148-61\n        ', 'name': 'Journal of health and social behavior'}",49.0,Effects of social support from various sources on depression in elderly persons.,1990.0
1170,5d4d53231b8f5ca35928b7b1dee45838e7257ec4,"Studies reveal that when people are exposed to emotional facial expressions, they spontaneously react with distinct facial electromyographic (EMG) reactions in emotion-relevant facial muscles. These reactions reflect, in part, a tendency to mimic the facial stimuli. We investigated whether corresponding facial reactions can be elicited when people are unconsciously exposed to happy and angry facial expressions. Through use of the backward-masking technique, the subjects were prevented from consciously perceiving 30-ms exposures of happy, neutral, and angry target faces, which immediately were followed and masked by neutral faces. Despite the fact that exposure to happy and angry faces was unconscious, the subjects reacted with distinct facial muscle reactions that corresponded to the happy and angry stimulus faces. Our results show that both positive and negative emotional reactions can be unconsciously evoked, and particularly that important aspects of emotional face-to-face communication can occur on an unconscious level.","[{'authorId': '4583182', 'name': 'U. Dimberg'}, {'authorId': '3924673', 'name': 'M. Thunberg'}, {'authorId': '114829343', 'name': 'Kurt Elmehed'}]",1614.0,"{'bibtex': '@Article{Dimberg2000UnconsciousFR,\n author = {U. Dimberg and M. Thunberg and Kurt Elmehed},\n journal = {Psychological Science},\n pages = {86 - 89},\n title = {Unconscious Facial Reactions to Emotional Facial Expressions},\n volume = {11},\n year = {2000}\n}\n'}",,"{'volume': '11', 'pages': '86 - 89', 'name': 'Psychological Science'}",32.0,Unconscious Facial Reactions to Emotional Facial Expressions,2000.0
1171,5d67168e92f571dcc5a738ebd59570d047f14cce,"To address the issue of mental health, therapy chatbots have been developed in recent years to deliver methods such as Cognitive Behavioural Therapy. Self-Attachment is an alternative form of therapy intended to assist individuals so they can regulate their emotional state through developing and maintaining a bond with their childhood self. In this project, we create a chatbot to suggest Self-Attachment protocols based on a user’s emotion and description of recent events leading to that emotion. The chatbot will assist in the scaling of the delivery of Self-Attachment protocols. We utilise a rule-based model within this chatbot to determine which suggestions should be presented to users and ensure that dialogue presented to users is safe and fully controllable to avoid the possibility of harming the user through potentially harmful dialogue being presented by the model. We discuss a trial of the chatbot we performed with 9 participants from the non-clinical population with prior knowledge of Self-Attachment protocols, which shows the chatbot can provide suitable suggestions for Self-Attachment protocols. The chatbot is evaluated by 3 clinicians who had not previously practised Self-Attachment protocols. We also discuss a survey produced during this project, where users were asked to rewrite prompts to be more empathetic. We discuss how it has contributed to the formation of a dataset that can be utilised to enhance the chatbot’s responses to be more empathetic and engaging.",[],2.0,"{'bibtex': '@Inproceedings{None,\n title = {Evaluation of a Virtual Agent in Guiding Users from the Non-Clinical Population in Self-Attachment Intervention},\n year = {2021}\n}\n'}","[{'paperId': '5f110c3a0c22ec1245b6abcb84a142faa9925c8b', 'title': 'An Empathetic AI Coach for Self-Attachment Therapy'}, {'paperId': '20c3d9ab04cc01e605ebc2b656c573d75d3133fb', 'title': 'A deep-learning assisted empathetic guide for self-attachment therapy'}]",,48.0,Evaluation of a Virtual Agent in Guiding Users from the Non-Clinical Population in Self-Attachment Intervention,2021.0
1172,5d912177051796ddc0af58d16a4fcac6d0a2d9aa,"In the research described here we extend past computational investigations of animal signaling by studying an artificial world in which a population of initially noncommunicating agents evolves to communicate about food sources and predators. Signaling in this world can be either beneficial (e.g., warning of nearby predators) or costly (e.g., attracting predators or competing agents). Our goals were twofold: to examine systematically environmental conditions under which grounded signaling does or does not evolve, and to determine how variations in assumptions made about the evolutionary process influence the outcome. Among other things, we found that agents warning of nearby predators were a common occurrence whenever predators had a significant impact on survival and signaling could interfere with predator success. The setting most likely to lead to food signaling was found to be difficult-to-locate food sources that each have relatively large amounts of food. Deviations from the selection methods typically used in traditional genetic algorithms were also found to have a substantial impact on whether communication evolved. For example, constraining parent selection and child placement to physically neighboring areas facilitated evolution of signaling in general, whereas basing parent selection upon survival alone rather than survival plus fitness measured as success in food acquisition was more conducive to the emergence of predator alarm signals. We examine the mechanisms underlying these and other results, relate them to existing experimental data about animal signaling, and discuss their implications for artificial life research involving evolution of communication.","[{'authorId': '1742049', 'name': 'J. Reggia'}, {'authorId': '2056298301', 'name': 'Reiner Schulz'}, {'authorId': '35943890', 'name': 'G. Wilkinson'}, {'authorId': '3242972', 'name': 'Juan Uriagereka'}]",31.0,"{'bibtex': '@Article{Reggia2000ConditionsET,\n author = {J. Reggia and Reiner Schulz and G. Wilkinson and Juan Uriagereka},\n journal = {Artificial Life},\n pages = {3-32},\n title = {Conditions Enabling the Evolution of Inter-Agent Signaling in an Artificial World},\n volume = {7},\n year = {2000}\n}\n'}",,"{'volume': '7', 'pages': '3-32', 'name': 'Artificial Life'}",45.0,Conditions Enabling the Evolution of Inter-Agent Signaling in an Artificial World,2000.0
1173,5d99adf84df0915204bc7f1767af89d4f95d8ac1,,"[{'authorId': '3353285', 'name': 'U. Fischbacher'}, {'authorId': '2162670', 'name': 'S. Gächter'}, {'authorId': '144327037', 'name': 'E. Fehr'}]",2789.0,"{'bibtex': '@Article{Fischbacher2001ArePC,\n author = {U. Fischbacher and S. Gächter and E. Fehr},\n journal = {Microeconomic Theory eJournal},\n title = {Are People Conditionally Cooperative? Evidence from a Public Goods Experiment},\n year = {2001}\n}\n'}",,{'name': 'Microeconomic Theory eJournal'},69.0,Are People Conditionally Cooperative? Evidence from a Public Goods Experiment,2001.0
1174,5da28d4ba572a5cad37798de84d1a8ff08c17e31,"Shopping is an integral part of our everyday lives. Common wisdom suggests that many consumers engage in shopping and buying as a means to repair their negative feelings — a notion commonly referred to as retail therapy . However, does retail therapy really work? The present monograph seeks to address this question by proposing a tripartite approach, reviewing and organizing relevant research in marketing and consumer psychology based on this tripartite framework: (1) motivational (the goals and motives that consumers have for shopping); (2) behavioral (the activities in which consumers engage during the shopping process); and (3) emotional (the feelings that consumers experience while shopping). Although accumulating evidence suggests that retail therapy does work to a certain extent, simultaneously considering the three perspectives in future empirical investigation helps to further improve our understanding of the antecedents, underlying mechanisms, and consequences of retail therapy. Accordingly, a number of questions and directions for future research on the topic of retail therapy are discussed, drawing upon the proposed tripartite framework.","[{'authorId': '46419509', 'name': 'Leonard Lee'}]",20.0,"{'bibtex': '@Inproceedings{Lee2015TheES,\n author = {Leonard Lee},\n title = {The Emotional Shopper: Assessing the Effectiveness of Retail Therapy},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",180.0,The Emotional Shopper: Assessing the Effectiveness of Retail Therapy,2015.0
1175,5db96ca077a2e595a4e368e6ba037f053c5c4dbc,"When stimuli are learned by repetition, they are remembered better and retained for a longer time. However, current findings are lacking as to whether the medial temporal lobe (MTL) and cortical regions are involved in the learning effect when subjects retrieve associative memory, and whether their activations differentially change over time due to learning experience. To address these issues, we designed an fMRI experiment in which face-scene pairs were learned once (L1) or six times (L6). Subjects learned the pairs at four retention intervals, 30-min, 1-day, 1-week and 1-month, after which they finished an associative recognition task in the scanner. The results showed that compared to learning once, learning six times led to stronger activation in the hippocampus, but weaker activation in the perirhinal cortex (PRC) as well as anterior ventrolateral prefrontal cortex (vLPFC). In addition, the hippocampal activation was positively correlated with that of the parahippocampal place area (PPA) and negatively correlated with that of the vLPFC when the L6 group was compared to the L1 group. The hippocampal activation decreased over time after L1 but remained stable after L6. These results clarified how the hippocampus and cortical regions interacted to support associative memory after different learning experiences.","[{'authorId': '1760914', 'name': 'Lexia Zhan'}, {'authorId': '51055208', 'name': 'Dingrong Guo'}, {'authorId': '143793345', 'name': 'Gang Chen'}, {'authorId': '2109811156', 'name': 'Jiongjiong Yang'}]",35.0,"{'bibtex': '@Article{Zhan2018EffectsOR,\n author = {Lexia Zhan and Dingrong Guo and Gang Chen and Jiongjiong Yang},\n journal = {Frontiers in Human Neuroscience},\n title = {Effects of Repetition Learning on Associative Recognition Over Time: Role of the Hippocampus and Prefrontal Cortex},\n volume = {12},\n year = {2018}\n}\n'}",,"{'volume': '12', 'name': 'Frontiers in Human Neuroscience'}",88.0,Effects of Repetition Learning on Associative Recognition Over Time: Role of the Hippocampus and Prefrontal Cortex,2018.0
1176,5dc2a215bd7cd5bdd3a0baa8c967575632696fac,"Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; θ) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V (s, g; θ) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.","[{'authorId': '1725157', 'name': 'T. Schaul'}, {'authorId': '48257711', 'name': 'Dan Horgan'}, {'authorId': '144717963', 'name': 'Karol Gregor'}, {'authorId': '145824029', 'name': 'David Silver'}]",896.0,"{'bibtex': '@Inproceedings{Schaul2015UniversalVF,\n author = {T. Schaul and Dan Horgan and Karol Gregor and David Silver},\n pages = {1312-1320},\n title = {Universal Value Function Approximators},\n year = {2015}\n}\n'}",,{'pages': '1312-1320'},26.0,Universal Value Function Approximators,2015.0
1177,5dd298037cba0067e9a6eab5da886ac1ec926a00,"Examined whether spontaneous facial expressions provide observers with sufficient information to distinguish accurately which of 7 affective states (6 emotional and 1 neutral) is being experienced by another person. Six undergraduate senders' facial expressions were covertly videotaped as they watched emotionally loaded slides. After each slide, senders nominated the emotions term that best described their affective reaction and also rated the pleasantness and strength of that reaction. Similar nominations of emotion terms and ratings were later made by 53 undergraduate receivers who viewed the senders' videotaped facial expression. The central measure of communication accuracy was the match between senders' and receivers' emotion nominations. Overall accuracy was significantly greater than chance, although it was not impressive in absolute terms. Only happy, angry, and disgusted expressions were recognized at above-chance rates, whereas surprised expressions were recognized at rates that were significantly worse than chance. Female Ss were significantly better senders than were male Ss. Although neither sex was found to be better at receiving facial expressions, female Ss were better receivers of female senders' expressions than of male senders' expressions. Female senders' neutral and surprised expressions were more accurately recognized than were those of male senders. The only sex difference found for decoding emotions was a tendency for male Ss to be more accurate at recognizing anger. (25 ref)","[{'authorId': '144257022', 'name': 'H. Wagner'}, {'authorId': '152422249', 'name': 'C. J. MacDonald'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",181.0,"{'bibtex': '@Article{Wagner1986CommunicationOI,\n author = {H. Wagner and C. J. MacDonald and A. Manstead},\n journal = {Journal of Personality and Social Psychology},\n pages = {737-743},\n title = {Communication of individual emotions by spontaneous facial expressions},\n volume = {50},\n year = {1986}\n}\n'}",,"{'volume': '50', 'pages': '737-743', 'name': 'Journal of Personality and Social Psychology'}",22.0,Communication of individual emotions by spontaneous facial expressions,1986.0
1178,5de802fb1c6c8761a5eb617a52bb25f2dead9934,"Social interaction and group coordination are important factors in the simulation of human crowd behavior. To date, few simulation methods have been informed by models of human group behavior from the social science studies. In this paper we advance a computational model informed by Common Ground (CG) Theory that both inherits the social realism provided by the CG model and is computationally tractable for a large number of groups and individuals. The task of navigation in a group is viewed as performing a joint activity among agents, which requires effective coordination among group members. Our model includes both macro and micro coordination, addressing the joint plans, and the actions for coordination respectively. These coordination activities and plans inform the high-level route and walking strategies of the agents. We demonstrate a series of studies to show the qualitative and quantitative differences in simulation results with and without incorporation of the CG model.","[{'authorId': '2108416569', 'name': 'Seung In Park'}, {'authorId': '1740663', 'name': 'Francis K. H. Quek'}, {'authorId': '48696060', 'name': 'Yong Cao'}]",16.0,"{'bibtex': '@Article{Park2012ModelingSG,\n author = {Seung In Park and Francis K. H. Quek and Yong Cao},\n journal = {Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)},\n pages = {1-12},\n title = {Modeling social groups in crowds using Common Ground Theory},\n year = {2012}\n}\n'}",,"{'pages': '1-12', 'name': 'Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)'}",28.0,Modeling social groups in crowds using Common Ground Theory,2012.0
1179,5e0a3df7e379cfa74dd8d432ed5c86513772db52,"Recent research in psychology and neuroscience has identified both the critical role of emotion in decision-making and social interaction, and some of the mechanisms mediating the functioning of emotion. Yet the majority of cognitive architectures do not include models of emotion. In this paper I first motivate the need for including emotion in cognitive architectures, and then describe a generic methodology for modeling the effects of emotion within a symbolic cognitive architecture, as well as some of the associated representational requirements. I then present preliminary results, and conclude with an outline of key issues and future work in emotion modeling.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",56.0,"{'bibtex': '@Inproceedings{Hudlicka2004BeyondCM,\n author = {E. Hudlicka},\n pages = {118-123},\n title = {Beyond Cognition: Modeling Emotion in Cognitive Architectures},\n year = {2004}\n}\n'}",,{'pages': '118-123'},29.0,Beyond Cognition: Modeling Emotion in Cognitive Architectures,2004.0
1180,5e237ef31e4a5d5a486ecbc86490ed9b43ebe48d,"In the 1990s, it was argued that age-related changes in the frontal lobes predict cognitive changes in older adults. However, evidence for this hypothesis from behavioural and neuroimaging studies were equivocal at best. This chapter reviews the following four issues. First, there is little strong evidence to support the conclusion that executive control is differentially affected by age in comparison with other cognitive functions. Second, there are differences in the pattern of deficits seen following focal frontal lobe damage and those accompanying the ageing process. Third, the effects of age on social and emotional functioning have been largely ignored, despite considerable evidence linking such functions to the frontal lobes of the brain. Fourth, functional neuroimaging data do not support a straightforward version of the frontal-lobe theory of ageing.","[{'authorId': '1994871', 'name': 'L. Phillips'}, {'authorId': '2149900272', 'name': 'J. Henry'}]",17.0,"{'bibtex': '@Inproceedings{Phillips2012AnEO,\n author = {L. Phillips and J. Henry},\n title = {An evaluation of the frontal lobe theory of cognitive aging},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,An evaluation of the frontal lobe theory of cognitive aging,2012.0
1181,5e42e01d04454aae6cbda0ad4bcf1228d00c5749,"Research on Augmented Reality (AR) in education has demonstrated that AR applications designed with diverse components boost student motivation in educational settings. However, most of the research conducted to date, does not define exactly what those components are and how these components positively affect student motivation. This study, therefore, attempts to identify some of the components that positively affect student motivation in mobile AR learning experiences to contribute to the design and development of motivational AR learning experiences for the Vocational Education and Training (VET) level of education. To identify these components, a research model constructed from the literature was empirically validated with data obtained from two sources: 35 students from four VET institutes interacting with an AR application for learning for a period of 20 days, and a self-report measure obtained from the Instructional Materials Motivation Survey (IMMS). We found that the following variables: use of scaffolding, real-time feedback, degree of success, time on-task and learning outcomes are positively correlated with the four dimensions of the ARCS model of motivation: Attention, Relevance, Confidence, and Satisfaction. Implications of these results are also described.","[{'authorId': '8696488', 'name': 'J. Bacca'}, {'authorId': '1728461', 'name': 'S. Baldiris'}, {'authorId': '1694654', 'name': 'R. Fabregat'}, {'authorId': '7910112', 'name': 'Kinshuk'}]",44.0,"{'bibtex': '@Article{Bacca2018InsightsIT,\n author = {J. Bacca and S. Baldiris and R. Fabregat and Kinshuk},\n journal = {Frontiers in Psychology},\n title = {Insights Into the Factors Influencing Student Motivation in Augmented Reality Learning Experiences in Vocational Education and Training},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'name': 'Frontiers in Psychology'}",58.0,Insights Into the Factors Influencing Student Motivation in Augmented Reality Learning Experiences in Vocational Education and Training,2018.0
1182,5e4bca08443acdd9343e5edf448fd0d9e959588c,"In recent years, with the emergence of relatively inexpensive and accessible virtual reality technologies, it is now possible to deliver compelling and realistic simulations of human-to-human interaction. Neuroimaging studies have shown that, when participants believe they are interacting via a virtual interface with another human agent, they show different patterns of brain activity compared to when they know that their virtual partner is computer-controlled. The suggestion is that users adopt an “intentional stance” by attributing mental states to their virtual partner. However, it remains unclear how beliefs in the agency of a virtual partner influence participants’ behaviour and subjective experience of the interaction. We investigated this issue in the context of a cooperative “joint attention” game in which participants interacted via an eye tracker with a virtual onscreen partner, directing each other’s eye gaze to different screen locations. Half of the participants were correctly informed that their partner was controlled by a computer algorithm (“Computer” condition). The other half were misled into believing that the virtual character was controlled by a second participant in another room (“Human” condition). Those in the “Human” condition were slower to make eye contact with their partner and more likely to try and guide their partner before they had established mutual eye contact than participants in the “Computer” condition. They also responded more rapidly when their partner was guiding them, although the same effect was also found for a control condition in which they responded to an arrow cue. Results confirm the influence of human agency beliefs on behaviour in this virtual social interaction context. They further suggest that researchers and developers attempting to simulate social interactions should consider the impact of agency beliefs on user experience in other social contexts, and their effect on the achievement of the application’s goals.","[{'authorId': '2135703', 'name': 'N. Caruana'}, {'authorId': '25623194', 'name': 'Dean Spirou'}, {'authorId': '48687598', 'name': 'Jon Brock'}]",25.0,"{'bibtex': '@Article{Caruana2017HumanAB,\n author = {N. Caruana and Dean Spirou and Jon Brock},\n journal = {PeerJ},\n title = {Human agency beliefs influence behaviour during virtual social interactions},\n volume = {5},\n year = {2017}\n}\n'}",,"{'volume': '5', 'name': 'PeerJ'}",24.0,Human agency beliefs influence behaviour during virtual social interactions,2017.0
1183,5e589249d06ed15583a25fb32b1b0076e8e5483d,"Correctly interpreting an interlocutor's emotional expression is paramount to a successful interaction. But what happens when one of the interlocutors is a machine? The facilitation of human-machine communication and cooperation is of growing importance as smartphones, autonomous cars, or social robots increasingly pervade human social spaces. Previous research has shown that emotionally expressive virtual characters generally elicit higher cooperation and trust than 'neutral' ones. Since emotional expressions are multi-modal, and given that virtual characters can be designed to our liking in all their components, would a mismatch in the emotion expressed in the face and voice influence people's cooperation with a virtual character? We developed a game where people had to cooperate with a virtual character in order to survive on the moon. The character's face and voice were designed to either smile or not, resulting in 4 conditions: smiling voice and face, neutral voice and face, smiling voice only (neutral face), smiling face only (neutral voice). The experiment was set up in a museum over the course of several weeks; we report preliminary results from over 500 visitors, showing that people tend to trust the virtual character in the mismatched condition with the smiling face and neutral voice more. This might be because the two channels express different aspects of an emotion, as previously suggested.","[{'authorId': '1996210170', 'name': 'Ilaria Torre'}, {'authorId': '3493587', 'name': 'E. Carrigan'}, {'authorId': '51216086', 'name': 'Killian McCabe'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '144686633', 'name': 'N. Harte'}]",14.0,"{'bibtex': '@Article{Torre2018SurvivalAT,\n author = {Ilaria Torre and E. Carrigan and Killian McCabe and R. Mcdonnell and N. Harte},\n journal = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},\n title = {Survival at the Museum: A Cooperation Experiment with Emotionally Expressive Virtual Characters},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Multimodal Interaction'},42.0,Survival at the Museum: A Cooperation Experiment with Emotionally Expressive Virtual Characters,2018.0
1184,5e8191d25f12804c928329cd4f53418ce7e4e22a,"The aim of the present research is to investigate a comparison between the effect of cooperative learning teaching method and lecture teaching method on students’ learning and satisfaction level. The research population consisted of all the fourth grade elementary school students of educational district 4 in Shiraz. The statistical population included 120 students (60 female and 60 male) of fifth grade elementary school that were selected randomly. The research method was semi-experimental and the research tools included a 40-item exam aimed at evaluating the students’ learning level and also a questionnaire aimed at measuring student’s satisfaction level that included 25 items. Validity was calculated by asking 14 educational science professors and 12 members of the board of education to examine the content of the items. The reliability of the test was confirmed through retesting (r=.88). For data analysis, t-test and variance analysis were used by utilizing SPSS software. The results showed that the cooperative learning teaching method has a higher effect on students learning than the lecture teaching method. Also the results showed that the cooperative learning method results in higher satisfaction in students that the lecture teaching method. Female students had higher satisfaction and learning levels in cooperative learning teaching method than male students did.","[{'authorId': '113635625', 'name': 'Farzad Mohammadjani'}, {'authorId': '84262259', 'name': 'F. Tonkaboni'}]",29.0,"{'bibtex': ""@Article{Mohammadjani2015ACB,\n author = {Farzad Mohammadjani and F. Tonkaboni},\n journal = {International Education Studies},\n pages = {107-112},\n title = {A Comparison between the Effect of Cooperative Learning Teaching Method and Lecture Teaching Method on Students' Learning and Satisfaction Level.},\n volume = {8},\n year = {2015}\n}\n""}",,"{'volume': '8', 'pages': '107-112', 'name': 'International Education Studies'}",22.0,A Comparison between the Effect of Cooperative Learning Teaching Method and Lecture Teaching Method on Students' Learning and Satisfaction Level.,2015.0
1185,5e87a17735a033a7d167f69d9f8e010b6025a7c7,"Abstract In the natural world, faces are not isolated objects but are rather encountered in the context of the whole body. Previous work has studied the perception of combined faces and bodies using behavioural and electrophysiological measurements, but the neural correlates of emotional face–body perception still remain unexplored. Here, we combined happy and fearful faces and bodies to investigate the influence of body expressions on the neural processing of the face, the effect of emotional ambiguity between the two and the role of the amygdala in this process. Our functional magnetic resonance imaging analyses showed that the activity in motor, prefrontal and visual areas increases when facial expressions are presented together with bodies rather than in isolation, consistent with the notion that seeing body expressions triggers both emotional and action-related processes. In contrast, psychophysiological interaction analyses revealed that amygdala modulatory activity increases after the presentation of isolated faces when compared to combined faces and bodies. Furthermore, a facial expression combined with a congruent body enhanced both cortical activity and amygdala functional connectivity when compared to an incongruent face–body compound. Finally, the results showed that emotional body postures influence the processing of facial expressions, especially when the emotion conveyed by the body implies danger.","[{'authorId': '27698609', 'name': 'Marta Poyo Solanas'}, {'authorId': '14162527', 'name': 'M. Zhan'}, {'authorId': '145843168', 'name': 'M. Vaessen'}, {'authorId': '1974324', 'name': 'R. Hortensius'}, {'authorId': '40378322', 'name': 'T. Engelen'}, {'authorId': '4628064', 'name': 'B. de Gelder'}]",23.0,"{'bibtex': '@Article{Solanas2017LookingAT,\n author = {Marta Poyo Solanas and M. Zhan and M. Vaessen and R. Hortensius and T. Engelen and B. de Gelder},\n journal = {Social Cognitive and Affective Neuroscience},\n pages = {135 - 144},\n title = {Looking at the face and seeing the whole body. Neural basis of combined face and body expressions},\n volume = {13},\n year = {2017}\n}\n'}",,"{'volume': '13', 'pages': '135 - 144', 'name': 'Social Cognitive and Affective Neuroscience'}",58.0,Looking at the face and seeing the whole body. Neural basis of combined face and body expressions,2017.0
1186,5e946e5dbcef1709f9a26a44165ae468a53bf2d8,"The authors first give a brief overview of how computers can afford multiple forms of transformational experiences. Some of these experiences can be purposely designed, for example, to detect and regulate students' affective states to improve aspects of their learning experiences. They can also be used in computer-based psychological interventions that treat psychological illness or that preventively promote wellbeing, healthy lifestyles, and mental health. This special section aims to contribute ideas, methods and case studies for how affective computing can move toward the goal of promoting wellbeing. The papers spread across some of the multiple modalities and techniques used to perceive and detect affect from text, physiology facial expressions, and gaze, and some of the approaches to analyze the expression of sentiments shared via online communities by depressed patients. The articles in this special section discuss how information that computers collect about our behaviour, cognition - and particularly affect - can be used in the further understanding, nurturing or development of wellbeing and human strengths: e.g. self-understanding, empathy, intrinsic motivation toward wellbeing healthy lifestyles.","[{'authorId': '144792845', 'name': 'R. Calvo'}, {'authorId': '144059813', 'name': 'G. Riva'}, {'authorId': '1779199', 'name': 'C. Lisetti'}]",7.0,"{'bibtex': '@Article{Calvo2014AffectAW,\n author = {R. Calvo and G. Riva and C. Lisetti},\n journal = {IEEE Trans. Affect. Comput.},\n pages = {215-216},\n title = {Affect and Wellbeing: Introduction to Special Section},\n volume = {5},\n year = {2014}\n}\n'}",,"{'volume': '5', 'pages': '215-216', 'name': 'IEEE Trans. Affect. Comput.'}",21.0,Affect and Wellbeing: Introduction to Special Section,2014.0
1187,5ea770c8e016072963b8a7497d4e3e1e26f83db5,,"[{'authorId': '145623773', 'name': 'K. Saeed'}, {'authorId': '3163017', 'name': 'T. Nagashima'}]",25.0,"{'bibtex': '@Inproceedings{Saeed2012BiometricsAK,\n author = {K. Saeed and T. Nagashima},\n title = {Biometrics and Kansei Engineering},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",39.0,Biometrics and Kansei Engineering,2012.0
1188,5f13bbb89d8d1919b9d5c34e4f864c9267918130,,"[{'authorId': '9174234', 'name': 'Jina Lee'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '145417478', 'name': 'Brent Lance'}]",86.0,"{'bibtex': '@Inproceedings{Lee2007TheRG,\n author = {Jina Lee and S. Marsella and D. Traum and J. Gratch and Brent Lance},\n pages = {296-303},\n title = {The Rickel Gaze Model: A Window on the Mind of a Virtual Human},\n year = {2007}\n}\n'}",,{'pages': '296-303'},18.0,The Rickel Gaze Model: A Window on the Mind of a Virtual Human,2007.0
1189,5f3feb987dc1f2108e4e19f7df897edcc6f5d334,"In this paper we present an analysis from a corpus of dyadic expert-novice knowledge sharing interactions. The analysis aims at investigating the relationship between observed non-verbal cues and first impressions formation of warmth and competence. We first obtained both discrete and continuous annotations of our data. Discrete descriptors include non-verbal cues such as type of gestures, arms rest poses, head movements and smiles. Continuous descriptors concern annotators' judgments of the expert's warmth and competence during the observed interaction with the novice. Then we computed Odds Ratios between those descriptors. Results highlight the role of smiling in warmth and competence impressions. Smiling is associated with increased levels of warmth and decreasing competence. It also affects the impact of others non-verbal cues (e.g. self-adaptors gestures) on warmth and competence. Moreover, our findings provide interesting insights about the role of rest poses, that are associated with decreased levels of warmth and competence impressions.","[{'authorId': '23567239', 'name': 'Béatrice Biancardi'}, {'authorId': '40123315', 'name': 'Angelo Cafaro'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",19.0,"{'bibtex': '@Article{Biancardi2017AnalyzingFI,\n author = {Béatrice Biancardi and Angelo Cafaro and C. Pelachaud},\n journal = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},\n title = {Analyzing first impressions of warmth and competence from observable nonverbal cues in expert-novice interactions},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Multimodal Interaction'},46.0,Analyzing first impressions of warmth and competence from observable nonverbal cues in expert-novice interactions,2017.0
1191,5f7ee3f7537ea6aa15d8651812f0c33ea7f9de57,,"[{'authorId': '2909056', 'name': 'M. Daoudi'}, {'authorId': '2507859', 'name': 'S. Berretti'}, {'authorId': '1767957', 'name': 'P. Pala'}, {'authorId': '1403292578', 'name': 'Y. Delevoye-Turrell'}, {'authorId': '8196487', 'name': 'A. Bimbo'}]",26.0,"{'bibtex': '@Inproceedings{Daoudi2017EmotionRB,\n author = {M. Daoudi and S. Berretti and P. Pala and Y. Delevoye-Turrell and A. Bimbo},\n pages = {550-560},\n title = {Emotion Recognition by Body Movement Representation on the Manifold of Symmetric Positive Definite Matrices},\n year = {2017}\n}\n'}",,{'pages': '550-560'},20.0,Emotion Recognition by Body Movement Representation on the Manifold of Symmetric Positive Definite Matrices,2017.0
1193,5fbfca240af7702b81ef58f90e3ede22647e4b5e,,"[{'authorId': '3897865', 'name': 'N. Szajnberg'}]",442.0,"{'bibtex': '@Article{Szajnberg2007OnAT,\n author = {N. Szajnberg},\n journal = {The International journal of psycho-analysis},\n pages = {\n          240-1; author reply 241-2\n        },\n title = {On: Attachment theory and psychoanalysis.},\n volume = {88 Pt 1},\n year = {2007}\n}\n'}",,"{'volume': '88 Pt 1', 'pages': '\n          240-1; author reply 241-2\n        ', 'name': 'The International journal of psycho-analysis'}",0.0,On: Attachment theory and psychoanalysis.,2007.0
1195,5fd3ce235f5fcebd3d2807f710b060add527183b,"Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.","[{'authorId': '51287873', 'name': 'C. Stanton'}, {'authorId': '2552141', 'name': 'J. Clune'}]",39.0,"{'bibtex': '@Article{Stanton2018DeepCS,\n author = {C. Stanton and J. Clune},\n journal = {ArXiv},\n title = {Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems},\n volume = {abs/1806.00553},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1806.00553', 'name': 'ArXiv'}",38.0,Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems,2018.0
1196,5fe003333d41dbf56f60a7c3c6c28a198e503ccd,"We are interested in the problem of understanding personal narratives (PN) - spoken or written - recollections of facts, events, and thoughts. For PNs, we define emotion carriers as the speech or text segments that best explain the emotional state of the narrator. Such segments may span from single to multiple words, containing for example verb or noun phrases. Advanced automatic understanding of PNs requires not only the prediction of the narrator’s emotional state but also to identify which events (e.g. the loss of a relative or the visit of grandpa) or people (e.g. the old group of high school mates) carry the emotion manifested during the personal recollection. This work proposes and evaluates an annotation model for identifying emotion carriers in spoken personal narratives. Compared to other text genres such as news and microblogs, spoken PNs are particularly challenging because a narrative is usually unstructured, involving multiple sub-events and characters as well as thoughts and associated emotions perceived by the narrator. In this work, we experiment with annotating emotion carriers in speech transcriptions from the Ulm State-of-Mind in Speech (USoMS) corpus, a dataset of PNs in German. We believe this resource could be used for experiments in the automatic extraction of emotion carriers from PN, a task that could provide further advancements in narrative understanding.","[{'authorId': '39823859', 'name': 'Aniruddha Tammewar'}, {'authorId': '32015706', 'name': 'Alessandra Cervone'}, {'authorId': '117656327', 'name': 'Eva-Maria Messner'}, {'authorId': '1719162', 'name': 'G. Riccardi'}]",9.0,"{'bibtex': '@Inproceedings{Tammewar2020AnnotationOE,\n author = {Aniruddha Tammewar and Alessandra Cervone and Eva-Maria Messner and G. Riccardi},\n pages = {1517-1525},\n title = {Annotation of Emotion Carriers in Personal Narratives},\n year = {2020}\n}\n'}",,{'pages': '1517-1525'},43.0,Annotation of Emotion Carriers in Personal Narratives,2020.0
1197,5ff9c37f98d551f71b1dca709dc07e913622b64c,"Developing an embodied conversational agent that is able to exhibit a human-like behavior while communicating with other virtual or human agents requires enriching a typical NLG architecture. The purpose of this paper is to describe our efforts in this direction and to illustrate our approach to the generation of an Agent that shows a personality, a social intelligence and is able to react emotionally to events occurring in the environment, consistently with her goals and with the context in which the conversation takes place.","[{'authorId': '1739256', 'name': 'B. D. Carolis'}, {'authorId': '1694255', 'name': 'V. Carofiglio'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",33.0,"{'bibtex': '@Inproceedings{Carolis2002FromDP,\n author = {B. D. Carolis and V. Carofiglio and C. Pelachaud},\n pages = {65-72},\n title = {From Discourse Plans to Believable Behavior Generation},\n year = {2002}\n}\n'}",,{'pages': '65-72'},11.0,From Discourse Plans to Believable Behavior Generation,2002.0
1198,601273c37a9670f00234f7e7092bb673e7c0b25c,,"[{'authorId': '144837978', 'name': 'S. Rosenberg'}, {'authorId': '41176052', 'name': 'C. Nelson'}, {'authorId': '17038348', 'name': 'P. S. Vivekananthan'}]",911.0,"{'bibtex': '@Article{Rosenberg1968AMA,\n author = {S. Rosenberg and C. Nelson and P. S. Vivekananthan},\n journal = {Journal of personality and social psychology},\n pages = {\n          283-94\n        },\n title = {A multidimensional approach to the structure of personality impressions.},\n volume = {9 4},\n year = {1968}\n}\n'}",,"{'volume': '9 4', 'pages': '\n          283-94\n        ', 'name': 'Journal of personality and social psychology'}",18.0,A multidimensional approach to the structure of personality impressions.,1968.0
1199,6036a2f5537297a384ac5ff611d3e4996fca2483,"Text categorization is a well-known task based essentially on statistical approaches using neural networks, Support Vector Machines and other machine learning algorithms. Texts are generally considered as bags of words without any order. Although these approaches have proven to be efficient, they do not provide users with comprehensive and reusable rules about their data. Such rules are, however, very important for users to describe trends in the data they have to analyze. In this framework, an association-rule based approach has been proposed by Bing Liu (CBA). We propose, in this paper, to extend this approach by using sequential patterns in the SPaC method (Sequential Patterns for Classification) for text categorization. Taking order into account allows us to represent the succession of words through a document without complex and time-consuming representations and treatments such as those performed in natural language and grammatical methods. The original method we propose here consists in mining sequential patterns in order to build a classifier. We experimentally show that our proposal is relevant, and that it is very interesting compared to other methods. In particular, our method outperforms CBA and provides better results than SVM on some corpus.","[{'authorId': '2476469', 'name': 'Simon Jaillet'}, {'authorId': '145472032', 'name': 'Anne Laurent'}, {'authorId': '1683304', 'name': 'M. Teisseire'}]",60.0,"{'bibtex': '@Article{Jaillet2006SequentialPF,\n author = {Simon Jaillet and Anne Laurent and M. Teisseire},\n journal = {Intell. Data Anal.},\n pages = {199-214},\n title = {Sequential patterns for text categorization},\n volume = {10},\n year = {2006}\n}\n'}",,"{'volume': '10', 'pages': '199-214', 'name': 'Intell. Data Anal.'}",43.0,Sequential patterns for text categorization,2006.0
1200,60541c51590d3eb7246d6e04aac0a1c284cc8d47,"Common guidelines followed in the animation community include the idea that cartoon characters should be exaggerated to better convey emotion and intent, whereas more realistic characters should have “matching” realistic motion. We investigated the effects of rendering style and amount of facial motion on perceptions of character likeability, intelligence, and extraversion. We used cartoon and more realistic-looking characters that were animated with tracked actor motion. The motion was exaggerated and damped in 10% increments up to a 40% difference from the original motion. We discovered that motion changes ±20% from original motion affected perceptions of likeability and intelligence differently in the realistic-looking and cartoon characters. The realistic-looking characters benefited from increased motion whereas the cartoon characters benefitted from damped motion. Furthermore, the amount of facial motion and perceptions of extraversion were significantly correlated.","[{'authorId': '38105277', 'name': 'Jennifer Hyde'}, {'authorId': '32134175', 'name': 'E. Carter'}, {'authorId': '47198673', 'name': 'S. Kiesler'}, {'authorId': '1788773', 'name': 'J. Hodgins'}]",42.0,"{'bibtex': '@Article{Hyde2013PerceptualEO,\n author = {Jennifer Hyde and E. Carter and S. Kiesler and J. Hodgins},\n journal = {2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},\n pages = {1-6},\n title = {Perceptual effects of damped and exaggerated facial motion in animated characters},\n year = {2013}\n}\n'}",,"{'pages': '1-6', 'name': '2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)'}",25.0,Perceptual effects of damped and exaggerated facial motion in animated characters,2013.0
1201,6085213f948cb197d5da626cc7d3b92210552d7d,"Animated characters are beginning to be used as pedagogical tools, as they have the power to capture students' attention and foster their motivation for discovery and learning. However, in order for them to be widely employed and accepted as a learning resource, they must be easy to use and friendly. In this paper we present an architecture that facilitates building interactive pedagogical chatbots that can interact with students in natural language. Our proposal provides a modular and scalable framework to develop such systems efficiently. Additionally, we present Geranium, a system that helps children to appreciate and protect their environment with an interactive chatbot developed following our scheme.","[{'authorId': '1804281', 'name': 'D. Griol'}, {'authorId': '67297965', 'name': 'Z. Callejas'}]",28.0,"{'bibtex': '@Article{Griol2013AnAT,\n author = {D. Griol and Z. Callejas},\n journal = {International Journal of Advanced Robotic Systems},\n title = {An Architecture to Develop Multimodal Educative Applications with Chatbots},\n volume = {10},\n year = {2013}\n}\n'}",,"{'volume': '10', 'name': 'International Journal of Advanced Robotic Systems'}",80.0,An Architecture to Develop Multimodal Educative Applications with Chatbots,2013.0
1203,6086c5ba570918a8c3b3d4623690860e784aca6d,,"[{'authorId': '34923062', 'name': 'M. Foster'}, {'authorId': '3263707', 'name': 'J. Oberlander'}]",56.0,"{'bibtex': '@Article{Foster2007CorpusbasedGO,\n author = {M. Foster and J. Oberlander},\n journal = {Language Resources and Evaluation},\n pages = {305-323},\n title = {Corpus-based generation of head and eyebrow motion for an embodied conversational agent},\n volume = {41},\n year = {2007}\n}\n'}",,"{'volume': '41', 'pages': '305-323', 'name': 'Language Resources and Evaluation'}",32.0,Corpus-based generation of head and eyebrow motion for an embodied conversational agent,2007.0
1204,6089cd4d2264d23b4f5580cd00af29a85ef3659b,"The general unavailability of and difficulty associated with obtaining records-based data on absenteeism suggests the potential value of self-report data for those conducting research on absenteeism. This should not be recommended, however, until the validity of these self-report measures is assessed. In this paper, we compare records-based and self-report measures of absenteeism for the same employees for the same period of time. We find that although the univariate descriptive data for the two measures are similar, the correlation between the two is .299. Although this is in the expected direction, its magnitude is small enough for us to question the validity of self- report measures of absenteeism. Several suggestions are offered for additional study of this.","[{'authorId': '39706207', 'name': 'C. Mueller'}, {'authorId': '31898312', 'name': 'D. Wakefield'}, {'authorId': '47051477', 'name': 'J. L. Price'}, {'authorId': '2054074831', 'name': 'J. Curry'}, {'authorId': '2980638', 'name': 'J. McCloskey'}]",22.0,"{'bibtex': '@Article{Mueller1987ANO,\n author = {C. Mueller and D. Wakefield and J. L. Price and J. Curry and J. McCloskey},\n journal = {Human Relations},\n pages = {117 - 123},\n title = {A Note on the Validity of Self-Reports of Absenteeism},\n volume = {40},\n year = {1987}\n}\n'}",,"{'volume': '40', 'pages': '117 - 123', 'name': 'Human Relations'}",12.0,A Note on the Validity of Self-Reports of Absenteeism,1987.0
1205,6097d5bd62277def74e8582b425d223b65056ee7,"In military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and complicated scripts, but nevertheless resulting in rigid, predictable performance. Autonomous agents that learn desired behaviors themselves using Machine Learning (ML) techniques can overcome these shortcomings. However, such techniques are not yet widely used and perhaps underappreciated. In this context, the latin expression ""multum in parvo"" (""much in little"") denotes that ML agents are able to yield a large variety of behavior, despite their compactness in terms of code and usage of physical memory. This paper attempts to provide some background on applicable Machine Learning solutions and their potential military application. The paper is based on the work of the NATO Research Task Group IST-121 Machine Learning Techniques for Autonomous Computer Generated Entities.","[{'authorId': '2398535', 'name': 'J. Roessingh'}, {'authorId': '3075138', 'name': 'A. Toubman'}, {'authorId': '1758865', 'name': 'J. V. Oijen'}, {'authorId': '14767528', 'name': 'G. Poppinga'}, {'authorId': '87359978', 'name': 'R. A. Løvlid'}, {'authorId': '2144273323', 'name': 'Ming Hou'}, {'authorId': '2200611', 'name': 'L. Luotsinen'}]",11.0,"{'bibtex': '@Article{Roessingh2017MachineLT,\n author = {J. Roessingh and A. Toubman and J. V. Oijen and G. Poppinga and R. A. Løvlid and Ming Hou and L. Luotsinen},\n journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},\n pages = {3445-3450},\n title = {Machine learning techniques for autonomous agents in military simulations — Multum in parvo},\n year = {2017}\n}\n'}",,"{'pages': '3445-3450', 'name': '2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)'}",28.0,Machine learning techniques for autonomous agents in military simulations — Multum in parvo,2017.0
1206,609cc7a1a08d498f80353b1f741a3a6fbed63fc9,"Psychometric studies of the organization of the ""natural language of personality"" have typically employed rating scales as measurement medium and factor analysis as statistical technique. The results of such investigations over the past 30 years have varied greatly, both with respect to number of factors and with respect to the constructs generated. Re-analysis of the correlations of six studies, including the classical work of Cattell, indicated that the domain appears to be well described by five factors, with some suggestion of a sixth. The five factors were related across studies, using the Kaiser-Hunka-Bianchini method. Generally, the factors were highly related, with most indices of relatedness exceeding .90. The five-factor model was tested by the multiple-group method, used to factor a large-scale study of teachers' ratings of children. With slight modification of the originally hypothesized structure, the five-factor model accounted for the observed relationships quite well. The five constructs suggested by the factors appear to be domains of research effort and theoretical concern which have long been of interest to psychologists.","[{'authorId': '3969220', 'name': 'J. M. Digman'}, {'authorId': '1455850295', 'name': 'N. Takemoto-Chock'}]",762.0,"{'bibtex': '@Article{Digman1981FactorsIT,\n author = {J. M. Digman and N. Takemoto-Chock},\n journal = {Multivariate behavioral research},\n pages = {\n          149-70\n        },\n title = {Factors In The Natural Language Of Personality: Re-Analysis, Comparison, And Interpretation Of Six Major Studies.},\n volume = {16 2},\n year = {1981}\n}\n'}",,"{'volume': '16 2', 'pages': '\n          149-70\n        ', 'name': 'Multivariate behavioral research'}",26.0,"Factors In The Natural Language Of Personality: Re-Analysis, Comparison, And Interpretation Of Six Major Studies.",1981.0
1207,60a06f72e8b4c6b3cd732682dfa7da5899a3f1ff,,"[{'authorId': '2462740', 'name': 'K. Scherer'}]",439.0,"{'bibtex': '@Article{Scherer1995ExpressionOE,\n author = {K. Scherer},\n journal = {Journal of voice : official journal of the Voice Foundation},\n pages = {\n          235-48\n        },\n title = {Expression of emotion in voice and music.},\n volume = {9 3},\n year = {1995}\n}\n'}",,"{'volume': '9 3', 'pages': '\n          235-48\n        ', 'name': 'Journal of voice : official journal of the Voice Foundation'}",94.0,Expression of emotion in voice and music.,1995.0
1208,60bef41c17fce4db5e8161184dbc7296264d0db2,"Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2,000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal, and (dis)liking intensity estimation.","[{'authorId': '3125761', 'name': 'Jean Kossaifi'}, {'authorId': '2616466', 'name': 'R. Walecki'}, {'authorId': '1780393', 'name': 'Yannis Panagakis'}, {'authorId': '46904799', 'name': 'Jie Shen'}, {'authorId': '144465731', 'name': 'Maximilian Schmitt'}, {'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '144610333', 'name': 'Jing Han'}, {'authorId': '35747600', 'name': 'Vedhas Pandit'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '2065199', 'name': 'Kam Star'}, {'authorId': '1988507', 'name': 'Elnar Hajiyev'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",148.0,"{'bibtex': '@Article{Kossaifi2019SEWADA,\n author = {Jean Kossaifi and R. Walecki and Yannis Panagakis and Jie Shen and Maximilian Schmitt and F. Ringeval and Jing Han and Vedhas Pandit and Björn Schuller and Kam Star and Elnar Hajiyev and M. Pantic},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1022-1040},\n title = {SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild},\n volume = {43},\n year = {2019}\n}\n'}",,"{'volume': '43', 'pages': '1022-1040', 'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence'}",88.0,SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild,2019.0
1209,60cd1fb49d4a4870c0f426ed0227a8ff67cbe69c,,"[{'authorId': '3440142', 'name': 'Marine Taffou'}, {'authorId': '2050262', 'name': 'Jan Ondřej'}, {'authorId': '1404017833', 'name': ""C. O'Sullivan""}, {'authorId': '2606936', 'name': 'O. Warusfel'}, {'authorId': '1401022809', 'name': 'I. Viaud-Delmon'}]",3.0,"{'bibtex': ""@Article{Taffou2017JudgingCS,\n author = {Marine Taffou and Jan Ondřej and C. O'Sullivan and O. Warusfel and I. Viaud-Delmon},\n journal = {Journal on Multimodal User Interfaces},\n pages = {57-65},\n title = {Judging crowds’ size by ear and by eye in virtual reality},\n volume = {11},\n year = {2017}\n}\n""}",,"{'volume': '11', 'pages': '57-65', 'name': 'Journal on Multimodal User Interfaces'}",38.0,Judging crowds’ size by ear and by eye in virtual reality,2017.0
1210,60d57df50f55a7b5d3ca15807896bced82e9d1f0,,"[{'authorId': '153544611', 'name': 'James P. Wolf'}]",25.0,"{'bibtex': '@Article{Wolf2008TheEO,\n author = {James P. Wolf},\n journal = {System},\n pages = {279-294},\n title = {The Effects of Backchannels on Fluency in L2 Oral Task Production.},\n volume = {36},\n year = {2008}\n}\n'}",,"{'volume': '36', 'pages': '279-294', 'name': 'System'}",60.0,The Effects of Backchannels on Fluency in L2 Oral Task Production.,2008.0
1211,60ed31580e9884f1a42c1666939ca9c13e9ec9d9,"The PRVAs, product recommendation virtual agent, are the agents that take part in the clerks on the onlineshopping. For their aims, it is very important for the PRVAs to be trusted by users. However, trustworthy the PRVA design was not be studied yet. In this paper, we suggest the user’s trust transition model that is consisted by two parameters. One parameter is user’s emotion, and the other is agent’s knowledge. We suggested the transition operators that transited these two parameters by executing when the PRVAs recommend. Emotion transition operators are agent’s smile and gestures. Knowledge transition operators is long product recommendation text. We carried on three experiments to estimate these model and transition operators. In experiment 1, we executed no transition operators. In experiment 2, we executed emotion transition operators and added knowledge transition operators in the latter half. In experiment 3, we executed knowledge transition operators in the first half and added emotion transition operators in the latter half. As a results, it is discovered that transition operators and a transition model are effective. In experiment 1, there are no transition in the participants’ trust state. In experiment 2, the participants’ knowledge perceived and trust for agent transited after executing knowledge transition operators. In experiment 3, the participants’ emotion transited after executing positive emotion operators, however, trust didn’t transited. From these result, we concluded that trust is based on each of the user’s emotion and the agent’s knowledge.","[{'authorId': '49201495', 'name': 'T. Matsui'}, {'authorId': '1679243', 'name': 'S. Yamada'}]",0.0,"{'bibtex': '@Article{Matsui2017DesignOA,\n author = {T. Matsui and S. Yamada},\n journal = {Transactions of The Japanese Society for Artificial Intelligence},\n title = {Design of a Product Recommendation Virtual Agent That Induces User’s Trust Operating User’s Trust},\n volume = {32},\n year = {2017}\n}\n'}",[],"{'name': 'Transactions of The Japanese Society for Artificial Intelligence', 'volume': '32'}",16.0,Design of a Product Recommendation Virtual Agent That Induces User’s Trust Operating User’s Trust,2017.0
1212,615d08513a90eb64649c3eff41006810cd572ec3,,"[{'authorId': '2698741', 'name': 'N. Bellomo'}, {'authorId': '2004264', 'name': 'L. Gibelli'}]",38.0,"{'bibtex': '@Article{Bellomo2016BehavioralCM,\n author = {N. Bellomo and L. Gibelli},\n journal = {Computers & Fluids},\n pages = {13-21},\n title = {Behavioral crowds: Modeling and Monte Carlo simulations toward validation},\n volume = {141},\n year = {2016}\n}\n'}",,"{'volume': '141', 'pages': '13-21', 'name': 'Computers & Fluids'}",27.0,Behavioral crowds: Modeling and Monte Carlo simulations toward validation,2016.0
1213,6187c0830dfd536cc748f5a06be6f15b750c7768,"The focus of this work is to investigate and quantify the ability of a humanoid ‘hybrid face’ robot to effectively convey emotion to a human observer by mapping their physiological (EEG) response to perceived emotional information. Specifically, we examine the event related response during two implicit emotion recognition experiments to determine the modulation of the face-specific N170 brain response component to robot facial expressions. EEG recordings were taken from a range of test subjects observing the BERT2 robot cycle through a range of facial emotions in each emotion recognition experiment. Results from both experiments demonstrate that the stimuli evoke the N170 component and that digital facial expressions with high correlations can be discriminated. Emotional expressions evoke a larger response relative to neutral stimuli, with negative evoking an increased amplitude and latency to positive emotions, and demonstrate that the response to robot facial expressions evoke similar brain activity to that of a human emotions. This study is the first of its nature to investigate and quantify the human physiological response to digital facial expressions as conveyed in real-time by a humanoid robot.","[{'authorId': '153888634', 'name': 'R. Craig'}, {'authorId': '144301847', 'name': 'R. Vaidyanathan'}, {'authorId': '2056775280', 'name': 'Christopher James'}, {'authorId': '1730409', 'name': 'C. Melhuish'}]",12.0,"{'bibtex': '@Article{Craig2010AssessmentOH,\n author = {R. Craig and R. Vaidyanathan and Christopher James and C. Melhuish},\n journal = {2010 10th IEEE-RAS International Conference on Humanoid Robots},\n pages = {647-652},\n title = {Assessment of human response to robot facial expressions through visual evoked potentials},\n year = {2010}\n}\n'}",,"{'pages': '647-652', 'name': '2010 10th IEEE-RAS International Conference on Humanoid Robots'}",27.0,Assessment of human response to robot facial expressions through visual evoked potentials,2010.0
1214,61957cd718d52659f7f2ae379cbe02ded3a8db05,,"[{'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}]",48.0,"{'bibtex': '@Inproceedings{Andrist2013ConversationalGA,\n author = {Sean Andrist and Bilge Mutlu and Michael Gleicher},\n pages = {249-262},\n title = {Conversational Gaze Aversion for Virtual Agents},\n year = {2013}\n}\n'}",,{'pages': '249-262'},22.0,Conversational Gaze Aversion for Virtual Agents,2013.0
1215,619bb8cac2c93e127ecb0331cb2fa994d10589ec,"Storytelling can develop children's emotional intelligence when they are asked to freely talk about their emotions. While parents are responsible for teaching emotional intelligence, studies in using affective technologies to help people become aware of their emotions have also been explored. In this paper, we investigate the opportunity of this technology in enabling children to recognize and express their emotions. We describe a chatbot that leverages storytelling strategies to listen to children as they share emotional events they experienced, then guides them through reflective discipline to devise the next course of action. We report the types of emotions children choose to share with the chatbot, the kinds of support that the chatbot provided, the challenges during the conversation and children's perception of the chatbot. From our findings, we suggest design considerations for a conversation flow that anchors on storytelling to support child-agent interaction.","[{'authorId': '1750919955', 'name': 'Kyle-Althea Santos'}, {'authorId': '8045848', 'name': 'Ethel Ong'}, {'authorId': '116323144', 'name': 'Ron R. Resurreccion'}]",27.0,"{'bibtex': ""@Article{Santos2020TherapistVC,\n author = {Kyle-Althea Santos and Ethel Ong and Ron R. Resurreccion},\n journal = {Proceedings of the Interaction Design and Children Conference},\n title = {Therapist vibe: children's expressions of their emotions through storytelling with a chatbot},\n year = {2020}\n}\n""}",,{'name': 'Proceedings of the Interaction Design and Children Conference'},49.0,Therapist vibe: children's expressions of their emotions through storytelling with a chatbot,2020.0
1216,61ba3584885142e46673943142a4f2280ac14387,"The idea of interacting with computers through natural language dates back to the 1960s, but recent technological advances have led to a renewed interest in conversational agents such as chatbots or digital assistants. In the customer service context, conversational agents promise to create a fast, convenient, and cost-effective channel for communicating with customers. Although numerous agents have been implemented in the past, most of them could not meet the expectations and disappeared. In this paper, we present our design science research project on how to design cooperative and social conversational agents to increase service quality in customer service. We discuss several issues that hinder the success of current conversational agents in customer service. Drawing on the cooperative principle of conversation and social response theory, we propose preliminary meta-requirements and design principles for cooperative and social conversational agents. Next, we will develop a prototype based on these design principles.","[{'authorId': '7807550', 'name': 'Ulrich Gnewuch'}, {'authorId': '3339327', 'name': 'Stefan Morana'}, {'authorId': '1806905', 'name': 'A. Maedche'}]",221.0,"{'bibtex': '@Inproceedings{Gnewuch2017TowardsDC,\n author = {Ulrich Gnewuch and Stefan Morana and A. Maedche},\n title = {Towards Designing Cooperative and Social Conversational Agents for Customer Service},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",79.0,Towards Designing Cooperative and Social Conversational Agents for Customer Service,2017.0
1217,61e47a6523858b1db68f7bc6f3572355eabae6cf,"Although consumer research began focusing on emotional response to advertising during the 1980s (Goodstein, Edell, and Chapman Moore. 1990; Burke and Edell, 1989; Aaker, Stayman, and Vezina, 1988; Holbrook and Batra, 1988), perhaps one of the most practical measures of affective response has only recently emerged. Part of the difficulty in developing measures of emotional response stems from the complexity of emotion itself (Plummer and Leckenby, 1985). Researchers have explored several different measurement formats including: verbal self-reports (adjective checklists), physiological techniques, photodecks, and dial-turning instruments.","[{'authorId': '1606560531', 'name': 'Jon D. Morris'}]",523.0,"{'bibtex': '@Article{Morris1995ObservationsST,\n author = {Jon D. Morris},\n journal = {Journal of Advertising Research},\n title = {Observations: SAM: The Self-Assessment Manikin An Efficient Cross-Cultural Measurement Of Emotional Response 1},\n year = {1995}\n}\n'}",,"{'volume': '', 'name': 'Journal of Advertising Research'}",19.0,Observations: SAM: The Self-Assessment Manikin An Efficient Cross-Cultural Measurement Of Emotional Response 1,1995.0
1219,61f043693282de0ac4cb9647c2846683dab58588,,"[{'authorId': '144603514', 'name': 'B. Parkinson'}, {'authorId': '1993236', 'name': 'P. Totterdell'}, {'authorId': '2236765', 'name': 'R. Briner'}, {'authorId': '4239362', 'name': 'S. Reynolds'}]",338.0,"{'bibtex': '@Inproceedings{Parkinson1996ChangingMT,\n author = {B. Parkinson and P. Totterdell and R. Briner and S. Reynolds},\n title = {Changing Moods: The Psychology of Mood and Mood Regulation},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Changing Moods: The Psychology of Mood and Mood Regulation,1996.0
1220,61f46053015c7269761d87f591e1dfcd9c213d2b,,"[{'authorId': '2066544348', 'name': 'Francesco Ferrari'}, {'authorId': '3366020', 'name': 'M. Paladino'}, {'authorId': '2515554', 'name': 'J. Jetten'}]",170.0,"{'bibtex': '@Article{Ferrari2016BlurringHD,\n author = {Francesco Ferrari and M. Paladino and J. Jetten},\n journal = {International Journal of Social Robotics},\n pages = {287-302},\n title = {Blurring Human–Machine Distinctions: Anthropomorphic Appearance in Social Robots as a Threat to Human Distinctiveness},\n volume = {8},\n year = {2016}\n}\n'}",,"{'volume': '8', 'pages': '287-302', 'name': 'International Journal of Social Robotics'}",58.0,Blurring Human–Machine Distinctions: Anthropomorphic Appearance in Social Robots as a Threat to Human Distinctiveness,2016.0
1221,6209274d7fd95c15627495d643758f67ef6a4a4c,"Turbulent vortices in fluid flows are crucial for a visually interesting appearance. Although there has been a significant amount of work on turbulence in graphics recently, these algorithms rely on the underlying simulation to resolve the flow around objects. We build upon work from classical fluid mechanics to design an algorithm that allows us to accurately precompute the turbulence being generated around an object immersed in a flow. This is made possible by modeling turbulence formation based on an averaged flow field, and relying on universal laws describing the flow near a wall. We precompute the confined vorticity in the boundary layer around an object, and simulate the boundary layer separation during a fluid simulation. Then, a turbulence model is used to identify areas where this separated layer will transition into actual turbulence. We sample these regions with vortex particles, and simulate the further dynamics of the vortices based on these particles. We will show how our method complements previous work on synthetic turbulence, and yields physically plausible results. In addition, we demonstrate that our method can efficiently compute turbulent flows around a variety of objects including cars, whisks, as well as boulders in a river flow. We can even apply our model to precomputed static flow fields, yielding turbulent dynamics without a costly simulation.","[{'authorId': '2054956', 'name': 'T. Pfaff'}, {'authorId': '1786445', 'name': 'N. Thürey'}, {'authorId': '1714992', 'name': 'Andrew Selle'}, {'authorId': '2257153235', 'name': 'M. Gross'}]",83.0,"{'bibtex': '@Article{Pfaff2009SyntheticTU,\n author = {T. Pfaff and N. Thürey and Andrew Selle and M. Gross},\n journal = {ACM SIGGRAPH Asia 2009 papers},\n title = {Synthetic turbulence using artificial boundary layers},\n year = {2009}\n}\n'}",,{'name': 'ACM SIGGRAPH Asia 2009 papers'},33.0,Synthetic turbulence using artificial boundary layers,2009.0
1222,6211dc610a4dafb5739befd6bbdd646f625f4d8d,"During the past three decades, researchers interested in emotions and cognition have attempted to understand the relationship that affect and emotions have with cognitive outcomes such as judgement and decision-making. Recent research has revealed the importance of examining more discrete emotions, showing that same-valence emotions (e.g., anger and fear) differentially impact judgement and decision-making outcomes. Narrative reviews of the literature (Lerner & Tiedens, 2006; Pham, 2007) have identified some under-researched topics, but provide a limited synthesis of findings. The purpose of this study was to review the research examining the influence of discrete emotions on judgement and decision-making outcomes and provide an assessment of the observed effects using a meta-analytic approach. Results, overall, show that discrete emotions have moderate to large effects on judgement and decision-making outcomes. However, moderator analyses revealed differential effects for study-design characteristics and emotion-manipulation characteristics by emotion type. Implications are discussed.","[{'authorId': '2698323', 'name': 'Amanda D. Angie'}, {'authorId': '145214017', 'name': 'S. Connelly'}, {'authorId': '2299204', 'name': 'Ethan P. Waples'}, {'authorId': '1946866', 'name': 'Vykinta Kligyte'}]",246.0,"{'bibtex': '@Article{Angie2011TheIO,\n author = {Amanda D. Angie and S. Connelly and Ethan P. Waples and Vykinta Kligyte},\n journal = {Cognition and Emotion},\n pages = {1393 - 1422},\n title = {The influence of discrete emotions on judgement and decision-making: A meta-analytic review},\n volume = {25},\n year = {2011}\n}\n'}",,"{'volume': '25', 'pages': '1393 - 1422', 'name': 'Cognition and Emotion'}",129.0,The influence of discrete emotions on judgement and decision-making: A meta-analytic review,2011.0
1223,6225f236047af5650ee1ad2d048470cd3516ab54,,"[{'authorId': '2240492577', 'name': 'Patrik Vuilleumier'}, {'authorId': '2240502473', 'name': 'Jorge L. Armony'}, {'authorId': '49910272', 'name': 'K. Clarke'}, {'authorId': '2240493136', 'name': 'M. Husain'}, {'authorId': '2240493136', 'name': 'M. Husain'}, {'authorId': '2240481342', 'name': 'Julia Driver'}, {'authorId': '2237446785', 'name': 'R. J. Dolan'}]",317.0,"{'bibtex': '@Article{Vuilleumier2002NeuralRT,\n author = {Patrik Vuilleumier and Jorge L. Armony and K. Clarke and M. Husain and M. Husain and Julia Driver and R. J. Dolan},\n journal = {Neuropsychologia},\n pages = {2156-2166},\n title = {Neural response to emotional faces with and without awareness: event-related fMRI in a parietal patient with visual extinction and spatial neglect},\n volume = {40},\n year = {2002}\n}\n'}",,"{'volume': '40', 'pages': '2156-2166', 'name': 'Neuropsychologia'}",56.0,Neural response to emotional faces with and without awareness: event-related fMRI in a parietal patient with visual extinction and spatial neglect,2002.0
1224,6231ae1aeba3768f4b6f33941736fa53eec204ba,"In this article, we describe how cross-cultural research methodologies have evolved, with each phase of research addressing limitations of a previous one. We describe briefly the three previous phases and argue for embarking on a fourth phase that empirically establishes linkages between the active cultural ingredients hypothesized to cause between-country differences and the observed differences themselves. We discuss theoretical considerations and possible empirical methods to establish such linkages, and urge researchers to seriously consider incorporating these kinds of linkage studies in their programs of research.","[{'authorId': '145413880', 'name': 'D. Matsumoto'}, {'authorId': '50313734', 'name': 'S. Yoo'}]",381.0,"{'bibtex': '@Article{Matsumoto2006TowardAN,\n author = {D. Matsumoto and S. Yoo},\n journal = {Perspectives on Psychological Science},\n pages = {234 - 250},\n title = {Toward a New Generation of Cross-Cultural Research},\n volume = {1},\n year = {2006}\n}\n'}",,"{'volume': '1', 'pages': '234 - 250', 'name': 'Perspectives on Psychological Science'}",152.0,Toward a New Generation of Cross-Cultural Research,2006.0
1225,624328b4d6ee529019b0654bca6ca65abde60218,,"[{'authorId': '33432486', 'name': 'Mei Si'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '1748597', 'name': 'D. Pynadath'}]",30.0,"{'bibtex': '@Inproceedings{Si2009DirectorialCI,\n author = {Mei Si and S. Marsella and D. Pynadath},\n pages = {221-233},\n title = {Directorial Control in a Decision-Theoretic Framework for Interactive Narrative},\n year = {2009}\n}\n'}",,{'pages': '221-233'},26.0,Directorial Control in a Decision-Theoretic Framework for Interactive Narrative,2009.0
1226,6257c0833dfa6fe89ef2e1133653b74f9dadc5f6,"Factor analyses of 75 facet scales from 2 major Big Five inventories, in the Eugene-Springfield community sample (N=481), produced a 2-factor solution for the 15 facets in each domain. These findings indicate the existence of 2 distinct (but correlated) aspects within each of the Big Five, representing an intermediate level of personality structure between facets and domains. The authors characterized these factors in detail at the item level by correlating factor scores with the International Personality Item Pool (L. R. Goldberg, 1999). These correlations allowed the construction of a 100-item measure of the 10 factors (the Big Five Aspect Scales [BFAS]), which was validated in a 2nd sample (N=480). Finally, the authors examined the correlations of the 10 factors with scores derived from 10 genetic factors that a previous study identified underlying the shared variance among the Revised NEO Personality Inventory facets (K. L. Jang et al., 2002). The correspondence was strong enough to suggest that the 10 aspects of the Big Five may have distinct biological substrates.","[{'authorId': '2542164', 'name': 'C. DeYoung'}, {'authorId': '3925408', 'name': 'L. Quilty'}, {'authorId': '33275982', 'name': 'Jordan B. Peterson'}]",1549.0,"{'bibtex': '@Article{DeYoung2007BetweenFA,\n author = {C. DeYoung and L. Quilty and Jordan B. Peterson},\n journal = {Journal of personality and social psychology},\n pages = {\n          880-96\n        },\n title = {Between facets and domains: 10 aspects of the Big Five.},\n volume = {93 5},\n year = {2007}\n}\n'}",,"{'volume': '93 5', 'pages': '\n          880-96\n        ', 'name': 'Journal of personality and social psychology'}",66.0,Between facets and domains: 10 aspects of the Big Five.,2007.0
1227,6258156445ec81e556a199ddc02c2c98f6553175,"Learning objectives After participating in this activity, learners should be better able to: • Evaluate the literature regarding the effectiveness of incorporating virtual reality (VR) in the treatment of psychiatric disorders • Assess the use of exposure-based intervention for anxiety disorders Abstract Virtual reality (VR) allows users to experience a sense of presence in a computer-generated, three-dimensional environment. Sensory information is delivered through a head-mounted display and specialized interface devices. These devices track head movements so that the movements and images change in a natural way with head motion, allowing for a sense of immersion. VR, which allows for controlled delivery of sensory stimulation via the therapist, is a convenient and cost-effective treatment. This review focuses on the available literature regarding the effectiveness of incorporating VR within the treatment of various psychiatric disorders, with particular attention to exposure-based intervention for anxiety disorders. A systematic literature search was conducted in order to identify studies implementing VR-based treatment for anxiety or other psychiatric disorders. This article reviews the history of the development of VR-based technology and its use within psychiatric treatment, the empirical evidence for VR-based treatment, and the benefits for using VR for psychiatric research and treatment. It also presents recommendations for how to incorporate VR into psychiatric care and discusses future directions for VR-based treatment and clinical research.","[{'authorId': '1402540811', 'name': 'J. Maples-Keller'}, {'authorId': '38746727', 'name': 'B. Bunnell'}, {'authorId': '48388374', 'name': 'Sae-Jin Kim'}, {'authorId': '1831766', 'name': 'B. Rothbaum'}]",378.0,"{'bibtex': '@Article{Maples-Keller2017TheUO,\n author = {J. Maples-Keller and B. Bunnell and Sae-Jin Kim and B. Rothbaum},\n journal = {Harvard Review of Psychiatry},\n pages = {103–113},\n title = {The Use of Virtual Reality Technology in the Treatment of Anxiety and Other Psychiatric Disorders},\n volume = {25},\n year = {2017}\n}\n'}",,"{'volume': '25', 'pages': '103–113', 'name': 'Harvard Review of Psychiatry'}",105.0,The Use of Virtual Reality Technology in the Treatment of Anxiety and Other Psychiatric Disorders,2017.0
1228,626bff3ed49434d09b03495dbd687ed74ca831db,,"[{'authorId': '2209528801', 'name': 'M. Santos'}, {'authorId': '2407518', 'name': 'Arno in Wolde Lübke'}, {'authorId': '35619125', 'name': 'Takafumi Taketomi'}, {'authorId': '4907124', 'name': 'Goshiro Yamamoto'}, {'authorId': '34880158', 'name': 'M. Rodrigo'}, {'authorId': '1806543', 'name': 'C. Sandor'}, {'authorId': '2262728792', 'name': 'H. Kato'}]",164.0,"{'bibtex': '@Article{Santos2016AugmentedRA,\n author = {M. Santos and Arno in Wolde Lübke and Takafumi Taketomi and Goshiro Yamamoto and M. Rodrigo and C. Sandor and H. Kato},\n journal = {Research and Practice in Technology Enhanced Learning},\n title = {Augmented reality as multimedia: the case for situated vocabulary learning},\n volume = {11},\n year = {2016}\n}\n'}",,"{'volume': '11', 'name': 'Research and Practice in Technology Enhanced Learning'}",74.0,Augmented reality as multimedia: the case for situated vocabulary learning,2016.0
1229,6294651c71329b9379642a8b3b268c7b9ed3fe49,,"[{'authorId': '3689632', 'name': 'Laura Hoffmann'}, {'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '1422324403', 'name': 'Anh Lam-chi'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",77.0,"{'bibtex': '@Inproceedings{Hoffmann2009MediaER,\n author = {Laura Hoffmann and N. Krämer and Anh Lam-chi and S. Kopp},\n pages = {159-165},\n title = {Media Equation Revisited: Do Users Show Polite Reactions towards an Embodied Agent?},\n year = {2009}\n}\n'}",,{'pages': '159-165'},14.0,Media Equation Revisited: Do Users Show Polite Reactions towards an Embodied Agent?,2009.0
1230,629ddee7f9b50eaed0fa6e866b134d1102d6451e,,"[{'authorId': '1742930', 'name': 'E. André'}, {'authorId': '2922093', 'name': 'Martin Klesen'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '152915261', 'name': 'S. Allen'}, {'authorId': '144984483', 'name': 'T. Rist'}]",251.0,"{'bibtex': '@Inproceedings{André1999IntegratingMO,\n author = {E. André and Martin Klesen and Patrick Gebhard and S. Allen and T. Rist},\n pages = {150-165},\n title = {Integrating Models of Personality and Emotions into Lifelike Characters},\n year = {1999}\n}\n'}",,{'pages': '150-165'},31.0,Integrating Models of Personality and Emotions into Lifelike Characters,1999.0
1232,62b0675332b18db814785656ac1c3fda10fd2294,"Embodied conversational agents (ECAs) are often designed to produce nonverbal behavior to complement or enhance their verbal communication. One such form of the nonverbal behavior is co-speech gesturing, which involves movements that the agent makes with its arms and hands that are paired with verbal communication. Co-speech gestures for ECAs can be created using different generation methods, divided into rule-based and data-driven processes, with the latter, gaining traction because of the increasing interest from the applied machine learning community. However, reports on gesture generation methods use a variety of evaluation measures, which hinders comparison. To address this, we present a systematic review on co-speech gesture generation methods for iconic, metaphoric, deictic, and beat gestures, including reported evaluation methods. We review 22 studies that have an ECA with a human-like upper body that uses co-speech gesturing in social human-agent interaction. This includes studies that use human participants to evaluate performance. We found most studies use a within-subject design and rely on a form of subjective evaluation, but without a systematic approach. We argue that the field requires more rigorous and uniform tools for co-speech gesture evaluation, and formulate recommendations for empirical evaluation, including standardized phrases and example scenarios to help systematically test generative models across studies. Furthermore, we also propose a checklist that can be used to report relevant information for the evaluation of generative models, as well as to evaluate co-speech gesture use.","[{'authorId': '88728223', 'name': 'Pieter Wolfert'}, {'authorId': '49257924', 'name': 'Nicole L. Robinson'}, {'authorId': '2301161', 'name': 'Tony Belpaeme'}]",36.0,"{'bibtex': '@Article{Wolfert2021ARO,\n author = {Pieter Wolfert and Nicole L. Robinson and Tony Belpaeme},\n journal = {IEEE Transactions on Human-Machine Systems},\n pages = {379-389},\n title = {A Review of Evaluation Practices of Gesture Generation in Embodied Conversational Agents},\n volume = {52},\n year = {2021}\n}\n'}",,"{'volume': '52', 'pages': '379-389', 'name': 'IEEE Transactions on Human-Machine Systems'}",98.0,A Review of Evaluation Practices of Gesture Generation in Embodied Conversational Agents,2021.0
1233,62d35be0b678c80df55e5bcd87f0cd99580f1290,"
 
 Glaive is a state-space planner based on Hoffmann and Nebel's Fast-Forward which solves the narrative planning problem defined by Riedl and Young — to construct a plan which achieves the author's goals out of steps which are clearly motivated and goal-oriented toward individual character goals. Glaive reasons about how characters cooperate and conflict based on causal structures and possible worlds. By leveraging the unique constraints of narrative planning, Glaive reduces its branching factor and calculates a more accurate heuristic. We evaluate it on 8 narrative planning problems and demonstrate that it can solve certain non-trivial problems in under 1 second.
 
","[{'authorId': '34810994', 'name': 'Stephen G. Ware'}, {'authorId': '145513579', 'name': 'R. Young'}]",82.0,"{'bibtex': '@Article{Ware2014GlaiveAS,\n author = {Stephen G. Ware and R. Young},\n journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},\n title = {Glaive: A State-Space Narrative Planner Supporting Intentionality and Conflict},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment'},27.0,Glaive: A State-Space Narrative Planner Supporting Intentionality and Conflict,2014.0
1234,638ed211f3e4722e214c2117b5654b529c493ea4,"How do emotional expressions of group members shape conformity versus deviance in groups? We hypothesized that angry and happy responses to a group member's deviating opinion are interpreted as signals of imminent rejection versus acceptance. In 5 studies, the majority's expressions of anger led the deviant individual to feel rejected, whereas expressions of happiness made the deviant feel accepted. Because conformity can be seen as strategic behavior aimed at gaining (re)acceptance, the effects of emotional expressions on conformity should be moderated by social-contextual factors that determine the motivation to be accepted by the group and by the extent to which conformity is a means to this end. Accordingly, in Study 2, the availability of alternative groups determined whether a deviant conformed to the current group or abandoned the group after an angry reaction. In Study 3, anger and happiness were only associated with conformity pressure in situations that were perceived as cooperative (rather than competitive). Employing an interactive group task in Study 4, we showed that individuals who received an angry reaction contributed less in a cooperative group task than did those who received a neutral or happy reaction. Finally, in Study 5, peripheral group members conformed more after an angry reaction than after a happy reaction, but prototypical group members did not. Moreover, conformity was still manifest 3 weeks after the experiment, and this effect was mediated by feelings of rejection. We discuss implications of these findings for theorizing about social functions of emotions and the role of emotions in groups.","[{'authorId': '5073127', 'name': 'Marc W. Heerdink'}, {'authorId': '5980688', 'name': 'Gerben A. van Kleef'}, {'authorId': '4392679', 'name': 'A. Homan'}, {'authorId': '7444483', 'name': 'A. Fischer'}]",88.0,"{'bibtex': '@Article{Heerdink2013OnTS,\n author = {Marc W. Heerdink and Gerben A. van Kleef and A. Homan and A. Fischer},\n journal = {Journal of personality and social psychology},\n pages = {\n          262-84\n        },\n title = {On the social influence of emotions in groups: interpersonal effects of anger and happiness on conformity versus deviance.},\n volume = {105 2},\n year = {2013}\n}\n'}",,"{'volume': '105 2', 'pages': '\n          262-84\n        ', 'name': 'Journal of personality and social psychology'}",133.0,On the social influence of emotions in groups: interpersonal effects of anger and happiness on conformity versus deviance.,2013.0
1235,6391e61329c4923ed307bb411203dd6986a72735,"This handbook offers therapists an approach to helping clients live in harmony with head and heart. Leslie Greenberg proposes that, rather than controlling or avoiding emotions, clients can learn from their own bodily reactions and begin to act sensibly on them. Expressing emotion in ways that are appropriate to context is a highly complex skill. Offering clinical wisdom, practical guidance and case illustration, the volume presents an empirically-supported model of training clients to attain emotional wisdom.","[{'authorId': '6539543', 'name': 'L. Greenberg'}]",838.0,"{'bibtex': '@Inproceedings{Greenberg2002EmotionFocusedTC,\n author = {L. Greenberg},\n title = {Emotion-Focused Therapy: Coaching Clients to Work Through Their Feelings},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",128.0,Emotion-Focused Therapy: Coaching Clients to Work Through Their Feelings,2002.0
1236,63e605de8d98df2c8f94182f8ccfac73ad980a51,"Artificial emotion is essential to robotics study.Typical implementation of artificial emotion is briefly summarized in this paper.In order to better implement emotion-learning control mechanisms,a new control architecture based on artificial emotion is proposed.It incorporates an evolution-control system based on a Genetic Algorithm with a neural and an artificial emotion control system.The neural system receives environmental information and makes decisions.The results of decision-making are fed back to the emotion-learning model.The emotion-learning model produces emotional factors(hormones)based on inner and outer conditions of the robot,and these factors is used to regulate the neural system.In the final step,the memorization and behavior module of the neural system is exported to the genetic environment.This control architecture enhances learning and adaptive capacities of robots in a dynamic environment.Simulation was made to confirm the validity of the control architecture.","[{'authorId': '121700059', 'name': 'X. Xiong'}]",2.0,"{'bibtex': '@Article{Xiong2008ImplementationOA,\n author = {X. Xiong},\n journal = {Caai Transactions on Intelligent Systems},\n title = {Implementation of an evolutionary control system based on artificial emotion},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': 'Caai Transactions on Intelligent Systems'}",0.0,Implementation of an evolutionary control system based on artificial emotion,2008.0
1237,63f3107f977480f8cb278132f05768294c9fdc16,"This paper explores the extraction of features from the speech wave to perform intelligent emotion recognition. A feature extract tool (openSmile) was used to obtain a baseline set of 998 acoustic features from a set of emotional speech recordings from a microphone. The initial features were reduced to the most important ones so recognition of emotions using a supervised neural network could be performed. Given that the future use of virtual education agents lies with making the agents more interactive, developing agents with the capability to recognise and adapt to the emotional state of humans is an important step.","[{'authorId': '150925866', 'name': 'A. Tickle'}, {'authorId': '2055868224', 'name': 'S. Raghu'}, {'authorId': '2999481', 'name': 'M. Elshaw'}]",23.0,"{'bibtex': '@Article{Tickle2013EmotionalRF,\n author = {A. Tickle and S. Raghu and M. Elshaw},\n journal = {Journal of Physics: Conference Series},\n pages = {012053},\n title = {Emotional recognition from the speech signal for a virtual education agent},\n volume = {450},\n year = {2013}\n}\n'}","[{'paperId': 'f822bbc27ac54371f717e590be0186c1e04870a3', 'title': 'Automatic speech based emotion recognition using paralinguistics features'}, {'paperId': '5901d7ad3060e32f510f7e94dea6c82bb26bb26c', 'title': 'Audio Sentiment Analysis using Spectrogram and Bag-of- Visual- Words'}, {'paperId': '29d3999417d53286adea5e667fe8ab256af4df6c', 'title': 'Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study'}, {'paperId': '4c695ecfa27df6ade802972a36dc92830e2fbc50', 'title': 'CyTex: Transforming speech to textured images for speech emotion recognition'}, {'paperId': '2bb65d63ec27900d21bf119f895214499253661a', 'title': 'Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts'}, {'paperId': '1d75ff11897b27a8c40e457f385c0dfb0116f08b', 'title': 'Cascaded Convolutional Neural Network Architecture for Speech Emotion Recognition in Noisy Conditions'}, {'paperId': '3bab08b76a12f9b27c253ffdcb218327feb56bdb', 'title': 'The use of Automatic Speech Recognition in Education for Identifying Attitudes of the Speakers'}, {'paperId': 'ca9477f3e58568bad26ab1d29bfbf65aeac564b4', 'title': 'Affective Computing and Loneliness: How This Approach Could Improve a Support System'}, {'paperId': '5faa1708e8f77e43fd5b3d60e3b28ac2b3b14b49', 'title': 'Emotion Recognition from Speech Using the Bag-of-Visual Words on Audio Segment Spectrograms'}, {'paperId': '1838ba15b1989e6d3ca191d7d2921f574cbafb9c', 'title': 'Utilizing Psychoacoustic Modeling to Improve Speech-Based Emotion Recognition'}, {'paperId': '8dd73679899e36c67689fbd473f15869630925bd', 'title': 'Using a PCA-based dataset similarity measure to improve cross-corpus emotion recognition'}, {'paperId': 'a4b74b7b0f7fccedc49fb35e3410218e0fb98afc', 'title': 'Identifikasi Emosi Dari Sinyal Suara Secara Real Time Menggunakan Linear Predictive Coding dan Backpropagation'}, {'paperId': '35ea98dbf1676c1c8472c35059d4c46abd00e9a2', 'title': 'A Non-Linguistic Approach for Human Emotion Recognition from Speech'}, {'paperId': 'e7ecf59e5f7a419772e8d1ae3b172e41d64d78ae', 'title': 'Emotion Recognition from Speech: A Classroom Experiment'}, {'paperId': '8e116de80fddde0790207b18481d776037a39299', 'title': 'Prediction of User Satisfaction in Naturalistic Human-Computer Interaction'}, {'paperId': '30cee30b6e09eceb311bc6e7808845f498b3be5c', 'title': 'Improving Speech-Based Emotion Recognition by Using Psychoacoustic Modeling and Analysis-by-Synthesis'}, {'paperId': '09afc86d4c73e0d70390afa1319749dc1fa6c8eb', 'title': 'Empowering Requirements Elicitation Interviews with Vocal and Biofeedback Analysis'}, {'paperId': 'f9b26550067ff59230b5526fd6fb9cc4cc5b5df2', 'title': 'Exploring dataset similarities using PCA-based feature selection'}, {'paperId': '0b92c43f7de91ad33e6144235da75639a94f27a6', 'title': 'Procedure for Cepstral Analysis in tracing unique voice segments'}, {'paperId': '6b35970c822ef5bf01669273ac4bb9c137016860', 'title': 'Dimensionality Reduction and Attention Mechanisms for Extracting Affective State from Sound Spectrograms'}, {'paperId': '97039a3e4a2d02c5c25c360e43a79bdaa7474768', 'title': 'A Short Survey on Methodology for Stress Recognition'}, {'paperId': '195566fdcb473d170562519d15f27a2517cc7dbd', 'title': 'Feature Extraction of Voice Segments Using Cepstral Analysis for Voice Regeneration'}, {'paperId': '0d4cedb5dc7207c5876ee207716a1da91cef1912', 'title': 'Development of a Personalized Agent Dialogue Framework for End Users'}]","{'name': 'Journal of Physics: Conference Series', 'pages': '012053', 'volume': '450'}",21.0,Emotional recognition from the speech signal for a virtual education agent,2013.0
1238,642aff2564f82828f3af379bc3adfd4f78435f35,,"[{'authorId': '3368397', 'name': 'P. Verhaeghen'}, {'authorId': '2105494', 'name': 'J. Cerella'}]",678.0,"{'bibtex': '@Article{Verhaeghen2002AgingEC,\n author = {P. Verhaeghen and J. Cerella},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {849-857},\n title = {Aging, executive control, and attention: a review of meta-analyses},\n volume = {26},\n year = {2002}\n}\n'}",,"{'volume': '26', 'pages': '849-857', 'name': 'Neuroscience & Biobehavioral Reviews'}",36.0,"Aging, executive control, and attention: a review of meta-analyses",2002.0
1239,64441a238d93fc38e7a27610c66f425b9cace422,,"[{'authorId': '144629783', 'name': 'C. Chevallier'}, {'authorId': '47879364', 'name': 'G. Kohls'}, {'authorId': '2562268', 'name': 'V. Troiani'}, {'authorId': '6588720', 'name': 'E. Brodkin'}, {'authorId': '145157155', 'name': 'R. Schultz'}]",1542.0,"{'bibtex': '@Article{Chevallier2012TheSM,\n author = {C. Chevallier and G. Kohls and V. Troiani and E. Brodkin and R. Schultz},\n journal = {Trends in Cognitive Sciences},\n pages = {231-239},\n title = {The social motivation theory of autism},\n volume = {16},\n year = {2012}\n}\n'}",,"{'volume': '16', 'pages': '231-239', 'name': 'Trends in Cognitive Sciences'}",112.0,The social motivation theory of autism,2012.0
1240,6478bfd55fb24619c38e62771e04795243b1c38f,"Cross-lagged panel correlation is a method for testing spuriousness by comparing cross-lagged correlations. True experiments control for spuriousness by random assignment, but random assignment limits true experimental studies to independent variables that can be manipulated. Like any statistical method, cross-lagged analysis is based on a set of assumptions: synchronicity and stationarity. Different forms of stationarity have different consequences for both the changes in the synchronous correlations over time and the difference between cross-lags. Homogeneous stability is a necessary assumption in the identification of both the source and direction of a causal effect. Cross-lagged analysis is a low-power test. It is better adapted than either multiple regression or factor analysis for many questions in panel studies. Multiple regression must assume no errors of measurement in the independent variables and no correlated errors, while factor analysis must specify a particular factor structure. Two extended examples of cross-lagged analysis are discussed with special emphasis placed on the issue of stationarity and the estimation of reliability ratios.","[{'authorId': '2060895', 'name': 'D. Kenny'}]",704.0,"{'bibtex': '@Article{Kenny1975CrosslaggedPC,\n author = {D. Kenny},\n journal = {Psychological Bulletin},\n pages = {887-903},\n title = {Cross-lagged panel correlation: A test for spuriousness.},\n volume = {82},\n year = {1975}\n}\n'}",,"{'volume': '82', 'pages': '887-903', 'name': 'Psychological Bulletin'}",36.0,Cross-lagged panel correlation: A test for spuriousness.,1975.0
1241,64878f1bf384daa26346840136b0215daf72c205,"Virtual reality can broaden the types of interaction between students and computer tutors. As in conventional simulation-based training, the computer can watch students practice tasks, responding to questions and offering advice. However, immersive virtual environments also allow the computer tutor to inhabit the virtual world with the student. Unlike previous, disembodied computer tutors, such a ""pedagogical agent"" can ""physically"" collaborate with students, enabling new types of interaction. To illustrate the possibilities, this paper describes Steve, a pedagogical agent for virtual environments that helps students learn procedural tasks. After providing an overview of Steve's capabilities, the paper focuses on the benefits and challenges of graphically representing Steve in the virtual environment.","[{'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '2019292', 'name': 'J. Rickel'}]",241.0,"{'bibtex': '@Article{Johnson1997SteveAA,\n author = {W. Johnson and J. Rickel},\n journal = {SIGART Bull.},\n pages = {16-21},\n title = {Steve: an animated pedagogical agent for procedural training in virtual environments},\n volume = {8},\n year = {1997}\n}\n'}",,"{'volume': '8', 'pages': '16-21', 'name': 'SIGART Bull.'}",21.0,Steve: an animated pedagogical agent for procedural training in virtual environments,1997.0
1243,648c572b96af0acebd85e11a8a4320a9a5eb7709,"Kansei engineering is the newly proposed engineering discipline having a novel and unique goal. While its aim has been considered, from the beginning, to construct methodology and technology capable of providing industrial products and services reflecting user's personal preference/requirement and being evaluated by satisfaction of users, its role is now growing up to consider a new leaf of more fundamental technical issues on Kansei informatics or human-computer interactions toward realising safety and pleasantness of individuals, which will be considered to be a most important fundamental problem in the coming information network society. In this paper, we try to describe an overview of Kansei engineering referring to biometrics. The contents include the background, objectives, present status, and present technical topics of Kansei engineering, where we also introduce concrete examples of developing researches. Future developments are also discussed.","[{'authorId': '3163017', 'name': 'T. Nagashima'}, {'authorId': '2115247635', 'name': 'Hidenori Tanaka'}, {'authorId': '2679920', 'name': 'T. Uozumi'}]",15.0,"{'bibtex': '@Article{Nagashima2008AnOO,\n author = {T. Nagashima and Hidenori Tanaka and T. Uozumi},\n journal = {Int. J. Biom.},\n pages = {3-19},\n title = {An overview of Kansei engineering: a proposal of Kansei informatics toward realising safety and pleasantness of individuals in information network society},\n volume = {1},\n year = {2008}\n}\n'}",,"{'volume': '1', 'pages': '3-19', 'name': 'Int. J. Biom.'}",13.0,An overview of Kansei engineering: a proposal of Kansei informatics toward realising safety and pleasantness of individuals in information network society,2008.0
1244,649d104d259791604078dc396a8b1f40fcbe0174,,[],530.0,"{'bibtex': '@Misc{None,\n title = {CURRENT DIRECTIONS IN PSYCHOLOGICAL SCIENCE Tend and Befriend Biobehavioral Bases of Affiliation Under Stress}\n}\n'}",,,0.0,CURRENT DIRECTIONS IN PSYCHOLOGICAL SCIENCE Tend and Befriend Biobehavioral Bases of Affiliation Under Stress,
1245,64d033b8939bb5f7235af10d29c8690bb19dcf98,"The ANGELICA project addresses the problem of modality choice in information presentation by embodied, humanlike agents. The output modalities available to such agents include both language and various nonverbal signals such as pointing and gesturing. For each piece of information to be presented by the agent it must be decided whether it should be expressed using language, a nonverbal signal, or both. In the ANGELICA project a model of the different factors influencing this choice will be developed and integrated in a natural language generation system. The application domain is the presentation of route descriptions by an embodied agent in a 3D environment. Evaluation and testing form an integral part of the project. In particular, we will investigate the effect of different modality choices on the effectiveness and naturalness of the generated presentations and on the user's perception of the agent's personality.","[{'authorId': '1742430', 'name': 'M. Theune'}]",15.0,"{'bibtex': '@Inproceedings{Theune2001ANGELICAC,\n author = {M. Theune},\n pages = {89-93},\n title = {ANGELICA : choice of output modality in an embodied agent},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '89-93', 'name': ''}",35.0,ANGELICA : choice of output modality in an embodied agent,2001.0
1246,652544919461a28f03fe0b429f087e4c6d214d77,"Virtual reality is rapidly evolving into a pragmatically usable technology for mental health (MH) applications. As the underlying enabling technologies continue to evolve and allow us to design more useful and usable structural virtual environments (VEs), the next important challenge will involve populating these environments with virtual representations of humans (avatars). This will be vital to create mental health VEs that leverage the use of avatars for applications that require human-human interaction and communication. As Alessi et al.1 pointed out at the 8th Annual Medicine Meets Virtual Reality Conference (MMVR8), virtual humans have mainly appeared in MH applications to ""serve the role of props, rather than humans."" More believable avatars inhabiting VEs would open up possibilities for MH applications that address social interaction, communication, instruction, assessment, and rehabilitation issues. They could also serve to enhance realism that might in turn promote the experience of presence in VR. Additionally, it will soon be possible to use computer-generated avatars that serve to provide believable dynamic facial and bodily representations of individuals communicating from a distance in real time. This could support the delivery, in shared virtual environments, of more natural human interaction styles, similar to what is used in real life between people. These techniques could enhance communication and interaction by leveraging our natural sensing and perceiving capabilities and offer the potential to model human-computer-human interaction after human-human interaction. To enhance the authenticity of virtual human representations, advances in the rendering of facial and gestural behaviors that support implicit communication will be needed. In this regard, the current paper presents data from a study that compared human raters' judgments of emotional expression between actual video clips of facial expressions and identical expressions rendered on a three-dimensional avatar using a performance-driven facial animation (PDFA) system developed at the University of Southern California Integrated Media Systems Center. PDFA offers a means for creating high-fidelity visual representations of human faces and bodies. This effort explores the feasibility of sensing and reproducing a range of facial expressions with a PDFA system. In order to test concordance of human ratings of emotional expression between video and avatar facial delivery, we first had facial model subjects observe stimuli that were designed to elicit naturalistic facial expressions. The emotional stimulus induction involved presenting text-based, still image, and video clips to subjects that were previously rated to induce facial expressions for the six universals2 of facial expression (happy, sad, fear, anger, disgust, and surprise), in addition to attentiveness, puzzlement and frustration. Videotapes of these induced facial expressions that best represented prototypic examples of the above emotional states and three-dimensional avatar animations of the same facial expressions were randomly presented to 38 human raters. The raters used open-end, forced choice and seven-point Likert-type scales to rate expression in terms of identification. The forced choice and seven-point ratings provided the most usable data to determine video/animation concordance and these data are presented. To support a clear understanding of this data, a website has been set up that will allow readers to view the video and facial animation clips to illustrate the assets and limitations of these types of facial expression-rendering methods (www. USCAvatars.com/MMVR). This methodological first step in our research program has served to provide valuable human user-centered feedback to support the iterative design and development of facial avatar characteristics for expression of emotional communication.","[{'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '143840663', 'name': 'U. Neumann'}, {'authorId': '2094498548', 'name': 'R. Enciso'}, {'authorId': '2042142', 'name': 'Douglas A. Fidaleo'}, {'authorId': '116454272', 'name': 'Jun-yong Noh'}]",48.0,"{'bibtex': '@Article{Rizzo2001PerformanceDrivenFA,\n author = {A. Rizzo and U. Neumann and R. Enciso and Douglas A. Fidaleo and Jun-yong Noh},\n journal = {Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society},\n pages = {\n          471-87\n        },\n title = {Performance-Driven Facial Animation: Basic Research on Human Judgments of Emotional State in Facial Avatars},\n volume = {4 4},\n year = {2001}\n}\n'}",,"{'volume': '4 4', 'pages': '\n          471-87\n        ', 'name': 'Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society'}",34.0,Performance-Driven Facial Animation: Basic Research on Human Judgments of Emotional State in Facial Avatars,2001.0
1247,653542a3e404962bff36eded6369241bb44aa29c,,"[{'authorId': '1894671', 'name': 'Ramin Yaghoubzadeh'}, {'authorId': '2054271039', 'name': 'Marcel Kramer'}, {'authorId': '1725787', 'name': 'K. Pitsch'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",107.0,"{'bibtex': '@Inproceedings{Yaghoubzadeh2013VirtualAA,\n author = {Ramin Yaghoubzadeh and Marcel Kramer and K. Pitsch and S. Kopp},\n pages = {79-91},\n title = {Virtual Agents as Daily Assistants for Elderly or Cognitively Impaired People - Studies on Acceptance and Interaction Feasibility},\n year = {2013}\n}\n'}",,{'pages': '79-91'},20.0,Virtual Agents as Daily Assistants for Elderly or Cognitively Impaired People - Studies on Acceptance and Interaction Feasibility,2013.0
1248,6549a9d851a16f356d64ebde9ab7743d7639d67a,,"[{'authorId': '145248978', 'name': 'P. Philip'}, {'authorId': '1398530312', 'name': 'J. Micoulaud-Franchi'}, {'authorId': '1813095', 'name': 'P. Sagaspe'}, {'authorId': '1761859', 'name': 'E. D. Sevin'}, {'authorId': '2074620344', 'name': 'J. Olive'}, {'authorId': '2759597', 'name': 'S. Bioulac'}, {'authorId': '48763838', 'name': 'A. Sauteraud'}]",99.0,"{'bibtex': '@Article{Philip2017VirtualHA,\n author = {P. Philip and J. Micoulaud-Franchi and P. Sagaspe and E. D. Sevin and J. Olive and S. Bioulac and A. Sauteraud},\n journal = {Scientific Reports},\n title = {Virtual human as a new diagnostic tool, a proof of concept study in the field of major depressive disorders},\n volume = {7},\n year = {2017}\n}\n'}",,"{'volume': '7', 'name': 'Scientific Reports'}",39.0,"Virtual human as a new diagnostic tool, a proof of concept study in the field of major depressive disorders",2017.0
1249,6552e28f7520caf27a85e41b89c4199f1175f186,"Following Yik and Russell (1999) a judgement paradigm was used to examine to what extent differential accuracy of recognition of facial expressions allows evaluation of the well-foundedness of different theoretical views on emotional expression. Observers judged photos showing facial expressions of seven emotions on the basis of: (1) discrete emotion categories; (2) social message types; (3) appraisal results; or (4) action tendencies, and rated their confidence in making choices. Emotion categories and appraisals were judged significantly more accurately and confidently than messages or action tendencies. These results do not support claims of primacy for message or action tendency views of facial expression. Based on a componential model of emotion it is suggested that judges can infer components from categories and vice versa.","[{'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '1797080', 'name': 'D. Grandjean'}]",124.0,"{'bibtex': '@Article{Scherer2008FacialEA,\n author = {K. Scherer and D. Grandjean},\n journal = {Cognition and Emotion},\n pages = {789 - 801},\n title = {Facial expressions allow inference of both emotions and their components},\n volume = {22},\n year = {2008}\n}\n'}",,"{'volume': '22', 'pages': '789 - 801', 'name': 'Cognition and Emotion'}",28.0,Facial expressions allow inference of both emotions and their components,2008.0
1250,655ba8740546ad401d201199cb871e2dac355dd3,,"[{'authorId': '3032876', 'name': 'H. Bibby'}, {'authorId': '143982280', 'name': 'S. McDonald'}]",235.0,"{'bibtex': '@Article{Bibby2005TheoryOM,\n author = {H. Bibby and S. McDonald},\n journal = {Neuropsychologia},\n pages = {99-114},\n title = {Theory of mind after traumatic brain injury},\n volume = {43},\n year = {2005}\n}\n'}",,"{'volume': '43', 'pages': '99-114', 'name': 'Neuropsychologia'}",64.0,Theory of mind after traumatic brain injury,2005.0
1251,6581b0470b1ce3ab4329c595ff0b3aaff85eba5a,"Despite much research on the function of the insular cortex, few studies have investigated functional subdivisions of the insula in humans. The present study used resting-state functional connectivity magnetic resonance imaging (MRI) to parcellate the human insular lobe based on clustering of functional connectivity patterns. Connectivity maps were computed for each voxel in the insula based on resting-state functional MRI (fMRI) data and segregated using cluster analysis. We identified 3 insular subregions with distinct patterns of connectivity: a posterior region, functionally connected with primary and secondary somatomotor cortices; a dorsal anterior to middle region, connected with dorsal anterior cingulate cortex, along with other regions of a previously described control network; and a ventral anterior region, primarily connected with pregenual anterior cingulate cortex. Applying these regions to a separate task data set, we found that dorsal and ventral anterior insula responded selectively to disgusting images, while posterior insula did not. These results demonstrate that clustering of connectivity patterns can be used to subdivide cerebral cortex into anatomically and functionally meaningful subregions; the insular regions identified here should be useful in future investigations on the function of the insula.","[{'authorId': '2305032', 'name': 'Ben Deen'}, {'authorId': '2273865', 'name': 'Naomi B. Pitskel'}, {'authorId': '9765768', 'name': 'K. Pelphrey'}]",611.0,"{'bibtex': '@Article{Deen2010ThreeSO,\n author = {Ben Deen and Naomi B. Pitskel and K. Pelphrey},\n journal = {Cerebral Cortex (New York, NY)},\n pages = {1498 - 1506},\n title = {Three Systems of Insular Functional Connectivity Identified with Cluster Analysis},\n volume = {21},\n year = {2010}\n}\n'}",,"{'volume': '21', 'pages': '1498 - 1506', 'name': 'Cerebral Cortex (New York, NY)'}",84.0,Three Systems of Insular Functional Connectivity Identified with Cluster Analysis,2010.0
1252,658dac82a4f292ecd7c631c6a61408db75f05d07,. The uncanny valley is a phenomenon first described in 1970 by Masahiro Mori. It characterizes the correlation between the degree of human likeness of e,"[{'authorId': '2249341176', 'name': 'Lisa Bode'}]",1891.0,"{'bibtex': '@Article{Bode2019TheUV,\n author = {Lisa Bode},\n journal = {The Animation Studies Reader},\n title = {The Uncanny Valley},\n year = {2019}\n}\n'}",,{'name': 'The Animation Studies Reader'},13.0,The Uncanny Valley,2019.0
1254,658ec0b77341a806846c395edcc561bc42892c08,,"[{'authorId': '3213354', 'name': 'B. Gelder'}]",34.0,"{'bibtex': '@Inproceedings{Gelder2016EmotionsAT,\n author = {B. Gelder},\n title = {Emotions and the Body},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotions and the Body,2016.0
1255,65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea,,"[{'authorId': '32201536', 'name': 'S. Moubayed'}, {'authorId': '1826819', 'name': 'J. Beskow'}, {'authorId': '1711959', 'name': 'Gabriel Skantze'}, {'authorId': '144966453', 'name': 'B. Granström'}]",247.0,"{'bibtex': '@Inproceedings{Moubayed2011FurhatAB,\n author = {S. Moubayed and J. Beskow and Gabriel Skantze and B. Granström},\n pages = {114-130},\n title = {Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction},\n year = {2011}\n}\n'}",,{'pages': '114-130'},25.0,Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction,2011.0
1256,660ed33f065a45527a0d3d3641cba5f54752d479,"Abstract Eye contact plays a critical role in many aspects of face processing, including the processing of smiles. We propose that this is achieved by a subcortical route, which is activated by eye contact and modulates the cortical areas involve in social cognition, including the processing of facial expression. This mechanism could be impaired in individuals with autism spectrum disorders.","[{'authorId': '1986858', 'name': 'P. Niedenthal'}, {'authorId': '2634712', 'name': 'M. Mermillod'}, {'authorId': '40152862', 'name': 'M. Maringer'}, {'authorId': '3067657', 'name': 'U. Hess'}]",491.0,"{'bibtex': '@Article{Niedenthal2010IsEC,\n author = {P. Niedenthal and M. Mermillod and M. Maringer and U. Hess},\n journal = {Behavioral and Brain Sciences},\n pages = {458 - 459},\n title = {Is eye contact the key to the social brain?},\n volume = {33},\n year = {2010}\n}\n'}",,"{'volume': '33', 'pages': '458 - 459', 'name': 'Behavioral and Brain Sciences'}",663.0,Is eye contact the key to the social brain?,2010.0
1258,6623524f5b3538dd15c6b618ad00c670597ffbf5,,"[{'authorId': '5099321', 'name': 'S. Mineka'}, {'authorId': '5860536', 'name': 'Eshkol Rafaeli'}, {'authorId': '35189498', 'name': 'Iftah Yovel'}]",71.0,"{'bibtex': '@Inproceedings{Mineka2003CognitiveBI,\n author = {S. Mineka and Eshkol Rafaeli and Iftah Yovel},\n title = {Cognitive biases in emotional disorders: Information processing and social-cognitive perspectives: Series in Affective Science},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Cognitive biases in emotional disorders: Information processing and social-cognitive perspectives: Series in Affective Science,2003.0
1259,663bc6fbd59fbcf97a646de81d1419eddc79b66c,"In contrast to this view, recent theoretical advances in brain imaging have revealed that the brain is an organ continually built and re-built by one's experience. We are now beginning to learn that many forms of psychotherapy, developed in the absence of any scientific understanding of the brain, are supported by neuroscientific findings. In fact, it could be argued that to be an effective psychotherapist these days it is essential to have some basic understanding of neuroscience. Louis Cozolino's The Neuroscience of Psychotherapy, Second Edition is the perfect place to start. In a beautifully written and accessible synthesis, Cozolino illustrates how the brain's architecture is related to the problems, passions, and aspirations of human beings. As the book so elegantly argues, all forms of psychotherapy--from psychoanalysis to behavioral interventions--are successful to the extent to which they enhance change in relevant neural circuits. Beginning with an overview of the intersecting fields of neuroscience and psychotherapy, this book delves into the brain's inner workings, from basic neuronal building blocks to complex systems of memory, language, and the organization of experience. It continues by explaining the development and organization of the healthy brain and the unhealthy brain. Common problems such as anxiety, trauma, and codependency are discussed from a scientific and clinical perspective. Throughout the book, the science behind the brain's working is applied to day-to-day experience and clinical practice. Written for psychotherapists and others interested in the relationship between brain and behavior, this book encourages us to consider the brain when attempting to understand human development, mental illness, and psychological health. Fully and thoroughly updated with the many neuroscientific developments that have happened in the eight years since the publication of the first edition, this revision to the bestselling book belongs on the shelf of all practitioners.","[{'authorId': '4571106', 'name': 'L. Cozolino'}]",58.0,"{'bibtex': '@Inproceedings{Cozolino2017TheNO,\n author = {L. Cozolino},\n title = {The Neuroscience of Psychotherapy: Healing the Social Brain},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Neuroscience of Psychotherapy: Healing the Social Brain,2017.0
1260,6673d412dc7a29d796693ef60cc5dc87cbc2eb6e,,"[{'authorId': '38546866', 'name': 'H. Ross'}]",1091.0,"{'bibtex': '@Article{Ross1964PrinciplesON,\n author = {H. Ross},\n journal = {Systematic Biology},\n pages = {106-108},\n title = {Principles of Numerical Taxonomy},\n volume = {13},\n year = {1964}\n}\n'}",,"{'volume': '13', 'pages': '106-108', 'name': 'Systematic Biology'}",0.0,Principles of Numerical Taxonomy,1964.0
1261,66a78ea7d0240ffb80826eebc9360270ed637826,,"[{'authorId': '2245673', 'name': 'A. Duchowski'}]",2372.0,"{'bibtex': '@Inproceedings{Duchowski2003EyeTM,\n author = {A. Duchowski},\n pages = {I-XVII, 1-251},\n title = {Eye Tracking Methodology: Theory and Practice},\n year = {2003}\n}\n'}",,"{'pages': 'I-XVII, 1-251'}",52.0,Eye Tracking Methodology: Theory and Practice,2003.0
1262,66ac496a44e8c80784b09b0db1d210d61178af9a,,"[{'authorId': '6662654', 'name': 'J. Lanzetta'}, {'authorId': '1889669', 'name': 'B. Englis'}]",376.0,"{'bibtex': ""@Article{Lanzetta1989ExpectationsOC,\n author = {J. Lanzetta and B. Englis},\n journal = {Journal of Personality and Social Psychology},\n pages = {543-554},\n title = {Expectations of Cooperation and Competition and Their Effects on Observers' Vicarious Emotional Responses},\n volume = {56},\n year = {1989}\n}\n""}",,"{'volume': '56', 'pages': '543-554', 'name': 'Journal of Personality and Social Psychology'}",53.0,Expectations of Cooperation and Competition and Their Effects on Observers' Vicarious Emotional Responses,1989.0
1263,675bb798f0cf542c0e10687c39482a8ff7e3318a,"In this paper, we present the SemEval-2019 Task 3 - EmoContext: Contextual Emotion Detection in Text. Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem. For instance, as humans, on reading “Why don’t you ever text me!” we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines. However, the context of dialogue can prove helpful in detection of the emotion. In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others. To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps. A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants. A total of 311 teams made submissions to this task. The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score. Our analysis of systems submitted to the task indicate that Bi-directional LSTM was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class.","[{'authorId': '2463157', 'name': 'Ankush Chatterjee'}, {'authorId': '34749306', 'name': 'Kedhar Nath Narahari'}, {'authorId': '1651892966', 'name': 'Meghana Joshi'}, {'authorId': '2067432800', 'name': 'Puneet Agrawal'}]",201.0,"{'bibtex': '@Inproceedings{Chatterjee2019SemEval2019T3,\n author = {Ankush Chatterjee and Kedhar Nath Narahari and Meghana Joshi and Puneet Agrawal},\n pages = {39-48},\n title = {SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text},\n year = {2019}\n}\n'}",,{'pages': '39-48'},46.0,SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text,2019.0
1264,67677fe62a14fa172606853b2e1d0e4d5d37d199,,"[{'authorId': '3065464', 'name': 'A. V. D. Pütten'}, {'authorId': '39546611', 'name': 'F. P. Schulte'}, {'authorId': '2374750', 'name': 'S. Eimler'}, {'authorId': '3163828', 'name': 'Sabrina Sobieraj'}, {'authorId': '3689632', 'name': 'Laura Hoffmann'}, {'authorId': '2148765', 'name': 'S. Maderwald'}, {'authorId': '145386353', 'name': 'M. Brand'}, {'authorId': '1750852', 'name': 'N. Krämer'}]",114.0,"{'bibtex': '@Article{Pütten2014InvestigationsOE,\n author = {A. V. D. Pütten and F. P. Schulte and S. Eimler and Sabrina Sobieraj and Laura Hoffmann and S. Maderwald and M. Brand and N. Krämer},\n journal = {Comput. Hum. Behav.},\n pages = {201-212},\n title = {Investigations on empathy towards humans and robots using fMRI},\n volume = {33},\n year = {2014}\n}\n'}",,"{'volume': '33', 'pages': '201-212', 'name': 'Comput. Hum. Behav.'}",83.0,Investigations on empathy towards humans and robots using fMRI,2014.0
1265,67ab08815dc8cb850e4464b878beeed989b10250,"A new model of the visual search process is developed which can improve the design of large symbol sets such as those used by nuclear power plant personnel, air traffic controllers, and battlefield troops. An experiment was conducted to determine whether the new, componential model or an already existing, discriminability model better explains visual search behavior. The results were consistent with the componential model. We show how to use the componential model to help automate selection of the optimal symbol set (i.e., the symbol set that minimizes the average time to find a target).","[{'authorId': '2024442', 'name': 'D. Fisher'}, {'authorId': '2060642430', 'name': 'Nancy S. Tanner'}]",31.0,"{'bibtex': '@Article{Fisher1992OptimalSS,\n author = {D. Fisher and Nancy S. Tanner},\n journal = {Human Factors: The Journal of Human Factors and Ergonomics Society},\n pages = {79 - 95},\n title = {Optimal Symbol Set Selection: A Semiautomated Procedure},\n volume = {34},\n year = {1992}\n}\n'}",,"{'volume': '34', 'pages': '79 - 95', 'name': 'Human Factors: The Journal of Human Factors and Ergonomics Society'}",19.0,Optimal Symbol Set Selection: A Semiautomated Procedure,1992.0
1266,67dccc9a856b60bdc4d058d83657a089b8ad4486,"We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. 
 
Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.","[{'authorId': '34838386', 'name': 'K. Simonyan'}, {'authorId': '1688869', 'name': 'Andrew Zisserman'}]",6744.0,"{'bibtex': '@Article{Simonyan2014TwoStreamCN,\n author = {K. Simonyan and Andrew Zisserman},\n journal = {ArXiv},\n title = {Two-Stream Convolutional Networks for Action Recognition in Videos},\n volume = {abs/1406.2199},\n year = {2014}\n}\n'}",,"{'volume': 'abs/1406.2199', 'name': 'ArXiv'}",33.0,Two-Stream Convolutional Networks for Action Recognition in Videos,2014.0
1267,67e088e6c2759485f443e6cd20d57c1d18dbbee4,"Research on the neural bases of emotion raises much controversy and few quantitative models exist that can help address the issues raised. Here we replicate and dissect one of those models, Armony and colleagues’ neurocomputational model of fear conditioning, which is based on LeDoux's dual-route hypothesis regarding the rat fear circuitry. The importance of the model's modular abstraction of the neuroanatomy, its use of population coding, and in particular the interplay between thalamo-amygdala and thalamo-cortical pathways are tested. We show that a trivially minimal version of the model can produce conditioning to a reinforced stimulus without recourse to the dual pathway structure, but a modification of the original model, which nevertheless preserves the thalamo-amygdala and (reduced) thalamo-cortical pathways, enables stronger conditioning to a conditioned stimulus. Implications for neurocomputational modelling approaches are discussed.","[{'authorId': '145094762', 'name': 'Robert J. Lowe'}, {'authorId': '72525901', 'name': 'M. Humphries'}, {'authorId': '2491309', 'name': 'T. Ziemke'}]",21.0,"{'bibtex': '@Article{Lowe2009TheDH,\n author = {Robert J. Lowe and M. Humphries and T. Ziemke},\n journal = {Connection Science},\n pages = {15 - 37},\n title = {The dual-route hypothesis: evaluating a neurocomputational model of fear conditioning in rats},\n volume = {21},\n year = {2009}\n}\n'}",,"{'volume': '21', 'pages': '15 - 37', 'name': 'Connection Science'}",58.0,The dual-route hypothesis: evaluating a neurocomputational model of fear conditioning in rats,2009.0
1268,6814d976154b17e41bc79f8694e2372534e50419,,"[{'authorId': '3442630', 'name': 'F. Faul'}, {'authorId': '3391328', 'name': 'E. Erdfelder'}, {'authorId': '123362370', 'name': 'Albert-Georg Lang'}, {'authorId': '39826407', 'name': 'A. Buchner'}]",39973.0,"{'bibtex': '@Article{Faul2007GPower3A,\n author = {F. Faul and E. Erdfelder and Albert-Georg Lang and A. Buchner},\n journal = {Behavior Research Methods},\n pages = {175-191},\n title = {G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences},\n volume = {39},\n year = {2007}\n}\n'}",,"{'volume': '39', 'pages': '175-191', 'name': 'Behavior Research Methods'}",86.0,"G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences",2007.0
1269,681c4a01bcc3deb78491fae30b196afa7fe9e458,,"[{'authorId': '1812107', 'name': 'J. Bentahar'}, {'authorId': '1727720', 'name': 'B. Moulin'}, {'authorId': '1399443272', 'name': 'B. Chaib-draa'}]",28.0,"{'bibtex': '@Inproceedings{Bentahar2004SpecifyingAI,\n author = {J. Bentahar and B. Moulin and B. Chaib-draa},\n pages = {130-148},\n title = {Specifying and Implementing a Persuasion Dialogue Game Using Commitments and Arguments},\n year = {2004}\n}\n'}",,{'pages': '130-148'},39.0,Specifying and Implementing a Persuasion Dialogue Game Using Commitments and Arguments,2004.0
1270,68385c20a25aea16a54bbaad4f4230a892c010c6,"A major goal of research on virtual humans is the animation of expressive characters that display distinct psychological attributes. Body motion is an effective way of portraying different personalities and differentiating characters. The purpose and contribution of this work is to describe a formal, broadly applicable, procedural, and empirically grounded association between personality and body motion and apply this association to modify a given virtual human body animation that can be represented by these formal concepts. Because the body movement of virtual characters may involve different choices of parameter sets depending on the context, situation, or application, formulating a link from personality to body motion requires an intermediate step to assist generalization. For this intermediate step, we refer to Laban Movement Analysis, which is a movement analysis technique for systematically describing and evaluating human motion. We have developed an expressive human motion generation system with the help of movement experts and conducted a user study to explore how the psychologically validated OCEAN personality factors were perceived in motions with various Laban parameters. We have then applied our findings to procedurally animate expressive characters with personality, and validated the generalizability of our approach across different models and animations via another perception study.","[{'authorId': '2643744', 'name': 'Funda Durupinar'}, {'authorId': '143980997', 'name': 'M. Kapadia'}, {'authorId': '2047954359', 'name': 'Susan Deutsch'}, {'authorId': '143687087', 'name': 'Michael Neff'}, {'authorId': '1699200', 'name': 'N. Badler'}]",50.0,"{'bibtex': '@Article{Durupinar2016PerformPA,\n author = {Funda Durupinar and M. Kapadia and Susan Deutsch and Michael Neff and N. Badler},\n journal = {ACM Trans. Graph.},\n title = {Perform: perceptual approach for adding OCEAN personality to human motion using laban movement analysis},\n volume = {36},\n year = {2016}\n}\n'}",,"{'volume': '36', 'name': 'ACM Trans. Graph.'}",57.0,Perform: perceptual approach for adding OCEAN personality to human motion using laban movement analysis,2016.0
1272,68642d447018744b30e113d476e03583d83cd82d,,"[{'authorId': '41154117', 'name': 'D. Szczygieł'}, {'authorId': '3868830', 'name': 'J. Buczny'}, {'authorId': '13512301', 'name': 'Róża Bazińska'}]",85.0,"{'bibtex': '@Article{Szczygieł2012EmotionRA,\n author = {D. Szczygieł and J. Buczny and Róża Bazińska},\n journal = {Personality and Individual Differences},\n pages = {433-437},\n title = {Emotion regulation and emotional information processing: The moderating effect of emotional awareness},\n volume = {52},\n year = {2012}\n}\n'}",,"{'volume': '52', 'pages': '433-437', 'name': 'Personality and Individual Differences'}",28.0,Emotion regulation and emotional information processing: The moderating effect of emotional awareness,2012.0
1273,686cfc59a276e9b209aa241361c67ec37d4afe5f,"We present a novel approach for analyzing the quality of multi‐agent crowd simulation algorithms. Our approach is data‐driven, taking as input a set of user‐defined metrics and reference training data, either synthetic or from video footage of real crowds. Given a simulation, we formulate the crowd analysis problem as an anomaly detection problem and exploit state‐of‐the‐art outlier detection algorithms to address it. To that end, we introduce a new framework for the visual analysis of crowd simulations. Our framework allows us to capture potentially erroneous behaviors on a per‐agent basis either by automatically detecting outliers based on individual evaluation metrics or by accounting for multiple evaluation criteria in a principled fashion using Principle Component Analysis and the notion of Pareto Optimality. We discuss optimizations necessary to allow real‐time performance on large datasets and demonstrate the applicability of our framework through the analysis of simulations created by several widely‐used methods, including a simulation from a commercial game.","[{'authorId': '39066972', 'name': 'Panayiotis Charalambous'}, {'authorId': '2478994', 'name': 'Ioannis Karamouzas'}, {'authorId': '35170565', 'name': 'S. Guy'}, {'authorId': '1706408', 'name': 'Y. Chrysanthou'}]",51.0,"{'bibtex': '@Article{Charalambous2014ADF,\n author = {Panayiotis Charalambous and Ioannis Karamouzas and S. Guy and Y. Chrysanthou},\n journal = {Computer Graphics Forum},\n title = {A Data‐Driven Framework for Visual Crowd Analysis},\n volume = {33},\n year = {2014}\n}\n'}",,"{'volume': '33', 'name': 'Computer Graphics Forum'}",30.0,A Data‐Driven Framework for Visual Crowd Analysis,2014.0
1275,686e9a849db9e99a84e86652bd2d9cca228dc12e,,"[{'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '1412884931', 'name': 'J. Fernández-Dols'}]",105.0,"{'bibtex': '@Inproceedings{Russell1997ThePO,\n author = {J. Russell and J. Fernández-Dols},\n title = {The psychology of facial expression: What does a facial expression mean?},\n year = {1997}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The psychology of facial expression: What does a facial expression mean?,1997.0
1276,687ede32e2fea9408584a2682125a8a378ea17a0,"This paper proposes a modification to the model-free Reinforcement learning algorithm Q-learning. It is implemented to train smart gift shopping-cart learning agents (SGSCLA). The aim of the modification is to empower the learning agent to reaching a goal by making appropriate compromises only. That is the way in which the measure models and emotional models, represented as new agent memory matrixes are introduced. This models show how the user perceives and evaluates the environment. The Shopping Center is represented by a multigraph in which the nodes represent three groups of shops. The edges illustrate the connections between the shops; the primary (major) and the secondary (minor) paths between them and the emotions, evoked in the customer under consideration by a visit to a particular shop. The user can be see the route suggested by the virtual agent and the made compromises to the goal.The emotion types chosen for the purpose of the experiment are boredom, joy and worry. The environment model allow for exploring and predicting the change in the customer’s mood as he/she moves from one shop to another.","[{'authorId': '1799528', 'name': 'D. Budakova'}, {'authorId': '1581448954', 'name': 'Veselka Petrova-Dimitrova'}, {'authorId': '1753312', 'name': 'L. Dakovski'}]",1.0,"{'bibtex': '@Article{Budakova2020VirtualAL,\n author = {D. Budakova and Veselka Petrova-Dimitrova and L. Dakovski},\n journal = {Computer Science},\n title = {Virtual Agents, Learning How to Reach a Goal by Making Appropriate Compromises},\n year = {2020}\n}\n'}","[{'paperId': 'd696912be0427bd5dade666658b05652a7a0c8ee', 'title': 'Virtual Agent Behavior Modeling in Case of a Risky Situation in a Virtual Electrical Substation'}]","{'name': 'Computer Science', 'volume': ''}",15.0,"Virtual Agents, Learning How to Reach a Goal by Making Appropriate Compromises",2020.0
1277,68ae872c5e7cf31d6950d91fd902c5c42f5dee6b,"To date, the field of emotion regulation (ER) has largely focused on intrinsic ER (i.e., regulation of one's own emotions) and has only recently started to investigate extrinsic ER (i.e., regulation of another person's emotions). This article selectively reviews current findings in order to answer the following questions: (a) What is extrinsic ER, and how can it be distinguished from related constructs such as emotion contagion, empathy, prosocial behavior, and social support? (b) How can we best model the processes through which extrinsic ER occurs as well as individual differences in extrinsic ER ability? The answers show that although extrinsic ER has much in common with intrinsic ER, the 2 cannot be equated. Research is therefore needed on the extrinsic side of ER. (PsycINFO Database Record (c) 2020 APA, all rights reserved).","[{'authorId': '6919876', 'name': 'Y. Nozaki'}, {'authorId': '5342207', 'name': 'Moïra Mikolajczak'}]",39.0,"{'bibtex': '@Article{Nozaki2019ExtrinsicER,\n author = {Y. Nozaki and Moïra Mikolajczak},\n journal = {Emotion},\n pages = {\n          10-15\n        },\n title = {Extrinsic emotion regulation.},\n volume = {20 1},\n year = {2019}\n}\n'}",,"{'volume': '20 1', 'pages': '\n          10-15\n        ', 'name': 'Emotion'}",38.0,Extrinsic emotion regulation.,2019.0
1278,68b9d8fe795e71905b2f646e8a5bbb52c145369c,"This paper describes the IVI Lab entry to the GENEA Challenge 2022. We formulate the gesture generation problem as a sequence-to-sequence conversion task with text, audio, and speaker identity as inputs and the body motion as the output. We use the Tacotron2 architecture as our backbone with the locality-constraint attention mechanism that guides the decoder to learn the dependencies from the neighboring latent features. The collective evaluation released by GENEA Challenge 2022 indicates that our two entries (FSH and USK) for the full body and upper body tracks statistically outperform the audio-driven and text-driven baselines on both two subjective metrics. Remarkably, our full-body entry receives the highest speech appropriateness (60.5% matched) among all submitted entries. We also conduct an objective evaluation to compare our motion acceleration and jerk with two autoregressive baselines. The result indicates that the motion distribution of our generated gestures is much closer to the distribution of natural gestures.","[{'authorId': '2152342254', 'name': 'Che-Jui Chang'}, {'authorId': '2175553614', 'name': 'Sen Zhang'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",17.0,"{'bibtex': '@Article{Chang2022TheIL,\n author = {Che-Jui Chang and Sen Zhang and Mubbasir Kapadia},\n journal = {Proceedings of the 2022 International Conference on Multimodal Interaction},\n title = {The IVI Lab entry to the GENEA Challenge 2022 – A Tacotron2 Based Method for Co-Speech Gesture Generation With Locality-Constraint Attention Mechanism},\n year = {2022}\n}\n'}",,{'name': 'Proceedings of the 2022 International Conference on Multimodal Interaction'},25.0,The IVI Lab entry to the GENEA Challenge 2022 – A Tacotron2 Based Method for Co-Speech Gesture Generation With Locality-Constraint Attention Mechanism,2022.0
1279,68cf46020b94c5e4cd155fed46a7348ed1b1b7f4,"How can you know when someone is bluffing? Paying attention? Genuinely interested? The answer, writes Sandy Pentland in Honest Signals, is that subtle patterns in how we interact with other people reveal our attitudes toward them. These unconscious social signals are not just a back channel or a complement to our conscious language; they form a separate communication network. Biologically based ""honest signaling,"" evolved from ancient primate signaling mechanisms, offers an unmatched window into our intentions, goals, and values. If we understand this ancient channel of communication, Pentland claims, we can accurately predict the outcomes of situations ranging from job interviews to first dates. Pentland, an MIT professor, has used a specially designed digital sensor worn like an ID badgea ""sociometer""to monitor and analyze the back-and-forth patterns of signaling among groups of people. He and his researchers found that this second channel of communication, revolving not around words but around social relations, profoundly influences major decisions in our liveseven though we are largely unaware of it. Pentland presents the scientific background necessary for understanding this form of communication, applies it to examples of group behavior in real organizations, and shows how by ""reading"" our social networks we can become more successful at pitching an idea, getting a job, or closing a deal. Using this ""network intelligence"" theory of social signaling, Pentland describes how we can harness the intelligence of our social network to become better managers, workers, and communicators.","[{'authorId': '1682773', 'name': 'A. Pentland'}, {'authorId': '3204796', 'name': 'T. Heibeck'}]",583.0,"{'bibtex': '@Inproceedings{Pentland2008HonestS,\n author = {A. Pentland and T. Heibeck},\n pages = {I-XVII, 1-184},\n title = {Honest Signals - How They Shape Our World},\n year = {2008}\n}\n'}",,"{'pages': 'I-XVII, 1-184'}",0.0,Honest Signals - How They Shape Our World,2008.0
1280,693a614718a96d61968ec573b2932a3301092c9a,"From the Publisher: 
This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.","[{'authorId': '1735300', 'name': 'S. Haykin'}]",21158.0,"{'bibtex': '@Inproceedings{Haykin1998NeuralNA,\n author = {S. Haykin},\n title = {Neural Networks: A Comprehensive Foundation},\n year = {1998}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Neural Networks: A Comprehensive Foundation,1998.0
1281,69519759a99d8bff7efc93b6503491deba492ad3,,"[{'authorId': '2133747', 'name': 'Hongchun Qu'}, {'authorId': '2115725295', 'name': 'Youlan Wang'}, {'authorId': '2524401', 'name': 'Linqin Cai'}, {'authorId': '2155391244', 'name': 'Ting Wang'}, {'authorId': '2110249032', 'name': 'Zhonghua Lu'}]",8.0,"{'bibtex': '@Article{Qu2012OrangeTS,\n author = {Hongchun Qu and Youlan Wang and Linqin Cai and Ting Wang and Zhonghua Lu},\n journal = {Simul. Model. Pract. Theory},\n pages = {19-35},\n title = {Orange tree simulation under heterogeneous environment using agent-based model ORASIM},\n volume = {23},\n year = {2012}\n}\n'}",,"{'volume': '23', 'pages': '19-35', 'name': 'Simul. Model. Pract. Theory'}",83.0,Orange tree simulation under heterogeneous environment using agent-based model ORASIM,2012.0
1283,69544fbb52663ad28b716773f0ebf06f1215c73c,"Decision making is usually based on the comparative evaluation of different alternatives by means of a decision criterion. The whole decision process is compacted into a criterion formula on the basis of which alternatives are compared. It is thus, impossible for an end user to understand why an alternative is good, or better than another. 
 
Recently, some decision criteria were articulated in terms of a two-steps argumentation process: i) an inference step in which arguments in favor/against each option are built and evaluated, and ii) a comparison step in which pairs of alternatives are compared on the basis of ""accepted"" arguments. Thus, not only the best alternative is provided to the user but also the reasons justifying this recommendation. However, a two steps approach is not in accordance with the principle of an argumentation system, whose accepted arguments are intended to support the ""good"" options. Moreover, with such an approach it is difficult to define proof procedures for testing directly whether a given option may be the best one without computing the whole ordering. Finally, it is difficult to analyze how an ordering is revised in light of a new argument. 
 
This paper proposes a novel approach for argumentation-based decision making. We propose a Dung style system that takes as input different arguments and a defeat relation among them, and returns as outputs a status for each option, and a total preordering on a set of options. The status is defined on the basis of different inference mechanisms. The total preordering privileges the option that is supported by the strongest argument, provided that this argument survives to the attacks. The properties of the system are investigated.","[{'authorId': '1726694', 'name': 'Leila Amgoud'}, {'authorId': '2460602', 'name': 'Yannis Dimopoulos'}, {'authorId': '1865895', 'name': 'Pavlos Moraitis'}]",52.0,"{'bibtex': '@Inproceedings{Amgoud2008MakingDT,\n author = {Leila Amgoud and Yannis Dimopoulos and Pavlos Moraitis},\n pages = {113-123},\n title = {Making Decisions through Preference-Based Argumentation},\n year = {2008}\n}\n'}",,{'pages': '113-123'},17.0,Making Decisions through Preference-Based Argumentation,2008.0
1284,6998ae456640b8eac72434958e67ee5654a2fd6d,,"[{'authorId': '5691792', 'name': 'I. Blanchette'}]",73.0,"{'bibtex': '@Article{Blanchette2006TheEO,\n author = {I. Blanchette},\n journal = {Memory & Cognition},\n pages = {1112-1125},\n title = {The effect of emotion on interpretation and logic in a conditional reasoning task},\n volume = {34},\n year = {2006}\n}\n'}",,"{'volume': '34', 'pages': '1112-1125', 'name': 'Memory & Cognition'}",56.0,The effect of emotion on interpretation and logic in a conditional reasoning task,2006.0
1285,69e65d9a3ca1ab6f90657463a8cf00b3b442dc4a,"In recent years, recurrent neural network language models (RNNLMs) have become increasingly popular for a range of applications including speech recognition. However, the training of RNNLMs is computationally expensive, which limits the quantity of data, and size of network, that can be used. In order to fully exploit the power of RNNLMs, efficient training implementations are required. This paper introduces an open-source toolkit, the CUED-RNNLM toolkit, which supports efficient GPU-based training of RNNLMs. RNNLM training with a large number of word level output targets is supported, in contrast to existing tools which used class-based output-targets. Support fotN-best and lattice-based rescoring of both HTK and Kaldi format lattices is included. An example of building and evaluating RNNLMs with this toolkit is presented for a Kaldi based speech recognition system using the AMI corpus. All necessary resources including the source code, documentation and recipe are available online1.","[{'authorId': '2109030402', 'name': 'Xie Chen'}, {'authorId': '150344273', 'name': 'Xunying Liu'}, {'authorId': '2480051', 'name': 'Y. Qian'}, {'authorId': '1740397', 'name': 'M. Gales'}, {'authorId': '1716393', 'name': 'P. Woodland'}]",82.0,"{'bibtex': '@Article{Chen2016CUEDRNNLMA,\n author = {Xie Chen and Xunying Liu and Y. Qian and M. Gales and P. Woodland},\n journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {6000-6004},\n title = {CUED-RNNLM — An open-source toolkit for efficient training and evaluation of recurrent neural network language models},\n year = {2016}\n}\n'}",,"{'pages': '6000-6004', 'name': '2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}",35.0,CUED-RNNLM — An open-source toolkit for efficient training and evaluation of recurrent neural network language models,2016.0
1286,69e76e16740ed69f4dc55361a3d319ac2f1293dd,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","[{'authorId': '3255983', 'name': 'Volodymyr Mnih'}, {'authorId': '36045539', 'name': 'Adrià Puigdomènech Badia'}, {'authorId': '153583218', 'name': 'Mehdi Mirza'}, {'authorId': '1753223', 'name': 'Alex Graves'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '3367786', 'name': 'Tim Harley'}, {'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}]",7225.0,"{'bibtex': '@Inproceedings{Mnih2016AsynchronousMF,\n author = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},\n pages = {1928-1937},\n title = {Asynchronous Methods for Deep Reinforcement Learning},\n year = {2016}\n}\n'}",,{'pages': '1928-1937'},43.0,Asynchronous Methods for Deep Reinforcement Learning,2016.0
1287,6a1c81d22308df709418311855aaba522e1f6d2e,"Emotion theorists assume certain facial displays to convey information about the expresser's emotional state. In contrast, behavioral ecologists assume them to indicate behavioral intentions or action requests. To test these contrasting positions, over 2,000 online participants were presented with facial expressions and asked what they revealed-feeling states, behavioral intentions, or action requests. The majority of the observers chose feeling states as the message of facial expressions of disgust, fear, sadness, happiness, and surprise, supporting the emotions view. Only the anger display tended to elicit more choices of behavioral intention or action request, partially supporting the behavioral ecology view. The results support the view that facial expressions communicate emotions, with emotions being multicomponential phenomena that comprise feelings, intentions, and wishes.","[{'authorId': '1935524', 'name': 'G. Horstmann'}]",243.0,"{'bibtex': '@Article{Horstmann2003WhatDF,\n author = {G. Horstmann},\n journal = {Emotion},\n pages = {\n          150-66\n        },\n title = {What do facial expressions convey: feeling states, behavioral intentions, or action requests?},\n volume = {3 2},\n year = {2003}\n}\n'}",,"{'volume': '3 2', 'pages': '\n          150-66\n        ', 'name': 'Emotion'}",44.0,"What do facial expressions convey: feeling states, behavioral intentions, or action requests?",2003.0
1289,6a2d3d7f5c3c2986d0f5e067f3fd1579337bc971,"This paper is a charter presenting a scientific posture shared by signatories in the use of simulation tools when dealing with complex systems. This posture is based on a cycling approach, in interaction with field processes, including discussion of assumptions and feedbacks on the field process. Confrontation between field and modelling processes has to be permanent because of openness and uncertainty features of these systems. This approach is used with two possible aims: learn on systems or support collective decision processes in these systems, which corresponds to an objective of increasing knowledge either for the scientist or the field actors, always through an interaction between them mediated by an evolutionary model. Both aims lead to different implementations of this companion modelling approach, but each one is side effect of the other one, and has to be taken in account as such. Scientists ready to work in that way and make this posture alive are kindly invited to join. (Resume d'auteur)","[{'authorId': '3280761', 'name': 'O. Barreteau'}]",314.0,"{'bibtex': '@Article{Barreteau2003OurCM,\n author = {O. Barreteau},\n journal = {J. Artif. Soc. Soc. Simul.},\n title = {Our Companion Modelling Approach},\n volume = {6},\n year = {2003}\n}\n'}",,"{'volume': '6', 'name': 'J. Artif. Soc. Soc. Simul.'}",0.0,Our Companion Modelling Approach,2003.0
1290,6a2e5bad38dad2f0277357c8607efc22f693ddc8,"Most of the researches so far have focused on emotional impulsive virtual characters, i.e virtual characters that express their own felt emotions without taking into account the socio-emotional context of the interaction. However, research in Human and Social Sciences has shown that during interpersonal interaction, people express very often emotions different from their felt emotions because they have to follow some sociocultural norms or they are pursuing specific goals. In this paper, we address the emotions that a virtual character should express to try to convince someone else during a negotiation. A model of an emotional persuasive virtual character and its implementation in the virtual world Second Life are presented. Such character is endowed with strategies of emotion expression that enable it to identify dynamically the emotion that it should express, depending on its interlocutor’s emotional reaction, to try to influence his opinion during a negotiation. The emotional persuasive virtual character has been evaluated in indirect interaction (i.e. when the user watches a conversation between two virtual characters). The results show than the proposed model of strategic expressions of emotion enables one to significantly improve the virtual character’s persuasiveness.","[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '2356111', 'name': 'H. Prendinger'}]",5.0,"{'bibtex': ""@Inproceedings{Ochs2010AVC,\n author = {M. Ochs and H. Prendinger},\n title = {A VIRTUAL CHARACTER'S EMOTIONAL PERSUASIVENESS},\n year = {2010}\n}\n""}",,"{'volume': '', 'name': ''}",21.0,A VIRTUAL CHARACTER'S EMOTIONAL PERSUASIVENESS,2010.0
1291,6a543508d46893d6fb8f86c86ad60902615c7140,"The impact of social relationships in human behavior is a complex topic but it is an important aspect to consider when creating believable agents. To achieve more believable social relations, we focus on a phenomenon of human interaction that is tightly coupled to social relationships but has been overlooked in most agent models of emotions: the “social sharing of emotions”. It corresponds to a common daily event where a person, the sharer, tells another, the listener, about an emotional episode he experienced in the past. This phenomenon happens on a daily basis and it is important for social relationships as it signals some level of intimacy. Therefore, we propose an agent model that endows agents with the capability to speak about their past emotional experiences, and provide a supportive response in case they are the listeners. For evaluation purposes, we developed a case study that consists in a simulation of a distressful situation in a 3D environment involving three autonomous characters. Two variations of this situation were designed: one in which our model causes a character to share the negative emotion it experiences and another where the character does not share any emotion. An user study was then conducted to assess the impact of the emotional sharing concerning the characters’ believability and perceived relationship with each other.","[{'authorId': '2969405', 'name': 'Nuno Salvador'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",1.0,"{'bibtex': '@Article{Salvador2016ConveyingSR,\n author = {Nuno Salvador and João Dias and S. Mascarenhas and Ana Paiva},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1415-1416},\n title = {Conveying Social Relations in Virtual Agents Through an Emotion Sharing and Response Model: (Extended Abstract)},\n year = {2016}\n}\n'}","[{'paperId': 'df7c6ac457b9f88562bb0f2774fa164b51cfe3e0', 'title': 'EEG Model: Emotional Episode Generation for Social Sharing of Emotions'}]",{'pages': '1415-1416'},6.0,Conveying Social Relations in Virtual Agents Through an Emotion Sharing and Response Model: (Extended Abstract),2016.0
1292,6a69fe470b4ab68123af3a8a1f9c5d23f16e0e87,"We have developed a systematic methodology for designing emotional facial expressions for humanoid robots, especially those with limited degrees of freedom. The methodology is firmly grounded in the psychological literature on human static and dynamic emotional facial expressions. We demonstrate the methodology by applying it to a recent humanoid robot and evaluate the results, confirming that the observed confusion matrix agrees qualitatively with the predictions of the methodology. We also investigate how robot facial emotion recognition compares for dynamic versus static expressions.","[{'authorId': '3269838', 'name': 'Mohammad Shayganfar'}, {'authorId': '144898921', 'name': 'C. Rich'}, {'authorId': '2668280', 'name': 'C. Sidner'}]",28.0,"{'bibtex': '@Article{Shayganfar2012ADM,\n author = {Mohammad Shayganfar and C. Rich and C. Sidner},\n journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n pages = {4577-4583},\n title = {A design methodology for expressing emotion on robot faces},\n year = {2012}\n}\n'}",,"{'pages': '4577-4583', 'name': '2012 IEEE/RSJ International Conference on Intelligent Robots and Systems'}",25.0,A design methodology for expressing emotion on robot faces,2012.0
1293,6a6f1b8ef3eefaf8027a76134859b44886a5f795,"Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutick’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.","[{'authorId': '1388437494', 'name': 'Muhammad Abdul-Mageed'}, {'authorId': '1412391493', 'name': 'L. Ungar'}]",271.0,"{'bibtex': '@Inproceedings{Abdul-Mageed2017EmoNetFE,\n author = {Muhammad Abdul-Mageed and L. Ungar},\n pages = {718-728},\n title = {EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks},\n year = {2017}\n}\n'}",,{'pages': '718-728'},63.0,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,2017.0
1294,6a78a121d918cd2a1f0dc337bbc5c0dae7f9c134,"Copyright It is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s) and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an open content licence (like Creative Commons). UvA-DARE is a service provided by the library of the University of Amsterdam (http://dare.uva.nl) We receive emotional signals from different sources, including the face, the whole body, and the natural scene. Previous research has shown the importance of context provided by the whole body and the scene on the recognition of facial expressions. This study measured physiological responses to face-body-scene combinations. Participants freely viewed emotionally congruent and incongruent face-body and body-scene pairs whilst eye fixations, pupil-size, and electromyography (EMG) responses were recorded. Participants attended more to angry and fearful vs. happy or neutral cues, independent of the source and relatively independent from whether the face body and body scene combinations were emotionally congruent or not. Moreover, angry faces combined with angry bodies and angry bodies viewed in aggressive social scenes elicited greatest pupil dilation. Participants' face expressions matched the valence of the stimuli but when face-body compounds were shown, the observed facial expression influenced EMG responses more than the posture. Together, our results show that the perception of emotional signals from faces, bodies and scenes depends on the natural context, but when threatening cues are presented, these threats attract attention, induce arousal, and evoke congruent facial reactions.","[{'authorId': '27448594', 'name': 'M. Kret'}, {'authorId': '71890283', 'name': 'Koen W.H. Roelofs'}, {'authorId': '32734830', 'name': 'J. Stekelenburg'}, {'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '2512383', 'name': 'M. Kret'}, {'authorId': '2229103738', 'name': 'Karin Roelofs'}, {'authorId': '32734830', 'name': 'J. Stekelenburg'}, {'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '3229047', 'name': 'John J. Foxe'}, {'authorId': '2074222905', 'name': 'Albert'}, {'authorId': '2440064', 'name': 'C. H. Attar'}]",137.0,"{'bibtex': ""@Misc{None,\n author = {M. Kret and Koen W.H. Roelofs and J. Stekelenburg and B. de Gelder and M. Kret and Karin Roelofs and J. Stekelenburg and B. de Gelder and John J. Foxe and Albert and C. H. Attar},\n title = {Source (or Part of the following Source): Type Article Title Emotional Signals from Faces, Bodies and Scenes Influence Observers' Face Expressions, Fixations and Pupil-size Author(s) Emotional Signals from Faces, Bodies and Scenes Influence Observers' Face Expressions, Fixations and Pupil-size}\n}\n""}",,,39.0,"Source (or Part of the following Source): Type Article Title Emotional Signals from Faces, Bodies and Scenes Influence Observers' Face Expressions, Fixations and Pupil-size Author(s) Emotional Signals from Faces, Bodies and Scenes Influence Observers' Face Expressions, Fixations and Pupil-size",
1295,6a884f9d3e898c771bdd2c093f1d1df93c39a7d9,"Research into the methods and techniques used in simulating crowds has developed extensively within the last few years, particularly in the areas of video games and film. Despite recent impressive results when simulating and rendering thousands of individuals, many challenges still exist in this area. The comparison of simulation with reality, the realistic appearance of virtual humans and their behavior, group structure and their motion, and collision avoidance are just some examples of these challenges. For most of the applications of crowds, it is now a requirement to have real-time simulations which is an additional challenge, particularly when crowds are very large. Crowd Simulation analyses these challenges in depth and suggests many possible solutions. Daniel Thalmann and Soraia Musse share their experiences and expertise in the application of: Population modeling Virtual human animation Behavioral models for crowds The connection between virtual and real crowds Path planning and navigation Visual attention models Geometric and populated semantic environments Crowd rendering The second edition presents techniques and methods developed since the authors first covered the simulation of crowds in 2007. Crowd Simulation includes in-depth discussions on the techniques of path planning, including a new hybrid approach between navigation graphs and potential-based methods. The importance of gaze attention individuals appearing conscious of their environment and of others is introduced, and a free-of-collision method for crowds is also discussed.","[{'authorId': '2253503834', 'name': 'Daniel Thalmann'}]",190.0,"{'bibtex': '@Inproceedings{Thalmann2007CrowdS,\n author = {Daniel Thalmann},\n pages = {I-XII, 1-242},\n title = {Crowd Simulation},\n year = {2007}\n}\n'}",,"{'pages': 'I-XII, 1-242'}",55.0,Crowd Simulation,2007.0
1296,6a96bac30730217d7a25d723466038352925954f,"In this paper we highlight the different challenges in modeling communicative gestures for Embodied Conversational Agents (ECAs). We describe models whose aim is to capture and understand the specific characteristics of communicative gestures in order to envision how an automatic communicative gesture production mechanism could be built. The work is inspired by research on how human gesture characteristics (e.g., shape of the hand, movement, orientation and timing with respect to the speech) convey meaning. We present approaches to computing where to place a gesture, which shape the gesture takes and how gesture shapes evolve through time. We focus on a particular model based on theoretical frameworks on metaphors and embodied cognition that argue that people can represent, reason about and convey abstract concepts using physical representations and processes, which can be conveyed through physical gestures.","[{'authorId': '1682486', 'name': 'Brian Ravenet'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '2049106', 'name': 'C. Clavel'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",29.0,"{'bibtex': '@Article{Ravenet2018AutomatingTP,\n author = {Brian Ravenet and C. Pelachaud and C. Clavel and S. Marsella},\n journal = {Frontiers in Psychology},\n title = {Automating the Production of Communicative Gestures in Embodied Characters},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'name': 'Frontiers in Psychology'}",74.0,Automating the Production of Communicative Gestures in Embodied Characters,2018.0
1297,6a97ba1d89ac978a134f5149f073312710959e75,,"[{'authorId': '33578779', 'name': 'I. Albrecht'}, {'authorId': '144951065', 'name': 'M. Schröder'}, {'authorId': '144213705', 'name': 'Jörg Haber'}, {'authorId': '145156858', 'name': 'H. Seidel'}]",111.0,"{'bibtex': '@Article{Albrecht2005MixedFE,\n author = {I. Albrecht and M. Schröder and Jörg Haber and H. Seidel},\n journal = {Virtual Reality},\n pages = {201-212},\n title = {Mixed feelings: expression of non-basic emotions in a muscle-based talking head},\n volume = {8},\n year = {2005}\n}\n'}",,"{'volume': '8', 'pages': '201-212', 'name': 'Virtual Reality'}",50.0,Mixed feelings: expression of non-basic emotions in a muscle-based talking head,2005.0
1298,6ab8b2554c43366c390ec4a0080b33417beb9f7d,"This paper highlights some of our recent research efforts in designing and evaluating life-like characters that are capable of entertaining affective and social communication with human users. The key novelty of our approach is the use of human physiological information: first, as a method to evaluate the effect of life-like character behavior on a moment-to-moment basis, and second, as an input modality for a new generation of interface agents that we call 'physiologically perceptive' life-like characters. By exploiting the stream of primarily involuntary human responses, such as autonomic nervous system activity or eye movements, those characters are expected to respond to users' affective and social needs in a truly sensitive, and hence effective, friendly, and beneficial way.","[{'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}]",18.0,"{'bibtex': '@Article{Prendinger2005HumanPA,\n author = {H. Prendinger and M. Ishizuka},\n journal = {IEICE Trans. Inf. Syst.},\n pages = {2453-2460},\n title = {Human Physiology as a Basis for Designing and Evaluating Affective Communication with Life-Like Characters},\n volume = {88-D},\n year = {2005}\n}\n'}",,"{'volume': '88-D', 'pages': '2453-2460', 'name': 'IEICE Trans. Inf. Syst.'}",42.0,Human Physiology as a Basis for Designing and Evaluating Affective Communication with Life-Like Characters,2005.0
1299,6ae7e6a11515cfc8d3fe6e5b53c951f4de31e0b7,"A problem which bedevils the study of emotions, and the study of consciousness, is that we assume a shared understanding of many everyday concepts, such as ‘emotion’, ‘feeling’, ‘pleasure’, ‘pain’, ‘desire’, ‘awareness’, etc. Unfortunately, these concepts are inherently very complex, ill-defined, and used with different meanings by different people. Moreover this goes unnoticed, so that people think they understand what they are referring to even when their understanding is very unclear. Consequently there is much discussion that is inherently vague, often at cross-purposes, and with apparent disagreements that arise out of people unwittingly talking about different things. We need a framework which explains how there can be all the diverse phenomena that different people refer to when they talk about emotions and other affective states and processes. The conjecture on which this paper is based is that adult humans have a type of information-processing architecture, with components which evolved at different times, including a rich and varied collection of components whose interactions can generate all the sorts of phenomena that different researchers have labelled “emotions”. Within this framework we can provide rational reconstructions of many everyday concepts of mind. We can also allow a variety of different architectures, found in children, brain damaged adults, other animals, robots, software agents, etc., where different architectures support different classes of states and processes, and therefore different mental ontologies. Thus concepts like ‘emotion’, ‘awareness’, etc. will need to be interpreted differently when referring to different architectures. We need to limit the class of architectures under consideration, since for any class of behaviours there are indefinitely many architectures which can produce those behaviours. One important constraint is to consider architectures which might have been produced by biological evolution. This leads to the notion of a human architecture composed of many components which evolved under the influence of the other components as well as environmental needs and pressures. From this viewpoint, a mind is a kind of ecosystem1 of co-evolved sub-organisms acquiring and using different kinds of information and processing it in different ways, sometimes cooperating with one another and sometimes competing. Within this framework we can hope to study not only mechanisms underlying affective states and processes, but also other mechanisms which are often studied in isolation, e.g. vision, action mechanisms, learning mechanisms, ‘alarm’ mechanisms, etc. We can also explain why some models, and corresponding conceptions of emotion, are shallow whereas others are deeper. Shallow models may be of practical use, e.g. in entertainment and interface design. Deeper models are required if we are to understand what we are, how we can go wrong, etc. This paper is a snapshot of a long term project addressing all these issues. 1 What kinds of emotions? The study of emotions has recently become fashionable within AI and Cognitive Science. Unfortunately all sorts of different things are labelled as ‘emotions’. This is perhaps understandable among young engineers who have not been trained in philosophy or psychology. However even among specialists there many different definitions of ‘emotion’ and related concepts, such as ‘feeling’, ‘affect’, ‘motivation’, ‘mood’, etc. For instance some define emotions in terms of observable physical behaviours (such as weeping, grimacing, smiling, jumping for joy, etc.). Some define them in terms of measurable physiological changes which need 1The published version of this paper used the word ‘ecology’ here. not be easily discernible externally, though they may be sensed internally (referred to by Picard as ‘sentic modulation’). Some define them in terms of the kinds of conscious experiences involved in having them – their phenomenology. Some define them in terms of the brain mechanisms which may be activated. Even when behavioural manifestations do occur they may be to some extent culturally determined, casting doubt on behavioural criteria for emotions. For instance the sounds people make when exhibiting pain can vary according to culture: ‘ouch’ in English is replaced by ‘eina’ in Afrikaans! Some researchers regard emotions as inherently social or cultural in nature, though this may be more true of having a guilty conscience than being terrified during an earthquake. There is also disagreement over what sorts of evidence can be taken as relevant to the study of emotions. For instance, some will regard the behaviour of skilled actors when asked to show certain emotions as demonstrating connections between emotions and externally observable behaviour. Others will object that that merely reveals what happens when people are asked to act as if they had certain emotions, whereas naturally occurring emotions may be quite different. In some cases they may have no external manifestations, since people can often conceal their emotions. For some researchers, emotions, by definition, are linked to and differentiable in observable behaviour, like weeping, grimacing, jumping for joy, growing tense, etc., whereas others are more interested in semantically rich emotions for which there are no characteristic, non-verbal, behavioural expressions, e.g. ‘Being worried that your work is not appreciated by your colleagues’ vs. ‘Being worried that your political party is going to lose the next election’, or ‘Being delighted that the there is a sunny weather forecast for the day you have planned a picnic’ vs. ‘Being delighted that someone you admire very much is impressed by your research’, etc. Most of the empirical, laboratory, research on emotions has studied only simple, shallow emotions, largely ignoring semantic content, whereas most of the important human emotions (the ones that are important in our social lives, and which are the subject matter of gossip, poems, stories, plays, etc.) are deep and semantically rich. Another common difficulty is that some people use the word ‘emotion’ so loosely that it covers almost any affective state, including having a desire or motive, whereas in ordinary parlance we do not normally describe someone as being emotional just because they have goals, purposes, or preferences, or because they are enjoying a meal or finding their chair uncomfortable to sit in. If all such affective states were included as emotions, it would follow that people constantly have a large number of different emotions, since we all have multiple enduring goals, ambitions, tastes, preferences, ideals, etc. Another source of confusion concerns whether having an emotion necessarily involves being conscious of the emotion. According to some this is a defining criterion, yet that does not square with the common observation that people can sometimes be angry, jealous, infatuated, or pleased at being flattered, etc. without being aware of being so, even though it may be obvious to others. Another problem with the criterion is that it may rule out certain animals having emotions if they lack the ability to monitor and characterise their own states or lack the conceptual framework required to classify some states as emotions. Presumably a newborn infant cannot classify its own mental states using our adult categories. Does that mean that it has no emotions? Perhaps it has them but does not feel them? Perhaps an infant’s behavioural manifestations of pain, distress, discomfort, pleasure, etc. are simply part of the biologically important process of generating appropriate nurturing behaviour in parents rather than being expressions of what the infant is aware of? There is no obvious way of resolving disagreements on these issues because of the ambiguities and confusion in the key concepts used. Yet another confusion concerns whether, in order to have emotions, an organism or machine must contain an emotion-producing module of some kind, or whether some or all emotions are simply states involving interactions between a host of processes which are not intrinsically emotional, as was argued in (Wright et al., 1996). On the first view it makes sense to ask how the emotion mechanism evolved, and what biological function it has, whereas on the second view such questions make no sense. Another possibility is that the ambiguous word ‘emotion’ sometimes refers to states and processes conforming to the first view, and sometimes to the second, because our usage is inconsistent. Because of this conceptual mess, anyone can produce a program, label some component of it the ‘emotion module’ and proudly announce that they have developed a robot or software agent which has emotions. It will be hard to argue against such claims when there is no agreement on what emotions are. This is an extreme form of the phenomenon in AI of attributing mental states and human capabilities to programs on the basis of very shallow analogies, for which McDermott chided the AI community in (McDermott, 1981) many years ago, though he was concerned with the undisciplined use of labels such as ‘plan’, ‘goal’, ‘infer’.","[{'authorId': '145788442', 'name': 'A. Sloman'}]",97.0,"{'bibtex': '@Inproceedings{Sloman2002HOWMS,\n author = {A. Sloman},\n title = {HOW MANY SEPARATELY EVOLVED EMOTIONAL BEASTIES LIVE WITHIN US},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",59.0,HOW MANY SEPARATELY EVOLVED EMOTIONAL BEASTIES LIVE WITHIN US,2002.0
1300,6b160dfde88dd8364136aab4aea9464fa6da2ca9,"Episodic memory is a neurocognitive (brain/mind) system, uniquely different from other memory systems, that enables human beings to remember past experiences. The notion of episodic memory was first proposed some 30 Years ago. At that time it was defined in terms of materials and tasks. It was subsequently refined and elaborated in terms of ideas such as self, subjective time, and autonoetic consciousness. This chapter provides a brief history of the concept of episodic memory, describes how it has changed (indeed greatly changed) since its inception, considers criticisms of it, and then discusses supporting evidence provided by (a) neuropsychological studies of patterns of memory impairment caused by brain damage, and (b) functional neuroimaging studies of patterns of brain activity of normal subjects engaged in various memory tasks. I also suggest that episodic memory is a true, even if as yet generally unappreciated, marvel of nature.","[{'authorId': '2249734750', 'name': 'Endel Tulving'}]",371.0,"{'bibtex': '@Article{Tulving2004EpisodicMF,\n author = {Endel Tulving},\n journal = {Revue neurologique},\n pages = {\n          S9-23\n        },\n title = {[Episodic memory: from mind to brain].},\n volume = {160 4 Pt 2},\n year = {2004}\n}\n'}",,"{'volume': '160 4 Pt 2', 'pages': '\n          S9-23\n        ', 'name': 'Revue neurologique'}",0.0,[Episodic memory: from mind to brain].,2004.0
1301,6b29199b5f7e796f7f266cd6e957fadb4494e0e3,"Socially intelligent robotics is the pursuit of creating robots capable of exhibiting natural-appearing social qualities. Beyond the basic capabilities of moving and acting autonomously, the field has focused on the use of the robot's physical embodiment to communicate and interact with users in a social and engaging manner. One of its components, socially assistive robotics, focuses on helping human users through social rather than physical interaction. Early results already demonstrate the promises of socially assistive robotics, a new interdisciplinary research area with large horizons of fascinating and much needed research. Even as socially assistive robotic technology is still in its early stages of development, the next decade promises systems that will be used in hospitals, schools, and homes in therapeutic programs that monitor, encourage, and assist their users. This is an important time in the development of the field, when the board technical community and the beneficiary populations must work together to shape the field toward its intended impact on improved human quality of life","[{'authorId': '1738469', 'name': 'A. Tapus'}, {'authorId': '1742183', 'name': 'M. Matarić'}, {'authorId': '1792053', 'name': 'B. Scassellati'}]",405.0,"{'bibtex': '@Article{Tapus2007SociallyAR,\n author = {A. Tapus and M. Matarić and B. Scassellati},\n journal = {IEEE Robotics & Automation Magazine},\n pages = {35-42},\n title = {Socially assistive robotics [Grand Challenges of Robotics]},\n volume = {14},\n year = {2007}\n}\n'}",,"{'volume': '14', 'pages': '35-42', 'name': 'IEEE Robotics & Automation Magazine'}",26.0,Socially assistive robotics [Grand Challenges of Robotics],2007.0
1302,6b31942478e7aca6f1998b8d1506167fd8a0bf86,,"[{'authorId': '1706062', 'name': 'R. Arkin'}, {'authorId': '69954988', 'name': 'M. Fujita'}, {'authorId': '2052774919', 'name': 'Tsuyoshi Takagi'}, {'authorId': '2062296789', 'name': 'R. Hasegawa'}]",309.0,"{'bibtex': '@Article{Arkin2003AnEA,\n author = {R. Arkin and M. Fujita and Tsuyoshi Takagi and R. Hasegawa},\n journal = {Robotics Auton. Syst.},\n pages = {191-201},\n title = {An ethological and emotional basis for human-robot interaction},\n volume = {42},\n year = {2003}\n}\n'}",,"{'volume': '42', 'pages': '191-201', 'name': 'Robotics Auton. Syst.'}",18.0,An ethological and emotional basis for human-robot interaction,2003.0
1303,6b3f8bab8b358cba5f81d7ff3ddf02dacdbdeb32,,"[{'authorId': '21052747', 'name': 'Anabela Marto'}, {'authorId': '8029017', 'name': 'H. Almeida'}, {'authorId': '3068581', 'name': 'Alexandrino Gonçalves'}]",30.0,"{'bibtex': '@Article{Marto2019UsingAR,\n author = {Anabela Marto and H. Almeida and Alexandrino Gonçalves},\n journal = {VipIMAGE 2019},\n title = {Using Augmented Reality in Patients with Autism: A Systematic Review},\n year = {2019}\n}\n'}",,{'name': 'VipIMAGE 2019'},45.0,Using Augmented Reality in Patients with Autism: A Systematic Review,2019.0
1304,6b6d357fb4ef19f2330596183ce00d2f3797740d,"We start by def ining conven t ions and t e rmino logy that will be used th roughou t this paper . String C = clc~ ... cp is a subsequence of string A = aja2 ""'"" am if there is a mapp ing F : {1, 2 . . . . , p} ~ {1, 2, ... , m} such that F(i) = k only if c~ = ak and F is a m o n o t o n e strictly increasing funct ion (i .e. F(i) = u, F(]) = v, and i < j imply that u < v). C can be fo rmed by delet ing m p (not necessari ly ad jacen t ) symbols f rom A . F o r example , "" c o u r s e "" is a subsequence of "" c o m p u t e r sc ience . "" Str ing C is a c o m m o n s ubs equence of strings A and B if C is a s u b s e q u e n c e of A and also a subsequence of B. String C is a longest c o m m o n subsequence (abbrev ia ted LCS) of string A and B if C is a c o m m o n subsequence of A and B of maximal length , i .e. there is no c o m m o n subsequence of A and B that has grea te r length. Th roughou t this paper , we assume that A and B are strings of lengths m and n , m _< n , that have an LCS C of (unknown) length p . We assume that the symbols that may appea r in these strings c o m e f rom some a lphabet of size t . A symbol can be s tored in m e m o r y by using log t bits, which we assume will fit in one word of memory . Symbols can be c o m p a r e d (a -< b?) in one t ime unit . The n u m b e r of di f ferent symbols that actual ly appear in string B is def ined to be s (which must be less than n and t). The longest c o m m o n s u b s e q u e n c e prob lem has been solved by using a recurs ion re la t ionship on the length of the solut ion [7, 12, 16, 21]. These are general ly appl icable a lgor i thms that take O ( m n ) t ime for any input strings o f lengths m and n even though the lower bound on t ime of O ( m n ) need not apply to all inputs [2]. We present a lgor i thms that , depend ing on the na ture of the Input, may not requ i re quadra t ic t ime to r ecove r an LCS. The first a lgor i thm is appl icable in the genera l case and requi res O ( p n + n log n) t ime. T h e second a lgor i thm requi res t ime b o u n d e d by O((m + 1 p )p log n). In the c o m m o n special case where p is close to m , this a lgor i thm takes t ime","[{'authorId': '2561382', 'name': 'D. Hirschberg'}]",817.0,"{'bibtex': '@Article{Hirschberg1977AlgorithmsFT,\n author = {D. Hirschberg},\n journal = {J. ACM},\n pages = {664-675},\n title = {Algorithms for the Longest Common Subsequence Problem},\n volume = {24},\n year = {1977}\n}\n'}",,"{'volume': '24', 'pages': '664-675', 'name': 'J. ACM'}",23.0,Algorithms for the Longest Common Subsequence Problem,1977.0
1305,6b84e730bfc174f7a02b15c0c16f2a75f609a1bb,"Recent years have seen a significant expansion in research on computational models of human emotional processes, driven both by their potential for basic research on emotion and cognition as well as their promise for an ever-increasing range of applications. This has led to a truly interdisciplinary, mutually beneficial partnership between emotion research in psychology and in computational science, of which this volume is an exemplar. To understand this partnership and its potential for transforming existing practices in emotion research across disciplines and for disclosing important novel areas of research, we explore in this chapter the history of work in computational models of emotion including the various uses to which they have been put, the theoretical traditions that have shaped their development, and how these uses and traditions are reflected in their underlying architectures. For an outsider to the field, the last 15 years have seen the development of a seemingly bewildering array of competing and complementary computational models. Figure 1.2.1 lists a ‘family tree’ of a few of the significant models and the theoretical traditions from which they stem. Although there has been a proliferation of work, the field is far from mature: the goals that a model is designed to achieve are not always clearly articulated; research is rarely incremental, more often returning to motivating theories rather than extending prior computational approaches; and rarely are models contrasted with each other in terms of their ability to achieve their set goals. Contributing to potential confusion is the reality that computational models are complex systems embodying a number of, sometimes unarticulated, design decisions and assumptions inherited from the psychological and computational traditions from which they emerged, a circumstance made worse by the lack of a commonly accepted lexicon even for designating these distinctions. In this chapter, we lay out the work on computational models of emotion in an attempt to reveal the common uses to which they may be put and the underlying techniques and assumptions from which the models are built. Our aim is to present conceptual distinctions and common terminology that can aid in discussion and comparison of competing models. Our hope is that this will not only facilitate an understanding of the field for outside researchers but work towards a lexicon that can help foster the maturity of the field towards more incremental research. In characterizing different computational models of emotion, we begin by describing interdisciplinary uses to which computational models may be put, including their uses in improving human–computer interaction, in enhancing general models of intelligence, and as methodological tools for furthering our understanding of human behaviour. We next discuss how models have been built, including the underlying theoretical traditions that have shaped their development. These differing theoretical perspectives often conceptualize emotion in quite different ways, emphasizing different scenarios and proposed functions, different component processes, and different linkages between these components. It should then come as no surprise that such","[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1764052', 'name': 'P. Petta'}]",392.0,"{'bibtex': '@Inproceedings{Marsella2010ComputationalMO,\n author = {S. Marsella and J. Gratch and P. Petta},\n title = {Computational models of emotion},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",113.0,Computational models of emotion,2010.0
1307,6b87cf6aaa8cc69123bc82698be6247f246e5326,,"[{'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '3347943', 'name': 'G. F. Siddiqui'}]",3.0,"{'bibtex': '@Inproceedings{Pontier2009AnAA,\n author = {M. Pontier and G. F. Siddiqui},\n pages = {33-47},\n title = {An Affective Agent Playing Tic-Tac-Toe as Part of a Healing Environment},\n year = {2009}\n}\n'}",,{'pages': '33-47'},28.0,An Affective Agent Playing Tic-Tac-Toe as Part of a Healing Environment,2009.0
1308,6b8df26dc31cbb7d64a31083b8b307ab2e0a6207,"Human interlocutors automatically adapt verbal and non-verbal signals so that different behaviors become synchronized over time. Multimodal communication comes naturally to humans, while this is not the case for Embodied Conversational Agents (ECAs). Knowing which behavioral channels synchronize within and across speakers and how they align seems critical in the development of ECAs. Yet, there exists little data-driven research that provides guidelines for the synchronization of different channels within an interlocutor. This study focuses on intrapersonal dependencies of multimodal behavior by using cross-recurrence analysis on a multimodal communication dataset to better understand the temporal relationships between language and gestural behavior channels. By shedding light on the intrapersonal synchronization of communicative channels in humans, we provide an initial manual for modality synchronisation in ECAs.","[{'authorId': '1394735166', 'name': 'P. A. Blomsma'}, {'authorId': '50816019', 'name': 'Guido M. Linders'}, {'authorId': '51211594', 'name': 'Julija Vaitonyte'}, {'authorId': '2073332', 'name': 'M. Louwerse'}]",3.0,"{'bibtex': '@Article{Blomsma2020IntrapersonalDI,\n author = {P. A. Blomsma and Guido M. Linders and Julija Vaitonyte and M. Louwerse},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Intrapersonal dependencies in multimodal behavior},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},50.0,Intrapersonal dependencies in multimodal behavior,2020.0
1309,6bb1c473507ac38923b42b2c7d4ba765327ee88a,,"[{'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '50716507', 'name': 'S. Blairy'}]",571.0,"{'bibtex': '@Article{Hess2001FacialMA,\n author = {U. Hess and S. Blairy},\n journal = {International journal of psychophysiology : official journal of the International Organization of Psychophysiology},\n pages = {\n          129-41\n        },\n title = {Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy.},\n volume = {40 2},\n year = {2001}\n}\n'}",,"{'volume': '40 2', 'pages': '\n          129-41\n        ', 'name': 'International journal of psychophysiology : official journal of the International Organization of Psychophysiology'}",44.0,Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy.,2001.0
1311,6bdacaf992b0394cc73ff94fcbf6b31483406286,"We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.","[{'authorId': '51064498', 'name': 'Zhengyou Zhang'}]",13100.0,"{'bibtex': '@Article{Zhang2000AFN,\n author = {Zhengyou Zhang},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {1330-1334},\n title = {A Flexible New Technique for Camera Calibration},\n volume = {22},\n year = {2000}\n}\n'}",,"{'volume': '22', 'pages': '1330-1334', 'name': 'IEEE Trans. Pattern Anal. Mach. Intell.'}",28.0,A Flexible New Technique for Camera Calibration,2000.0
1312,6bde69925620980e9daa734e3491aa199be6b75f,,"[{'authorId': '2497939', 'name': 'Séverin Lemaignan'}, {'authorId': '39150210', 'name': 'Julia Fink'}, {'authorId': '1727799', 'name': 'F. Mondada'}, {'authorId': '1799133', 'name': 'P. Dillenbourg'}]",29.0,"{'bibtex': ""@Inproceedings{Lemaignan2015YoureDI,\n author = {Séverin Lemaignan and Julia Fink and F. Mondada and P. Dillenbourg},\n pages = {390-400},\n title = {You're Doing It Wrong! Studying Unexpected Behaviors in Child-Robot Interaction},\n year = {2015}\n}\n""}",,{'pages': '390-400'},12.0,You're Doing It Wrong! Studying Unexpected Behaviors in Child-Robot Interaction,2015.0
1313,6bfb5f4915932b1da1d95cbe4feb20e4f58d2b6f,"The human face is capable of producing an astonishing variety of expressions—expressions for which sometimes the smallest difference changes the perceived meaning considerably. Producing realistic-looking facial animations that are able to transmit this degree of complexity continues to be a challenging research topic in computer graphics. One important question that remains to be answered is: When are facial animations good enough? Here we present an integrated framework in which psychophysical experiments are used in a first step to systematically evaluate the perceptual quality of several different computer-generated animations with respect to real-world video sequences. The first experiment provides an evaluation of several animation techniques, exposing specific animation parameters that are important to achieve perceptual fidelity. In a second experiment, we then use these benchmarked animation techniques in the context of perceptual research in order to systematically investigate the spatiotemporal characteristics of expressions. A third and final experiment uses the quality measures that were developed in the first two experiments to examine the perceptual impact of changing facial features to improve the animation techniques. Using such an integrated approach, we are able to provide important insights into facial expressions for both the perceptual and computer graphics community.","[{'authorId': '1793750', 'name': 'C. Wallraven'}, {'authorId': '2016068', 'name': 'M. Breidt'}, {'authorId': '1790148', 'name': 'D. Cunningham'}, {'authorId': '1747836', 'name': 'H. Bülthoff'}]",82.0,"{'bibtex': '@Article{Wallraven2008EvaluatingTP,\n author = {C. Wallraven and M. Breidt and D. Cunningham and H. Bülthoff},\n journal = {ACM Trans. Appl. Percept.},\n pages = {4:1-4:20},\n title = {Evaluating the perceptual realism of animated facial expressions},\n volume = {4},\n year = {2008}\n}\n'}",,"{'volume': '4', 'pages': '4:1-4:20', 'name': 'ACM Trans. Appl. Percept.'}",39.0,Evaluating the perceptual realism of animated facial expressions,2008.0
1314,6c134a4a6b1aae03decf7f38a23d762d6baed8ad,,"[{'authorId': '1678537', 'name': 'D. Heylen'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}]",82.0,"{'bibtex': '@Inproceedings{Heylen2008TheNS,\n author = {D. Heylen and S. Kopp and S. Marsella and C. Pelachaud and H. Vilhjálmsson},\n pages = {270-280},\n title = {The Next Step towards a Function Markup Language},\n year = {2008}\n}\n'}",,{'pages': '270-280'},31.0,The Next Step towards a Function Markup Language,2008.0
1315,6c2b28f9354f667cd5bd07afc0471d8334430da7,"A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.","[{'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '36037226', 'name': 'Réjean Ducharme'}, {'authorId': '120247189', 'name': 'Pascal Vincent'}, {'authorId': '1909943744', 'name': 'Christian Janvin'}]",6981.0,"{'bibtex': '@Article{Bengio2003ANP,\n author = {Yoshua Bengio and Réjean Ducharme and Pascal Vincent and Christian Janvin},\n journal = {J. Mach. Learn. Res.},\n pages = {1137-1155},\n title = {A Neural Probabilistic Language Model},\n volume = {3},\n year = {2003}\n}\n'}",,"{'volume': '3', 'pages': '1137-1155', 'name': 'J. Mach. Learn. Res.'}",39.0,A Neural Probabilistic Language Model,2003.0
1316,6caf0c6ef11b641c283e15652c1d05acbad8d6e4,"Quienes somos y como hemos llegado a ser quienes somos son los eternos enigmas a cuya resolucion se consagran, directa o indirectamente, multiples disciplinas en un intento por resolver una parte del puzzle. J. Turner tambien se embarca a traves de las paginas de la presente obra en esta investigacion concretando la pregunta en un aspecto de nuestra naturaleza, a saber, el social, transformando asi el enigma inicial en como hemos llegado a ser seres sociales. Para responder a ello el presente libro aporta pistas sumamente valiosas en la reconstruccion del puzzle, reparando en una pieza olvidada y al mismo tiempo clave para entender la identidad de los humanos: las emociones, llegando asi a defender la sorprendente y poco frecuente tesis de que las emociones son el fundamento de los lazos sociales en los humanos. On the origins of human emotions trata, pues, de descifrar mediante una historia logica y factible como opero la seleccion para hacer de los humanos los animales mas emocionales que existen sobre la tierra.1 Para ello J. Turner, profesor de sociologia de las emociones en la Universidad de California (Riverside), realiza un recorrido interdisciplinar, sin por ello abandonar su enfoque sociologico, con el fin de anclar su tesis en distintos puertos intelectuales, mostrando asi, como el mismo senala, que la trasgresion de estas fronteras en el conocimiento se perfila cada vez mas necesaria en cualquier estudio del ser humano. Asi, con el fin de entender mejor la dinamica de las emociones en las relaciones humanas desde una perspectiva sociologica y evolucionista, J. Turner elabora un argumento original y riguroso, transportandonos al comienzodenuestra historia como especie. Para el autor se hace evidente, siendo este el punto central de toda su argumentacion, que la capacidad emocional fue la manera mas eficaz que tuvo la evolucion para garantizar nuestra supervivencia; y esto","[{'authorId': '118688586', 'name': 'J. Turner'}]",251.0,"{'bibtex': '@Inproceedings{Turner2000OnTO,\n author = {J. Turner},\n title = {On the Origins of Human Emotions: A Sociological Inquiry into the Evolution of Human Affect},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,On the Origins of Human Emotions: A Sociological Inquiry into the Evolution of Human Affect,2000.0
1317,6d067a072b9cf8f226eabca90d7bb1d93867a8f6,"The chameleon effect refers to nonconscious mimicry of the postures, mannerisms, facial expressions, and other behaviors of one's interaction partners, such that one's behavior passively and unintentionally changes to match that of others in one's current social environment. The authors suggest that the mechanism involved is the perception-behavior link, the recently documented finding (e.g., J. A. Bargh, M. Chen, & L. Burrows, 1996) that the mere perception of another's behavior automatically increases the likelihood of engaging in that behavior oneself. Experiment 1 showed that the motor behavior of participants unintentionally matched that of strangers with whom they worked on a task. Experiment 2 had confederates mimic the posture and movements of participants and showed that mimicry facilitates the smoothness of interactions and increases liking between interaction partners. Experiment 3 showed that dispositionally empathic individuals exhibit the chameleon effect to a greater extent than do other people.","[{'authorId': '6026289', 'name': 'T. Chartrand'}, {'authorId': '2536558', 'name': 'J. Bargh'}]",3950.0,"{'bibtex': '@Article{Chartrand1999TheCE,\n author = {T. Chartrand and J. Bargh},\n journal = {Journal of personality and social psychology},\n pages = {\n          893-910\n        },\n title = {The chameleon effect: the perception-behavior link and social interaction.},\n volume = {76 6},\n year = {1999}\n}\n'}",,"{'volume': '76 6', 'pages': '\n          893-910\n        ', 'name': 'Journal of personality and social psychology'}",110.0,The chameleon effect: the perception-behavior link and social interaction.,1999.0
1321,6d0ce443cee21915038992df97c707dce792ef83,,"[{'authorId': '52248191', 'name': 'Byu Scholarsarchive'}, {'authorId': '2064142748', 'name': 'A. Manning'}, {'authorId': '39477448', 'name': 'Scott McCloud'}]",1671.0,"{'bibtex': '@Inproceedings{Scholarsarchive2018UnderstandingCT,\n author = {Byu Scholarsarchive and A. Manning and Scott McCloud},\n title = {Understanding Comics: The Invisible Art},\n year = {2018}\n}\n'}",,,0.0,Understanding Comics: The Invisible Art,2018.0
1322,6d0d7f499249cad3e2663a44797a67c7a7612017,"Interactive virtual worlds provide an immersive and effective environment for training, education, and entertainment purposes. So far, there have been considerable advances in 3D and AI design to enhance virtual environments. Virtual characters are one of the crucial aspects of interactive narratives. The interaction of rich virtual characters can produce interesting narrative and as a result, enhance the experience of virtual environments. As one of those characters, the user would feel immersed and engaged when interacting with compelling characters. There are many characteristics that improve believable behavior generation, including beliefs, goals, desires, affect, and personality. My goal is to propose an affective model of personality for multi-agent narrative planning systems that is domain independent and thus minimizes authorial burden. I aim to combine existing models into a unified framework. Although the framework may oversimply those models, it would not overlook their key ingredients.","[{'authorId': '144650313', 'name': 'A. Shirvani'}]",2.0,"{'bibtex': '@Article{Shirvani2019TowardsMB,\n author = {A. Shirvani},\n booktitle = {Artificial Intelligence and Interactive Digital Entertainment Conference},\n pages = {230-232},\n title = {Towards More Believable Characters Using Personality and Emotion},\n year = {2019}\n}\n'}","[{'paperId': '6a3eb85e6294046c136d3b35b8317d13d0d21653', 'title': 'A Formalization of Emotional Planning for Strong-Story Systems'}, {'paperId': '05a2a43edfff98334bdceeff94b26a296be9f681', 'title': 'Camelot: A Modular Customizable Sandbox for Visualizing Interactive Narratives'}]",{'pages': '230-232'},21.0,Towards More Believable Characters Using Personality and Emotion,2019.0
1323,6d624ea0367fdaa0cf8c7810615f404633630358,,"[{'authorId': '2656777', 'name': 'A. Damasio'}]",458.0,"{'bibtex': '@Article{Damasio1998EmotionIT,\n author = {A. Damasio},\n journal = {Brain Research Reviews},\n pages = {83-86},\n title = {Emotion in the perspective of an integrated nervous system\n 1\n Published on the World Wide Web on 27 January 1998.\n \n \n 1\n \n},\n volume = {26},\n year = {1998}\n}\n'}",,"{'volume': '26', 'pages': '83-86', 'name': 'Brain Research Reviews'}",19.0,"Emotion in the perspective of an integrated nervous system
 1
 Published on the World Wide Web on 27 January 1998.
 
 
 1
 
",1998.0
1325,6d75df4360a3d56514dcb775c832fdc572bab64b,"We present here new evidence of cross-cultural agreement in the judgement of facial expression. Subjects in 10 cultures performed a more complex judgment task than has been used in previous cross-cultural studies. Instead of limiting the subjects to selecting only one emotion term for each expression, this task allowed them to indicate that multiple emotions were evident and the intensity of each emotion. Agreement was very high across cultures about which emotion was the most intense. The 10 cultures also agreed about the second most intense emotion signaled by an expression and about the relative intensity among expressions of the same emotion. However, cultural differences were found in judgments of the absolute level of emotional intensity.","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}, {'authorId': '1456140296', 'name': 'Maureen O’Sullivan'}, {'authorId': '2223648361', 'name': 'Anthony Chan'}, {'authorId': '2223647683', 'name': 'Irene Diacoyanni-Tarlatzis'}, {'authorId': '8729784', 'name': 'K. Heider'}, {'authorId': '50392108', 'name': 'R. Krause'}, {'authorId': '5121013', 'name': 'W. LeCompte'}, {'authorId': '4481972', 'name': 'T. Pitcairn'}, {'authorId': '147835851', 'name': 'P. Ricci-Bitti'}, {'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '113860467', 'name': 'M. Tomita'}, {'authorId': '40029778', 'name': 'A. Tzavaras'}]",1608.0,"{'bibtex': '@Article{Ekman1987UniversalsAC,\n author = {P. Ekman and Wallace V. Friesen and Maureen O’Sullivan and Anthony Chan and Irene Diacoyanni-Tarlatzis and K. Heider and R. Krause and W. LeCompte and T. Pitcairn and P. Ricci-Bitti and K. Scherer and M. Tomita and A. Tzavaras},\n journal = {Journal of personality and social psychology},\n pages = {\n          712-7\n        },\n title = {Universals and cultural differences in the judgments of facial expressions of emotion.},\n volume = {53 4},\n year = {1987}\n}\n'}",,"{'volume': '53 4', 'pages': '\n          712-7\n        ', 'name': 'Journal of personality and social psychology'}",25.0,Universals and cultural differences in the judgments of facial expressions of emotion.,1987.0
1328,6d800d776db1c0a126d2184b90df033dee75ab8d,"When learning from others, it is important to take a critical stance—evaluating both the informants themselves as well as the content of their claims. In addition to accuracy, one can evaluate claims based on quality. The current study investigates developmental change in learners’ evaluations of evidence that varies in quality—inductive strength based on typicality or diversity. We found that while younger children track which informant provides which examples, they do not have clear preferences for the informant who provides stronger examples. Older children, on the other hand, are in the middle of a developmental transition. They rate informants who provide inductively strong examples as more trustworthy, but only reliably choose the informant who provides diverse examples.","[{'authorId': '2895785', 'name': 'A. Landrum'}, {'authorId': '2426641', 'name': 'Joshua Cloudy'}, {'authorId': '3210220', 'name': 'Patrick Shafto'}]",3.0,"{'bibtex': '@Article{Landrum2015MoreTT,\n author = {A. Landrum and Joshua Cloudy and Patrick Shafto},\n journal = {Cognitive Science},\n title = {More than true: Developmental changes in use of the inductive strength for selective trust},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': 'Cognitive Science'}",18.0,More than true: Developmental changes in use of the inductive strength for selective trust,2015.0
1329,6d8795b5aed51e4951763974cb391047ca94b75b,"Purpose Existing virtual agents (VAs) present in dialogue systems are either information retrieval based or static goal-driven. However, in real-world situations, end-users might not have a known and fixed goal beforehand for the task, i.e., they may upgrade/downgrade/update their goal components in real-time to maximize their utility values. Existing VAs are unable to handle such dynamic goal-oriented situations. Methodology Due to the absence of any related dialogue dataset where such choice deviations are present, we have created a conversational dataset called Deviation adapted Virtual Agent(DevVA), with the manual annotation of its corresponding intents, slots, and sentiment labels. A Dynamic Goal Driven Dialogue Agent (DGDVA) has been developed by incorporating a Dynamic Goal Driven Module (GDM) on top of a deep reinforcement learning based dialogue manager. In the course of a conversation, the user sentiment provides grounded feedback about agent behavior, including goal serving action. User sentiment appears to be an appropriate indicator for goal discrepancy that guides the agent to complete the user’s desired task with gratification. The negative sentiment expressed by the user about an aspect of the provided choice is treated as a discrepancy that is being resolved by the GDM depending upon the observed discrepancy and current dialogue state. The goal update capability and the VA’s interactiveness trait enable end-users to accomplish their desired task satisfactorily. Findings The obtained experimental results illustrate that DGDVA can handle dynamic goals with maximum user satisfaction and a significantly higher success rate. The interaction drives the user to decide its final goal through the latent specification of possible choices and information retrieved and provided by the dialogue agent. Through the experimental results (qualitative and quantitative), we firmly conclude that the proposed sentiment-aware VA adapts users’ dynamic behavior for its goal setting with substantial efficacy in terms of primary objective i.e., task success rate (0.88). Practical implications In real world, it can be argued that many people do not have a predefined and fixed goal for tasks such as online shopping, movie booking & restaurant booking, etc. They tend to explore the available options first which are aligned with their minimum requirements and then decide one amongst them. The DGDVA provides maximum user satisfaction as it enables them to accomplish a dynamic goal that leads to additional utilities along with the essential ones. Originality To the best of our knowledge, this is the first effort towards the development of A Dynamic Goal Adapted Task-Oriented Dialogue Agent that can serve user goals dynamically until the user is satisfied.","[{'authorId': '2063522518', 'name': 'Abhisek Tiwari'}, {'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '2062808558', 'name': 'Shubhashis Sengupta'}, {'authorId': '40585053', 'name': 'Anutosh Maitra'}, {'authorId': '3040439', 'name': 'Roshni Ramnani'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",9.0,"{'bibtex': '@Article{Tiwari2021ADG,\n author = {Abhisek Tiwari and Tulika Saha and S. Saha and Shubhashis Sengupta and Anutosh Maitra and Roshni Ramnani and P. Bhattacharyya},\n journal = {PLoS ONE},\n title = {A dynamic goal adapted task oriented dialogue agent},\n volume = {16},\n year = {2021}\n}\n'}",,"{'volume': '16', 'name': 'PLoS ONE'}",73.0,A dynamic goal adapted task oriented dialogue agent,2021.0
1330,6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,,"[{'authorId': '1384255355', 'name': 'Aleix M. Martinez'}]",3983.0,"{'bibtex': '@Inproceedings{Martinez1998TheAF,\n author = {Aleix M. Martinez},\n title = {The AR face database},\n volume = {24},\n year = {1998}\n}\n'}",,"{'volume': '24', 'name': ''}",0.0,The AR face database,1998.0
1331,6d9b0e221edff099785584c19818c950654e03df,"Complex and natural social interaction between artificial agents (computer-generated or robotic) and humans necessitates the display of rich emotions in order to be believable, socially relevant, and accepted, and to generate the natural emotional responses that humans show in the context of social interaction, such as engagement or empathy. Whereas some robots use faces to display (simplified) emotional expressions, for other robots such as Nao, body language is the best medium available given their inability to convey facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should significantly improve naturalness. This research investigates the creation of an affect space for the generation of emotional body language to be displayed by humanoid robots. To do so, three experiments investigating how emotional body language displayed by agents is interpreted were conducted. The first experiment compared the interpretation of emotional body language displayed by humans and agents. The results showed that emotional body language displayed by an agent or a human is interpreted in a similar way in terms of recognition. Following these results, emotional key poses were extracted from an actor's performances and implemented in a Nao robot. The interpretation of these key poses was validated in a second study where it was found that participants were better than chance at interpreting the key poses displayed. Finally, an affect space was generated by blending key poses and validated in a third study. Overall, these experiments confirmed that body language is an appropriate medium for robots to display emotions and suggest that an affect space for body expressions can be used to improve the expressiveness of humanoid robots.","[{'authorId': '2609243', 'name': 'Aryel Beck'}, {'authorId': '46998955', 'name': 'B. Stevens'}, {'authorId': '2063584', 'name': 'K. Bard'}, {'authorId': '1713009', 'name': 'L. Cañamero'}]",79.0,"{'bibtex': '@Article{Beck2012EmotionalBL,\n author = {Aryel Beck and B. Stevens and K. Bard and L. Cañamero},\n journal = {ACM Trans. Interact. Intell. Syst.},\n pages = {2:1-2:29},\n title = {Emotional body language displayed by artificial agents},\n volume = {2},\n year = {2012}\n}\n'}",,"{'volume': '2', 'pages': '2:1-2:29', 'name': 'ACM Trans. Interact. Intell. Syst.'}",52.0,Emotional body language displayed by artificial agents,2012.0
1332,6db2b93a2d4007371030644173f1001c959214d2,"Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice’s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.","[{'authorId': '14487640', 'name': 'Ari Holtzman'}, {'authorId': '144685020', 'name': 'Jan Buys'}, {'authorId': '39191185', 'name': 'Maxwell Forbes'}, {'authorId': '2691021', 'name': 'Antoine Bosselut'}, {'authorId': '145798491', 'name': 'David Golub'}, {'authorId': '1699545', 'name': 'Yejin Choi'}]",194.0,"{'bibtex': '@Article{Holtzman2018LearningTW,\n author = {Ari Holtzman and Jan Buys and Maxwell Forbes and Antoine Bosselut and David Golub and Yejin Choi},\n journal = {ArXiv},\n title = {Learning to Write with Cooperative Discriminators},\n volume = {abs/1805.06087},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1805.06087', 'name': 'ArXiv'}",57.0,Learning to Write with Cooperative Discriminators,2018.0
1333,6de6ed8cadd0359097aca243f49c80f6c2995d67,Speaking is a physiological process that manifests in the acoustic and in the optic domain and hence is audible and visible. These two modalities influence each other in perception. Under normal circumstances the speech information in both channels is coherent and complementary and integrated to a percept. But if the information is conflicting and nevertheless integrated then the percept in one of the modalities might be changed by the other modality. The experiment described here discovers that when the video of an utterance spoken in one emotion is dubbed with the audio of the utterance spoken in another emotion the perceived emotion might be a third – neither present in the auditory nor in the visual modality.,"[{'authorId': '1703808', 'name': 'Sascha Fagel'}]",37.0,"{'bibtex': '@Inproceedings{Fagel2006EmotionalME,\n author = {Sascha Fagel},\n title = {Emotional McGurk Effect},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",11.0,Emotional McGurk Effect,2006.0
1334,6e2e9e86bb94e9cf382b0d085da91561df39b1ff,"Sentences and tweets are often annotated for sentiment simply by asking respondents to label them as positive, negative, or neutral. This works well for simple expressions of sentiment; however, for many other types of sentences, respondents are unsure of how to annotate, and produce inconsistent labels. In this paper, we outline several types of sentences that are particularly challenging for manual sentiment annotation. Next we propose two annotation schemes that address these challenges, and list benefits and limitations for both.","[{'authorId': '143880621', 'name': 'Saif M. Mohammad'}]",86.0,"{'bibtex': '@Inproceedings{Mohammad2016APG,\n author = {Saif M. Mohammad},\n pages = {174-179},\n title = {A Practical Guide to Sentiment Annotation: Challenges and Solutions},\n year = {2016}\n}\n'}",,{'pages': '174-179'},15.0,A Practical Guide to Sentiment Annotation: Challenges and Solutions,2016.0
1335,6e9d2a77d174f95145a30e6d0b0114d5ffddcd95,"Traditional approaches to the study of cognition emphasize an information-processing view that has generally excluded emotion. In contrast, the recent emergence of cognitive neuroscience as an inspiration for understanding human cognition has highlighted its interaction with emotion. This review explores insights into the relations between emotion and cognition that have resulted from studies of the human amygdala. Five topics are explored: emotional learning, emotion and memory, emotion's influence on attention and perception, processing emotion in social stimuli, and changing emotional responses. Investigations into the neural systems underlying human behavior demonstrate that the mechanisms of emotion and cognition are intertwined from early perception to reasoning. These findings suggest that the classic division between the study of emotion and cognition may be unrealistic and that an understanding of human cognition requires the consideration of emotion.","[{'authorId': '2471431', 'name': 'E. Phelps'}]",1596.0,"{'bibtex': '@Article{Phelps2012EmotionAC,\n author = {E. Phelps},\n journal = {Annual review of psychology},\n pages = {\n          27-53\n        },\n title = {Emotion and cognition: insights from studies of the human amygdala.},\n volume = {57},\n year = {2012}\n}\n'}",,"{'volume': '57', 'pages': '\n          27-53\n        ', 'name': 'Annual review of psychology'}",148.0,Emotion and cognition: insights from studies of the human amygdala.,2012.0
1336,6e9e10b250492d44b807b0d0298cc7a8161bea61,"Virtual agents are increasingly being integrated in our everyday life thanks to their communicative skills and abilities to express social affects like emotions and attitudes. The goal of this work is to evaluate the perception of agents expressing interpersonal attitudes through non-verbal behaviors. The interpretation of these behaviors depends on how they are sequenced and coordinated over time. To encompass the sequentiality and the dynamics of non-verbal signals, we rely on temporal sequence mining. From a multimodal corpus, this algorithm produces meaningful sequences resulting in more adapted expression of social attitudes of the agent.","[{'authorId': '8447202', 'name': 'Soumia Dermouche'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",1.0,"{'bibtex': '@Article{Dermouche2018AttitudeMF,\n author = {Soumia Dermouche and C. Pelachaud},\n journal = {Proceedings of the 5th International Conference on Movement and Computing},\n title = {Attitude Modeling for Virtual Character Based on Temporal Sequence Mining: Extraction and Evaluation},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 5th International Conference on Movement and Computing'},41.0,Attitude Modeling for Virtual Character Based on Temporal Sequence Mining: Extraction and Evaluation,2018.0
1337,6ea74bbce2629136e1ebe2d4bc9ba2913c131130,"Previous research has indicated that nonverbal teacher behaviors such as smiling, vocal expressiveness, movement about the classroom, and relaxed body position are salient low‐inference variables of a process which results in a product of increased cognitive and affective learning. This study identified a set of verbal teacher immediacy behaviors which similarly relate to increased student learning. Results indicated differentiated use of various types of verbal immediacy messages between small and larger classes, and that the impact of teacher immediacy behaviors (both verbal and nonverbal) on learning is coincidentally enhanced as class size increases. The study provides empirical definition of a specific set of low‐inference verbal variables which, in combination with previously identified nonverbal variables, clarify a single process‐product model for effective instructional interaction.","[{'authorId': '47054519', 'name': 'J. Gorham'}]",848.0,"{'bibtex': '@Article{Gorham1988TheRB,\n author = {J. Gorham},\n journal = {Communication Education},\n pages = {40-53},\n title = {The relationship between verbal teacher immediacy behaviors and student learning},\n volume = {37},\n year = {1988}\n}\n'}",,"{'volume': '37', 'pages': '40-53', 'name': 'Communication Education'}",16.0,The relationship between verbal teacher immediacy behaviors and student learning,1988.0
1338,6eb96e0ee473155ba8911b42ffa96426667cb05b,"Immersive collaborative virtual environments (CVEs) are simulations in which geographically separated individuals interact in a shared, three-dimensional, digital space using immersive virtual environment technology. Unlike videoconference technology, which transmits direct video streams, immersive CVEs accurately track movements of interactants and render them nearly simultaneously (i.e., in real time) onto avatars, three-dimensional digital representations of the interactants. Nonverbal behaviors of interactants can be rendered veridically or transformed strategically (i.e., rendered nonveridically). This research examined augmented gaze, a transformation in which a given interactant's actual head movements are transformed by an algorithm that renders his or her gaze directly at multiple interactants simultaneously, such that each of the others perceives that the transformed interactant is gazing only at him or her. In the current study, a presenter read a persuasive passage to two listeners under various transformed gaze conditions, including augmented gaze. Results showed that women agreed with a persuasive message more during augmented gaze than other gaze conditions. Men recalled more verbal information from the passage than women. Implications for theories of social interaction and computer-mediated communication are discussed.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2386187', 'name': 'J. Loomis'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '144097660', 'name': 'M. Turk'}]",161.0,"{'bibtex': '@Article{Bailenson2005TransformedSI,\n author = {J. Bailenson and A. Beall and J. Loomis and J. Blascovich and M. Turk},\n journal = {Human Communication Research},\n pages = {511-537},\n title = {Transformed social interaction, augmented gaze, and social influence in immersive virtual environments},\n volume = {31},\n year = {2005}\n}\n'}",,"{'volume': '31', 'pages': '511-537', 'name': 'Human Communication Research'}",60.0,"Transformed social interaction, augmented gaze, and social influence in immersive virtual environments",2005.0
1339,6ec802c9a6032391c68c75bf5c7b3d0057c17e31,,"[{'authorId': '145658080', 'name': 'Saima Aman'}, {'authorId': '66114340', 'name': 'S. Szpakowicz'}]",455.0,"{'bibtex': '@Inproceedings{Aman2007IdentifyingEO,\n author = {Saima Aman and S. Szpakowicz},\n pages = {196-205},\n title = {Identifying Expressions of Emotion in Text},\n year = {2007}\n}\n'}",,{'pages': '196-205'},22.0,Identifying Expressions of Emotion in Text,2007.0
1340,6ed3deb057d7469959901368a17493e49344e291,"Accurately estimating the person's head position and orientation is an important task for a wide range of applications such as driver awareness and human-robot interaction. Over the past two decades, many approaches have been suggested to solve this problem, each with its own advantages and disadvantages. In this paper, we present a probabilistic framework called generalized adaptive viewbased appearance model (GAVAM) which integrates the advantages from three of these approaches: (1) the automatic initialization and stability of static head pose estimation, (2) the relative precision and user-independence of differential registration, and (3) the robustness and bounded drift of keyframe tracking. In our experiments, we show how the GAVAM model can be used to estimate head position and orientation in real-time using a simple monocular camera. Our experiments on two previously published datasets show that the GAVAM framework can accurately track for a long period of time (>2 minutes) with an average accuracy of 3.5deg and 0.75 in with an inertial sensor and a 3D magnetic sensor.","[{'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '143973061', 'name': 'J. Whitehill'}, {'authorId': '1741200', 'name': 'J. Movellan'}]",112.0,"{'bibtex': '@Article{Morency2008GeneralizedAV,\n author = {Louis-Philippe Morency and J. Whitehill and J. Movellan},\n journal = {2008 8th IEEE International Conference on Automatic Face & Gesture Recognition},\n pages = {1-8},\n title = {Generalized adaptive view-based appearance model: Integrated framework for monocular head pose estimation},\n year = {2008}\n}\n'}",,"{'pages': '1-8', 'name': '2008 8th IEEE International Conference on Automatic Face & Gesture Recognition'}",33.0,Generalized adaptive view-based appearance model: Integrated framework for monocular head pose estimation,2008.0
1341,6ef28652cc35b3fce218c937315435ee39c59ce9,"The past years have seen increasing cooperation between psychology and computer science in the field of computational modeling of emotion. However, to realize its potential, the exchange between the two disciplines, as well as the intradisciplinary coordination, should be further improved. We make three proposals for how this could be achieved. The proposals refer to: 1) systematizing and classifying the assumptions of psychological emotion theories; 2) formalizing emotion theories in implementation-independent formal languages (set theory, agent logics); and 3) modeling emotions using general cognitive architectures (such as Soar and ACT-R), general agent architectures (such as the BDI architecture) or general-purpose affective agent architectures. These proposals share two overarching themes. The first is a proposal for modularization: deconstruct emotion theories into basic assumptions; modularize architectures. The second is a proposal for unification and standardization: Translate different emotion theories into a common informal conceptual system or a formal language, or implement them in a common architecture.","[{'authorId': '3213879', 'name': 'R. Reisenzein'}, {'authorId': '2348728', 'name': 'E. Hudlicka'}, {'authorId': '1707738', 'name': 'M. Dastani'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1751831', 'name': 'K. Hindriks'}, {'authorId': '1698475', 'name': 'E. Lorini'}, {'authorId': '1691228', 'name': 'J. Meyer'}]",140.0,"{'bibtex': '@Article{Reisenzein2013ComputationalMO,\n author = {R. Reisenzein and E. Hudlicka and M. Dastani and J. Gratch and K. Hindriks and E. Lorini and J. Meyer},\n journal = {IEEE Transactions on Affective Computing},\n pages = {246-266},\n title = {Computational Modeling of Emotion: Toward Improving the Inter- and Intradisciplinary Exchange},\n volume = {4},\n year = {2013}\n}\n'}",,"{'volume': '4', 'pages': '246-266', 'name': 'IEEE Transactions on Affective Computing'}",236.0,Computational Modeling of Emotion: Toward Improving the Inter- and Intradisciplinary Exchange,2013.0
1343,6ef6adf8777c50c4f4f0a7cc7804cf9d3317084b,"Narrative, and in particular storytelling, is an important part of the human experience. Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. One of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. There are many factors - logical and aesthetic - that contribute to the success of a narrative artifact. Central to this success is its understandability. We argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability. Character believability is the perception by the audience that the actions performed by characters do not negatively impact the audience's suspension of disbelief. Specifically, characters must be perceived by the audience to be intentional agents. In this article, we explore the use of refinement search as a technique for solving the narrative generation problem - to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. We describe a novel refinement search planning algorithm - the Intent-based Partial Order Causal Link (IPOCL) planner - that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. We present the results of an empirical evaluation that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.","[{'authorId': '2757194', 'name': 'Mark O. Riedl'}, {'authorId': '145513579', 'name': 'R. Young'}]",540.0,"{'bibtex': '@Article{Riedl2010NarrativePB,\n author = {Mark O. Riedl and R. Young},\n journal = {ArXiv},\n title = {Narrative Planning: Balancing Plot and Character},\n volume = {abs/1401.3841},\n year = {2010}\n}\n'}",,"{'volume': 'abs/1401.3841', 'name': 'ArXiv'}",95.0,Narrative Planning: Balancing Plot and Character,2010.0
1344,6f0ff5d9beb65b490c79cde1c8cafeaa1210fcee,,"[{'authorId': '2812935', 'name': 'A. Héloir'}, {'authorId': '145616714', 'name': 'Michael Kipp'}]",65.0,"{'bibtex': '@Article{Héloir2009EMBRAR,\n author = {A. Héloir and Michael Kipp},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-2},\n title = {EMBR: A realtime animation engine for interactive embodied agents},\n year = {2009}\n}\n'}",,"{'pages': '1-2', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",19.0,EMBR: A realtime animation engine for interactive embodied agents,2009.0
1345,6f41b743eb7805e3eeef2fdbede769c1da07ee44,"Noise in the form of random errors in implementing a choice is a common problem in real-world interactions. Recent research has identified three approaches to coping with noise: adding generosity to a reciprocating strategy; adding contrition to a reciprocating strategy; and using an entirely different strategy, Pavlov, based on the idea of switching choice whenever the previous payoff was low. Tournament studies, ecological simulation, and theoretical analysis demonstrate (1) a generous version of tit-for-tat is a highly effective strategy when the players it meets have not adapted to noise; (2) if the other players have adapted to noise, a contrite version of tit-for-tat is even more effective at quickly restoring mutual cooperation without the risk of exploitation; and (3) Pavlov is not robust.","[{'authorId': '2144138994', 'name': 'J. Wu'}, {'authorId': '145012155', 'name': 'R. Axelrod'}]",320.0,"{'bibtex': ""@Article{Wu1995HowTC,\n author = {J. Wu and R. Axelrod},\n journal = {Journal of Conflict Resolution},\n pages = {183 - 189},\n title = {How to Cope with Noise in the Iterated Prisoner's Dilemma},\n volume = {39},\n year = {1995}\n}\n""}",,"{'volume': '39', 'pages': '183 - 189', 'name': 'Journal of Conflict Resolution'}",20.0,How to Cope with Noise in the Iterated Prisoner's Dilemma,1995.0
1346,6f522352a3f42333e8eb0820fc95233eecc79baa,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}]",500.0,"{'bibtex': '@Article{Mehrabian1969SignificanceOP,\n author = {A. Mehrabian},\n journal = {Psychological bulletin},\n pages = {\n          359-72\n        },\n title = {Significance of posture and posiion in the communication of attitude and status relationships.},\n volume = {71 5},\n year = {1969}\n}\n'}",,"{'volume': '71 5', 'pages': '\n          359-72\n        ', 'name': 'Psychological bulletin'}",0.0,Significance of posture and posiion in the communication of attitude and status relationships.,1969.0
1347,6f6c8098453655cb7ed5ce4e90d64bef8ecaef93,"During the last half of the twentieth century, psychologists and anthropologists have studied proxemics, or spacing behavior, among people in many contexts. As we enter the twenty-first century, immersive virtual environment technology promises new experimental venues in which researchers can study proxemics. Immersive virtual environments provide realistic and compelling experimental settings without sacrificing experimental control. The experiment reported here tested Argyle and Dean's (1965) equilibrium theory's specification of an inverse relationship between mutual gaze, a nonverbal cue signaling intimacy, and interpersonal distance. Participants were immersed in a three-dimensional virtual room in which a virtual human representation (that is, an embodied agent) stood. Under the guise of a memory task, participants walked towards and around the agent. Distance between the participant and agent was tracked automatically via our immersive virtual environment system. All participants maintained more space around agents than they did around similarly sized and shaped but nonhuman-like objects. Female participants maintained more interpersonal distance between themselves and agents who engaged them in eye contact (that is, mutual gaze behavior) than between themselves and agents who did not engage them in eye contact, whereas male participants did not. Implications are discussed for the study of proxemics via immersive virtual environment technology, as well as the design of virtual environments and virtual humans.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '1990740', 'name': 'Christopher Rex'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2386187', 'name': 'J. Loomis'}]",473.0,"{'bibtex': '@Article{Bailenson2001EquilibriumTR,\n author = {J. Bailenson and Christopher Rex and A. Beall and J. Loomis},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {583-598},\n title = {Equilibrium Theory Revisited: Mutual Gaze and Personal Space in Virtual Environments},\n volume = {10},\n year = {2001}\n}\n'}",,"{'volume': '10', 'pages': '583-598', 'name': 'Presence: Teleoperators & Virtual Environments'}",68.0,Equilibrium Theory Revisited: Mutual Gaze and Personal Space in Virtual Environments,2001.0
1349,6f9c548b05b06f86a7c49e2e8eaf1027d4145bdb,"In this paper, we present a novel framework for creating cartoon facial animation from multi-view hand-drawn sketches. The input sketches are first employed to construct a base mesh model by using a hybrid sketch-based method. The model is then deformed for each key viewpoint, yielding a set of models that closely match the corresponding sketches. We introduce a view-dependent facial expression space defined by the key viewpoints and the basic emotions to generate various facial expressions viewed from arbitrary angles. The output facial animation conforms to the input sketches and maintains frame-to-frame correspondence. We demonstrate the potential of our approach through an easy-to-use system, where the animating of cartoon faces is automated once the user accomplishes sketching and configuration. Copyright © 2010 John Wiley & Sons, Ltd. 
 
In this paper, we present a sketch-based framework for the creation of cartoon faces, which are animated depending on both the viewpoints and the emotions. For each basic emotion, the multi-view sketches (a) are employed to construct a view-dependent model (b) that closely matches the input sketches. A view-dependent facial expression space defined by the key viewpoints and the basic emotions is introduced to generate various facial expressions (c) viewed from arbitrary angles.","[{'authorId': '2144440849', 'name': 'Xiang Li'}, {'authorId': '2150637563', 'name': 'Jun Xu'}, {'authorId': '1690266', 'name': 'Yangchun Ren'}, {'authorId': '2257808072', 'name': 'Weidong Geng'}]",125.0,"{'bibtex': '@Article{Li2010AnimatingCF,\n author = {Xiang Li and Jun Xu and Yangchun Ren and Weidong Geng},\n journal = {Computer Animation and Virtual Worlds},\n pages = {193-201},\n title = {Animating cartoon faces by multi-view drawings},\n volume = {21},\n year = {2010}\n}\n'}",,"{'volume': '21', 'pages': '193-201', 'name': 'Computer Animation and Virtual Worlds'}",642.0,Animating cartoon faces by multi-view drawings,2010.0
1350,6fa16f9d4e50a6923b4d0525ffe3057147a2c4d3,,"[{'authorId': '2253263941', 'name': 'Jean. Steinier'}, {'authorId': '2253276011', 'name': 'Yves. Termonia'}, {'authorId': '2253290223', 'name': 'Jules. Deltour'}]",16218.0,"{'bibtex': '@Article{Steinier1964SmoothingAD,\n author = {Jean. Steinier and Yves. Termonia and Jules. Deltour},\n journal = {Analytical chemistry},\n pages = {\n          1906-9\n        },\n title = {Smoothing and differentiation of data by simplified least square procedure.},\n volume = {44 11},\n year = {1964}\n}\n'}",,"{'volume': '44 11', 'pages': '\n          1906-9\n        ', 'name': 'Analytical chemistry'}",6.0,Smoothing and differentiation of data by simplified least square procedure.,1964.0
1351,6fb4b2c5a2e1b401ee1ded242425b3e3a854bbdd,"This study verifies the effectiveness of the robotic emotional expressions developed using non-verbal languages. An emotional robot known as ""Mung"" was developed by applying the nonverbal expressions of bruises (blue or dark blue) and complexions (red or pink) using full color LEDs, to express robotic emotions. With these emotional expressions, Mung functioned as a language purifier by responding to humans' use of language emotionally. In order to evaluate emotional expressions used in Mung, an experiment was performed to compare the robotic emotions using a verbal language with those using non-verbal languages. A 2x2 between-groups factorial design was used for this experiment. The between-groups factors were expression type and emotion type. According to the results of a post-experiment survey, emotional expressions through bruises and complexion are as effective as speech for the delivery of robotic emotions. This result emphasizes the effectiveness of the developed robotic emotional expressions.","[{'authorId': '145391043', 'name': 'E. Kim'}, {'authorId': '1743779', 'name': 'Sonya S. Kwak'}, {'authorId': '3352492', 'name': 'Jeonghye Han'}, {'authorId': '9446832', 'name': 'Y. Kwak'}]",9.0,"{'bibtex': '@Inproceedings{Kim2009EvaluationOT,\n author = {E. Kim and Sonya S. Kwak and Jeonghye Han and Y. Kwak},\n pages = {362-365},\n title = {Evaluation of the expressions of robotic emotions of the emotional robot, ""Mung""},\n year = {2009}\n}\n'}",,{'pages': '362-365'},24.0,"Evaluation of the expressions of robotic emotions of the emotional robot, ""Mung""",2009.0
1352,6fc660745dcbafa5f3f2756bcec5519cecbcad58,"According to the basic emotions theory,this paper puts forward a new artificial psychology model.It is believed that people’s general emotions are mixed states of basic emotions and are impacted by “drives” which reflect people’s general emotions theory.This artificial psychology model is constructed based on self-organization,fuzzy and optimization theories.The emotion characters constructed in this model can be changed to generate different personality by adjusting emotion parameters.Computer simulation demonstrates the validity of this model.","[{'authorId': '1934202', 'name': 'Wang Zhiliang'}]",8.0,"{'bibtex': '@Article{Zhiliang2005ApplicationOB,\n author = {Wang Zhiliang},\n journal = {Computer Engineering},\n title = {Application of Basic Emotions Theory in Construction of Artificial Psychology Model},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': 'Computer Engineering'}",0.0,Application of Basic Emotions Theory in Construction of Artificial Psychology Model,2005.0
1353,6fd70f797ab9bad027098af2b38b7188185237b5,"What makes some near-human characters scary while others are merely laughable? More important, why do some human and humanlike characters fail to arouse our sympathy? Visual artists and roboticists face these questions as they seek to alternately frighten and endear. Recent attempts to create accurate human replicas have brought these questions to the fore with increased urgency.","[{'authorId': '2263008863', 'name': 'Tom Geller'}]",88.0,"{'bibtex': '@Article{Geller2008OvercomingTU,\n author = {Tom Geller},\n journal = {IEEE Computer Graphics and Applications},\n title = {Overcoming the Uncanny Valley},\n volume = {28},\n year = {2008}\n}\n'}",,"{'volume': '28', 'name': 'IEEE Computer Graphics and Applications'}",0.0,Overcoming the Uncanny Valley,2008.0
1354,6ff61f1ea6d4acfcda1bfd1be5721b44d673e1ed,"In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.","[{'authorId': '1388702112', 'name': 'L. Rojas-Barahona'}, {'authorId': '33870107', 'name': 'Bo-Hsiang Tseng'}, {'authorId': '30087809', 'name': 'Yinpei Dai'}, {'authorId': '80905709', 'name': 'Clare Mansfield'}, {'authorId': '2065760904', 'name': 'Osman Ramadan'}, {'authorId': '2295429', 'name': 'Stefan Ultes'}, {'authorId': '2114765392', 'name': 'Michael Crawford'}, {'authorId': '51175233', 'name': 'M. Gašić'}]",20.0,"{'bibtex': '@Article{Rojas-Barahona2018DeepLF,\n author = {L. Rojas-Barahona and Bo-Hsiang Tseng and Yinpei Dai and Clare Mansfield and Osman Ramadan and Stefan Ultes and Michael Crawford and M. Gašić},\n journal = {ArXiv},\n title = {Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy},\n volume = {abs/1809.00640},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1809.00640', 'name': 'ArXiv'}",51.0,Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy,2018.0
1355,6ffb361144360de3271a9453dc034a721dca8ad5,"Randolph M. Jones Soar Technology, Inc. and Colby College rjones@soartech.com ABSTRACT Increasingly, across the broad spectrum of modeling and simulation, as well as in battlefield information and control systems, autonomous reasoning ability is required, and the “intelligence” developed and tested in simulation is migrating to real -world entities. Cognitive and agent architectures represent maturing computational approaches t o intelligence that can provide robust, scalable, and realistic intelligence. This tutorial will provide an introduction to cognitive architectures, concentrating on production system computation. Examples will be presented from the ACT-R, GOMS, and Soar cognitive architectures. Differences and similarities in cognitive modeling and human behavior representation will be discussed. Attendees will also learn to recognize some of the requirements that suggest the need for a cognitive architecture as compar ed to other approaches and be better able to assess risks, costs, and benefits of different approaches. ABOUT THE AUTHOR Randolph M. Jones , PhD is Chief Scientist at Soar Technology, Inc. and Assistant Professor of Computer Science at Colby College. He has been principal investigator for a variety of Soar Technology’s advanced R&D projects funded by ONR, ARI, DMSO, DARPA and other DOD agencies. He has previously held research positions at the University of Michigan, the University of Pittsburgh, and Carnegie Mellon University. His general areas of research include computational models of human learning and problem solving, executable psychological models, and automated intelligent actors for training and entertainment systems. He earned a BS in Mathematics and Computer Science at UCLA, and MS (1987) and PhD (1989) degrees from the Department of Information and Computer Science at the University of California, Irvine.","[{'authorId': '153788834', 'name': 'Randolph M. Jones'}]",11.0,"{'bibtex': '@Inproceedings{Jones2004AnIT,\n author = {Randolph M. Jones},\n title = {An Introduction to Cognitive Architectures for Modeling and Simulation},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",8.0,An Introduction to Cognitive Architectures for Modeling and Simulation,2004.0
1356,70593a5a6c9607c73fc44c7b16640ed50786042d,,"[{'authorId': '2025591', 'name': 'K. Bergmann'}, {'authorId': '2747728', 'name': 'M. Macedonia'}]",48.0,"{'bibtex': ""@Inproceedings{Bergmann2013AVA,\n author = {K. Bergmann and M. Macedonia},\n pages = {139-148},\n title = {A Virtual Agent as Vocabulary Trainer: Iconic Gestures Help to Improve Learners' Memory Performance},\n year = {2013}\n}\n""}",,{'pages': '139-148'},20.0,A Virtual Agent as Vocabulary Trainer: Iconic Gestures Help to Improve Learners' Memory Performance,2013.0
1357,7066dfda88fbd0ecc803753ca41fb28c176bde44,"Building pedagogical applications in virtual worlds is a multi-disciplinary endeavor that involves learning theories, application development framework, and mediated communication theories. This paper presents a project that integrates game-based learning, multi-agent system architecture (MAS), and the theory of Transformed Social Interaction (TSI), The project implements a group of engaging, affectionate and effective pedagogical agents equipped with abilities of selfrepresentation, emotional states reasoning and situational awareness. A prototype of a virtual quiz show, QuizMASter, has been implemented to realize these abilities, and will be used to test for the effectiveness of the approach. Keywordsvirtual quiz games, pedagogical agent, transformed social interaction(TSI) theory, BDI agent, multiagent systems.","[{'authorId': '2052862892', 'name': 'Steve Leung'}, {'authorId': '3261352', 'name': 'Sandeep Virwaney'}, {'authorId': '1757617', 'name': 'F. Lin'}, {'authorId': '153280757', 'name': 'A. J. Armstrong'}, {'authorId': '2004143', 'name': 'Adien Dubbelboer'}]",18.0,"{'bibtex': '@Article{Leung2013TSIEnhancedPA,\n author = {Steve Leung and Sandeep Virwaney and F. Lin and A. J. Armstrong and Adien Dubbelboer},\n journal = {Int. J. Distance Educ. Technol.},\n pages = {1-13},\n title = {TSI-Enhanced Pedagogical Agents to Engage Learners in Virtual Worlds},\n volume = {11},\n year = {2013}\n}\n'}",,"{'volume': '11', 'pages': '1-13', 'name': 'Int. J. Distance Educ. Technol.'}",29.0,TSI-Enhanced Pedagogical Agents to Engage Learners in Virtual Worlds,2013.0
1358,707c4efd5452de0ef4f3806f8f90529b41f995bd,,"[{'authorId': '2059684142', 'name': 'Barbara Messing'}]",4022.0,"{'bibtex': '@Article{Messing2003AnIT,\n author = {Barbara Messing},\n journal = {Künstliche Intell.},\n pages = {58-},\n title = {An Introduction to MultiAgent Systems},\n volume = {17},\n year = {2003}\n}\n'}",,"{'volume': '17', 'pages': '58-', 'name': 'Künstliche Intell.'}",0.0,An Introduction to MultiAgent Systems,2003.0
1359,709d561c6b5fc8da1c34d56df8c19bafe136ee16,Deux etudes examinent les relations entre differents types d'appreciation et de disposition a l'action et des categories emotionnelles variees dans des epreuves de memorisation d'experiences d'etats emotionnels,"[{'authorId': '49584958', 'name': 'N. Frijda'}, {'authorId': '2075154709', 'name': 'P. Kuipers'}, {'authorId': '1484741405', 'name': 'Elisabeth ter Schure'}]",2142.0,"{'bibtex': '@Article{Frijda1989RelationsAE,\n author = {N. Frijda and P. Kuipers and Elisabeth ter Schure},\n journal = {Journal of Personality and Social Psychology},\n pages = {212-228},\n title = {Relations among emotion, appraisal, and emotional action readiness},\n volume = {57},\n year = {1989}\n}\n'}",,"{'volume': '57', 'pages': '212-228', 'name': 'Journal of Personality and Social Psychology'}",41.0,"Relations among emotion, appraisal, and emotional action readiness",1989.0
1360,70b7ba7423778df1e80a820b83af68cda41978e1,,"[{'authorId': '2114012102', 'name': 'Ran Zhao'}, {'authorId': '1710287', 'name': 'A. Papangelis'}, {'authorId': '145431806', 'name': 'Justine Cassell'}]",122.0,"{'bibtex': '@Inproceedings{Zhao2014TowardsAD,\n author = {Ran Zhao and A. Papangelis and Justine Cassell},\n pages = {514-527},\n title = {Towards a Dyadic Computational Model of Rapport Management for Human-Virtual Agent Interaction},\n year = {2014}\n}\n'}",,{'pages': '514-527'},40.0,Towards a Dyadic Computational Model of Rapport Management for Human-Virtual Agent Interaction,2014.0
1362,70c93e5e38a8176590f69c0491fd63ab2a9e67c4,"Confirmation bias, as the term is typically used in the psychological literature, connotes the seeking or interpreting of evidence in ways that are partial to existing beliefs, expectations, or a hypothesis in hand. The author reviews evidence of such a bias in a variety of guises and gives examples of its operation in several practical contexts. Possible explanations are considered, and the question of its utility or disutility is discussed.","[{'authorId': '145898516', 'name': 'R. Nickerson'}]",5552.0,"{'bibtex': '@Article{Nickerson1998ConfirmationBA,\n author = {R. Nickerson},\n journal = {Review of General Psychology},\n pages = {175 - 220},\n title = {Confirmation Bias: A Ubiquitous Phenomenon in Many Guises},\n volume = {2},\n year = {1998}\n}\n'}",,"{'volume': '2', 'pages': '175 - 220', 'name': 'Review of General Psychology'}",323.0,Confirmation Bias: A Ubiquitous Phenomenon in Many Guises,1998.0
1363,70d4d8111fe5591dc38ac4335e494d34fbf7e695,"Constructing valid robotic models of social development requires that we accurately characterize the social learning and interaction that can take place between a robotic agent and a human adult. To that end, this study examined the effect of perceived attitudinal similarity on human-robot interaction. 28 participants rated toys by order of preference and then interacted with a small, socially-expressive robot to determine the robot's preferences for the same toys. The robot displayed either the same preferences as the participant or exactly the opposite preferences. Participants in the Similar-Preferences condition rated the robot as significantly friendlier than did participants in the Dissimilar-Preferences condition. However, there was no difference between conditions in how participants rated their enjoyment of the interaction. These findings have interesting implications for human-robot interaction studies in general, and for work in robotic models of developmental social cognition specifically.","[{'authorId': '34255085', 'name': 'Emily P. Bernier'}, {'authorId': '1792053', 'name': 'B. Scassellati'}]",35.0,"{'bibtex': '@Article{Bernier2010TheSE,\n author = {Emily P. Bernier and B. Scassellati},\n journal = {2010 IEEE 9th International Conference on Development and Learning},\n pages = {286-290},\n title = {The similarity-attraction effect in human-robot interaction},\n year = {2010}\n}\n'}",,"{'pages': '286-290', 'name': '2010 IEEE 9th International Conference on Development and Learning'}",15.0,The similarity-attraction effect in human-robot interaction,2010.0
1364,71054b3a96ca9feb4753749fabbe697145c0fdcf,,"[{'authorId': '2896960', 'name': 'J. Burgoon'}, {'authorId': '3061342', 'name': 'J. Bonito'}, {'authorId': '118258992', 'name': 'Bjorn Bengtsson'}, {'authorId': '12830628', 'name': 'C. Cederberg'}, {'authorId': '3478732', 'name': 'M. Lundeberg'}, {'authorId': '116526038', 'name': 'L. Allspach'}]",352.0,"{'bibtex': '@Inproceedings{Burgoon2000InteractivityIH,\n author = {J. Burgoon and J. Bonito and Bjorn Bengtsson and C. Cederberg and M. Lundeberg and L. Allspach},\n pages = {553-574},\n title = {Interactivity in human–computer interaction: a study of credibility, understanding, and influence},\n volume = {16},\n year = {2000}\n}\n'}",,"{'volume': '16', 'pages': '553-574', 'name': ''}",45.0,"Interactivity in human–computer interaction: a study of credibility, understanding, and influence",2000.0
1365,7133faf529e2ced7a2a69615ab048e148ba5cef3,"In two experiments, a total of 126 subjects judged the seven emotional expressions of Ekman & Friesen's (1976) pictures of facial affect presented upright or inverted. Inversion reduced accuracy for sad, fear, anger and disgust, and sad was identified as neutral. However, happy was identified almost perfectly on upright and inverted faces, and both anger and disgust were identified significantly often on inverted faces. In addition, the classic confusions between surprise and fear and between disgust and anger occurred on both upright and inverted faces. It is argued that expressions are difficult to identify on inverted faces when they are based on configural information. However, accurate performance on inverted faces and similar confusions on upright and inverted faces are due to componential processing.","[{'authorId': '145639166', 'name': 'S. McKelvie'}]",169.0,"{'bibtex': '@Article{McKelvie1995EmotionalEI,\n author = {S. McKelvie},\n journal = {The British journal of social psychology},\n pages = {\n          325-34\n        },\n title = {Emotional expression in upside-down faces: evidence for configurational and componential processing.},\n volume = {34 ( Pt 3)},\n year = {1995}\n}\n'}",,"{'volume': '34 ( Pt 3)', 'pages': '\n          325-34\n        ', 'name': 'The British journal of social psychology'}",0.0,Emotional expression in upside-down faces: evidence for configurational and componential processing.,1995.0
1366,713a59adab39245fcee72a11767ac68053a5eec1,"We have conducted a study analyzing motion capture data of bodily expressions of human emotions towards the goal of building a social expressive robot that interacts with and supports hospitalized children. Although modeling emotional expression (and recognition) in (by) robots in terms of discrete categories presents advantages such as ease and clarity of interpretation, our results show that this approach also poses a number of problems. The main issues relate to the loss of subtle expressions and feelings, individual features, context, and social interaction elements that are present in real life.","[{'authorId': '145620241', 'name': 'Matthew Lewis'}, {'authorId': '46219050', 'name': 'Lola Cañamero'}]",21.0,"{'bibtex': '@Article{Lewis2013AreDE,\n author = {Matthew Lewis and Lola Cañamero},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {97-102},\n title = {Are Discrete Emotions Useful in Human-Robot Interaction? Feedback from Motion Capture Analysis},\n year = {2013}\n}\n'}",,"{'pages': '97-102', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",12.0,Are Discrete Emotions Useful in Human-Robot Interaction? Feedback from Motion Capture Analysis,2013.0
1367,7142b58e2b74991b4b4090180648b4cf98426f44,"Two experiments focused on nonverbal mirroring in a group discussion. In Experiment 1, each participant interacted with two confederates. Confederates disagreed with each other and with the participant during discussion. One confederate mirrored the nonverbal behavior of the participant; the other did not. Participants rated the imitating confederate as more confident and persuasive. However, they were not more likely to change their viewpoint to match that of this confederate. Independent coders, unaware of the hypotheses, did not rate the two confederates as significantly different. In Experiment 2, each participant again interacted with two confederates. One confederate agreed with the participant during the discussion, and the other disagreed. One confederate rubbed his or her face during the discussion. The other shook his or her foot. The hypothesis that participants would be more likely to mirror the nonverbal behavior of the confederate who agreed with them during discussion received no support.","[{'authorId': '4931453', 'name': 'L. M. Swol'}]",58.0,"{'bibtex': '@Article{Swol2003TheEO,\n author = {L. M. Swol},\n journal = {Commun. Res.},\n pages = {461-480},\n title = {The Effects of Nonverbal Mirroring on Perceived Persuasiveness, Agreement with an Imitator, and Reciprocity in a Group Discussion},\n volume = {30},\n year = {2003}\n}\n'}",,"{'volume': '30', 'pages': '461-480', 'name': 'Commun. Res.'}",30.0,"The Effects of Nonverbal Mirroring on Perceived Persuasiveness, Agreement with an Imitator, and Reciprocity in a Group Discussion",2003.0
1368,7176ab988d90a6297ed8539f82b54bce5bebbfc3,,"[{'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '2710492', 'name': 'Maike Paetzel'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",7.0,"{'bibtex': '@Inproceedings{Perugia2020OnTR,\n author = {G. Perugia and Maike Paetzel and Ginevra Castellano},\n pages = {120-131},\n title = {On the Role of Personality and Empathy in Human-Human, Human-Agent, and Human-Robot Mimicry},\n year = {2020}\n}\n'}",,{'pages': '120-131'},44.0,"On the Role of Personality and Empathy in Human-Human, Human-Agent, and Human-Robot Mimicry",2020.0
1369,71ae7d2e8f070e51c7c3310e8cd3e6ab026245cf,"Lesion and imaging studies have implicated the ventromedial prefrontal cortex (vmPFC) in economic decisions and social interactions, yet its exact functions remain unclear. Here, we investigated the hypothesis that the vmPFC represents the subjective value or desirability of future outcomes during social decision-making. Both vmPFC-damaged patients and control participants acted as the responder in a single-round ultimatum game. To test outcome valuation, we contrasted concrete, immediately available gains with abstract, future ones. To test social valuation, we contrasted interactions with a human partner and those involving a computer. We found that, compared to controls, vmPFC patients substantially reduced their acceptance rate of unfair offers from a human partner, but only when financial gains were presented as abstract amounts to be received later. When the gains were visible and readily available, the vmPFC patients' acceptance of unfair offers was normal. Furthermore, unlike controls, vmPFC patients did not distinguish between unfair offers from a human agent and those from a computerized opponent. We conclude that the vmPFC encodes the expected value of abstract, future goals in a common neural currency that takes into account both reward and social signals in order to optimize economic decision-making.","[{'authorId': '46979318', 'name': 'L. Moretti'}, {'authorId': '3057223', 'name': 'D. Dragone'}, {'authorId': '31905076', 'name': 'G. Pellegrino'}]",112.0,"{'bibtex': '@Article{Moretti2009RewardAS,\n author = {L. Moretti and D. Dragone and G. Pellegrino},\n journal = {Journal of Cognitive Neuroscience},\n pages = {128-140},\n title = {Reward and Social Valuation Deficits following Ventromedial Prefrontal Damage},\n volume = {21},\n year = {2009}\n}\n'}",,"{'volume': '21', 'pages': '128-140', 'name': 'Journal of Cognitive Neuroscience'}",83.0,Reward and Social Valuation Deficits following Ventromedial Prefrontal Damage,2009.0
1370,71b7178df5d2b112d07e45038cb5637208659ff7,,"[{'authorId': '33493200', 'name': 'Tsung-Yi Lin'}, {'authorId': '145854440', 'name': 'M. Maire'}, {'authorId': '50172592', 'name': 'Serge J. Belongie'}, {'authorId': '48966748', 'name': 'James Hays'}, {'authorId': '1690922', 'name': 'P. Perona'}, {'authorId': '1770537', 'name': 'Deva Ramanan'}, {'authorId': '3127283', 'name': 'Piotr Dollár'}, {'authorId': '1699161', 'name': 'C. L. Zitnick'}]",31573.0,"{'bibtex': '@Inproceedings{Lin2014MicrosoftCC,\n author = {Tsung-Yi Lin and M. Maire and Serge J. Belongie and James Hays and P. Perona and Deva Ramanan and Piotr Dollár and C. L. Zitnick},\n pages = {740-755},\n title = {Microsoft COCO: Common Objects in Context},\n year = {2014}\n}\n'}",,{'pages': '740-755'},52.0,Microsoft COCO: Common Objects in Context,2014.0
1371,71bfcb2c163f42b03a8ac19cbbf9f0b9e1abd106,,"[{'authorId': '144658641', 'name': 'Tim Miller'}, {'authorId': '3199529', 'name': 'S. Pedell'}, {'authorId': '1401292565', 'name': 'A. Lopez-Lorca'}, {'authorId': '6857534', 'name': 'Antonette Mendoza'}, {'authorId': '145977411', 'name': 'L. Sterling'}, {'authorId': '3011330', 'name': 'Alen Keirnan'}]",90.0,"{'bibtex': '@Article{Miller2015EmotionledMF,\n author = {Tim Miller and S. Pedell and A. Lopez-Lorca and Antonette Mendoza and L. Sterling and Alen Keirnan},\n journal = {J. Syst. Softw.},\n pages = {54-71},\n title = {Emotion-led modelling for people-oriented requirements engineering: The case study of emergency systems},\n volume = {105},\n year = {2015}\n}\n'}",,"{'volume': '105', 'pages': '54-71', 'name': 'J. Syst. Softw.'}",70.0,Emotion-led modelling for people-oriented requirements engineering: The case study of emergency systems,2015.0
1372,71f8ad4c05896529cedfddb3356aaf27b3db2ab9,,"[{'authorId': '153845524', 'name': 'J. Lyons'}]",2.0,"{'bibtex': '@Inproceedings{Lyons2015THEMP,\n author = {J. Lyons},\n pages = {74-82},\n title = {THE MORAL PREMISE},\n year = {2015}\n}\n'}",,"{'volume': '', 'pages': '74-82', 'name': ''}",0.0,THE MORAL PREMISE,2015.0
1373,7225c3408bd6f8e80df0aeee9e0167a573ad3c0e,,"[{'authorId': '2230836', 'name': 'Tobias Baur'}, {'authorId': '3048626', 'name': 'Ionut Damian'}, {'authorId': '2565410', 'name': 'Florian Lingenfelser'}, {'authorId': '6164138', 'name': 'J. Wagner'}, {'authorId': '1742930', 'name': 'E. André'}]",41.0,"{'bibtex': '@Inproceedings{Baur2013NovAAA,\n author = {Tobias Baur and Ionut Damian and Florian Lingenfelser and J. Wagner and E. André},\n pages = {160-171},\n title = {NovA: Automated Analysis of Nonverbal Signals in Social Interactions},\n year = {2013}\n}\n'}",,{'pages': '160-171'},39.0,NovA: Automated Analysis of Nonverbal Signals in Social Interactions,2013.0
1374,724e1943cd21cc41cc517d8367889e28ceb5628a,"This paper reports our research efforts on social robots that recognize interpersonal relationships. These investigations are carried out by observing group behaviors while the robot interacts with people. Our humanoid robot interacts with children by speaking and making various gestures. It identifies individual children by using a wireless tag system, which helps to promote interaction such as the robot calling a child by name. Accordingly, the robot is capable of interacting with many children, causing spontaneous group behavior from the children around it. Here, group behavior is associated with social relationships among the children themselves. For example, a child may be accompanied by his or her friends and then play together with them. We propose the hypothesis that our interactive robot prompts a child’s friends to accompany him or her; thus, we can estimate their friendship by simply observing their accompanying behaviors.
 
We conducted a field experiment for two weeks in a Japanese elementary school to verify this hypothesis. In the experiment, two “Robovie” robots were placed where children could freely interact with them during recesses. As a result, we found that they mostly prompted friend-accompanying behavior. Moreover, we could estimate some of their friendly relationships, in particular among the children who often appeared around the robot. For example, we could estimate 5% of all friendships with 80% accuracy, and 15% of them with nearly 50% accuracy. Thus, this result basically supports our hypothesis on friendship estimation from an interactive humanoid robot. We believe that this ability to estimate human relationships is essential for robots to behave socially.","[{'authorId': '48309591', 'name': 'T. Kanda'}, {'authorId': '1687808', 'name': 'H. Ishiguro'}]",18.0,"{'bibtex': '@Article{Kanda2006AnAF,\n author = {T. Kanda and H. Ishiguro},\n journal = {Interaction Studies},\n pages = {369-403},\n title = {An approach for a social robot to understand human relationships: Friendship estimation through interaction with robots},\n volume = {7},\n year = {2006}\n}\n'}",,"{'volume': '7', 'pages': '369-403', 'name': 'Interaction Studies'}",48.0,An approach for a social robot to understand human relationships: Friendship estimation through interaction with robots,2006.0
1375,7254ce176d69ac8708217ab84ae34e0cdb10763a,,"[{'authorId': '2027127', 'name': 'M. Swain'}]",850.0,"{'bibtex': '@Inproceedings{Swain2005TheOH,\n author = {M. Swain},\n pages = {495-508},\n title = {The Output Hypothesis: Theory and Research},\n year = {2005}\n}\n'}",,"{'volume': '', 'pages': '495-508', 'name': ''}",0.0,The Output Hypothesis: Theory and Research,2005.0
1376,726fe9544b2e28133d32bc8fdcd967c1e6f89524,"In this article the role of different categories of postures in the detection, recognition, and interpretation of emotion in contextually rich scenarios, including ironic items, is investigated. Animated scenarios are designed with 3D virtual agents in order to test 3 conditions: In the “still” condition, the narrative content was accompanied by emotional facial expressions without any body movements; in the “idle” condition, emotionally neutral body movements were introduced; and in the “congruent” condition, emotional body postures congruent with the character's facial expressions were displayed. Those conditions were examined by 27 subjects, and their impact on the viewers’ attentional and emotional processes was assessed. The results highlight the importance of the contextual information to emotion recognition and irony interpretation. It is also shown that both idle and emotional postures improve the detection of emotional expressions. Moreover, emotional postures increase the perceived intensity of emotions and the realism of the animations.","[{'authorId': '1742939', 'name': 'S. Buisine'}, {'authorId': '3237926', 'name': 'M. Courgeon'}, {'authorId': '2058881883', 'name': 'Aurélien Charles'}, {'authorId': '1724799', 'name': 'C. Clavel'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}, {'authorId': '2057776275', 'name': 'Ning Tan'}, {'authorId': '2791712', 'name': 'O. Grynszpan'}]",29.0,"{'bibtex': '@Article{Buisine2014TheRO,\n author = {S. Buisine and M. Courgeon and Aurélien Charles and C. Clavel and Jean-Claude Martin and Ning Tan and O. Grynszpan},\n journal = {International Journal of Human–Computer Interaction},\n pages = {52 - 62},\n title = {The Role of Body Postures in the Recognition of Emotions in Contextually Rich Scenarios},\n volume = {30},\n year = {2014}\n}\n'}",,"{'volume': '30', 'pages': '52 - 62', 'name': 'International Journal of Human–Computer Interaction'}",61.0,The Role of Body Postures in the Recognition of Emotions in Contextually Rich Scenarios,2014.0
1377,72bb12707969ce9779c5229b7d3a480a2571471b,"The current study examined how assessments of copresence in an immersive virtual environment are influenced by variations in how much an embodied agent resembles a human being in appearance and behavior. We measured the extent to which virtual representations were both perceived and treated as if they were human via self-report, behavioral, and cognitive dependent measures. Distinctive patterns of findings emerged with respect to the behavior and appearance of embodied agents depending on the definition and operationalization of copresence. Independent and interactive effects for appearance and behavior were found suggesting that assessing the impact of behavioral realism on copresence without taking into account the appearance of the embodied agent (and vice versa) can lead to misleading conclusions. Consistent with the results of previous research, copresence was lowest when there was a large mismatch between the appearance and behavioral realism of an embodied agent.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '2396884', 'name': 'Kimberly R. Swinth'}, {'authorId': '32047738', 'name': 'Crystal L. Hoyt'}, {'authorId': '2380439', 'name': 'S. Persky'}, {'authorId': '50403647', 'name': 'Alex Dimov'}, {'authorId': '2307657', 'name': 'J. Blascovich'}]",366.0,"{'bibtex': '@Article{Bailenson2005TheIA,\n author = {J. Bailenson and Kimberly R. Swinth and Crystal L. Hoyt and S. Persky and Alex Dimov and J. Blascovich},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {379-393},\n title = {The Independent and Interactive Effects of Embodied-Agent Appearance and Behavior on Self-Report, Cognitive, and Behavioral Markers of Copresence in Immersive Virtual Environments},\n volume = {14},\n year = {2005}\n}\n'}",,"{'volume': '14', 'pages': '379-393', 'name': 'Presence: Teleoperators & Virtual Environments'}",44.0,"The Independent and Interactive Effects of Embodied-Agent Appearance and Behavior on Self-Report, Cognitive, and Behavioral Markers of Copresence in Immersive Virtual Environments",2005.0
1378,72d942abe12e74c521fb7304d359da9b0b70410f,"Darwin regarded emotions as predispositions to act adaptively, thereby suggesting that characteristic body movements are associated with each emotional state. To this date, investigations of emotional cognition have predominantly concentrated on processes associated with viewing facial expressions. However, expressive body movements may be just as important for understanding the neurobiology of emotional behavior. Here, we used functional MRI to clarify how the brain recognizes happiness or fear expressed by a whole body. Our results indicate that observing fearful body expressions produces increased activity in brain areas narrowly associated with emotional processes and that this emotion-related activity occurs together with activation of areas linked with representation of action and movement. The mechanism of fear contagion hereby suggested may automatically prepare the brain for action.","[{'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '143809175', 'name': 'Josh Snyder'}, {'authorId': '2064575931', 'name': 'Doug Greve'}, {'authorId': '2076692723', 'name': 'George Gerard'}, {'authorId': '2830760', 'name': 'N. Hadjikhani'}]",450.0,"{'bibtex': '@Article{Gelder2004FearFF,\n author = {B. de Gelder and Josh Snyder and Doug Greve and George Gerard and N. Hadjikhani},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n pages = {\n          16701-6\n        },\n title = {Fear fosters flight: a mechanism for fear contagion when perceiving emotion expressed by a whole body.},\n volume = {101 47},\n year = {2004}\n}\n'}",,"{'volume': '101 47', 'pages': '\n          16701-6\n        ', 'name': 'Proceedings of the National Academy of Sciences of the United States of America'}",69.0,Fear fosters flight: a mechanism for fear contagion when perceiving emotion expressed by a whole body.,2004.0
1379,72ecd8711e0cfaf356c55fcd7a415336cd504db1,"We propose a direct anthropomorphization to improve human-agent interaction. It agentizes an artifact by attaching humanoid parts to it. There have been many studies that can provide valuable information on using spoken directions and gestures via anthropomorphic agents such as CG (computer graphics) agents and communication robots. Our method directly anthropomorphizes the artifact through robotic bodily parts shaped like those of humans. An anthropomorphized artifact with these parts can provide information to people by giving them spoken directions and expressing themselves through body language. This makes people pay more attentions to the artifact, than when using anthropomorphic CG or robot agents. We conducted an experiment to verify the difference between receiving an explanation of the functions of the artifact using the direct anthropomorphization method and that from using the independent humanoid agent ""Robovie"". The results from participants' questionnaires and gazes during the experiment indicated that they noticed the target artifact and memorized the functions more quickly and easily from using the direct anthropomorphization method than from the ""Robovie"".","[{'authorId': '145978249', 'name': 'Hirotaka Osawa'}, {'authorId': '29348114', 'name': 'Yuji Matsuda'}, {'authorId': '34922599', 'name': 'Ren Ohmura'}, {'authorId': '1752970', 'name': 'M. Imai'}]",19.0,"{'bibtex': '@Article{Osawa2008EmbodimentOA,\n author = {Hirotaka Osawa and Yuji Matsuda and Ren Ohmura and M. Imai},\n journal = {2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},\n pages = {484-490},\n title = {Embodiment of an Agent by Anthropomorphization of a Common Object},\n volume = {2},\n year = {2008}\n}\n'}",,"{'volume': '2', 'pages': '484-490', 'name': '2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology'}",23.0,Embodiment of an Agent by Anthropomorphization of a Common Object,2008.0
1380,72f463e6a50bc9738a45aea57062b495331f4031,"We present SimSensei Kiosk, an implemented virtual human interviewer designed to create an engaging face-to-face interaction where the user feels comfortable talking and sharing information. SimSensei Kiosk is also designed to create interactional situations favorable to the automatic assessment of distress indicators, defined as verbal and nonverbal behaviors correlated with depression, anxiety or post-traumatic stress disorder (PTSD). In this paper, we summarize the design methodology, performed over the past two years, which is based on three main development cycles: (1) analysis of face-to-face human interactions to identify potential distress indicators, dialogue policies and virtual human gestures, (2) development and analysis of a Wizard-of-Oz prototype system where two human operators were deciding the spoken and gestural responses, and (3) development of a fully automatic virtual interviewer able to engage users in 15-25 minute interactions. We show the potential of our fully automatic virtual human interviewer in a user study, and situate its performance in relation to the Wizard-of-Oz prototype.","[{'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '2038490', 'name': 'Ron Artstein'}, {'authorId': '31624455', 'name': 'G. Benn'}, {'authorId': '2069141814', 'name': 'Teresa Dey'}, {'authorId': '2432742', 'name': 'Edward Fast'}, {'authorId': '49406416', 'name': 'Alesia Gainer'}, {'authorId': '3194430', 'name': 'Kallirroi Georgila'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '1930380', 'name': 'Margot Lhommet'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '2551269', 'name': 'Angela Nazarian'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '50562753', 'name': 'Apar Suri'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '2072346682', 'name': 'Rachel Wood'}, {'authorId': '1884967', 'name': 'Yuyu Xu'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",524.0,"{'bibtex': '@Inproceedings{DeVault2014SimSenseiKA,\n author = {D. DeVault and Ron Artstein and G. Benn and Teresa Dey and Edward Fast and Alesia Gainer and Kallirroi Georgila and J. Gratch and Arno Hartholt and Margot Lhommet and Gale M. Lucas and S. Marsella and Fabrizio Morbini and Angela Nazarian and Stefan Scherer and Giota Stratou and Apar Suri and D. Traum and Rachel Wood and Yuyu Xu and A. Rizzo and Louis-Philippe Morency},\n pages = {1061-1068},\n title = {SimSensei kiosk: a virtual human interviewer for healthcare decision support},\n year = {2014}\n}\n'}",,{'pages': '1061-1068'},40.0,SimSensei kiosk: a virtual human interviewer for healthcare decision support,2014.0
1385,73461123840be9cde94e6e5d0cbeca9ecc1d7c09,,"[{'authorId': '2116964984', 'name': 'J. Su'}, {'authorId': '4344061', 'name': 'D. Duan'}, {'authorId': '2108069824', 'name': 'Xuemin Zhang'}, {'authorId': '6767169', 'name': 'Huanyu Lei'}, {'authorId': '48585702', 'name': 'Chundi Wang'}, {'authorId': '2110655266', 'name': 'Heng Guo'}, {'authorId': '4765443', 'name': 'Xiaoqian Yan'}]",8.0,"{'bibtex': '@Article{Su2017TheEO,\n author = {J. Su and D. Duan and Xuemin Zhang and Huanyu Lei and Chundi Wang and Heng Guo and Xiaoqian Yan},\n journal = {Neuroscience Letters},\n pages = {15-20},\n title = {The effect of negative emotion on multiple object tracking task: An ERP study},\n volume = {641},\n year = {2017}\n}\n'}",,"{'volume': '641', 'pages': '15-20', 'name': 'Neuroscience Letters'}",33.0,The effect of negative emotion on multiple object tracking task: An ERP study,2017.0
1386,7352ed5024a83b1b289b481991ce5675967e8d4a,"Although robots are becoming an ever-growing presence in society, we do not hold the same expectations for robots as we do for humans, nor do we treat them the same. As such, the ability to recognize cues to human animacy is fundamental for guiding social interactions. We review literature that demonstrates cortical networks associated with person perception, action observation and mentalizing are sensitive to human animacy information. In addition, we show that most prior research has explored stimulus properties of artificial agents (humanness of appearance or motion), with less investigation into knowledge cues (whether an agent is believed to have human or artificial origins). Therefore, currently little is known about the relationship between stimulus and knowledge cues to human animacy in terms of cognitive and brain mechanisms. Using fMRI, an elaborate belief manipulation, and human and robot avatars, we found that knowledge cues to human animacy modulate engagement of person perception and mentalizing networks, while stimulus cues to human animacy had less impact on social brain networks. These findings demonstrate that self–other similarities are not only grounded in physical features but are also shaped by prior knowledge. More broadly, as artificial agents fulfil increasingly social roles, a challenge for roboticists will be to manage the impact of pre-conceived beliefs while optimizing human-like design.","[{'authorId': '1850742', 'name': 'Emily S. Cross'}, {'authorId': '144772502', 'name': 'Richard Ramsey'}, {'authorId': '1721092', 'name': 'Roman Liepelt'}, {'authorId': '144374107', 'name': 'W. Prinz'}, {'authorId': '145213359', 'name': 'A. Hamilton'}]",60.0,"{'bibtex': '@Article{Cross2016TheSO,\n author = {Emily S. Cross and Richard Ramsey and Roman Liepelt and W. Prinz and A. Hamilton},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n title = {The shaping of social perception by stimulus and knowledge cues to human animacy},\n volume = {371},\n year = {2016}\n}\n'}",,"{'volume': '371', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",78.0,The shaping of social perception by stimulus and knowledge cues to human animacy,2016.0
1387,73b0ded0ea6704dd0e02a60b4693891ece22d8b2,,[],144.0,"{'bibtex': '@Misc{None,\n title = {EUROGRAPHICS 2006 STAR – State of The Art Report Building Expression into Virtual Characters}\n}\n'}",,,0.0,EUROGRAPHICS 2006 STAR – State of The Art Report Building Expression into Virtual Characters,
1391,73bd1e471a222aa14cdfffd769f6a4af84fd2290,,"[{'authorId': '2080636017', 'name': 'Brenda Volkaert'}, {'authorId': '4742900', 'name': 'Laura Wante'}, {'authorId': '49147545', 'name': 'M. Van Beveren'}, {'authorId': '47691208', 'name': 'L. Vervoort'}, {'authorId': '3768756', 'name': 'C. Braet'}]",19.0,"{'bibtex': '@Article{Volkaert2019TrainingAE,\n author = {Brenda Volkaert and Laura Wante and M. Van Beveren and L. Vervoort and C. Braet},\n journal = {Cognitive Therapy and Research},\n pages = {678-696},\n title = {Training Adaptive Emotion Regulation Skills in Early Adolescents: The Effects of Distraction, Acceptance, Cognitive Reappraisal, and Problem Solving},\n volume = {44},\n year = {2019}\n}\n'}",,"{'volume': '44', 'pages': '678-696', 'name': 'Cognitive Therapy and Research'}",115.0,"Training Adaptive Emotion Regulation Skills in Early Adolescents: The Effects of Distraction, Acceptance, Cognitive Reappraisal, and Problem Solving",2019.0
1392,73db90c046b72db8a17b36135fdb46f60095e6a9,,"[{'authorId': '6426903', 'name': 'G. Glavin'}]",319.0,"{'bibtex': '@Article{Glavin1985StressAB,\n author = {G. Glavin},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {233-243},\n title = {Stress and brain noradrenaline: A review},\n volume = {9},\n year = {1985}\n}\n'}",,"{'volume': '9', 'pages': '233-243', 'name': 'Neuroscience & Biobehavioral Reviews'}",66.0,Stress and brain noradrenaline: A review,1985.0
1393,74052224879bbdf1323c3bde8ad2cac85912bdd5,"Facial expressions have long been considered the ""universal language of emotion."" Yet consistent cultural differences in the recognition of facial expressions contradict such notions (e.g., R. E. Jack, C. Blais, C. Scheepers, P. G. Schyns, & R. Caldara, 2009). Rather, culture--as an intricate system of social concepts and beliefs--could generate different expectations (i.e., internal representations) of facial expression signals. To investigate, they used a powerful psychophysical technique (reverse correlation) to estimate the observer-specific internal representations of the 6 basic facial expressions of emotion (i.e., happy, surprise, fear, disgust, anger, and sad) in two culturally distinct groups (i.e., Western Caucasian [WC] and East Asian [EA]). Using complementary statistical image analyses, cultural specificity was directly revealed in these representations. Specifically, whereas WC internal representations predominantly featured the eyebrows and mouth, EA internal representations showed a preference for expressive information in the eye region. Closer inspection of the EA observer preference revealed a surprising feature: changes of gaze direction, shown primarily among the EA group. For the first time, it is revealed directly that culture can finely shape the internal representations of common facial expressions of emotion, challenging notions of a biologically hardwired ""universal language of emotion.""","[{'authorId': '2143019', 'name': 'Rachael E. Jack'}, {'authorId': '3489043', 'name': 'R. Caldara'}, {'authorId': '2287417', 'name': 'P. Schyns'}]",234.0,"{'bibtex': '@Article{Jack2012InternalRR,\n author = {Rachael E. Jack and R. Caldara and P. Schyns},\n journal = {Journal of experimental psychology. General},\n pages = {\n          19-25\n        },\n title = {Internal representations reveal cultural diversity in expectations of facial expressions of emotion.},\n volume = {141 1},\n year = {2012}\n}\n'}",,"{'volume': '141 1', 'pages': '\n          19-25\n        ', 'name': 'Journal of experimental psychology. General'}",31.0,Internal representations reveal cultural diversity in expectations of facial expressions of emotion.,2012.0
1394,744311d96e757e4f5a05c56031e738501ffb92e8,,"[{'authorId': '145091664', 'name': 'L. Steels'}, {'authorId': '145005045', 'name': 'A. Campbell'}]",454.0,"{'bibtex': '@Inproceedings{Steels1995ProgressIA,\n author = {L. Steels and A. Campbell},\n title = {Progress in Artificial Intelligence},\n volume = {990},\n year = {1995}\n}\n'}",,{'volume': '990'},0.0,Progress in Artificial Intelligence,1995.0
1395,74b45bcb68590e8387f230f7dd882813432e6336,,"[{'authorId': '5375813', 'name': 'H. Snellen'}]",55.0,"{'bibtex': '@Article{Snellen1863ArtXF,\n author = {H. Snellen},\n journal = {The American Journal of the Medical Sciences},\n pages = {520},\n title = {Art. XXIV.—Test-Types for the Determination of the Acuteness of Vision.},\n volume = {44},\n year = {1863}\n}\n'}",,"{'volume': '44', 'pages': '520', 'name': 'The American Journal of the Medical Sciences'}",0.0,Art. XXIV.—Test-Types for the Determination of the Acuteness of Vision.,1863.0
1396,74c9dc1d9c5fe5c5bfc7b2db7d96aad0d00bb0fc,"Embodied conversational agents (ECAs) benefit from non-verbal behavior for natural and efficient interaction with users. Gesticulation - hand and arm movements accompanying speech - is an essential part of non-verbal behavior. Gesture generation models have been developed for several decades: starting with rule-based and ending with mainly data-driven methods. To date, recent end-to-end gesture generation methods have not been evaluated in a real-time interaction with users. We present a proof-of-concept framework, which is intended to facilitate evaluation of modern gesture generation models in interaction. We demonstrate an extensible open-source framework that contains three components: 1) a 3D interactive agent; 2) a chatbot backend; 3) a gesticulating system. Each component can be replaced, making the proposed framework applicable for investigating the effect of different gesturing models in real-time interactions with different communication modalities, chatbot backends, or different agent appearances. The code and video are available at the project page https://nagyrajmund.github.io/project/gesturebot.","[{'authorId': '2051502134', 'name': 'Rajmund Nagy'}, {'authorId': '145372964', 'name': 'Taras Kucherenko'}, {'authorId': '117376362', 'name': 'Birger Moell'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '9167315', 'name': 'Hedvig Kjellstrom'}, {'authorId': '2964656', 'name': 'Ulysses Bernardet'}]",6.0,"{'bibtex': '@Inproceedings{Nagy2021AFF,\n author = {Rajmund Nagy and Taras Kucherenko and Birger Moell and André Pereira and Hedvig Kjellstrom and Ulysses Bernardet},\n pages = {1779-1781},\n title = {A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents},\n year = {2021}\n}\n'}",,{'pages': '1779-1781'},23.0,A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents,2021.0
1397,74d9c58c2f238b66a8f842801e14aa3d3406c250,"Recently, scholars from a wide variety of disciplines, using a variety of scientific techniques, have begun to study the influence of attention, facial mimicry, and social context on emotional contagion. In this paper we will review the classic evidence documenting the role of attention, facial mimicry, and feedback in sparking primitive emotional contagion. Then we will discuss the new evidence which scholars have amassed to help us better understand the role of facial mimicry in fostering contagion and the ability to “read” others’ thoughts, feelings, and emotions. Finally, we will briefly speculate as to where future research might be headed.","[{'authorId': '48279878', 'name': 'E. Hatfield'}, {'authorId': '52403871', 'name': 'L. Bensman'}, {'authorId': '2067418271', 'name': 'Paul Thornton'}, {'authorId': '8611261', 'name': 'Richard L. Rapson'}]",188.0,"{'bibtex': '@Article{Hatfield2014NewPO,\n author = {E. Hatfield and L. Bensman and Paul Thornton and Richard L. Rapson},\n journal = {Interpersona: an international journal on personal relationships},\n pages = {159-179},\n title = {New Perspectives on Emotional Contagion: A Review of Classic and Recent Research on Facial Mimicry and Contagion},\n volume = {8},\n year = {2014}\n}\n'}",,"{'volume': '8', 'pages': '159-179', 'name': 'Interpersona: an international journal on personal relationships'}",114.0,New Perspectives on Emotional Contagion: A Review of Classic and Recent Research on Facial Mimicry and Contagion,2014.0
1398,74fdaeb2678aba886c3d899f66b4197b901483d7,Deep neural networks (DNNs) are widely used in machine translation (MT). This article gives an overview of DNN applications in various aspects of MT.,"[{'authorId': '38358352', 'name': 'Jiajun Zhang'}, {'authorId': '2423168', 'name': 'Chengqing Zong'}]",183.0,"{'bibtex': '@Article{Zhang2015DeepNN,\n author = {Jiajun Zhang and Chengqing Zong},\n journal = {IEEE Intelligent Systems},\n pages = {16-25},\n title = {Deep Neural Networks in Machine Translation: An Overview},\n volume = {30},\n year = {2015}\n}\n'}",,"{'volume': '30', 'pages': '16-25', 'name': 'IEEE Intelligent Systems'}",32.0,Deep Neural Networks in Machine Translation: An Overview,2015.0
1399,7518aca48e6b8cd2257c98b64ed7ee3db69c0c1a,,"[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}]",563.0,"{'bibtex': '@Inproceedings{Ekman2019FacialAC,\n author = {P. Ekman and Wallace V. Friesen},\n title = {Facial action coding system},\n year = {2019}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Facial action coding system,2019.0
1400,75259e9c2b49b0df8f7810ad5cef80c848b0af82,,"[{'authorId': '48644058', 'name': 'Yang Hongwei'}, {'authorId': '31151520', 'name': 'Pan Zhigeng'}, {'authorId': '1411327472', 'name': 'Liu Gengdai'}]",11.0,"{'bibtex': '@Article{Hongwei2008ACC,\n author = {Yang Hongwei and Pan Zhigeng and Liu Gengdai},\n journal = {Journal of Computer Research and Development},\n pages = {579},\n title = {A Comprehensive Computational Model of Emotions},\n volume = {45},\n year = {2008}\n}\n'}",,"{'volume': '45', 'pages': '579', 'name': 'Journal of Computer Research and Development'}",0.0,A Comprehensive Computational Model of Emotions,2008.0
1403,7527ea4b387d5030a8af0e72157da28f2bbc65ee,,"[{'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '2642748', 'name': 'Bernd Tietz'}, {'authorId': '2487649', 'name': 'G. Bente'}]",52.0,"{'bibtex': '@Inproceedings{Krämer2003EffectsOE,\n author = {N. Krämer and Bernd Tietz and G. Bente},\n pages = {292-300},\n title = {Effects of Embodied Interface Agents and Their Gestural Activity},\n year = {2003}\n}\n'}",,{'pages': '292-300'},16.0,Effects of Embodied Interface Agents and Their Gestural Activity,2003.0
1404,75308067ddd3c53721430d7984295838c81d4106,"Facial expressions of emotion provide relevant cues for understanding social interactions and the affective processes involved in emotion perception. Virtual human faces are useful for conducting controlled experiments. However, little is known regarding the possible differences between physiological responses elicited by virtual versus real human facial expressions. The aim of the current study was to determine if virtual and real emotional faces elicit the same rapid facial reactions for the perception of facial expressions of joy, anger, and sadness. Facial electromyography (corrugator supercilii, zygomaticus major, and depressor anguli) was recorded in 30 participants during the presentation of dynamic or static and virtual or real faces. For the perception of dynamic facial expressions of joy and anger, analyses of electromyography data revealed that rapid facial reactions were stronger when participants were presented with real faces compared with virtual faces. These results suggest that the processes underlying the perception of virtual versus real emotional faces might differ.","[{'authorId': '2560627', 'name': 'L. Philip'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}, {'authorId': '1724799', 'name': 'C. Clavel'}]",15.0,"{'bibtex': '@Article{Philip2018RapidFR,\n author = {L. Philip and Jean-Claude Martin and C. Clavel},\n journal = {i-Perception},\n title = {Rapid Facial Reactions in Response to Facial Expressions of Emotion Displayed by Real Versus Virtual Faces},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'name': 'i-Perception'}",54.0,Rapid Facial Reactions in Response to Facial Expressions of Emotion Displayed by Real Versus Virtual Faces,2018.0
1405,755551038d6bb1749b0d5deddc38f482988c632d,"It is evident that the classification of personality disorder is shifting toward a dimensional trait model and, more specifically, the five-factor model (FFM). The purpose of this paper is to provide an overview of the FFM of personality disorder. It will begin with a description of this dimensional model of normal and abnormal personality functioning, followed by a comparison with a proposal for future revisions to DSM-5 and a discussion of its potential advantages as an integrative hierarchical model of normal and abnormal personality structure.","[{'authorId': '4503414', 'name': 'T. Trull'}, {'authorId': '5197542', 'name': 'T. Widiger'}]",157.0,"{'bibtex': '@Article{Trull2013DimensionalMO,\n author = {T. Trull and T. Widiger},\n journal = {Dialogues in Clinical Neuroscience},\n pages = {135 - 146},\n title = {Dimensional models of personality: the five-factor model and the DSM-5},\n volume = {15},\n year = {2013}\n}\n'}",,"{'volume': '15', 'pages': '135 - 146', 'name': 'Dialogues in Clinical Neuroscience'}",99.0,Dimensional models of personality: the five-factor model and the DSM-5,2013.0
1406,75b3ef23d97b10b4599d7efa3c3084407dbc522c,"This study examined the relationship of trust to self-disclosure. A measure of individualized trust was developed and used in conjunction with a multidimensional measure of disclosure to reassess the relationship between the two. A modest, linear relationship between individualized trust and various dimensions of self-disclosure was discovered. Moreover, a higher level of trust (as opposed to lesser trust as well as distrust) was found to be associated with more consciously intended disclosure and a greater amount of disclosure.","[{'authorId': '118894644', 'name': 'Lawrence R. Wheeless'}, {'authorId': '114495308', 'name': 'Janis Grotz'}]",453.0,"{'bibtex': '@Article{Wheeless1977THEMO,\n author = {Lawrence R. Wheeless and Janis Grotz},\n journal = {Human Communication Research},\n pages = {250-257},\n title = {THE MEASUREMENT OF TRUST AND ITS RELATIONSHIP TO SELF‐DISCLOSURE},\n volume = {3},\n year = {1977}\n}\n'}",,"{'volume': '3', 'pages': '250-257', 'name': 'Human Communication Research'}",23.0,THE MEASUREMENT OF TRUST AND ITS RELATIONSHIP TO SELF‐DISCLOSURE,1977.0
1407,764e1560eb5a6d05cfca459adf52a8fbd989509a,"Over the past decade, computer games and other interactive technologies have shown great potential when used in innovative ways to enhance learning. It is now known that learning is associated not only with cognitive ability but also with affect. The incorporation of affective embodied pedagogical agents (EPAs) in computer programs for learning can significantly influence learner beliefs and efficacy. However, there have been a number of criticisms and contradictory empirical findings calling into question earlier results and theoretical claims on the effectiveness of using EPAs. Therefore, this paper reviewed studies that evaluated the effectiveness of affect in EPAs in a computer-based learning environment (CLE). The statistical meta-analytic review (k = 30, n = 2,150) found that the use of affect in EPAs has a significant and moderate impact (r = .35) on students’ learning motivation and a relatively smaller impact on knowledge retention (r = .29) and knowledge transfer (r = .26). The effect sizes of the three outcome variables in this review are stronger compared with previous meta-analysis on the impact of the embodiment level of EPAs, indicating that the use of affect is more desirable and effective in CLEs. Finally, implications for both designers and researchers of using affective EPAs in CLEs are discussed.","[{'authorId': '1799289', 'name': 'Yanru Guo'}, {'authorId': '1731861', 'name': 'D. Goh'}]",32.0,"{'bibtex': '@Article{Guo2015AffectIE,\n author = {Yanru Guo and D. Goh},\n journal = {Journal of Educational Computing Research},\n pages = {124 - 149},\n title = {Affect in Embodied Pedagogical Agents},\n volume = {53},\n year = {2015}\n}\n'}",,"{'volume': '53', 'pages': '124 - 149', 'name': 'Journal of Educational Computing Research'}",67.0,Affect in Embodied Pedagogical Agents,2015.0
1408,7657870b28a5386ce22a8219e904e4d76a9ee63b,"With the advancements in AI, agents (i.e., smart products, robots, software agents) are increasingly capable of working closely together with humans in a variety of ways while benefiting from each other. These human-agent collaborations have gained growing attention in the HCI community; however, the field lacks clear guidelines on how to design the agents’ behaviors in collaborations. In this paper, the qualities that are relevant for designers to create robust and pleasant human-agent collaborations were investigated. Bratman's Shared Cooperative Activity framework was used to identify the core characteristics of collaborations and survey the most important issues in the design of human-agent collaborations, namely code-of-conduct, task delegation, autonomy and control, intelligibility, common ground, offering help and requesting help. The aim of this work is to add structure to this growing and important facet of HCI research and operationalize the concept of human-agent collaboration with concrete design considerations.","[{'authorId': '3396650', 'name': 'Nazli Cila'}]",14.0,"{'bibtex': '@Article{Cila2022DesigningHC,\n author = {Nazli Cila},\n journal = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},\n title = {Designing Human-Agent Collaborations: Commitment, responsiveness, and support},\n year = {2022}\n}\n'}",,{'name': 'Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems'},176.0,"Designing Human-Agent Collaborations: Commitment, responsiveness, and support",2022.0
1409,76ad25c4d0693d17710fb4f334c6ef68b732a624,"One of life's great challenges is successfully regulating emotions. Do some emotion regulation strategies have more to recommend them than others? According to Gross's (1998, Review of General Psychology, 2, 271-299) process model of emotion regulation, strategies that act early in the emotion-generative process should have a different profile of consequences than strategies that act later on. This review focuses on two commonly used strategies for down-regulating emotion. The first, reappraisal, comes early in the emotion-generative process. It consists of changing the way a situation is construed so as to decrease its emotional impact. The second, suppression, comes later in the emotion-generative process. It consists of inhibiting the outward signs of inner feelings. Experimental and individual-difference studies find reappraisal is often more effective than suppression. Reappraisal decreases emotion experience and behavioral expression, and has no impact on memory. By contrast, suppression decreases behavioral expression, but fails to decrease emotion experience, and actually impairs memory. Suppression also increases physiological responding for suppressors and their social partners. This review concludes with a consideration of five important directions for future research on emotion regulation processes.","[{'authorId': '1775321', 'name': 'J. Gross'}]",3716.0,"{'bibtex': '@Article{Gross2002EmotionRA,\n author = {J. Gross},\n journal = {Psychophysiology},\n pages = {\n          281-91\n        },\n title = {Emotion regulation: affective, cognitive, and social consequences.},\n volume = {39 3},\n year = {2002}\n}\n'}",,"{'volume': '39 3', 'pages': '\n          281-91\n        ', 'name': 'Psychophysiology'}",83.0,"Emotion regulation: affective, cognitive, and social consequences.",2002.0
1410,76cc49341006018c2c9d12d6a9b4092316900b17,,"[{'authorId': '144157104', 'name': 'Catherine Brennan'}]",265.0,"{'bibtex': '@Article{Brennan2020Status,\n author = {Catherine Brennan},\n journal = {Max Weber on Power and Social Stratification},\n title = {Status},\n year = {2020}\n}\n'}",,{'name': 'Max Weber on Power and Social Stratification'},0.0,Status,2020.0
1411,76d3c851ab8a6d028d0047a8e6d0d999949ea44e,,"[{'authorId': '32733837', 'name': 'H. Wilson'}, {'authorId': '2042936', 'name': 'Gunter Loffler'}, {'authorId': '2040767', 'name': 'F. Wilkinson'}]",183.0,"{'bibtex': '@Article{Wilson2002SyntheticFF,\n author = {H. Wilson and Gunter Loffler and F. Wilkinson},\n journal = {Vision Research},\n pages = {2909-2923},\n title = {Synthetic faces, face cubes, and the geometry of face space},\n volume = {42},\n year = {2002}\n}\n'}",,"{'volume': '42', 'pages': '2909-2923', 'name': 'Vision Research'}",67.0,"Synthetic faces, face cubes, and the geometry of face space",2002.0
1412,772ce66315c2f6d0108e6404ce91034fae3a37ae,"The expression of emotion can play a significant role in strategic decision-making. In this study, we hypothesized that emotion expression alters behavior in morally charged negotiation. We investigated the impact of facial displays of discrete emotions, specifically anger and sadness, in a morally charged multi-issue negotiation task. Our results indicate that if a negotiator associated moral significance to the object of the negotiation, displays of anger resulted in reduced concession making whereas displays of sadness increased concession making. Moral significance of the issues fostered an emotional matching mechanism of sorrow, where a sorrow expression from one party elicited a sorrow expression from the other. Taken together, the results indicate that emotional expressions can affect morally charged negotiation in ways that can inhibit as well as promote cooperation.","[{'authorId': '145707560', 'name': 'Morteza Dehghani'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",34.0,"{'bibtex': '@Article{Dehghani2014InterpersonalEO,\n author = {Morteza Dehghani and P. Carnevale and J. Gratch},\n journal = {Judgment and Decision Making},\n title = {Interpersonal effects of expressed anger and sorrow in morally charged negotiation},\n year = {2014}\n}\n'}",,{'name': 'Judgment and Decision Making'},46.0,Interpersonal effects of expressed anger and sorrow in morally charged negotiation,2014.0
1413,772fdadb73f9c62d47b855539b1d6840f5c3e0de,,"[{'authorId': '2113606401', 'name': 'Keith Anderson'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '2230836', 'name': 'Tobias Baur'}, {'authorId': '2282728', 'name': 'S. Bernardini'}, {'authorId': '40325099', 'name': 'Mathieu Chollet'}, {'authorId': '2111629', 'name': 'E. Chryssafidou'}, {'authorId': '3048626', 'name': 'Ionut Damian'}, {'authorId': '31894925', 'name': 'Cathy Ennis'}, {'authorId': '2479558', 'name': 'A. Egges'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '31600786', 'name': 'H. Jones'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1400276226', 'name': 'K. Porayska-Pomsta'}, {'authorId': '1773424', 'name': 'Paola Rizzo'}, {'authorId': '1731432', 'name': 'N. Sabouret'}]",125.0,"{'bibtex': '@Inproceedings{Anderson2013TheTF,\n author = {Keith Anderson and E. André and Tobias Baur and S. Bernardini and Mathieu Chollet and E. Chryssafidou and Ionut Damian and Cathy Ennis and A. Egges and Patrick Gebhard and H. Jones and M. Ochs and C. Pelachaud and K. Porayska-Pomsta and Paola Rizzo and N. Sabouret},\n pages = {476-491},\n title = {The TARDIS Framework: Intelligent Virtual Agents for Social Coaching in Job Interviews},\n year = {2013}\n}\n'}",,{'pages': '476-491'},37.0,The TARDIS Framework: Intelligent Virtual Agents for Social Coaching in Job Interviews,2013.0
1415,775bba31770cebbf8a81ac08e34f6c3471388361,"PRIMER is a proof-of-concept system designed to show the potential of immersive dialogue agents and virtual environments that adapt and respond to both direct verbal input and indirect emotional input. The system has two novel interfaces: (1) for the user, an immersive VR environment and an animated virtual agent both of which adapt and react to the user’s direct input as well as the user’s perceived emotional state, and (2) for an observer, an interface that helps track the perceived emotional state of the user, with visualizations to provide insight into the system’s decision making process. While the basic system architecture can be adapted for many potential real world applications, the initial version of this system was designed to assist clinical social workers in helping children cope with bullying. The virtual agent produces verbal and non-verbal behaviors guided by a plan for the counseling session, based on in-depth discussions with experienced counselors, but is also reactive to both initiatives that the user takes, e.g. asking their own questions, and the user’s perceived emotional state.","[{'authorId': '40325729', 'name': 'Carla Gordon'}, {'authorId': '3201827', 'name': 'A. Leuski'}, {'authorId': '31624455', 'name': 'G. Benn'}, {'authorId': '144984764', 'name': 'E. Klassen'}, {'authorId': '2432742', 'name': 'Edward Fast'}, {'authorId': '2138666', 'name': 'Matt Liewer'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '144518646', 'name': 'D. Traum'}]",11.0,"{'bibtex': '@Inproceedings{Gordon2019PRIMERAE,\n author = {Carla Gordon and A. Leuski and G. Benn and E. Klassen and Edward Fast and Matt Liewer and Arno Hartholt and D. Traum},\n title = {PRIMER: An Emotionally Aware Virtual Agent},\n year = {2019}\n}\n'}",,"{'volume': '', 'name': ''}",12.0,PRIMER: An Emotionally Aware Virtual Agent,2019.0
1416,776ab65a1caac831574f2a034110c841a44e1e90,,"[{'authorId': '1402222068', 'name': 'Enilda Romero-Hall'}]",3.0,"{'bibtex': '@Inproceedings{Romero-Hall2016AnimatedPA,\n author = {Enilda Romero-Hall},\n pages = {225-237},\n title = {Animated Pedagogical Agents and Emotion},\n year = {2016}\n}\n'}",,"{'volume': '', 'pages': '225-237', 'name': ''}",36.0,Animated Pedagogical Agents and Emotion,2016.0
1417,778e341d5440bda2202971e198809d0ec43a33c2,"We present an interactive algorithm to model physics-based interactions in multi-agent simulations. Our approach is capable of modeling both physical forces and interactions between agents and obstacles, while allowing the agents to anticipate and avoid collisions for local navigation. We combine velocity-based collision-avoidance algorithms with external physical forces. The overall formulation can approximately simulate various physical effects, including collisions, pushing, deceleration and resistive forces. We have integrated our approach with an open-source physics engine and use the resulting system to model plausible behaviors of and interactions among large numbers of agents in dense environments. Our algorithm can simulate a few thousand agents at interactive rates and can generate many emergent behaviors. The overall approach is useful for interactive applications that require plausible physical behavior, including games and virtual worlds.","[{'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '35170565', 'name': 'S. Guy'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",48.0,"{'bibtex': '@Inproceedings{Kim2013VelocitybasedMO,\n author = {Sujeong Kim and S. Guy and Dinesh Manocha},\n pages = {125-133},\n title = {Velocity-based modeling of physical interactions in multi-agent simulations},\n year = {2013}\n}\n'}",,{'pages': '125-133'},36.0,Velocity-based modeling of physical interactions in multi-agent simulations,2013.0
1418,77b77b5f8e7a3aa01a325c19c15c321da22150da,"ChatBot has potential as a language learning tool, especially for learning Chinese vocabulary. This study aimed to investigate the impact of using a newly developed ChatBot to learn Chinese vocabulary by comparing how it works in different learning environments and to explore the ChatBot with reference to the Technology Acceptance Model (TAM). This study was conducted with 58 students divided into two independent groups. The control group used ChatBot in a one-on-many classroom. The experimental group applied the ChatBot in one-on-one tutor sessions. A pretest and a posttest were used to measure the effect of the ChatBot, while TAM was explored through questionnaire and interview. Data analysis includes a paired-sample t test, analysis of covariance, and levels of effect. The results indicated that the ChatBot significantly improved the students’ learning achievement and that having a one-on-one environment may lead to better outcome than what could be achieved in a classroom. The TAM model was tested using partial least square. The result showed that perceived usefulness was the predictor of behavioral intention, whereas perceived ease of use was not. The students agreed that the ChatBot benefited their learning of Chinese vocabulary, with several adjustments need to be made for further progress.","[{'authorId': '2145278581', 'name': 'Hsiu-Ling Chen'}, {'authorId': '1844297259', 'name': 'Gracia Vicki Widarso'}, {'authorId': '32143875', 'name': 'H. Sutrisno'}]",56.0,"{'bibtex': '@Article{Chen2020ACF,\n author = {Hsiu-Ling Chen and Gracia Vicki Widarso and H. Sutrisno},\n journal = {Journal of Educational Computing Research},\n pages = {1161 - 1189},\n title = {A ChatBot for Learning Chinese: Learning Achievement and Technology Acceptance},\n volume = {58},\n year = {2020}\n}\n'}",,"{'volume': '58', 'pages': '1161 - 1189', 'name': 'Journal of Educational Computing Research'}",62.0,A ChatBot for Learning Chinese: Learning Achievement and Technology Acceptance,2020.0
1419,77f6e019783cc34aa586da8283a77ac1d48ca03e,,"[{'authorId': '3358799', 'name': 'Steve Yohanan'}, {'authorId': '1796517', 'name': 'Karon E Maclean'}]",239.0,"{'bibtex': '@Article{Yohanan2012TheRO,\n author = {Steve Yohanan and Karon E Maclean},\n journal = {International Journal of Social Robotics},\n pages = {163-180},\n title = {The Role of Affective Touch in Human-Robot Interaction: Human Intent and Expectations in Touching the Haptic Creature},\n volume = {4},\n year = {2012}\n}\n'}",,"{'volume': '4', 'pages': '163-180', 'name': 'International Journal of Social Robotics'}",60.0,The Role of Affective Touch in Human-Robot Interaction: Human Intent and Expectations in Touching the Haptic Creature,2012.0
1420,7809503308492b62e67d125b9c13f5a706b39171,"Emotional intelligence is an important branch of artificial intelligence, it is an important way to achieve the truly intelligent robot. This paper regards an Agent living in a virtual environment as the research object and proposes a fuzzy emotion model based on particle system, and combines the feedback signal of emotion with associative learning mechanism based on CMAC neural network. Finally, combined the environment, emotion model and CMAC network we propose a new Agent's behavior decision-making model based on artificial emotion, the results show that this model can improve the performance of the agent.","[{'authorId': '3174138', 'name': 'Fuping Yang'}, {'authorId': '2053079410', 'name': 'Xuewen Zhen'}]",6.0,"{'bibtex': ""@Article{Yang2014ResearchOT,\n author = {Fuping Yang and Xuewen Zhen},\n journal = {The Journal of Information and Computational Science},\n pages = {2723-2733},\n title = {Research on the Agent's Behavior Decision-making Based on Artificial Emotion},\n volume = {11},\n year = {2014}\n}\n""}","[{'paperId': '2f49c1af268c60cffdc962a601f3a85f5e58b52c', 'title': 'Emotions in the models of Artificial Intelligence'}, {'paperId': '194a68c44be44c87d916520c76f0f7a8865ac449', 'title': 'Behavior Models of Emotion-Featured Robots: A Survey'}, {'paperId': 'e5f2b49abbb27c55b972b2fc252f35bcf6350a10', 'title': 'Behavior Models of Emotion-Featured Robots: A Survey'}, {'paperId': '7b4585bd5d6264a8c949108297c079dfd5784d75', 'title': 'Tendencies and Perspectives of the Emotions Usage in Robotics'}, {'paperId': '20a1e72a36153d5ae1120159b261dcbbcbc536ca', 'title': 'An extended Q learning system with emotion state to make up an agent with individuality'}, {'paperId': '66a9c93892c5d69de50bd1a3a45ddddac118953f', 'title': 'An Emotional Robot'}]","{'name': 'The Journal of Information and Computational Science', 'pages': '2723-2733', 'volume': '11'}",0.0,Research on the Agent's Behavior Decision-making Based on Artificial Emotion,2014.0
1421,7829c1be330980199cbe1b4e043f1254df84833b,"Intelligent behaviours have been traditionally considered exclusively as products of pure rationality processes. Considering other influential factors, such as emotions, has been accused of being “non scientific”. However, pure rational processes fail when trying to explain most of human behaviours, in which emotion plays a key role. Nevertheless, emotional factors add an extra complexity to agent architectures, making them, hitherto, either few efficient or few reusable. This paper presents a context independent cognitive architecture for agents that combine emotions and rationality, named COGNITIVA, which bets on adaptivity as a weapon to fight against that complexity. Key-Words: Cognitive agent architecture, specification process, emotion, personality, virtual characters, adaptivity 1 The Role of Emotions in the Rational Process 1.1 Emotion and Reason, Oil and Water? Emotions and reason have been traditionally considered as two sides of the same coin and, therefore, antagonistic and non combinable. Emotions are something rather irrational that plays down value to human rationality [1], something “non scientific” [2]. However, recent theories [3] [4] suggest that emotions are an essential part of human intelligence, playing a critical role in processes such as perception, learning, attention, memory, rational decision making and other abilities usually associated to intelligent behaviours. The initial approach fails, probably, in considering “emotional systems” as systems that lose the desirable rationality and control. However, it is not right considering laws and rational norms as the unique and more important parts when interpreting human behaviour and intelligence. It is also an error considering human behaviour independent of any emotional process. Up to this point, it is worth to remark that, from the neurological perspective, no polarization, or clean dividing line occurs between thinking and emotions [2]. 1.2 Emotional Architectures Most of the theoretical models of emotion, coming from Psychology are not appropriate to be applied into computer systems, since they weren’t conceived with that purpose. The adaptation of these approaches and the development of new theories, more suitable to the automation of their elements and processes, have reduced the number of theoretical models of emotion present in most of the emotional systems: appraisal models, motivational models, dimensional models... However, neither these models nor the architectures and systems developed from them incorporate successfully emotions to the general rational process. Some deficiency or drawback is always imputed to each one, although, depending on the contexts and problems, they also prove sometimes to be acceptably adequate. The empirical results of these approaches show that emotional factors cannot be considered as yet another component in the agent's architecture, but all the architecture must exhibit an emotional orientation. 1.3 Adaptivity vs. Specificity or Generality Behind these architectures (usually agent-oriented) underlies a very intricate structure. Sometimes, their elements and dynamics are interwoven with the restrictions and particularities of the application context, mingling with them, making these approaches few reusable (cf. [5], [6], [7], [8], [9]). Other times, architectures are intrinsically very generic, independent from any specific problem (cf. [10], [11], [12]). The lack of an adaptation to the particular needs of the problem originates less-efficient, computationally demanding mechanisms. This forces to reconsider the whole structure of the architecture and to simplify some of their original capabilities, in order to offer viable developments. In our opinion, nowadays solutions do not provide the level of desired quality/satisfaction because they – just– fail in the “attitudes” with which complexity is faced: instead of looking for specificity or generality, the key is in adaptivity. A new perspective is needed. A perspective considering the complexity of this kind of systems in an efficient, reusable way; a perspective allowing an adaptation to the specific problem and context needs without loosing the generic purpose of the architecture; a perspective maintaining a structure, components and processes coherent and understandable. 2 Adaptable Cognitive Architecture This paper proposes the definition of a generic architecture, named COGNITIVA, to develop agents whose behaviours are emotionally influenced. Considering an agent as a continuous perception-cognition-action cycle, the scope of this architecture is circumscribed to its cognitive activity, although it does not restrict any of the other two modules (perceptual and actuation). Different from the generic preceding architectures, COGNITIVA provides mechanisms to be adapted to each specific context and problem, from a double perspective: • Adaptation of its structure: COGNITIVA is a multilayered architecture that covers several kinds of behaviour: reactive, deliberative and social. Besides, it includes a flexible model that allows establishing dependencies and influences among elements such as personality traits, attitudes, physical states, concerns and emotions. Even more, both the behaviour modes and the elements are configurable, according to the particular needs of every situation. • Adaptation of its process of application: from the generic architecture, a progressive specification process has been designed, to apply it to every particular context. This process begins with a functional specification of the architecture, which provides a particular design and implementation of each one of the information structures and functions defined in the generic architecture. This first specification is yet too context independent to the application context. In fact, the same functional specification may be used as a basis for many different contexts. The approach to each one of them is made in a second specification phase, the contextual specification, in which all the particular values and procedures of the application environment are included. In the following sections, COGNITIVA and its main components are described with a deeper detail, along with some results obtained from its application. 3 Description of COGNITIVA Internally, COGNITIVA can be considered as a hybrid architecture, combining reactive, deliberative and social skills. Fig. 1 shows a schematic perspective of the three quasi-horizontal levels, namely: • Reactive layer, to provide the agent with immediate responses to the perceived changes in the environment. • Deliberative layer, to provide the agent with goal-directed behaviours, from its individual abilities point of view. • Social layer, to provide the agent with behaviours in which the existence of other agents and the interaction with them is considered. Fig. 1. General Schema of COGNITIVA. 3.1 Management of the Current State of the Agent: Beliefs With independence of the decision making process carried out in each one of the layers, it is very difficult for an agent to exhibit coherent behaviours exclusively from the perceptual input coming from the sensors (perceptual module). It is necessary to consider other information sources, such as its knowledge about the environment, about other agents and, even more, about itself. All this information is represented internally as a beliefs set. To manage the beliefs, a taxonomy has been defined. In a first level, the taxonomy differentiates the object of the belief: places –physical, conceptual or virtual–, objects, individuals and the current situation. Besides, the agent beliefs related to places, objects and individuals are classified into: • Beliefs related to defining characteristics (DCs), that describe the general traits of places, objects and individuals. DCs are fundamental to understand them. The DCs value hardly changes over the time. • Beliefs related to transitory states (TSs), characteristics whose values represent the current state of places, objects and individuals. TSs’ values have a much more dynamic nature, compared to DCs. • Beliefs related to attitudes, useful to determine the behaviour of an agent towards other environment components (places, objects and individuals). Attitudes’ values are less variable than TSs’, but more than DCs’. Among the whole set of agent's beliefs, COGNITIVA distinguishes a small subset related to the agent itself, which is fundamental in the architecture. This subset constitutes what is called the agent's personal model, and includes DCs such as its personality traits, whose values determine the coherent and stable behaviour of the individual; TSs such as its moods and its physical states, identifying the state of the mind and the body of the agent, respectively; and also its attitudes towards others. Many of these characteristics are intrinsically related, and exert some influence on other beliefs of the personal model. So, for instance, the personality traits influence the value of the emotions. 3.2 Management of the Past State: History Agent behaviours that do not take into account events occurred in past moments are specially disappointing for human observers. The architecture proposed considers two mechanisms to maintain the agent's past history information: • Accumulative management of the past: this is an implicit mechanism, related to the way in which beliefs are managed. External changes in the environment or internal modifications in the agent internal state may produce an update of the agent beliefs. However, this update is performed as a variation ---of higher or lower intensity--of the previous beliefs, avoiding abrupt alterations in the individual behaviour. • Explicit management of the past state: an accumulative effect of the past events may not be enough to manage efficiently the past state, because it does not consider information related to the events themselves or to the temporal instant in which they took place. Our ","[{'authorId': '2096718980', 'name': 'R. Imbert'}]",2.0,"{'bibtex': '@Inproceedings{Imbert2005AgentsTC,\n author = {R. Imbert},\n title = {Agents that Combine Emotions and Rationality : a Context Independent Cognitive Architecture},\n year = {2005}\n}\n'}",,,19.0,Agents that Combine Emotions and Rationality : a Context Independent Cognitive Architecture,2005.0
1422,782ab35a0c12981e43db607ebfbf3e34cd04f4d5,"American Psychological Association | Division 12 http://www.div12.org/ Exposure therapy is a psychological treatment that was developed to help people confront their fears. When people are fearful of something, they tend to avoid the feared objects, activities, or situations. Although this avoidance might help reduce feelings of fear in the short term, over the long term it can make the fear become even worse. In such situations, a psychologist might recommend a program of exposure therapy in order to help break the pattern of avoidance and fear. In this form of therapy, psychologists create a safe environment in which to ""expose"" individuals to the things they fear and avoid. The exposure to the feared objects, activities, or situations in a safe environment helps reduce fear and decrease avoidance.","[{'authorId': '16015593', 'name': 'Steve Colori'}]",61.0,"{'bibtex': '@Article{Colori2018ExposureT,\n author = {Steve Colori},\n journal = {Schizophrenia bulletin},\n pages = {\n          229-230\n        },\n title = {Exposure Therapy.},\n volume = {44 2},\n year = {2018}\n}\n'}",,"{'volume': '44 2', 'pages': '\n          229-230\n        ', 'name': 'Schizophrenia bulletin'}",5.0,Exposure Therapy.,2018.0
1423,782ce13c8cd76aa39e8ca01b8d1a121dcdec1523,,"[{'authorId': '35145855', 'name': 'S. Fiore'}, {'authorId': '46384423', 'name': 'G. Harrison'}, {'authorId': '32827434', 'name': 'C. Hughes'}, {'authorId': '145349330', 'name': 'E. Rutström'}]",129.0,"{'bibtex': '@Article{Fiore2009VirtualEA,\n author = {S. Fiore and G. Harrison and C. Hughes and E. Rutström},\n journal = {Journal of Environmental Economics and Management},\n pages = {65-86},\n title = {Virtual Experiments and Environmental Policy},\n volume = {57},\n year = {2009}\n}\n'}",,"{'volume': '57', 'pages': '65-86', 'name': 'Journal of Environmental Economics and Management'}",181.0,Virtual Experiments and Environmental Policy,2009.0
1425,787002ee8b7e7902b212e5943b51a12b74d9a395,,"[{'authorId': '145822452', 'name': 'J. Duncan'}]",255.0,"{'bibtex': '@Inproceedings{Duncan1996CooperatingBS,\n author = {J. Duncan},\n title = {Cooperating brain systems in selective perception and action.},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Cooperating brain systems in selective perception and action.,1996.0
1426,788744fd40fcef45949a78be7f2925db484295cb,"We have developed an autonomous virtual character guided by emotions. The agent is a virtual character who lives in a three-dimensional maze world. We found that emotion drivers can induce the behavior of a trained agent. Our approach is a case of goal parameterized reinforcement learning. Thus, we create conditioning between emotion drivers and a set of goals that determine the behavioral profile of a virtual character. We train agents who can randomly assume these goals while trying to maximize a reward function based on intrinsic and extrinsic motivations. A mapping between motivation and emotion was carried out. So, the agent learned a behavior profile as a training goal. The developed approach was integrated with the Advantage Actor-Critic (A3C) algorithm. Experiments showed that this approach produces behaviors consistent with the objectives given to agents, and has potential for the development of believable virtual characters.","[{'authorId': '153468821', 'name': 'Gilzamir Gomes'}, {'authorId': '1872209', 'name': 'C. Vidal'}, {'authorId': '1870162', 'name': 'J. B. C. Neto'}, {'authorId': '2889936', 'name': 'Y. L. Nogueira'}]",2.0,"{'bibtex': '@Inproceedings{Gomes2020AnAE,\n author = {Gilzamir Gomes and C. Vidal and J. B. C. Neto and Y. L. Nogueira},\n pages = {27-44},\n title = {An Autonomous Emotional Virtual Character: An Approach with Deep and Goal-Parameterized Reinforcement Learning},\n volume = {11},\n year = {2020}\n}\n'}","[{'paperId': '245e484081a097803199061ef27e9af00b5c0940', 'title': 'An Overview of Emotion in Artificial Intelligence'}, {'paperId': '28ad33130eac87f4ff702e95114135087c1783bb', 'title': '3D Modeling Design of Multirole Virtual Character Based on Visual Communication in Wireless Sensor Networks'}]","{'name': '', 'pages': '27-44', 'volume': '11'}",38.0,An Autonomous Emotional Virtual Character: An Approach with Deep and Goal-Parameterized Reinforcement Learning,2020.0
1427,788b15af6cf5f787dd41b20cf4f6323e148d9ca6,"The development of non-human robots and virtual agents is focused on encouraging affective communication between humans and agents while supporting the daily life of individuals. For an agent to achieve affective communication with a wide range of users, understanding emotion recognition of an agent's expression is important. In previous studies, age has had an obvious effect on emotion recognition. However, the effect of age on emotion recognition in the context of non-human agents is not yet well understood. In this study, we investigated emotion recognition of young and elderly users when confronted with a non-human agent's expressions. A questionnaire with a seven-emotion alternative forced choice task was used to analyze emotion recognition in 62 young and 39 elderly users. Dynamically formed virtual agent expressions were used to analyze the effect of non-human expressions on emotion recognition. The elderly users had a higher variability of emotion recognition compared with the young users. Studying the individual characteristics of emotion recognition should be prioritized would allow for more affective communication between elderly users and non-human agents.","[{'authorId': '2069073454', 'name': 'Takashi Numata'}, {'authorId': '2661328', 'name': 'Yasuhiro Asa'}, {'authorId': '1391465021', 'name': 'T. Kitagaki'}, {'authorId': '2087690212', 'name': 'T. Hashimoto'}, {'authorId': '3323578', 'name': 'K. Karasawa'}]",4.0,"{'bibtex': ""@Book{Numata2019YoungAE,\n author = {Takashi Numata and Yasuhiro Asa and T. Kitagaki and T. Hashimoto and K. Karasawa},\n booktitle = {International Conference on Human-Agent Interaction},\n journal = {Proceedings of the 7th International Conference on Human-Agent Interaction},\n title = {Young and Elderly Users' Emotion Recognition of Dynamically Formed Expressions Made by a Non-Human Virtual Agent},\n year = {2019}\n}\n""}","[{'paperId': '149dc74d9a5d8802f73784d57ccdfd34e6375280', 'title': 'Fintech Agents: Technologies and Theories'}, {'paperId': 'dfff521c01494781fa4eaf1c238ed31fd587a16f', 'title': 'Effects of virtual agent interactivity on pro-environmental behavior promotion'}, {'paperId': '460d304be9ff4a9999fb57e7949e87eac7a7584b', 'title': 'The Zoomorphic Miro Robot’s Affective Expression Design and Perceived Appearance'}, {'paperId': '8aa0d78a161ca3287bf98f5441aeebbaebbece41', 'title': 'An Experimental Study on Promotion of Pro-Environmental Behavior Focusing on ""Vanity"" for Interactive Agent'}]",{'name': 'Proceedings of the 7th International Conference on Human-Agent Interaction'},14.0,Young and Elderly Users' Emotion Recognition of Dynamically Formed Expressions Made by a Non-Human Virtual Agent,2019.0
1428,78b780eb6f07e0b36aeaf3af2bf743ee67594373,,"[{'authorId': '15102546', 'name': 'N. Eisenberg'}, {'authorId': '117097339', 'name': 'Cindy Shea'}, {'authorId': '144124312', 'name': 'G. Carlo'}, {'authorId': '15673391', 'name': 'G. Knight'}]",148.0,"{'bibtex': '@Inproceedings{Eisenberg2014EmpathyRelatedRA,\n author = {N. Eisenberg and Cindy Shea and G. Carlo and G. Knight},\n pages = {85-110},\n title = {Empathy-Related Responding and Cognition: A “Chicken and the Egg” Dilemma},\n year = {2014}\n}\n'}",,"{'volume': '', 'pages': '85-110', 'name': ''}",0.0,Empathy-Related Responding and Cognition: A “Chicken and the Egg” Dilemma,2014.0
1429,78c72d21b62656f71c294133f544a655546c6af6,"Emotion-Driven Reinforcement Learning Robert P. Marinier III, John E. Laird ({rmarinie,laird}@umich.edu) Electrical Engineering and Computer Science Department, 2260 Hayward Ann Arbor, MI 48109 USA augmented Soar with a new module, our emotion system, described below. Abstract Existing computational models of emotion are primarily concerned with creating more realistic agents, with recent efforts looking into matching human data, including qualitative emotional responses and dynamics. In this paper, our work focuses on the functional benefits of emotion in a cognitive system where emotional feedback helps drive reinforcement learning. Our system is an integration of our emotion theory with Soar, an independently-motivated cognitive architecture. Integrating Appraisal Theories and Cognition Our work is grounded in appraisal theories (Roseman & Smith, 2001, for an overview) and Newell’s (1990) PEACTIDM (pronounced PEE-ACK-TEH-DIM). Appraisal theories hypothesize that an emotional reaction to a stimulus is the result of an evaluation of that stimulus along a number of dimensions, most of which relate it to current goals. The particular appraisals that our system uses is a subset of the appraisals described by Scherer (2001) (see Table 1). The subset our system uses can be split into two main groups: appraisals that help the agent decide which stimulus attend to (Suddenness, Unpredictability, Intrinsic Pleasantness, Relevance) and those appraisals that help the agent decide what do in response to an attended stimulus (causal agent and motive, outcome probability, discrepancy from expectation, conduciveness, control, power). Appraisal theories generally do not give much detail about how appraisals are generated or why. That is, the details of the process are left unspecified. Thus, computational models must fill in those details, but with little or no direction from appraisal theory, the details are often arbitrary. PEACTIDM, on the other hand, describes necessary and sufficient processes for immediate behavior (see Table 2). The PEACTIDM hypothesis is that stimuli are Perceived and Encoded so cognition can work with them. Then Attend focuses cognition on one stimulus to process, which is then Comprehended. Tasking is managing tasks and goals (e.g., in response to a change in the situation), whereas Intend is determining what actions to take. Decode and Motor are translating cognitive choices into physical actions. PEACTIDM, however, does not describe the data upon which these processes operate. The basis of our theory is that appraisals are the information upon which the PEACTIDM theory operates (Marinier & Laird, 2006) (see Table 3). For example, the attend process determines what to process next based on appraisal information generated by the perceive and encode processes (i.e., suddenness, unpredictability, intrinsic pleasantness, and relevance). Thus, appraisals not only determine the information that PEACTIDM processes, PEACTIDM also imposes dependencies between the appraisals (e.g., the appraisals for Comprehend cannot occur until after appraisals for Attend have been generated). Keywords: Emotion, reinforcement learning, intrinsic reward, cognitive architecture, appraisal theories. Introduction Folk psychology often casts emotions in a negative light. For example, in Star Trek, Vulcans are portrayed as superior to humans because they are not distracted by emotions, and thus can make purely logical decisions. As far back as Phineas Gage, however, it has been clear that emotions play a critical role in proper functioning in humans, and over the last several decades psychological research has explored how emotions influence behavior. We are interested in exploring how some of the functional capabilities of emotion can be utilized in computational agents; that is, we want to bring the functionality of emotions to artificial intelligence. This is in contrast to most existing computational models of emotion, which focused primarily on creating believable agents (Gratch & Marsella, 2004; Hudlicka, 2004), modeling human data (Marsella & Gratch 2006; Gratch, Marsella, and Mao, 2006), or entertainment (Loyall, Neal Reilly, Bates, and Weyhrauch, 2004). In this paper, we present work in which reinforcement learning is driven by emotion. Intuitively, feelings serve as a reward signal. The agent learns to behave in a way that makes it feel good while avoiding feeling bad. Coupled with a task that the agent wants to complete, the agent learns that completing the task makes it feel good. This work contributes not only to research on emotion in providing a functional computational grounding for feelings, but it also contributes to research in reinforcement learning by providing a detailed theory of the origin and basis of intrinsically-motivated reward. Background Our system in implemented in the Soar cognitive architecture (Newell, 1990). Soar is a complete agent framework composed of interacting, task-independent memory and processing modules that include short- and long-term memory, decision making, learning, and perception. We have","[{'authorId': '144843807', 'name': 'Robert P. Marinier'}, {'authorId': '1715438', 'name': 'J. Laird'}]",70.0,"{'bibtex': '@Inproceedings{Marinier2008EmotionDrivenRL,\n author = {Robert P. Marinier and J. Laird},\n title = {Emotion-Driven Reinforcement Learning},\n volume = {30},\n year = {2008}\n}\n'}",,"{'volume': '30', 'name': ''}",18.0,Emotion-Driven Reinforcement Learning,2008.0
1430,7915637dde7f78fbfd654b77eb7d558e7be7d4b4,,"[{'authorId': '47985333', 'name': 'A. Kendon'}]",2036.0,"{'bibtex': '@Article{Kendon1967SomeFO,\n author = {A. Kendon},\n journal = {Acta psychologica},\n pages = {\n          22-63\n        },\n title = {Some functions of gaze-direction in social interaction.},\n volume = {26 1},\n year = {1967}\n}\n'}",,"{'volume': '26 1', 'pages': '\n          22-63\n        ', 'name': 'Acta psychologica'}",30.0,Some functions of gaze-direction in social interaction.,1967.0
1431,7925e138e94f926836f3373c7f32b03653077f46,The IRM4S model (Influence Reaction Model for Simulation) is an adaptation of the formalism of [2] for multiagent based simulations (MABS). The goal of IRM4S is to provide a framework that eases the use of the Influence/Reaction principle within MABS.,"[{'authorId': '1750451', 'name': 'F. Michel'}]",68.0,"{'bibtex': '@Inproceedings{Michel2007TheIM,\n author = {F. Michel},\n pages = {133},\n title = {The IRM4S model: the influence/reaction principle for multiagent based simulation},\n year = {2007}\n}\n'}",,{'pages': '133'},8.0,The IRM4S model: the influence/reaction principle for multiagent based simulation,2007.0
1432,79439ceba4f6731a900445afff086d1f16c4750b,"To facilitate natural interactions between humans and embodied conversational agents (ECAs), we need to endow the latter with the same nonverbal cues that humans use to communicate. Gaze cues in particular are integral in mechanisms for communication and management of attention in social interactions, which can trigger important social and cognitive processes, such as establishment of affiliation between people or learning new information. The fundamental building blocks of gaze behaviors are gaze shifts: coordinated movements of the eyes, head, and body toward objects and information in the environment. In this article, we present a novel computational model for gaze shift synthesis for ECAs that supports parametric control over coordinated eye, head, and upper body movements. We employed the model in three studies with human participants. In the first study, we validated the model by showing that participants are able to interpret the agent’s gaze direction accurately. In the second and third studies, we showed that by adjusting the participation of the head and upper body in gaze shifts, we can control the strength of the attention signals conveyed, thereby strengthening or weakening their social and cognitive effects. The second study shows that manipulation of eye--head coordination in gaze enables an agent to convey more information or establish stronger affiliation with participants in a teaching task, while the third study demonstrates how manipulation of upper body coordination enables the agent to communicate increased interest in objects in the environment.","[{'authorId': '2633572', 'name': 'T. Pejsa'}, {'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}]",49.0,"{'bibtex': '@Article{Pejsa2015GazeAA,\n author = {T. Pejsa and Sean Andrist and Michael Gleicher and Bilge Mutlu},\n journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},\n pages = {1 - 34},\n title = {Gaze and Attention Management for Embodied Conversational Agents},\n volume = {5},\n year = {2015}\n}\n'}",,"{'volume': '5', 'pages': '1 - 34', 'name': 'ACM Transactions on Interactive Intelligent Systems (TiiS)'}",86.0,Gaze and Attention Management for Embodied Conversational Agents,2015.0
1433,79596de8da82cbb219e88b45c808b052606f765d,"Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for “quick and dirty” methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability","[{'authorId': '145239431', 'name': 'J. Brooke'}]",10931.0,"{'bibtex': ""@Inproceedings{Brooke1996SUSA,\n author = {J. Brooke},\n pages = {207-212},\n title = {SUS: A 'Quick and Dirty' Usability Scale},\n year = {1996}\n}\n""}",,"{'volume': '', 'pages': '207-212', 'name': ''}",5.0,SUS: A 'Quick and Dirty' Usability Scale,1996.0
1434,7959718ba698e515eef07a6e02bb30fa5bf19e72,"Recent advances in technology have paved the way for humanagent interactions to become ubiquitous in our daily lives, and decades worth of research on virtual agents have enhanced these interactions. However, for the most part, the effect of different types of agents on the human brain is unknown, and the neuroscience of human-agent interactions is rarely studied. In this study, we examine the underlying neural systems involved in processing and responding to different types of negotiating agents. More specifically, we show that different brain patterns are observed for various types of virtual agents; consequently, we can decode the strategy and emotional display of the agent based on the counterpart’s brain activity. Using fMRI data, we analyzed participants’ brain activity during negotiations with agents who show three different emotional expressions and use two different types of negotiation strategies. We demonstrate that, using Multi-Voxel Pattern Analysis, we can reliably decode agents’ emotional expressions based on the activity in the left dorsal anterior insula, and also agents’ strategies based on the activity in the frontal pole.","[{'authorId': '47056370', 'name': 'Eunkyung Kim'}, {'authorId': '40026778', 'name': 'Sarah I. Gimbel'}, {'authorId': '144423532', 'name': 'A. Litvinova'}, {'authorId': '2253951', 'name': 'J. Kaplan'}, {'authorId': '145707560', 'name': 'Morteza Dehghani'}]",1.0,"{'bibtex': ""@Article{Kim2017DecodingVA,\n author = {Eunkyung Kim and Sarah I. Gimbel and A. Litvinova and J. Kaplan and Morteza Dehghani},\n booktitle = {Annual Meeting of the Cognitive Science Society},\n journal = {Cognitive Science},\n title = {Decoding Virtual Agent's Emotion and Strategy from Brain Patterns},\n year = {2017}\n}\n""}","[{'paperId': '37cee6eab71ee934d8851a9ae9c60c77768c95fa', 'title': 'Extending the wisdom of crowds: How to harness the wisdom of the inner crowd'}]","{'name': 'Cognitive Science', 'volume': ''}",28.0,Decoding Virtual Agent's Emotion and Strategy from Brain Patterns,2017.0
1435,79a424a51e65d50e2532162f8e734683cded4f5e,"
 
 In this paper we present two studies supporting a plan-based model of narrative generation that reasons about both intentionality and belief. First we compare the believability of agent plans taken from the spaces of valid classical plans, intentional plans, and belief plans. We show that the plans that make the most sense to humans are those in the overlapping regions of the intentionality and belief spaces. Second, we validate the model’s approach to representing anticipation, where characters form plans that involve actions they expect other characters to take. Using a short interactive scenario we demonstrate that players not only find it believable when NPCs anticipate their actions, but sometimes actively anticipate the actions of NPCs in a way that is consistent with the model.
 
","[{'authorId': '144650313', 'name': 'A. Shirvani'}, {'authorId': '38518031', 'name': 'Rachelyn Farrell'}, {'authorId': '34810994', 'name': 'Stephen G. Ware'}]",24.0,"{'bibtex': '@Inproceedings{Shirvani2018CombiningIA,\n author = {A. Shirvani and Rachelyn Farrell and Stephen G. Ware},\n pages = {222-228},\n title = {Combining Intentionality and Belief: Revisiting Believable Character Plans},\n year = {2018}\n}\n'}",,{'pages': '222-228'},28.0,Combining Intentionality and Belief: Revisiting Believable Character Plans,2018.0
1436,79b46da995fb2d941ba32f571fab4b83f87ac6fc,"This paper presents the first dynamic 3D FACS data set for facial expression research, containing 10 subjects performing between 19 and 97 different AUs both individually and in combination. In total the corpus contains 519 AU sequences. The peak expression frame of each sequence has been manually FACS coded by certified FACS experts. This provides a ground truth for 3D FACS based AU recognition systems. In order to use this data, we describe the first framework for building dynamic 3D morphable models. This includes a novel Active Appearance Model (AAM) based 3D facial registration and mesh correspondence scheme. The approach overcomes limitations in existing methods that require facial markers or are prone to optical flow drift. We provide the first quantitative assessment of such 3D facial mesh registration techniques and show how our proposed method provides more reliable correspondence.","[{'authorId': '1792288', 'name': 'D. Cosker'}, {'authorId': '50755536', 'name': 'Eva G. Krumhuber'}, {'authorId': '144046599', 'name': 'A. Hilton'}]",149.0,"{'bibtex': '@Article{Cosker2011AFV,\n author = {D. Cosker and Eva G. Krumhuber and A. Hilton},\n journal = {2011 International Conference on Computer Vision},\n pages = {2296-2303},\n title = {A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling},\n year = {2011}\n}\n'}",,"{'pages': '2296-2303', 'name': '2011 International Conference on Computer Vision'}",27.0,A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling,2011.0
1437,79bf3910522048053206146bedd54d61564f63cb,"One of the most effective ways to improve quality of life in dementia is by exposing people to meaningful activities. The study of engagement is crucial to identify which activities are significant for persons with dementia and customize them. Previous work has mainly focused on developing assessment tools and the only available model of engagement for people with dementia focused on factors influencing engagement or influenced by engagement. This article focuses on the internal functioning of engagement and presents the development and testing of a model specifying the components of engagement, their measures, and the relationships they entertain. We collected behavioral and physiological data while participants with dementia (N = 14) were involved in six sessions of play, three of game-based cognitive stimulation and three of robot-based free play. We tested the concurrent validity of the measures employed to gauge engagement and ran factorial analysis and Structural Equation Modeling to determine whether the components of engagement and their relationships were those hypothesized. The model we constructed, which we call the ENGAGE-DEM, achieved excellent goodness of fit and can be considered a scaffold to the development of affective computing frameworks for measuring engagement online and offline, especially in HCI and HRI.","[{'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '1413855572', 'name': 'Marta Díaz-Boladeras'}, {'authorId': '1423350437', 'name': 'Andreu Català-Mallofré'}, {'authorId': '145741020', 'name': 'E. Barakova'}, {'authorId': '1736974', 'name': 'G.W.M. Rauterberg'}]",17.0,"{'bibtex': '@Article{Perugia2020ENGAGEDEMAM,\n author = {G. Perugia and Marta Díaz-Boladeras and Andreu Català-Mallofré and E. Barakova and G.W.M. Rauterberg},\n journal = {IEEE Transactions on Affective Computing},\n pages = {926-943},\n title = {ENGAGE-DEM: A Model of Engagement of People With Dementia},\n volume = {13},\n year = {2020}\n}\n'}",,"{'volume': '13', 'pages': '926-943', 'name': 'IEEE Transactions on Affective Computing'}",110.0,ENGAGE-DEM: A Model of Engagement of People With Dementia,2020.0
1438,79f2e8a93993d51574a14ee08ba81a73abd8065d,"While collision avoidance has been the most active topic in pedestrian simulation, the modelling of other kinds of behaviours appears to be essential for better realism. Thus higher cognitive levels of perception and behaviour improve simulation quality. Furthermore, giving an agent the possibility to choose the nature of its interactions with the others can not only improve simulation realism but also bring heterogeneity in the simulated population because each agent individually perceives the situation according to its own characteristics. In this paper, we aim at providing the pedestrian agent the ability to obtain an individual representation of the environment that allows him to adapt its behaviour according to the situation. We base our work on the analysis and interpretation of the environment, which makes the agent decide the behaviour it is going to adopt. We focus on two kinds of behaviours, following and group avoidance behaviours, and on their integration in classical avoidance simulations. We integrate recent works about following behaviour and propose to model interactions directly with groups of people instead of individuals. We aim at providing perception rules totally independent from the collision avoidance model used in the simulation. Because of the improved perception process, we observe emerging speed waves, group behaviours and lane formation in our simulations. Our results demonstrate the interest of modelling such behaviours to obtain more realistic simulations and show that specific patterns and collective behaviours emerge when using several types of behaviours in simulations. Copyright © 2015 John Wiley & Sons, Ltd.","[{'authorId': '2341240', 'name': 'S. Lemercier'}, {'authorId': '1974013', 'name': 'J. Auberlet'}]",23.0,"{'bibtex': '@Article{Lemercier2016TowardsMB,\n author = {S. Lemercier and J. Auberlet},\n journal = {Computer Animation and Virtual Worlds},\n pages = {24 - 34},\n title = {Towards more behaviours in crowd simulation},\n volume = {27},\n year = {2016}\n}\n'}",,"{'volume': '27', 'pages': '24 - 34', 'name': 'Computer Animation and Virtual Worlds'}",47.0,Towards more behaviours in crowd simulation,2016.0
1439,7a13d003b31fe8bc55b3bf7f25d9e901e9fad294,,"[{'authorId': '143830962', 'name': 'G. Thierry'}, {'authorId': '32279869', 'name': 'Clara D. Martin'}, {'authorId': '2062365', 'name': 'P. Downing'}, {'authorId': '1861118', 'name': 'A. Pegna'}]",239.0,"{'bibtex': '@Article{Thierry2007ControllingFI,\n author = {G. Thierry and Clara D. Martin and P. Downing and A. Pegna},\n journal = {Nature Neuroscience},\n pages = {505-511},\n title = {Controlling for interstimulus perceptual variance abolishes N170 face selectivity},\n volume = {10},\n year = {2007}\n}\n'}",,"{'volume': '10', 'pages': '505-511', 'name': 'Nature Neuroscience'}",45.0,Controlling for interstimulus perceptual variance abolishes N170 face selectivity,2007.0
1440,7a1a617ab95f87c876251069771f0754f4161ae9,"The ninth Audio-Visual Emotion Challenge and workshop AVEC 2019 was held in conjunction with ACM Multimedia'19. This year, the AVEC series addressed major novelties with three distinct tasks: State-of-Mind Sub-challenge (SoMS), Detecting Depression with Artificial Intelligence Sub-challenge (DDS), and Cross-cultural Emotion Sub-challenge (CES). The SoMS was based on a novel dataset (USoM corpus) that includes self-reported mood (10-point Likert scale) after the narrative of personal stories (two positive and two negative). The DDS was based on a large extension of the DAIC-WOZ corpus (c.f. AVEC 2016) that includes new recordings of patients suffering from depression with the virtual agent conducting the interview being, this time, wholly driven by AI, i.e., without any human intervention. The CES was based on the SEWA dataset (c.f. AVEC 2018) that has been extended with the inclusion of new participants in order to investigate how emotion knowledge of Western European cultures (German, Hungarian) can be transferred to the Chinese culture. In this summary, we mainly describe participation and conditions of the AVEC Challenge.","[{'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '1709997', 'name': 'N. Cummins'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",17.0,"{'bibtex': ""@Article{Ringeval2019AVEC19AE,\n author = {F. Ringeval and Björn Schuller and M. Valstar and N. Cummins and R. Cowie and M. Pantic},\n booktitle = {ACM Multimedia},\n journal = {Proceedings of the 27th ACM International Conference on Multimedia},\n title = {AVEC'19: Audio/Visual Emotion Challenge and Workshop},\n year = {2019}\n}\n""}","[{'paperId': '2ab2577b74473996f9eb6fea626868b5748bc935', 'title': 'Systematic review of machine learning in PTSD studies for automated diagnosis evaluation'}, {'paperId': 'b3d6b5c6bf824cdaa6170bf34082b66b0ec5f7ed', 'title': 'Enhancing Depression Detection: Employing Autoencoders and Linguistic Feature Analysis with BERT and LSTM Model'}, {'paperId': '909e899de4b40ee6563d4be95f7095ef76ae237b', 'title': 'On the Automatic Detection of Comorbidity of Mental Disorders Using Audio-Visual Data'}, {'paperId': '0c9447f906e971eb07a4e7da7a7e8cd363712f0d', 'title': 'Speech-based Evaluation of Emotions-Depression Correlation'}, {'paperId': '3854257087a3f3ed4e79739ee87c08017430e915', 'title': 'Temporal Attentive Adversarial Domain Adaption for Cross Cultural Affect Recognition'}, {'paperId': '92ac9389fd065c535d666b30bd416ea7597d2acc', 'title': 'Do Deepfakes Feel Emotions? A Semantic Approach to Detecting Deepfakes Via Emotional Inconsistencies'}, {'paperId': 'fc991cc2638c62553e042373d44a26cc6870b66a', 'title': 'Multi-modality Depression Detection via Multi-scale Temporal Dilated CNNs'}, {'paperId': 'bea2c2379608da156ad8a2bd2f8fcfac44a79d28', 'title': 'Adversarial Domain Adaption for Multi-Cultural Dimensional Emotion Recognition in Dyadic Interactions'}, {'paperId': '56a77b2810dfa3fedf22bdfda0eb33b5b2ca882f', 'title': 'Multimodal Fusion of BERT-CNN and Gated CNN Representations for Depression Detection'}, {'paperId': '8d9ff2905896fa52fd9296d75e2de27ffa59f9f1', 'title': 'Efficient Spatial Temporal Convolutional Features for Audiovisual Continuous Affect Recognition'}, {'paperId': '2bd07c1a90282571c374a7d40deb294759000347', 'title': 'Predicting Depression and Emotions in the Cross-roads of Cultures, Para-linguistics, and Non-linguistics'}, {'paperId': 'c6955562c2f324948bc214b724e9a904d1e4a2e5', 'title': 'A Multi-Modal Hierarchical Recurrent Neural Network for Depression Detection'}, {'paperId': 'b13f84a52d02acf297b41f199320b745cd933246', 'title': 'A Multimodal Framework for State of Mind Assessment with Sentiment Pre-classification'}, {'paperId': '95be3458a94439c0810d506adf74bbb8a6958458', 'title': 'Multi-level Attention Network using Text, Audio and Video for Depression Prediction'}, {'paperId': '4232e9b7f480a1bd39a46b9537f68a8b93ab18c1', 'title': 'Multimodal-multisensor affect detection'}, {'paperId': 'd09ada0f35f21bc376ff8da6bea3097af93a4e25', 'title': 'Generation and detection of manipulated multimodal audiovisual content: Advances, trends and open challenges'}, {'paperId': '6128c369a5ac7ea8a1f0709f398dec1beb6a9b85', 'title': 'An Investigation on the Audio-Video Data Based Estimation of Emotion Regulation Difficulties and Their Association With Mental Disorders'}]",{'name': 'Proceedings of the 27th ACM International Conference on Multimedia'},12.0,AVEC'19: Audio/Visual Emotion Challenge and Workshop,2019.0
1441,7a20a3c31bf313f1c91784bd10b136b67231569c,,"[{'authorId': '3113645', 'name': 'Preben Wik'}, {'authorId': '2524288', 'name': 'Anna Hjalmarsson'}]",136.0,"{'bibtex': '@Article{Wik2009EmbodiedCA,\n author = {Preben Wik and Anna Hjalmarsson},\n journal = {Speech Commun.},\n pages = {1024-1037},\n title = {Embodied conversational agents in computer assisted language learning},\n volume = {51},\n year = {2009}\n}\n'}",,"{'volume': '51', 'pages': '1024-1037', 'name': 'Speech Commun.'}",42.0,Embodied conversational agents in computer assisted language learning,2009.0
1442,7a2bbe6de1084900e2a7022b53a5a36aaff66668,,"[{'authorId': '40325099', 'name': 'Mathieu Chollet'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",33.0,"{'bibtex': '@Inproceedings{Chollet2014FromNS,\n author = {Mathieu Chollet and M. Ochs and C. Pelachaud},\n pages = {120-133},\n title = {From Non-verbal Signals Sequence Mining to Bayesian Networks for Interpersonal Attitudes Expression},\n year = {2014}\n}\n'}",,{'pages': '120-133'},48.0,From Non-verbal Signals Sequence Mining to Bayesian Networks for Interpersonal Attitudes Expression,2014.0
1444,7a3362a372a888dfd4662a6b7dffb33b0a70f582,"The Active Disturbance Rejection Control (ADRC) prefers the cascaded integral system for a convenient design or better control effect and takes it as a typical form. However, the state variables of practical system do not necessarily have a cascaded integral relationship. Therefore, this paper proposes an algebraic substitution method and its structure, which can convert a noncascaded integral system of PID control into a cascaded integral form. The adjusting parameters of the ADRC controller are also demonstrated. Meanwhile, a numerical example and the oscillation control of a flexible arm are demonstrated to show the conversion, controller design, and control effect. The converted system is proved to be more suitable for a direct ADRC control. In addition, for the numerical example, its control effect for the converted system is compared with a PID controller under different disturbances. The result shows that the converted system can achieve a better control effect under the ADRC than that of a PID. The theory is a guide before practice. This converting method not only solves the ADRC control problem of some noncascaded integral systems in theory and simulation but also expands the application scope of the ADRC method.","[{'authorId': '144829273', 'name': 'Zhijian Huang'}, {'authorId': '2111162773', 'name': 'Yudong Li'}, {'authorId': '2015975461', 'name': 'Yihua Liu'}, {'authorId': '153799202', 'name': 'Wenbo Sui'}, {'authorId': '3214953', 'name': 'Gui-chen Zhang'}]",92.0,"{'bibtex': '@Article{Huang2018AnAM,\n author = {Zhijian Huang and Yudong Li and Yihua Liu and Wenbo Sui and Gui-chen Zhang},\n journal = {Mathematical Problems in Engineering},\n title = {An ADRC Method for Noncascaded Integral Systems Based on Algebraic Substitution Method and Its Structure},\n year = {2018}\n}\n'}",,{'name': 'Mathematical Problems in Engineering'},32.0,An ADRC Method for Noncascaded Integral Systems Based on Algebraic Substitution Method and Its Structure,2018.0
1445,7a3c932c871d46b8d834d0383d59bfd03164d0c2,,"[{'authorId': '2870739', 'name': 'A. Kleinsmith'}, {'authorId': '2107479541', 'name': 'P. D. Silva'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}]",34.0,"{'bibtex': '@Inproceedings{Kleinsmith2005GroundingAD,\n author = {A. Kleinsmith and P. D. Silva and N. Bianchi-Berthouze},\n pages = {263-270},\n title = {Grounding Affective Dimensions into Posture Features},\n year = {2005}\n}\n'}",,{'pages': '263-270'},22.0,Grounding Affective Dimensions into Posture Features,2005.0
1446,7a3db5fe8dc49d893851e4bc0ffa9d87944c8cea,,"[{'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '9174234', 'name': 'Jina Lee'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}]",169.0,"{'bibtex': '@Inproceedings{Traum2008MultipartyMM,\n author = {D. Traum and S. Marsella and J. Gratch and Jina Lee and Arno Hartholt},\n pages = {117-130},\n title = {Multi-party, Multi-issue, Multi-strategy Negotiation for Multi-modal Virtual Agents},\n year = {2008}\n}\n'}",,{'pages': '117-130'},34.0,"Multi-party, Multi-issue, Multi-strategy Negotiation for Multi-modal Virtual Agents",2008.0
1447,7a49d92e9d7f7a526996f27f139b9ff6fab27dbf,"Most negotiations are ill-structured situations, and the ability to identify novel options is likely to be crucial for success. This study, therefore, examined how creativity impacts negotiation processes and outcomes, and how this effect is moderated by positive arousal. The negotiators’ creative personality and their state of positive arousal were measured before they participated in a simulated negotiation, with the results demonstrating that the level of creativity in negotiation dyads was positively related to the negotiators’ joint outcome. Negotiators in high creativity dyads searched for more information by asking questions about priorities and were less narrowly focused by providing fewer single-issue offers than negotiators in low creativity dyads. Positive arousal did not affect outcome directly, but moderated the effect of creativity on joint outcomes; the effect of creativity was strongest under high levels of positive arousal. The discussion section emphasizes that future research may find creativity to have even more of a positive effect when negotiations become more complex.","[{'authorId': '4235481', 'name': 'Vidar Schei'}]",9.0,"{'bibtex': '@Article{Schei2013CreativePC,\n author = {Vidar Schei},\n journal = {Creativity Research Journal},\n pages = {408 - 417},\n title = {Creative People Create Values: Creativity and Positive Arousal in Negotiations},\n volume = {25},\n year = {2013}\n}\n'}",,"{'volume': '25', 'pages': '408 - 417', 'name': 'Creativity Research Journal'}",46.0,Creative People Create Values: Creativity and Positive Arousal in Negotiations,2013.0
1448,7a6581473d906c43fe1be3a28f880ad3c5760167,"In recent years, emotion recognition has become a hot topic in human-computer interaction. If computers could understand human emotions, they could interact better with end users (Peter & Beale, 2008). In psychology, emotion is defined as a complex state that consists of a subjective experience (how we experience emotion), a physiological response (how our bodies react to emotion), and an expressive response (how we behave in response to emotion) (Smith & Lazarus, 1990). Emotion expression includes not only facial expression but also vocal and postural expression. The observable aspects of emotion (physiological and expressive components) might be able to be used as indicators of emotional state, such as facial expressions, speech, physiological parameters, gestures, and body movements (Peter & Beale, 2008). As the common use of modalities to recognizing emotional states in human-human interaction, various clues have been used in affective computing, such as facial expressions (e.g., Kenji, 1991), physiological signals (e.g., Picard, Vyzas, & Healey, 2001), linguistic information (e.g., Alm, Roth, & Sproat, 2005) and acoustic features (e.g., Dellaert, Polzin, & Waibel, 1996). Other than that, it is feasible to recognize specific affective states using gait. To investigate how gait features are effective in characterizing and recognizing emotions, gait features were used for modeling to identify different emotions. By utilizing 59 participants’ gait data with emotion labels, machine learning models are trained to detect individual emotion (Li et al, 2016).","[{'authorId': '2115890855', 'name': 'Jingying Wang'}, {'authorId': '40479455', 'name': 'Baobin Li'}, {'authorId': '3005060', 'name': 'Changye Zhu'}, {'authorId': '2116622659', 'name': 'Shun Li'}, {'authorId': '2127808', 'name': 'T. Zhu'}]",5.0,"{'bibtex': '@Inproceedings{Wang2018AutomaticER,\n author = {Jingying Wang and Baobin Li and Changye Zhu and Shun Li and T. Zhu},\n pages = {132-143},\n title = {Automatic Emotion Recognition Based on Non-Contact Gaits Information},\n year = {2018}\n}\n'}",,"{'volume': '', 'pages': '132-143', 'name': ''}",35.0,Automatic Emotion Recognition Based on Non-Contact Gaits Information,2018.0
1449,7aa83dc8d507fc38aa97e22233d96fd878ff7e51,"W hat determines how two human beings will act toward each other when they meet? Is this initial response a product of learning from culture, family experiences, and other socialization processes? Or is the response the expression of a neurobiological process that is programmed into the very DNA of our species? If the response has a neurobiological basis, are there specific features of the other person’s behavior that trigger either feelings of safety, love, and comfort or feelings of danger? Why do some children cuddle and warmly conform to embraces, yet others stiffen and pull back from the same overture? Why do some children smile and actively engage a new person, while others avert their gaze and withdraw? Does knowledge of human biology help us to understand the triggers and mechanisms of these behaviors during normal development? If we learn how behavioral features trigger neural circuits that facilitate social behavior, will we be better able to help children with severe developmental disabilities, such as autism, improve their social behavior? By processing information from the environment through the senses, the nervous system continually evaluates risk. I have coined the term neuroception to describe how PHOTO: MARILYN NOLT","[{'authorId': '4226466', 'name': 'S. Porges'}]",132.0,"{'bibtex': '@Article{Porges2004NEUROCEPTIONAS,\n author = {S. Porges},\n journal = {Zero to Three},\n pages = {19-24},\n title = {NEUROCEPTION: A Subconscious System for Detecting Threats and Safety},\n volume = {24},\n year = {2004}\n}\n'}",,"{'volume': '24', 'pages': '19-24', 'name': 'Zero to Three'}",8.0,NEUROCEPTION: A Subconscious System for Detecting Threats and Safety,2004.0
1450,7ac39f2da12e4a4d25b2806d47e6655d1f9e4cbf,"We investigated subjects’ responses to a synthesized talking face displayed on a computer screen in the context of a questionnaire study. Compared to subjects who answered questions presented via text display on a screen, subjects who answered the same questions spoken by a talking face spent more time, made fewer mistakes, and wrote more comments. When we compared responses to two different talking faces, subjects who answered questions spoken by a stern face, compared to subjects who answered questions spoken by a neutral face, spent more time, made fewer mistakes, and wrote more comments. They also liked the experience and the face less. We interpret this study in the light of desires to anthropomorphize computer interfaces and suggest that incautiously adding human characteristics, like face, voice, and facial expressions, could make the experience for users worse rather than better.","[{'authorId': '1683823', 'name': 'A. Sutcliffe'}, {'authorId': '2613856', 'name': 'Peter Faraday'}]",261.0,"{'bibtex': '@Article{Sutcliffe1994DesigningPI,\n author = {A. Sutcliffe and Peter Faraday},\n journal = {Conference Companion on Human Factors in Computing Systems},\n title = {Designing presentation in multimedia interfaces},\n year = {1994}\n}\n'}",,{'name': 'Conference Companion on Human Factors in Computing Systems'},60.0,Designing presentation in multimedia interfaces,1994.0
1452,7acd939a05ff4fb7480d21dc38b6a5b06d4e36de,"The CyberCode is a visual tagging system based on a 2D-barcode technology and provides several features not provided by other tagging systems. CyberCode tags can be recognized by the low-cost CMOS or CCD cameras found in more and more mobile devices, and it can also be used to determine the 3D position of the tagged object as well as its ID number. This paper describes examples of augmented reality applications based on CyberCode, and discusses some key characteristics of tagging technologies that must be taken into account when designing augmented reality environments.","[{'authorId': '1685962', 'name': 'J. Rekimoto'}, {'authorId': '2446448', 'name': 'Y. Ayatsuka'}]",517.0,"{'bibtex': '@Inproceedings{Rekimoto2000CyberCodeDA,\n author = {J. Rekimoto and Y. Ayatsuka},\n pages = {1-10},\n title = {CyberCode: designing augmented reality environments with visual tags},\n year = {2000}\n}\n'}",,{'pages': '1-10'},25.0,CyberCode: designing augmented reality environments with visual tags,2000.0
1453,7ad1c4547b76d51398c1e2ca66173ffb2f78f838,,"[{'authorId': '48279878', 'name': 'E. Hatfield'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}, {'authorId': '8611261', 'name': 'Richard L. Rapson'}]",557.0,"{'bibtex': '@Inproceedings{Hatfield1992PrimitiveEC,\n author = {E. Hatfield and J. Cacioppo and Richard L. Rapson},\n title = {Primitive emotional contagion.},\n year = {1992}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Primitive emotional contagion.,1992.0
1454,7b221cce8fdbc1105956b27c938730fce8c1fc10,"
 
 Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.
 
","[{'authorId': '144751955', 'name': 'Hao Zhou'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '50615630', 'name': 'Tianyang Zhang'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '2149124481', 'name': 'Bing-Qian Liu'}]",633.0,"{'bibtex': '@Inproceedings{Zhou2017EmotionalCM,\n author = {Hao Zhou and Minlie Huang and Tianyang Zhang and Xiaoyan Zhu and Bing-Qian Liu},\n pages = {730-739},\n title = {Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory},\n year = {2017}\n}\n'}",,{'pages': '730-739'},61.0,Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory,2017.0
1455,7b329ca8d2afe053c8eb13c3562b3aa37acdb54f,,"[{'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '144102217', 'name': 'A. Mehrabian'}]",1691.0,"{'bibtex': '@Article{Russell1977EvidenceFA,\n author = {J. Russell and A. Mehrabian},\n journal = {Journal of Research in Personality},\n pages = {273-294},\n title = {Evidence for a three-factor theory of emotions},\n volume = {11},\n year = {1977}\n}\n'}",,"{'volume': '11', 'pages': '273-294', 'name': 'Journal of Research in Personality'}",28.0,Evidence for a three-factor theory of emotions,1977.0
1456,7b388c66af4cf854beffaffbf43abce123a557d4,"Recognizing human facial expression and emotion by computer is an interesting and challenging problem. Many have investigated emotional contents in speech alone, or recognition of human facial expressions solely from images. However, relatively little has been done in combining these two modalities for recognizing human emotions. L.C. De Silva et al. (1997) studied human subjects' ability to recognize emotions from viewing video clips of facial expressions and listening to the corresponding emotional speech stimuli. They found that humans recognize some emotions better by audio information, and other emotions better by video. They also proposed an algorithm to integrate both kinds of inputs to mimic human's recognition process. While attempting to implement the algorithm, we encountered difficulties which led us to a different approach. We found these two modalities to be complimentary. By using both, we show it is possible to achieve higher recognition rates than either modality alone.","[{'authorId': '2108579847', 'name': 'Lawrence S. Chen'}, {'authorId': '153652752', 'name': 'Thomas S. Huang'}, {'authorId': '2407080', 'name': 'T. Miyasato'}, {'authorId': '2458123', 'name': 'R. Nakatsu'}]",194.0,"{'bibtex': '@Article{Chen1998MultimodalHE,\n author = {Lawrence S. Chen and Thomas S. Huang and T. Miyasato and R. Nakatsu},\n journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},\n pages = {366-371},\n title = {Multimodal human emotion/expression recognition},\n year = {1998}\n}\n'}",,"{'pages': '366-371', 'name': 'Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition'}",17.0,Multimodal human emotion/expression recognition,1998.0
1457,7b6805e028b02a5be1ca5112c9bd366c3c211629,"The study of social dilemmas is the study of the tension between individual and collective rationality. In a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. The first part of this review is a discussion of categories of social dilemmas and how they are modeled. The key two-person social dilemmas (Prisoner's Dilemma, Assurance, Chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. The second part is an extended treatment of possible solutions for social dilemmas. These solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: Motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. Strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. Solu...","[{'authorId': '1873272', 'name': 'P. Kollock'}]",1662.0,"{'bibtex': '@Article{Kollock1998SOCIALDT,\n author = {P. Kollock},\n journal = {Review of Sociology},\n pages = {183-214},\n title = {SOCIAL DILEMMAS: The Anatomy of Cooperation},\n volume = {24},\n year = {1998}\n}\n'}",,"{'volume': '24', 'pages': '183-214', 'name': 'Review of Sociology'}",135.0,SOCIAL DILEMMAS: The Anatomy of Cooperation,1998.0
1458,7b70ff3fbc3e9fdee619b445af17d34c3a964f68,,"[{'authorId': '153914091', 'name': 'R. Khan'}, {'authorId': '34919047', 'name': 'A. D. Angeli'}]",32.0,"{'bibtex': '@Inproceedings{Khan2009TheAS,\n author = {R. Khan and A. D. Angeli},\n pages = {85-97},\n title = {The Attractiveness Stereotype in the Evaluation of Embodied Conversational Agents},\n year = {2009}\n}\n'}",,{'pages': '85-97'},27.0,The Attractiveness Stereotype in the Evaluation of Embodied Conversational Agents,2009.0
1459,7b9038848463f0306bc527a46241c16b28b12411,"We present a probabilistic model to monitor a user's emotions and engagement during the interaction with educational games. We illustrate how our probabilistic model assesses affect by integrating evidence on both possible causes of the user's emotional arousal (i.e., the state of the interaction) and its effects (i.e., bodily expressions that are known to be influenced by emotional reactions). The probabilistic model relies on a Dynamic Decision Network to leverage any indirect evidence on the user's emotional state, in order to estimate this state and any other related variable in the model. This is crucial in a modeling task in which the available evidence usually varies with the user and with each particular interaction. The probabilistic model we present is to be used by decision theoretic pedagogical agents to generate interventions aimed at achieving the best tradeoff between a user's learning and engagement during the interaction with educational games.","[{'authorId': '1692714', 'name': 'C. Conati'}]",579.0,"{'bibtex': ""@Article{Conati2002ProbabilisticAO,\n author = {C. Conati},\n journal = {Applied Artificial Intelligence},\n pages = {555 - 575},\n title = {Probabilistic assessment of user's emotions in educational games},\n volume = {16},\n year = {2002}\n}\n""}",,"{'volume': '16', 'pages': '555 - 575', 'name': 'Applied Artificial Intelligence'}",44.0,Probabilistic assessment of user's emotions in educational games,2002.0
1461,7ba39024ac05a64dfc3841ba7e15ff43518c764e,"This study tested whether computers embedded with the most minimal gender cues will evoke gender-based stereotypic responses. Using an experimental paradigm (N = 40) that involved computers with voice output, the study tested 3 gender-based stereotypes under conditions in which all suggestions of gender were removed, with the sole exception of vocal cues. In all 3 cases, gender-stereotypic responses were obtained. Because the experimental manipulation involved no deception regarding the source of the voices. this study presents evidence that the tendency to gender stereotype is extremely powerful, extending even to stereotyping of machines.","[{'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '33875827', 'name': 'Youngme Moon'}, {'authorId': '2053842834', 'name': 'Nancy Green'}]",504.0,"{'bibtex': '@Article{Nass1997AreMG,\n author = {C. Nass and Youngme Moon and Nancy Green},\n journal = {Journal of Applied Social Psychology},\n pages = {864-876},\n title = {Are Machines Gender Neutral? Gender‐Stereotypic Responses to Computers With Voices},\n volume = {27},\n year = {1997}\n}\n'}",,"{'volume': '27', 'pages': '864-876', 'name': 'Journal of Applied Social Psychology'}",13.0,Are Machines Gender Neutral? Gender‐Stereotypic Responses to Computers With Voices,1997.0
1462,7babda4f724d3914f3d0c9aeb7c2a209b0ccb919,,"[{'authorId': '144605639', 'name': 'R. Boone'}, {'authorId': '33400830', 'name': 'R. Buck'}]",222.0,"{'bibtex': '@Article{Boone2003EmotionalEA,\n author = {R. Boone and R. Buck},\n journal = {Journal of Nonverbal Behavior},\n pages = {163-182},\n title = {Emotional Expressivity and Trustworthiness: The Role of Nonverbal Behavior in the Evolution of Cooperation},\n volume = {27},\n year = {2003}\n}\n'}",,"{'volume': '27', 'pages': '163-182', 'name': 'Journal of Nonverbal Behavior'}",45.0,Emotional Expressivity and Trustworthiness: The Role of Nonverbal Behavior in the Evolution of Cooperation,2003.0
1463,7bb9285380b76cb1a330129a94e7b7c3c86551f6,"Traditional temporal relations propagating is based on Allen's Interval Algebra. This paper proposes an alternative method to propagate temporal relations among intervals, in which 5 2 5 matrices are used to represent temporal relations of intervals. Hence, the propagation of temporal relations is transformed into a numerical computation. For efficiency, we use the special values of the thirteen matrices to determine the possible temporal relations between two given intervals by using only the final resultant matrix so as to optimize the propagation. To evaluate the utility of the proposed technique, we have implemented the matrix representation in Java. The experimental results demonstrate that the approach is efficient and promising.","[{'authorId': '1734695', 'name': 'Shichao Zhang'}, {'authorId': '32076894', 'name': 'Chengqi Zhang'}]",24.0,"{'bibtex': '@Article{Zhang2002PropagatingTR,\n author = {Shichao Zhang and Chengqi Zhang},\n journal = {Applied Artificial Intelligence},\n pages = {1 - 27},\n title = {Propagating temporal relations of intervals by matrix},\n volume = {16},\n year = {2002}\n}\n'}",,"{'volume': '16', 'pages': '1 - 27', 'name': 'Applied Artificial Intelligence'}",48.0,Propagating temporal relations of intervals by matrix,2002.0
1464,7bf2d2bdde14b0dcd9a73f97f8df4743bc33630b,"How do social cues such as gesturing, facial expression, eye gaze, and human-like movement affect multimedia learning with onscreen agents? To help address this question, students were asked to twice view a 4-min narrated presentation on how solar cells work in which the screen showed an animated pedagogical agent standing to the left of 11 successive slides. Across three experiments, learners performed better on a transfer test when a human-voiced agent displayed human-like gestures, facial expression, eye gaze, and body movement than when the agent did not, yielding an embodiment effect. In Experiment 2 the embodiment effect was found when the agent spoke in a human voice but not in a machine voice. In Experiment 3, the embodiment effect was found both when students were told the onscreen agent was consistent with their choice of agent characteristics and when inconsistent. Students who viewed a highly embodied agent also rated the social attributes of the agent more positively than did students who viewed a nongesturing agent. The results are explained by social agency theory, in which social cues in a multimedia message prime a feeling of social partnership in the learner, which leads to deeper cognitive processing during learning, and results in a more meaningful learning outcome as reflected in transfer test performance.","[{'authorId': '1819200', 'name': 'R. Mayer'}, {'authorId': '8359211', 'name': 'C. Dapra'}]",213.0,"{'bibtex': '@Article{Mayer2012AnEE,\n author = {R. Mayer and C. Dapra},\n journal = {Journal of experimental psychology. Applied},\n pages = {\n          239-52\n        },\n title = {An embodiment effect in computer-based learning with animated pedagogical agents.},\n volume = {18 3},\n year = {2012}\n}\n'}",,"{'volume': '18 3', 'pages': '\n          239-52\n        ', 'name': 'Journal of experimental psychology. Applied'}",68.0,An embodiment effect in computer-based learning with animated pedagogical agents.,2012.0
1466,7c0220f4a3f19b24fce735382a863b60d7f28c5e,,"[{'authorId': '1729279', 'name': 'Timo Partala'}, {'authorId': '1718377', 'name': 'Veikko Surakka'}]",747.0,"{'bibtex': '@Article{Partala2003PupilSV,\n author = {Timo Partala and Veikko Surakka},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {185-198},\n title = {Pupil size variation as an indication of affective processing},\n volume = {59},\n year = {2003}\n}\n'}",,"{'volume': '59', 'pages': '185-198', 'name': 'Int. J. Hum. Comput. Stud.'}",51.0,Pupil size variation as an indication of affective processing,2003.0
1467,7c1d4fbaff6a275e8cd384c281668ffb6d038dd5,,"[{'authorId': '2633572', 'name': 'T. Pejsa'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}]",16.0,"{'bibtex': '@Inproceedings{Pejsa2017WhoMH,\n author = {T. Pejsa and Michael Gleicher and Bilge Mutlu},\n pages = {347-359},\n title = {Who, Me? How Virtual Agents Can Shape Conversational Footing in Virtual Reality},\n year = {2017}\n}\n'}",,{'pages': '347-359'},31.0,"Who, Me? How Virtual Agents Can Shape Conversational Footing in Virtual Reality",2017.0
1468,7c287f054c96d16f643eb2d7bfbfffc23147d00b,"Affecting to around 1% of the population, Autism is sometimes described as a different approach to interacting with the world. Adapting the surrounding objects and systems can improve their experience and their relative's. This project is based on previous research where it has been shown that toys can influence positively in a child's development. Also, new technologies as Augmented Reality (AR) can be beneficial for these children in attracting and keeping their attention. The proposed game would engage the player by first creating a customized monster with the help of different AR markers. In a second stage, the player would try to guess the emotion of different monsters or virtual humans. The game will be tested in further stages to check its suitability for the AS children and the effect on their emotion recognition skills.","[{'authorId': '2072583196', 'name': 'Vicente Lopez Trompo Daniel'}, {'authorId': '2072508081', 'name': 'Han Ting'}, {'authorId': '3320826', 'name': 'P. Ratsamee'}, {'authorId': '2204175823', 'name': 'Haruo Takemura'}]",4.0,"{'bibtex': '@Article{Daniel2019AnAP,\n author = {Vicente Lopez Trompo Daniel and Han Ting and P. Ratsamee and Haruo Takemura},\n journal = {Proceedings of the 3rd International Conference on Digital Technology in Education},\n title = {An AR Puzzle Application for Improving Emotion Recognition for AS Children},\n year = {2019}\n}\n'}",,{'name': 'Proceedings of the 3rd International Conference on Digital Technology in Education'},30.0,An AR Puzzle Application for Improving Emotion Recognition for AS Children,2019.0
1469,7c3708e5cf52f6354916d7b565f33b35b5595e92,,"[{'authorId': '1713369', 'name': 'L. Devillers'}, {'authorId': '1766081', 'name': 'L. Vidrascu'}, {'authorId': '145204681', 'name': 'L. Lamel'}]",359.0,"{'bibtex': '@Article{Devillers2005ChallengesIR,\n author = {L. Devillers and L. Vidrascu and L. Lamel},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          407-22\n        },\n title = {Challenges in real-life emotion annotation and machine learning based detection},\n volume = {18 4},\n year = {2005}\n}\n'}",,"{'volume': '18 4', 'pages': '\n          407-22\n        ', 'name': 'Neural networks : the official journal of the International Neural Network Society'}",39.0,Challenges in real-life emotion annotation and machine learning based detection,2005.0
1470,7c4a4b065cf18f7b13e6648ef12c42fd8bb79735,"In order to achieve more believable interactions with artificial agents, there is a need to produce dialogue that is not only relevant, but also emotionally appropriate and consistent. This paper presents a comprehensive system that models the emotional state of users and an agent to dynamically adapt dialogue utterance selection. A Partially Observable Markov Decision Process (POMDP) with an online solver is used to model user reactions in real-time. The model decides the emotional content of the next utterance based on the rewards from the users and the agent. The previous approaches are extended through jointly modeling the user and agent emotions, maintaining this model over time with a memory, and enabling interactions with multiple users. A proof of concept user study is used to demonstrate that the system can deliver and maintain distinct agent personalities during multiparty interactions.","[{'authorId': '9538117', 'name': 'Bahar Irfan'}, {'authorId': '1999878327', 'name': 'Anika Narayanan'}, {'authorId': '143953789', 'name': 'James Kennedy'}]",3.0,"{'bibtex': '@Article{Irfan2020DynamicEL,\n author = {Bahar Irfan and Anika Narayanan and James Kennedy},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Dynamic Emotional Language Adaptation in Multiparty Interactions with Agents},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},47.0,Dynamic Emotional Language Adaptation in Multiparty Interactions with Agents,2020.0
1471,7c79e32240ade9b3c51dc9d6369f7f7b8df0d297,"A variety of theoretical frameworks predict the resemblance of behaviors between two people engaged in communication, in the form of coordination, mimicry, or alignment. However, little is known about the time course of the behavior matching, even though there is evidence that dyads synchronize oscillatory motions (e.g., postural sway). This study examined the temporal structure of nonoscillatory actions-language, facial, and gestural behaviors-produced during a route communication task. The focus was the temporal relationship between matching behaviors in the interlocutors (e.g., facial behavior in one interlocutor vs. the same facial behavior in the other interlocutor). Cross-recurrence analysis revealed that within each category tested (language, facial, gestural), interlocutors synchronized matching behaviors, at temporal lags short enough to provide imitation of one interlocutor by the other, from one conversational turn to the next. Both social and cognitive variables predicted the degree of temporal organization. These findings suggest that the temporal structure of matching behaviors provides low-level and low-cost resources for human interaction.","[{'authorId': '2073332', 'name': 'M. Louwerse'}, {'authorId': '144301561', 'name': 'Rick Dale'}, {'authorId': '2695814', 'name': 'E. Bard'}, {'authorId': '1961178', 'name': 'Patrick Jeuniaux'}]",261.0,"{'bibtex': '@Article{Louwerse2012BehaviorMI,\n author = {M. Louwerse and Rick Dale and E. Bard and Patrick Jeuniaux},\n journal = {Cognitive science},\n pages = {\n          1404-26\n        },\n title = {Behavior Matching in Multimodal Communication Is Synchronized},\n volume = {36 8},\n year = {2012}\n}\n'}",,"{'volume': '36 8', 'pages': '\n          1404-26\n        ', 'name': 'Cognitive science'}",75.0,Behavior Matching in Multimodal Communication Is Synchronized,2012.0
1472,7caaedb86c80b6f97f08068a3786dbf4a6c9dfa6,"Background Computer Networks have a tendency to grow at an unprecedented scale. Modern networks involve not only computers but also a wide variety of other interconnected devices ranging from mobile phones to other household items fitted with sensors. This vision of the ""Internet of Things"" (IoT) implies an inherent difficulty in modeling problems. Purpose It is practically impossible to implement and test all scenarios for large-scale and complex adaptive communication networks as part of Complex Adaptive Communication Networks and Environments (CACOONS). The goal of this study is to explore the use of Agent-based Modeling as part of the Cognitive Agent-based Computing (CABC) framework to model a Complex communication network problem. Method We use Exploratory Agent-based Modeling (EABM), as part of the CABC framework, to develop an autonomous multi-agent architecture for managing carbon footprint in a corporate network. To evaluate the application of complexity in practical scenarios, we have also introduced a company-defined computer usage policy. Results The conducted experiments demonstrated two important results: Primarily CABC-based modeling approach such as using Agent-based Modeling can be an effective approach to modeling complex problems in the domain of IoT. Secondly, the specific problem of managing the Carbon footprint can be solved using a multiagent system approach.","[{'authorId': '2069416', 'name': 'S. Laghari'}, {'authorId': '1795560', 'name': 'M. Niazi'}]",54.0,"{'bibtex': '@Article{Laghari2016ModelingTI,\n author = {S. Laghari and M. Niazi},\n journal = {PLoS ONE},\n title = {Modeling the Internet of Things, Self-Organizing and Other Complex Adaptive Communication Networks: A Cognitive Agent-Based Computing Approach},\n volume = {11},\n year = {2016}\n}\n'}",,"{'volume': '11', 'name': 'PLoS ONE'}",61.0,"Modeling the Internet of Things, Self-Organizing and Other Complex Adaptive Communication Networks: A Cognitive Agent-Based Computing Approach",2016.0
1473,7cb63a03c225c8032a872a28e60fbec9cb88fc99,"Judgments of trustworthiness from faces determine basic approach/avoidance responses and approximate the valence evaluation of faces that runs across multiple person judgments. Here, based on trustworthiness judgments and using a computer model for face representation, we built a model for representing face trustworthiness (study 1). Using this model, we generated novel faces with an increased range of trustworthiness and used these faces as stimuli in a functional Magnetic Resonance Imaging study (study 2). Although participants did not engage in explicit evaluation of the faces, the amygdala response changed as a function of face trustworthiness. An area in the right amygdala showed a negative linear response-as the untrustworthiness of faces increased so did the amygdala response. Areas in the left and right putamen, the latter area extended into the anterior insula, showed a similar negative linear response. The response in the left amygdala was quadratic--strongest for faces on both extremes of the trustworthiness dimension. The medial prefrontal cortex and precuneus also showed a quadratic response, but their response was strongest to faces in the middle range of the trustworthiness dimension.","[{'authorId': '145441940', 'name': 'A. Todorov'}, {'authorId': '2235458', 'name': 'S. Baron'}, {'authorId': '1896078', 'name': 'N. Oosterhof'}]",369.0,"{'bibtex': '@Article{Todorov2008EvaluatingFT,\n author = {A. Todorov and S. Baron and N. Oosterhof},\n journal = {Social cognitive and affective neuroscience},\n pages = {\n          119-27\n        },\n title = {Evaluating face trustworthiness: a model based approach.},\n volume = {3 2},\n year = {2008}\n}\n'}",,"{'volume': '3 2', 'pages': '\n          119-27\n        ', 'name': 'Social cognitive and affective neuroscience'}",42.0,Evaluating face trustworthiness: a model based approach.,2008.0
1474,7cbac9e38d6d0bf3d156093b7f7b605b7d04d4c2,,"[{'authorId': '1381933697', 'name': 'M. S. El-Nasr'}, {'authorId': '143674364', 'name': 'J. Yen'}, {'authorId': '1681317', 'name': 'T. Ioerger'}]",522.0,"{'bibtex': '@Article{El-Nasr2000FLAMEFuzzyLA,\n author = {M. S. El-Nasr and J. Yen and T. Ioerger},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {219-257},\n title = {FLAME—Fuzzy Logic Adaptive Model of Emotions},\n volume = {3},\n year = {2000}\n}\n'}",,"{'volume': '3', 'pages': '219-257', 'name': 'Autonomous Agents and Multi-Agent Systems'}",97.0,FLAME—Fuzzy Logic Adaptive Model of Emotions,2000.0
1482,7cbb8d75d25ae34d41a0fe7299275c7b238f8314,"Emotional mimicry—quick and spontaneous matching of another’s expressions—is a well-documented phenomenon that is associated with numerous social outcomes. Although the mechanisms underlying mimicry are not fully understood, there is growing awareness that it is more than a one-to-one motor matching of others’ expressions and may be the result of neural simulation. If true, it is possible that mimicry could extend to other parts of the body, even in the absence of visual information from that body part. Indeed, we found that passively viewing anger and fear expressions, without accompanying information from the body, voice or other channels, produced both facial mimicry and corresponding responses in arm muscles that make a fist or a defensive posture. This suggests that observers simulated observed expressions and that activity may have spilled over to other areas to create a body response.","[{'authorId': '48254408', 'name': 'E. Moody'}, {'authorId': '145182723', 'name': 'C. Reed'}, {'authorId': '1753404815', 'name': 'Tara Van Bommel'}, {'authorId': '145972787', 'name': 'B. App'}, {'authorId': '21464189', 'name': 'D. McIntosh'}]",18.0,"{'bibtex': '@Article{Moody2018EmotionalMB,\n author = {E. Moody and C. Reed and Tara Van Bommel and B. App and D. McIntosh},\n journal = {Social Psychological and Personality Science},\n pages = {844 - 852},\n title = {Emotional Mimicry Beyond the Face?},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'pages': '844 - 852', 'name': 'Social Psychological and Personality Science'}",52.0,Emotional Mimicry Beyond the Face?,2018.0
1483,7cc71e4c0bb39c2d3ff3cd039c5e62f726f6d493,"Understanding emotions of others requires a theory of mind approach providing knowledge of internal appraisal and regulation processes of emotions. Multi-modal social signal classification is insufficient for understanding emotional expressions. Mainly, because many communicative, emotional expressions are not directly related to internal emotional states. Moreover, the recognition of the expression's direction is neglected so far. Even if social signals reveal emotional aspects, the recognition with signal classifiers cannot explain internal appraisal or regulation processes. Using that information is one approach for building cognitive empathic agents with the ability to address observations and motives in an empathic dialogue. In this paper, we introduce a computational model of user emotions for empathic agents. It combines a simulation of appraisal and regulation processes with a social signal interpretation taking directions of expressions into account. Our evaluation shows that social signal sequences can be related to emotion regulation processes. Their recognition and using appraisal and regulation knowledge enables our agent to react empathically.","[{'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '2375527', 'name': 'T. Schneeberger'}, {'authorId': '2230836', 'name': 'Tobias Baur'}, {'authorId': '1742930', 'name': 'E. André'}]",22.0,"{'bibtex': '@Inproceedings{Gebhard2018MARSSIMO,\n author = {Patrick Gebhard and T. Schneeberger and Tobias Baur and E. André},\n pages = {497-506},\n title = {MARSSI: Model of Appraisal, Regulation, and Social Signal Interpretation},\n year = {2018}\n}\n'}",,{'pages': '497-506'},80.0,"MARSSI: Model of Appraisal, Regulation, and Social Signal Interpretation",2018.0
1484,7ce45f9effe7f4345d50076dcdb30dcb2095f4ca,"We are using the results of the study to improve the design of both programs. We plan to repeat these evaluations several times as development of both programs progresses. Evaluation with kindergarten and elementary school deaf children and their teachers will be done in collaboration with the Indiana School for the Deaf and will start in the fall of 2009. We will report the results in a future article. A strong need exists for solutions that allow deaf users to communicate and interact in an environment free of prejudice, stigma, technological barrier, or other obstacles. The fact that all children were able to engage with and complete the tasks in both test systems is encouraging.","[{'authorId': '1403309968', 'name': 'N. Adamo-Villani'}, {'authorId': '2465833', 'name': 'R. Wilbur'}]",31.0,"{'bibtex': '@Article{Adamo-Villani2008TwoNT,\n author = {N. Adamo-Villani and R. Wilbur},\n journal = {IEEE MultiMedia},\n title = {Two Novel Technologies for Accessible Math and Science Education},\n volume = {15},\n year = {2008}\n}\n'}",,"{'volume': '15', 'name': 'IEEE MultiMedia'}",27.0,Two Novel Technologies for Accessible Math and Science Education,2008.0
1485,7ce7b3bbd6457356215f822211cbe40ab619d65e,"We describe ubiGaze, a novel wearable ubiquitous method to augment any real-world object with invisible messages through gaze gestures that lock the message into the object. This enables a context and location dependent messaging service, which users can utilize discreetly and effortlessly. Further, gaze gestures can be used as an authentication method, even when the augmented object is publicly known. We developed a prototype using two wearable devices: a Pupil eye tracker equipped with a scene camera and a Sony Smartwatch 3. The eye tracker follows the users' gaze, the scene camera captures distinct features from the selected real-world object, and the smartwatch provides both input and output modalities for selecting and displaying messages. We describe the concept, design, and implementation of our real-world system. Finally, we discuss research implications and address future work.","[{'authorId': '31944767', 'name': 'Mihai Bâce'}, {'authorId': '35202419', 'name': 'T. Leppänen'}, {'authorId': '40249938', 'name': 'David Gil de Gomez'}, {'authorId': '21475281', 'name': 'Argenis Ramirez Gomez'}]",37.0,"{'bibtex': '@Article{Bâce2016ubiGazeUA,\n author = {Mihai Bâce and T. Leppänen and David Gil de Gomez and Argenis Ramirez Gomez},\n journal = {SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},\n title = {ubiGaze: ubiquitous augmented reality messaging using gaze gestures},\n year = {2016}\n}\n'}",,{'name': 'SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications'},19.0,ubiGaze: ubiquitous augmented reality messaging using gaze gestures,2016.0
1486,7cf137c73fbf3febc37570b2f74d1e034672a5db,,"[{'authorId': '1714997', 'name': 'K. Doya'}]",652.0,"{'bibtex': '@Article{Doya2002MetalearningAN,\n author = {K. Doya},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          495-506\n        },\n title = {Metalearning and neuromodulation},\n volume = {15 4-6},\n year = {2002}\n}\n'}",,"{'volume': '15 4-6', 'pages': '\n          495-506\n        ', 'name': 'Neural networks : the official journal of the International Neural Network Society'}",88.0,Metalearning and neuromodulation,2002.0
1487,7d0531aaaa4dbf4033da964289843e7812262702,"There is a rich variety of data sets for sentiment analysis (viz., polarity and subjectivity classification). For the more challenging task of detecting discrete emotions following the definitions of Ekman and Plutchik, however, there are much fewer data sets, and notably no resources for the social media domain. This paper contributes to closing this gap by extending the SemEval 2016 stance and sentiment dataset with emotion annotation. We (a) analyse annotation reliability and annotation merging; (b) investigate the relation between emotion annotation and the other annotation layers (stance, sentiment); (c) report modelling results as a baseline for future work.","[{'authorId': '7959237', 'name': 'Hendrik Schuff'}, {'authorId': '144435436', 'name': 'Jeremy Barnes'}, {'authorId': '25079660', 'name': 'Julian Mohme'}, {'authorId': '1708581', 'name': 'Sebastian Padó'}, {'authorId': '66339110', 'name': 'Roman Klinger'}]",78.0,"{'bibtex': '@Inproceedings{Schuff2017AnnotationMA,\n author = {Hendrik Schuff and Jeremy Barnes and Julian Mohme and Sebastian Padó and Roman Klinger},\n pages = {13-23},\n title = {Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus},\n year = {2017}\n}\n'}",,{'pages': '13-23'},54.0,"Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus",2017.0
1488,7d15d87384b4542fc1c72be6a4a9d9380190f614,"Human emotional behavior, personality, and body language are the essential elements in the recognition of a believable synthetic story character. This paper presents an approach using story scripts and action descriptions in a form similar to the content description of storyboards to predict specific personality and emotional states. By adopting the Abridged Big Five Circumplex (AB5C) Model of personality from the study of psychology as a basis for a computational model, we construct a hierarchical fuzzy rule-based system to facilitate the personality and emotion control of the body language of a dynamic story character. The story character can consistently perform specific postures and gestures based on his/her personality type. Story designers can devise a story context in the form of our story interface which predictably motivates personality and emotion values to drive the appropriate movements of the story characters. Our system takes advantage of relevant knowledge described by psychologists and researchers of storytelling, nonverbal communication, and human movement. Our ultimate goal is to facilitate the high-level control of a synthetic character","[{'authorId': '3062715', 'name': 'Wen-Poh Su'}, {'authorId': '144809479', 'name': 'Binh Pham'}, {'authorId': '144305327', 'name': 'Aster Wardhani'}]",62.0,"{'bibtex': '@Article{Su2007PersonalityAE,\n author = {Wen-Poh Su and Binh Pham and Aster Wardhani},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n title = {Personality and Emotion-Based High-Level Control of Affective Story Characters},\n volume = {13},\n year = {2007}\n}\n'}",,"{'volume': '13', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",42.0,Personality and Emotion-Based High-Level Control of Affective Story Characters,2007.0
1491,7d5b2388945b5ba53512ab775d80f4659092307f,"Enabling the machines with empathetic abilities to provide context-consistent responses is crucial on both semantic and emotional levels. The task of empathetic dialogue generation is proposed to address this problem. However, lacking external knowledge makes it difficult to perceive implicit emotions from limited dialogue history. To address the above challenges, we propose to leverage multi-type knowledge, i.e, the commonsense knowledge and emotional lexicon, to explicitly understand and express emotions in empathetic dialogue generation. We first enrich the dialogue history by jointly interacting with two-type knowledge and construct an emotional context graph. Then we introduce a multi-type knowledge-aware context encoder to learn emotional context representations and distill emotional signals, which are the prerequisites to predicate emotions expressed in responses. Finally, we propose an emotional cross-attention mechanism to exploit the emotional dependencies between the emotional context graph and the target empathetic response. Conducted on a benchmark dataset, extensive experimental results show that our proposed framework outperforms state-of-the-art baselines in terms of automatic metrics and human evaluations.","[{'authorId': '47422209', 'name': 'Qintong Li'}, {'authorId': '2193560', 'name': 'Piji Li'}, {'authorId': '1721165', 'name': 'Zhumin Chen'}, {'authorId': '2780667', 'name': 'Z. Ren'}]",16.0,"{'bibtex': '@Article{Li2020TowardsED,\n author = {Qintong Li and Piji Li and Zhumin Chen and Z. Ren},\n journal = {arXiv: Computation and Language},\n title = {Towards Empathetic Dialogue Generation over Multi-type Knowledge.},\n year = {2020}\n}\n'}",,"{'volume': '', 'name': 'arXiv: Computation and Language'}",55.0,Towards Empathetic Dialogue Generation over Multi-type Knowledge.,2020.0
1492,7d62f1b94a62f9f649d27eb1396379654498feee,"We present a real-time facial expression recognition toolkit that can automatically code the expressions of multiple people simultaneously. The toolkit is available across major mobile and desktop platforms (Android, iOS, Windows). The system is trained on the world's largest dataset of facial expressions and has been optimized to operate on mobile devices and with very few false detections. The toolkit offers the potential for the design of novel interfaces that respond to users' emotional states based on their facial expressions. We present a demonstration application that provides real-time visualization of the expressions captured by the camera.","[{'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '3100369', 'name': 'A. Mahmoud'}, {'authorId': '3396867', 'name': 'Mohammad Mavadati'}, {'authorId': '145354041', 'name': 'M. Amr'}, {'authorId': '40463348', 'name': 'Jay Turcot'}, {'authorId': '1754451', 'name': 'R. E. Kaliouby'}]",282.0,"{'bibtex': '@Article{McDuff2016AFFDEXSA,\n author = {Daniel J. McDuff and A. Mahmoud and Mohammad Mavadati and M. Amr and Jay Turcot and R. E. Kaliouby},\n journal = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},\n title = {AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems'},10.0,AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit,2016.0
1493,7dca0cf429291117f5f5d494defff1190481d972,,"[{'authorId': '144006457', 'name': 'R. Moreno'}, {'authorId': '1819200', 'name': 'R. Mayer'}]",1148.0,"{'bibtex': '@Article{Moreno2007InteractiveML,\n author = {R. Moreno and R. Mayer},\n journal = {Educational Psychology Review},\n pages = {309-326},\n title = {Interactive Multimodal Learning Environments},\n volume = {19},\n year = {2007}\n}\n'}",,"{'volume': '19', 'pages': '309-326', 'name': 'Educational Psychology Review'}",89.0,Interactive Multimodal Learning Environments,2007.0
1494,7dd06cf1132d8244a7db77768d024be7362022c5,,"[{'authorId': '51118484', 'name': 'Samuel S. Sohn'}, {'authorId': '2108143874', 'name': 'Xun Zhang'}, {'authorId': '31899849', 'name': 'F. Geraci'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",8.0,"{'bibtex': '@Inproceedings{Sohn2018AnEA,\n author = {Samuel S. Sohn and Xun Zhang and F. Geraci and Mubbasir Kapadia},\n pages = {2250-2252},\n title = {An Emotionally Aware Embodied Conversational Agent},\n year = {2018}\n}\n'}",,{'pages': '2250-2252'},17.0,An Emotionally Aware Embodied Conversational Agent,2018.0
1496,7dd4a0fb7368a510840cffd7dcad618962d3b8e4,,"[{'authorId': '145207410', 'name': 'J. Bates'}, {'authorId': '2695299', 'name': 'A. B. Loyall'}, {'authorId': '145034916', 'name': 'W. S. Reilly'}]",321.0,"{'bibtex': '@Inproceedings{Bates1992AnAF,\n author = {J. Bates and A. B. Loyall and W. S. Reilly},\n pages = {55-68},\n title = {An Architecture for Action, Emotion, and Social Behavior},\n year = {1992}\n}\n'}",,{'pages': '55-68'},37.0,"An Architecture for Action, Emotion, and Social Behavior",1992.0
1498,7e00c5095b474571ee74642a5c8e6dcf5fc5c23b,,"[{'authorId': '1689001', 'name': 'C. Jonker'}, {'authorId': '21143294', 'name': 'Joost J. P. Schalken'}, {'authorId': '145639856', 'name': 'J. Theeuwes'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",66.0,"{'bibtex': '@Inproceedings{Jonker2004HumanEI,\n author = {C. Jonker and Joost J. P. Schalken and J. Theeuwes and Jan Treur},\n pages = {206-220},\n title = {Human Experiments in Trust Dynamics},\n year = {2004}\n}\n'}",,{'pages': '206-220'},16.0,Human Experiments in Trust Dynamics,2004.0
1499,7e121dac778725f891ab8e43463db2ec216039d0,,"[{'authorId': '38542466', 'name': 'Xiaojie Wang'}, {'authorId': '2006373', 'name': 'Caixia Yuan'}]",22.0,"{'bibtex': '@Article{Wang2016RecentAO,\n author = {Xiaojie Wang and Caixia Yuan},\n journal = {CAAI Trans. Intell. Technol.},\n pages = {303-312},\n title = {Recent Advances on Human-Computer Dialogue},\n volume = {1},\n year = {2016}\n}\n'}",,"{'volume': '1', 'pages': '303-312', 'name': 'CAAI Trans. Intell. Technol.'}",66.0,Recent Advances on Human-Computer Dialogue,2016.0
1500,7e297a8431974c92e779fa16a2f40dec32b60e43,"Delivering appealing virtual characters conveying personality is becoming extremely important in the entertainment industry and beyond. A theory called the 'Uncanny Valley' has been used to describe the phenomenon that the appearance of a virtual character can contribute to negative/positive audience reactions to that character [Mori 1970]. Since the style used to render a character strongly changes the appearance, we investigate whether a difference in render style can indirectly influence audience reaction, which we measure based on perception of personality. Based on psychology research, we first scripted original character dialogues in order to convey a range of ten typical personality types. Then, a professional actor was recruited to act out these dialogues, while his face and body motion and audio were recorded. The performances were mapped onto a virtual character rendered in two styles that differ in appearance: an appealing cartoon style and unappealing ill style (Figure 1). In our experiment, participants were asked questions about the character's personality in order for us to test if the difference in render style causes differences in personality perception. Our results found an indirect effect of render style where the cartoon style was rated as having a more agreeable personality than the ill style. This result has implications for developers interested in creating appealing virtual humans, avoiding the 'Uncanny Valley' phenomenon.","[{'authorId': '1710384', 'name': 'Katja Zibrek'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",39.0,"{'bibtex': '@Article{Zibrek2014DoesRS,\n author = {Katja Zibrek and R. Mcdonnell},\n journal = {Proceedings of the ACM Symposium on Applied Perception},\n title = {Does render style affect perception of personality in virtual humans?},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the ACM Symposium on Applied Perception'},26.0,Does render style affect perception of personality in virtual humans?,2014.0
1502,7ebc5f16a9ace2d00036ffb6a81562760a5e042e,,"[{'authorId': '2109116473', 'name': 'Chen Wang'}, {'authorId': '23567239', 'name': 'Béatrice Biancardi'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '40123315', 'name': 'Angelo Cafaro'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1809085', 'name': 'T. Pun'}, {'authorId': '2343145', 'name': 'G. Chanel'}]",2.0,"{'bibtex': '@Inproceedings{Wang2020ImpressionDA,\n author = {Chen Wang and Béatrice Biancardi and M. Mancini and Angelo Cafaro and C. Pelachaud and T. Pun and G. Chanel},\n pages = {260-278},\n title = {Impression Detection and Management Using an Embodied Conversational Agent},\n year = {2020}\n}\n'}",,{'pages': '260-278'},42.0,Impression Detection and Management Using an Embodied Conversational Agent,2020.0
1503,7ec9a8bbffca6867a9b225d6c7c3e606b2a02894,"Steering is a challenging task, required by nearly all agents in virtual worlds. There is a large and growing number of approaches for steering, and it is becoming increasingly important to ask a fundamental question: how can we objectively compare steering algorithms? To our knowledge, there is no standard way of evaluating or comparing the quality of steering solutions. This paper presents SteerBench: a benchmark framework for objectively evaluating steering behaviors for virtual agents. We propose a diverse set of test cases, metrics of evaluation, and a scoring method that can be used to compare different steering algorithms. Our framework can be easily customized by a user to evaluate specific behaviors and new test cases. We demonstrate our benchmark process on two example steering algorithms, showing the insight gained from our metrics. We hope that this framework can grow into a standard for steering evaluation. Copyright © 2009 John Wiley & Sons, Ltd.","[{'authorId': '38940063', 'name': 'Shawn Singh'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '1737527', 'name': 'P. Faloutsos'}, {'authorId': '1718128', 'name': 'Glenn D. Reinman'}]",97.0,"{'bibtex': '@Article{Singh2009SteerBenchAB,\n author = {Shawn Singh and Mubbasir Kapadia and P. Faloutsos and Glenn D. Reinman},\n journal = {Computer Animation and Virtual Worlds},\n title = {SteerBench: a benchmark suite for evaluating steering behaviors},\n volume = {20},\n year = {2009}\n}\n'}",,"{'volume': '20', 'name': 'Computer Animation and Virtual Worlds'}",28.0,SteerBench: a benchmark suite for evaluating steering behaviors,2009.0
1504,7ed2bfa5487756e7de8ed3a895644c8e4d4cb9e1,"Psychotherapy is among the most effective techniques for combating mental health issues, and virtual reality is beginning to be explored as a way to enhance the efficacy of various psychotherapeutic treatments. In this paper we propose an immersive virtual reality mobile platform for Self-Attachment psychotherapy. Under the SelfAttachment therapeutic framework, the causes of disorders such as chronic anxiety and depression are traced back to the quality of the individual’s attachment with their primary caregiver during childhood. Our proposed platform aims to assist the user in enhancing their capacities for self-regulation of emotion, by means of earning secure attachment through the experience of positive attachment interactions, missed in their childhood. In the virtual environment provided by the platform, the adult-self of the user learns to create and strengthen an affectional and supportive bond with the inner-child. It is hypothesised that by long term potentiation and neuroplasticity, the user gradually develops new neural pathways and matures into an effective secure attachment object for the inner-child, thereby enabling the self-regulation of emotions.","[{'authorId': '2585368', 'name': 'David Cittern'}, {'authorId': '1694989', 'name': 'A. Edalat'}, {'authorId': '2485535', 'name': 'I. Ghaznavi'}]",6.0,"{'bibtex': '@Inproceedings{Cittern2017AnIV,\n author = {David Cittern and A. Edalat and I. Ghaznavi},\n title = {An immersive virtual reality mobile platform for self-attachment},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",31.0,An immersive virtual reality mobile platform for self-attachment,2017.0
1506,7f00fdba4fdcb1c8887dd375d3a7186486121356,,"[{'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '1975046', 'name': 'Jeeheon Ryu'}, {'authorId': '48977180', 'name': 'E. Shen'}]",69.0,"{'bibtex': '@Inproceedings{Baylor2003TheEO,\n author = {A. L. Baylor and Jeeheon Ryu and E. Shen},\n pages = {452-458},\n title = {The Effects of Pedagogical Agent Voice and Animation on Learning, Motivation and Perceived Persona},\n volume = {2003},\n year = {2003}\n}\n'}",,"{'volume': '2003', 'pages': '452-458', 'name': ''}",23.0,"The Effects of Pedagogical Agent Voice and Animation on Learning, Motivation and Perceived Persona",2003.0
1507,7f290e4bbb14e01308144ed37b41d3165131fe60,,"[{'authorId': '4021295', 'name': 'M. Mikulincer'}, {'authorId': '32963249', 'name': 'P. Shaver'}]",708.0,"{'bibtex': '@Article{Mikulincer2012AnAP,\n author = {M. Mikulincer and P. Shaver},\n journal = {World Psychiatry},\n title = {An attachment perspective on psychopathology},\n volume = {11},\n year = {2012}\n}\n'}",,"{'volume': '11', 'name': 'World Psychiatry'}",41.0,An attachment perspective on psychopathology,2012.0
1508,7f4ad37feac723ae60701d83b70862c7921817d8,"Emotional reasoning can be an important contribution to automated tutoring and training systems. This paper describes Emile, a model of emotional reasoning that builds upon existing approaches and significantly generalizes and extends their capabilities. The main contribution is to show how an explicit planning model allows a more general treatment of several stages of the reasoning process. The model supports educational applications by allowing agents to appraise the emotional significance of events as they relate to students' (or their own) plans and goals, model and predict the emotional state of others, and alter behavior accordingly.","[{'authorId': '145438097', 'name': 'J. Gratch'}]",176.0,"{'bibtex': '@Inproceedings{Gratch2000ÉmileMP,\n author = {J. Gratch},\n pages = {325-332},\n title = {Émile: Marshalling passions in training and education},\n year = {2000}\n}\n'}",,{'pages': '325-332'},40.0,Émile: Marshalling passions in training and education,2000.0
1509,7f4b6b566d1ecfbd47868249fd8f463940e699b9,,"[{'authorId': '145037594', 'name': 'Sarel van Vuuren'}, {'authorId': '1889011', 'name': 'L. Cherney'}]",45.0,"{'bibtex': '@Article{Vuuren2014AVT,\n author = {Sarel van Vuuren and L. Cherney},\n journal = {Intelligent virtual agents : ... International Workshop, IVA ... proceedings. IVA},\n pages = {\n          438-448\n        },\n title = {A Virtual Therapist for Speech and Language Therapy},\n volume = {8637},\n year = {2014}\n}\n'}",,"{'volume': '8637', 'pages': '\n          438-448\n        ', 'name': 'Intelligent virtual agents : ... International Workshop, IVA ... proceedings. IVA'}",26.0,A Virtual Therapist for Speech and Language Therapy,2014.0
1510,7f62f4a76e5b2909600a01584e69871984ba23fa,"This study evaluated the effects of affective intensity and thematic content of foreground photographic stimuli on various physiological response systems. This was accomplished by assessing responses to pictures that varied systematically in these parameters. Along with overall effects of picture valence reported in previous work, we found effects of thematic content (i.e., specific nature of objects/events depicted) for all measures except heart rate. In addition, we found that the magnitude of startle blink, skin conductance, and corrugator muscle reactions increased with increasing affective intensity of pictures. Additionally, for these three measures, intensity effects also interacted with effects of picture content. These results indicate that stimulus parameters of intensity and thematic content exert separate-and in some cases interactive-modulatory effects on physiological reactions to emotional pictures.","[{'authorId': '2825853', 'name': 'E. Bernat'}, {'authorId': '2133311', 'name': 'C. Patrick'}, {'authorId': '5105463', 'name': 'Stephen D. Benning'}, {'authorId': '116114697', 'name': 'A. Tellegen'}]",188.0,"{'bibtex': '@Article{Bernat2006EffectsOP,\n author = {E. Bernat and C. Patrick and Stephen D. Benning and A. Tellegen},\n journal = {Psychophysiology},\n pages = {\n          93-103\n        },\n title = {Effects of picture content and intensity on affective physiological response.},\n volume = {43 1},\n year = {2006}\n}\n'}",,"{'volume': '43 1', 'pages': '\n          93-103\n        ', 'name': 'Psychophysiology'}",39.0,Effects of picture content and intensity on affective physiological response.,2006.0
1511,7f8baa24bdbefb0c5b1a0a98a52fa34defad8258,"Hypothesis I: There exists, in the human organism, a drive to evaluate his opinions and his abilities. While opinions and abilities may, at first glance, seem to be quite different things, there is a close functional tie between them. They act together in the manner in which they affect behavior. A person’s cognition (his opinions and beliefs) about the situation in which he exists and his appraisals of what he is capable of doing (his evaluation of his abilities) will together have bearing on his behavior. The holding of incorrect opinions and/or inaccurate appraisals of one’s abilities can be punishing or even fatal in many situations. It is necessary, before we proceed, to clarify the distinction between opinions and evaluations of abilities since at first glance it may seem that one’s evaluation of one’s own ability is an opinion about it. Abilities are of course manifested only through performance which is assumed to depend upon the particular ability. The clarity of the manifestation or performance can vary from instances where there is no clear ordering criterion of the ability to instances where the performance which reflects the ability can be clearly ordered. In the former case, the evaluation of the ability does function like other opinions which are not directly testable in “objective reality’. For example, a person’s evaluation of his ability to write poetry will depend to a large extent on the opinions which others have of his ability to write poetry. In cases where the criterion is unambiguous and can be clearly ordered, this furnishes an objective reality for the evaluation of one’s ability so that it depends less on the opinions of other persons and depends more on actual comparison of one’s performance with the performance of others. Thus, if a person evaluates his running ability, he will do so by comparing his time to run some distance with the times that other persons have taken. In the following pages, when we talk about evaluating an ability, we shall mean specifically the evaluation of that ability in situations where the performance is unambiguous and is known. Most situations in real life will, of course, present situations which are a mixture of opinion and ability evaluation. In a previous article (7) the author posited the existence of a drive to determine whether or not one’s opinions were “correct”. We are here stating that this same drive also produces behavior in people oriented toward obtaining an accurate appraisal of their abilities. The behavioral implication of the existence of such a drive is that we would expect to observe behaviour on the part of persons which enables them to ascertain whether or not their opinions are correct and also behavior which enables them accurately to evaluate their abilities. It is consequently","[{'authorId': '5281667', 'name': 'L. Festinger'}]",17507.0,"{'bibtex': '@Article{Festinger1954ATO,\n author = {L. Festinger},\n journal = {Human Relations},\n pages = {117 - 140},\n title = {A Theory of Social Comparison Processes},\n volume = {7},\n year = {1954}\n}\n'}",,"{'volume': '7', 'pages': '117 - 140', 'name': 'Human Relations'}",30.0,A Theory of Social Comparison Processes,1954.0
1512,7f9e01261bb47905775bc3f0181fef1faa411334,"How the processing of emotional expression is influenced by perceived gaze remains a debated issue. Discrepancies between previous results may stem from differences in the nature of stimuli and task characteristics. Here we used a highly controlled set of computer-generated animated faces combining dynamic emotional expressions with varying intensity, and gaze shifts either directed at or averted from the observer. We predicted that perceived self-relevance of fearful faces would be higher with averted gaze-signaling a nearby danger; whereas conversely, direct gaze would be more relevant for angry faces-signaling aggressiveness. This interaction pattern was observed behaviorally for emotion intensity ratings, and neurally for functional magnetic resonance imaging activation in amygdala, as well as fusiform and medial prefrontal cortices, but only for mild- and not high-intensity expressions. These results support an involvement of human amygdala in the appraisal of self-relevance and reveal a crucial role of expression intensity in emotion and gaze interactions.","[{'authorId': '2097603', 'name': ""K. N'diaye""}, {'authorId': '143868107', 'name': 'D. Sander'}, {'authorId': '2152501', 'name': 'P. Vuilleumier'}]",169.0,"{'bibtex': ""@Article{N'diaye2009SelfrelevancePI,\n author = {K. N'diaye and D. Sander and P. Vuilleumier},\n journal = {Emotion},\n pages = {\n          798-806\n        },\n title = {Self-relevance processing in the human amygdala: gaze direction, facial expression, and emotion intensity.},\n volume = {9 6},\n year = {2009}\n}\n""}",,"{'volume': '9 6', 'pages': '\n          798-806\n        ', 'name': 'Emotion'}",44.0,"Self-relevance processing in the human amygdala: gaze direction, facial expression, and emotion intensity.",2009.0
1513,7fc366a2deea99d42f2ac1babfb512f8782dc274,,"[{'authorId': '2553201', 'name': 'Wilma A. Bainbridge'}, {'authorId': '1802400', 'name': 'Justin W. Hart'}, {'authorId': '1748636', 'name': 'Elizabeth S. Kim'}, {'authorId': '1792053', 'name': 'B. Scassellati'}]",386.0,"{'bibtex': '@Article{Bainbridge2011TheBO,\n author = {Wilma A. Bainbridge and Justin W. Hart and Elizabeth S. Kim and B. Scassellati},\n journal = {International Journal of Social Robotics},\n pages = {41-52},\n title = {The Benefits of Interactions with Physically Present Robots over\xa0Video-Displayed Agents},\n volume = {3},\n year = {2011}\n}\n'}",,"{'volume': '3', 'pages': '41-52', 'name': 'International Journal of Social Robotics'}",31.0,The Benefits of Interactions with Physically Present Robots over Video-Displayed Agents,2011.0
1514,7fcf062048186e814cf9df48a7754eab381a57ad,"Recent proliferation of available virtual reality (VR) tools has seen increased use in psychological research. This is due to a number of advantages afforded over traditional experimental apparatus such as tighter control of the environment and the possibility of creating more ecologically valid stimulus presentation and response protocols. At the same time, higher levels of immersion and visual fidelity afforded by VR do not necessarily evoke presence or elicit a “realistic” psychological response. The current paper reviews some current uses for VR environments in psychological research and discusses some ongoing questions for researchers. Finally, we focus on the area of visual perception, where both the advantages and challenges of VR are particularly salient.","[{'authorId': '50026019', 'name': 'Christopher J. Wilson'}, {'authorId': '47154417', 'name': 'A. Soranzo'}]",176.0,"{'bibtex': '@Article{Wilson2015TheUO,\n author = {Christopher J. Wilson and A. Soranzo},\n journal = {Computational and Mathematical Methods in Medicine},\n title = {The Use of Virtual Reality in Psychology: A Case Study in Visual Perception},\n volume = {2015},\n year = {2015}\n}\n'}",,"{'volume': '2015', 'name': 'Computational and Mathematical Methods in Medicine'}",94.0,The Use of Virtual Reality in Psychology: A Case Study in Visual Perception,2015.0
1515,7fd3991f364514acb8cdea1e8181baca15c86d32,"Fear extinction refers to the ability to adapt as situations change by learning to suppress a previously learned fear. This process involves a gradual reduction in the capacity of a fear-conditioned stimulus to elicit fear by presenting the conditioned stimulus repeatedly on its own. Fear extinction is context-dependent and is generally considered to involve the establishment of inhibitory control of the prefrontal cortex over amygdala-based fear processes. In this paper, we review research progress on the neural basis of fear extinction with a focus on the role of the amygdala and the prefrontal cortex. We evaluate two competing hypotheses for how the medial prefrontal cortex inhibits amygdala output. In addition, we present new findings showing that lesions of the basal amygdala do not affect fear extinction. Based on this result, we propose an updated model for integrating hippocampal-based contextual information with prefrontal-amygdala circuitry.","[{'authorId': '1401568019', 'name': 'Francisco Sotres-Bayon'}, {'authorId': '47799076', 'name': 'D. Bush'}, {'authorId': '2332694', 'name': 'Joseph E LeDoux'}]",413.0,"{'bibtex': '@Article{Sotres-Bayon2004EmotionalPA,\n author = {Francisco Sotres-Bayon and D. Bush and Joseph E LeDoux},\n journal = {Learning & memory},\n pages = {\n          525-35\n        },\n title = {Emotional perseveration: an update on prefrontal-amygdala interactions in fear extinction.},\n volume = {11 5},\n year = {2004}\n}\n'}",,"{'volume': '11 5', 'pages': '\n          525-35\n        ', 'name': 'Learning & memory'}",187.0,Emotional perseveration: an update on prefrontal-amygdala interactions in fear extinction.,2004.0
1516,80140e972720787e7b295550f0659d8514112d64,"In two studies, healthy elderly adults were poor at recognizing certain emotions. In study one, an emotion face morphed to express a new emotion. The elderly were impaired when recognizing anger and sadness, whereas no differences were found between the two age groups in recognizing fear or happiness, or in a task requiring reasoning about non=emotion stimuli. In study two, the elderly were impaired when judging which of two faces was more angry, sad, or fearful, but they were not impaired when judging other emotions or when judging which of two beakers was more full. The elderly were also impaired when matching emotion sounds to angry, sad, and disgusted faces, but not to other emotions and not when matching non-emotion (e.g., machine) sounds to machines. Elderly deficits were independent of performance on a task requiring basic face processing (gender recognition). Overall, the results provide support for an age-related decline in the recognition of some emotions that is independent of changes in perceptual abilities, processing speed, fluid IQ, basic face processing abilities, and reasoning- about non-face stimuli. Recognition of emotion stimuli might be mediated by regions of the brain that are independent from those associated with a more general cognitive decline","[{'authorId': '116432126', 'name': 'Susan Sullivan'}, {'authorId': '3890791', 'name': 'T. Ruffman'}]",268.0,"{'bibtex': '@Article{Sullivan2004EMOTIONRD,\n author = {Susan Sullivan and T. Ruffman},\n journal = {International Journal of Neuroscience},\n pages = {403 - 432},\n title = {EMOTION RECOGNITION DEFICITS IN THE ELDERLY},\n volume = {114},\n year = {2004}\n}\n'}",,"{'volume': '114', 'pages': '403 - 432', 'name': 'International Journal of Neuroscience'}",60.0,EMOTION RECOGNITION DEFICITS IN THE ELDERLY,2004.0
1517,80183519a1e9c67b6996bea274cd5e6c251e6683,"Augmented Reality (AR) provides a unique opportunity to situate learning content in one's environment. In this work, we investigated how AR could be developed to provide an interactive context-based language learning experience. Specifically, we developed a novel handheld-AR app for learning case grammar by dynamically creating quizzes, based on real-life objects in the learner's surroundings. We compared this to the experience of learning with a non-contextual app that presented the same quizzes with static photographic images. Participants found AR suitable for use in their everyday lives and enjoyed the interactive experience of exploring grammatical relationships in their surroundings. Nonetheless, Bayesian tests provide substantial evidence that the interactive and context-embedded AR app did not improve case grammar skills, vocabulary retention, and usability over the experience with equivalent static images. Based on this, we propose how language learning apps could be designed to combine the benefits of contextual AR and traditional approaches.","[{'authorId': '1396850289', 'name': 'Fiona Draxler'}, {'authorId': '1666377025', 'name': 'Audrey Labrie'}, {'authorId': '145823914', 'name': 'A. Schmidt'}, {'authorId': '144710398', 'name': 'L. Chuang'}]",23.0,"{'bibtex': '@Article{Draxler2020AugmentedRT,\n author = {Fiona Draxler and Audrey Labrie and A. Schmidt and L. Chuang},\n journal = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n title = {Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems'},44.0,Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions,2020.0
1518,8018cb26bae66c31862199e3722291e3b1def203,"Information is traditionally confined to paper or digitally to a screen. In this paper, we introduce WUW, a wearable gestural interface, which attempts to bring information out into the tangible world. By using a tiny projector and a camera mounted on a hat or coupled in a pendant like wearable device, WUW sees what the user sees and visually augments surfaces or physical objects the user is interacting with. WUW projects information onto surfaces, walls, and physical objects around us, and lets the user interact with the projected information through natural hand gestures, arm movements or interaction with the object itself.","[{'authorId': '145231524', 'name': 'Pranav Mistry'}, {'authorId': '1701876', 'name': 'P. Maes'}, {'authorId': '2122834469', 'name': 'Liyan Chang'}]",470.0,"{'bibtex': ""@Article{Mistry2009WUWW,\n author = {Pranav Mistry and P. Maes and Liyan Chang},\n journal = {CHI '09 Extended Abstracts on Human Factors in Computing Systems},\n title = {WUW - wear Ur world: a wearable gestural interface},\n year = {2009}\n}\n""}",,"{'name': ""CHI '09 Extended Abstracts on Human Factors in Computing Systems""}",21.0,WUW - wear Ur world: a wearable gestural interface,2009.0
1519,801afc8b3823ddba9e67689980aa48fcd9dc5a55,,"[{'authorId': '7518542', 'name': 'L. Fairbanks'}, {'authorId': '5054307', 'name': 'M. Mcguire'}, {'authorId': '150949871', 'name': 'C. J. Harris'}]",57.0,"{'bibtex': '@Article{Fairbanks1982NonverbalIO,\n author = {L. Fairbanks and M. Mcguire and C. J. Harris},\n journal = {Journal of abnormal psychology},\n pages = {\n          109-19\n        },\n title = {Nonverbal interaction of patients and therapists during psychiatric interviews.},\n volume = {91 2},\n year = {1982}\n}\n'}",,"{'volume': '91 2', 'pages': '\n          109-19\n        ', 'name': 'Journal of abnormal psychology'}",28.0,Nonverbal interaction of patients and therapists during psychiatric interviews.,1982.0
1520,802fd9b4d336a0456567119d6145bad0b5fe2e8a,,"[{'authorId': '1402194434', 'name': 'Héctor Rafael Orozco-Aguirre'}, {'authorId': '145054210', 'name': 'Félix F. Ramos'}, {'authorId': '144781300', 'name': 'M. Ramos'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}]",13.0,"{'bibtex': '@Article{Orozco-Aguirre2011AnAS,\n author = {Héctor Rafael Orozco-Aguirre and Félix F. Ramos and M. Ramos and D. Thalmann},\n journal = {The Visual Computer},\n pages = {275-285},\n title = {An action selection process to simulate the human behavior in\xa0virtual humans with real personality},\n volume = {27},\n year = {2011}\n}\n'}",,"{'volume': '27', 'pages': '275-285', 'name': 'The Visual Computer'}",32.0,An action selection process to simulate the human behavior in virtual humans with real personality,2011.0
1523,806066bcd2b380a84c988b9175ab61ef89b915eb,"We designed an fMRI experiment comparing perception of human faces and robotic faces producing emotional expressions. The purpose of our experiment was to investigate engagement of different parts of the social brain by viewing these animate and inanimate agents. Both human and robotic face expressions evoked activity in face-responsive regions in the fusiform gyrus and STS and in the putative human mirror neuron system. These results suggest that these areas mediate perception of agency, independently of whether the agents are living or not. By contrast, the human faces evoked stronger activity than did robotic faces in the medial pFC and the anterior temporal cortex—areas associated with the representation of others' mental states (theory of mind), whereas robotic faces evoked stronger activity in areas associated with perception of objects and mechanical movements. Our data demonstrate that the representation of the distinction between animate and inanimate agents involves areas that participate in attribution of mental stance.","[{'authorId': '1982106', 'name': 'M. Gobbini'}, {'authorId': '49986724', 'name': 'C. Gentili'}, {'authorId': '3136019', 'name': 'E. Ricciardi'}, {'authorId': '50672082', 'name': 'C. Bellucci'}, {'authorId': '153723511', 'name': 'P. Salvini'}, {'authorId': '1736793', 'name': 'C. Laschi'}, {'authorId': '1927285', 'name': 'M. Guazzelli'}, {'authorId': '1977452', 'name': 'P. Pietrini'}]",93.0,"{'bibtex': '@Article{Gobbini2011DistinctNS,\n author = {M. Gobbini and C. Gentili and E. Ricciardi and C. Bellucci and P. Salvini and C. Laschi and M. Guazzelli and P. Pietrini},\n journal = {Journal of Cognitive Neuroscience},\n pages = {1911-1920},\n title = {Distinct Neural Systems Involved in Agency and Animacy Detection},\n volume = {23},\n year = {2011}\n}\n'}",,"{'volume': '23', 'pages': '1911-1920', 'name': 'Journal of Cognitive Neuroscience'}",84.0,Distinct Neural Systems Involved in Agency and Animacy Detection,2011.0
1524,8073de0cf614f37c229265b57313b6131a7172fe,,"[{'authorId': '1977041', 'name': 'P. Wagner'}, {'authorId': '3018492', 'name': 'Zofia Malisz'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",302.0,"{'bibtex': '@Article{Wagner2014GestureAS,\n author = {P. Wagner and Zofia Malisz and S. Kopp},\n journal = {Speech Commun.},\n pages = {209-232},\n title = {Gesture and speech in interaction: An overview},\n volume = {57},\n year = {2014}\n}\n'}",,"{'volume': '57', 'pages': '209-232', 'name': 'Speech Commun.'}",197.0,Gesture and speech in interaction: An overview,2014.0
1525,807ac8ede9ac06de13142f269a394d53415c33a6,,"[{'authorId': '49647443', 'name': 'João Dias'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",229.0,"{'bibtex': '@Inproceedings{Dias2014FAtiMAMT,\n author = {João Dias and S. Mascarenhas and Ana Paiva},\n pages = {44-56},\n title = {FAtiMA Modular: Towards an Agent Architecture with a Generic Appraisal Framework},\n year = {2014}\n}\n'}",,{'pages': '44-56'},16.0,FAtiMA Modular: Towards an Agent Architecture with a Generic Appraisal Framework,2014.0
1529,80d9af06cad91d85e95a1504ce3f018c64cc9f17,"Conversational agents’ ability to communicate in natural language through voice and text interfaces poses an opportunity in helping children with autism spectrum disorder (ASD) develop their social communication skills. In this paper, we describe the design of Amy, a conversational agent that explains social situations to help guide a child in understanding when to use socially appropriate behavior. A breathing exercise feature for emotion regulation is activated when a negative emotion is detected from the child’s input. Interviews with parents and a child psychologist informed the design of Amy as well as the 12 social stories themes that Amy shares with children. Interview inputs and previous works suggested four design considerations for social-emotional learning companions to facilitate better interaction by including relevant social story themes, formulating open-domain conversation flow, incorporating appropriate and guided emotion regulation exercise, and using lively visual user interface.","[{'authorId': '2172981412', 'name': 'Isser Troy Gagan'}, {'authorId': '2172981437', 'name': 'Maria Angela Mikaela Matias'}, {'authorId': '2172791700', 'name': 'Ivy Tan'}, {'authorId': '2172969825', 'name': 'Christianne Marie Vinco'}, {'authorId': '8045848', 'name': 'Ethel Ong'}, {'authorId': '116323144', 'name': 'Ron R. Resurreccion'}]",4.0,"{'bibtex': '@Article{Gagan2022DesigningAV,\n author = {Isser Troy Gagan and Maria Angela Mikaela Matias and Ivy Tan and Christianne Marie Vinco and Ethel Ong and Ron R. Resurreccion},\n booktitle = {International Conference on Interaction Design and Children},\n journal = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},\n title = {Designing A Virtual Talking Companion to Support the Social-Emotional Learning of Children with ASD},\n year = {2022}\n}\n'}","[{'paperId': '9bf9d66d53fe3dc633773cc4e6bb1ab49754ff52', 'title': 'Investigation into Stress Triggers in Autistic Adults for the Development of Technological Self-Interventions'}, {'paperId': '09668b9f3ef1bc27eaaf926f57cbea32bb46c23b', 'title': 'Programmable Floor Robot Robotito and its Tangible and Virtual Interface'}, {'paperId': '9c52f28c936221923b575acc29e9f5526733e87a', 'title': 'Understanding how technology can support social-emotional learning of children: a dyadic trauma-informed participatory design with proxies'}, {'paperId': '9c586828f49d58734aa33dad9cb2c911e73b6d6c', 'title': 'Preparing Children with Level 1 ASD for Social Interactions through Storytelling with Amy: An Exploratory Study'}]",{'name': 'Proceedings of the 21st Annual ACM Interaction Design and Children Conference'},65.0,Designing A Virtual Talking Companion to Support the Social-Emotional Learning of Children with ASD,2022.0
1530,80e053e9d8ff85d5ad7db0333b4dd5ec34d80b2a,"The delivery of mental health interventions via ubiquitous devices has shown much promise. A natural conversational interface that allows longitudinal symptom tracking and appropriate just-in-time interventions would be extremely valuable. However, the task of designing emotionally-aware agents is still poorly understood. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through two human-subject experiments with N=39 participants (one-week, and two-week long respectively). EMMA conducts experience sampling in an empathetic manner and provides emotionally appropriate micro-activities. We show the system can be extended to detect a user's mood purely from smartphone sensor data. Our results show that extraverts preferred EMMA significantly more than introverts and that our personalized machine learning model worked as well as relying on gold-standard self-reports of emotion from users. Finally, we provide a set of guidelines for the design of bots for mHealth.","[{'authorId': '2214185', 'name': 'Asma Ghandeharioun'}, {'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '1817251', 'name': 'M. Czerwinski'}, {'authorId': '36516124', 'name': 'Kael Rowan'}]",14.0,"{'bibtex': '@Article{Ghandeharioun2018EMMAAE,\n author = {Asma Ghandeharioun and Daniel J. McDuff and M. Czerwinski and Kael Rowan},\n journal = {ArXiv},\n title = {EMMA: An Emotionally Intelligent Personal Assistant for Improving Wellbeing},\n volume = {abs/1812.11423},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1812.11423', 'name': 'ArXiv'}",58.0,EMMA: An Emotionally Intelligent Personal Assistant for Improving Wellbeing,2018.0
1531,80f30d2f1875e7637bd79ad98f3d22aa05cd4136,,"[{'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '145431802', 'name': 'Barry D. Reich'}, {'authorId': '1736049', 'name': 'B. Webber'}]",59.0,"{'bibtex': '@Inproceedings{Badler1997TowardsPF,\n author = {N. Badler and Barry D. Reich and B. Webber},\n pages = {43-57},\n title = {Towards Personalities for Animated Agents with Reactive and Planning Behaviors},\n year = {1997}\n}\n'}",,{'pages': '43-57'},44.0,Towards Personalities for Animated Agents with Reactive and Planning Behaviors,1997.0
1533,810baae4153f5d97eb213142e3144edde3f7d76a,"A six-step, iterative, empirical, human factors design methodology was used to develop <bold><italic>CAL,</italic></bold>a natural language computer application to help computer-naive business professionals manage their personal calendars. Language is processed by a simple, non-parsing algorithm having limited storage requirements and a quick response time. <bold><italic>CAL</italic></bold> allows unconstrained English inputs from users with no training (except for a 5 minute introduction to the keyboard and display) and no manual (except for a two-page overview of the system). In a controlled test of performance, <bold><italic>CAL</italic></bold> correctly responded to between 86% and 97% of the inputs it received, according to various criteria. This research demonstrates that the methodological tools of the engineering psychologist can help build user-friendly software that accommodates the unruly language of computer-naive, first-time users by eliciting the cooperation of such users as partners in an iterative, empirical development process.
 The principal purpose of the research reported here was to design and test a systematic, empirical methodology for developing natural language computer applications. This paper describes that methodology and its successful use in the development of a natural language computer application: <bold><italic>CAL,</italic></bold><bold><italic>C</italic></bold>alendar <bold><italic>A</italic></bold>ccess <bold><italic>L</italic></bold>anguage. The limited context or domain in which the application operates is the management of a personal calendar, or appointment book, data base by computer-naive business professionals.","[{'authorId': '2072865746', 'name': 'J. F. Kelley'}]",269.0,"{'bibtex': '@Inproceedings{Kelley1983AnEM,\n author = {J. F. Kelley},\n pages = {193-196},\n title = {An empirical methodology for writing user-friendly natural language computer applications},\n year = {1983}\n}\n'}",,{'pages': '193-196'},7.0,An empirical methodology for writing user-friendly natural language computer applications,1983.0
1534,81113eab5fd886c371868bdbdfc875acf124072e,"Despite their many advantages, e-businesses lag behind brick and mortar businesses in several fundamental respects. This paper concerns one of these: relationships based on trust and reputation. Recent studies on simple reputation systems for e-Businesses such as eBay have pointed to the importance of such rating systems for deterring moral hazard and encouraging trusting interactions. However, despite numerous studies on trust and reputation systems, few have taken studies across disciplines to provide an integrated account of these concepts and their relationships. This paper first surveys existing literatures on trust, reputation and a related concept: reciprocity. Based on sociological and biological understandings of these concepts, a computational model is proposed. This model can be implemented in a real system to consistently calculate agents' trust and reputation scores.","[{'authorId': '145124469', 'name': 'L. Mui'}, {'authorId': '2725157', 'name': 'M. Mohtashemi'}, {'authorId': '39863484', 'name': 'A. Halberstadt'}]",1062.0,"{'bibtex': '@Article{Mui2002ACM,\n author = {L. Mui and M. Mohtashemi and A. Halberstadt},\n journal = {Proceedings of the 35th Annual Hawaii International Conference on System Sciences},\n pages = {2431-2439},\n title = {A computational model of trust and reputation},\n year = {2002}\n}\n'}",,"{'pages': '2431-2439', 'name': 'Proceedings of the 35th Annual Hawaii International Conference on System Sciences'}",62.0,A computational model of trust and reputation,2002.0
1535,8120bee1fa392ece012356de0a9bc9f2aadc1f1b,,"[{'authorId': '1740889', 'name': 'K. Isbister'}]",138.0,"{'bibtex': '@Inproceedings{Isbister2006BetterGC,\n author = {K. Isbister},\n pages = {I-XXVII, 1-336},\n title = {Better Game Characters by Design - A Psychological Approach},\n year = {2006}\n}\n'}",,"{'pages': 'I-XXVII, 1-336'}",0.0,Better Game Characters by Design - A Psychological Approach,2006.0
1536,8142668ad01565997cf8e1bd15fb23549f4b9a8d,"This paper describes a Conceptual Framework underpinning “Systems that Care” in terms of educational systems that take account of motivation, metacognition and affect, in addition to cognition. The main focus is on motivation, as learning requires the student to put in effort and be engaged, in other words to be motivated to learn. But motivation is not the whole story as it is strongly related to metacognition and affect. Traditional intelligent educational systems, whether learner-centred or teacher-centred in their pedagogy, are characterised as having deployed their intelligence to assist in the development of the learner's knowledge or skill in some domain. They have operated largely at the cognitive level and have assumed that the learner is already able to manage her own learning, is already in an appropriate affective state and also is already motivated to learn. This paper starts by outlining theories of motivation and their interactions with affect and with metacognition, as developed in the psychological and educational literatures. It then describes how such theories have been implemented in intelligent educational systems. The first part of the Conceptual Framework develops the notion of a partial hierarchy of systems in terms of their pedagogic focus. These range from traditional, cognitively intelligent systems, essentially concerned with cognition up to “Systems that Care”. Intermediate classes of system include Metacognitively Intelligent systems, Affectively Intelligent systems and Motivationally Intelligent systems. The second part of the Conceptual Framework is concerned with the design of systems. This is characterised in terms of (i) the kinds of diagnostic input data (such as the learner's facial expression offering clues as to her demeanour) and (ii) the repertoire of tactical and strategic pedagogic moves (such as offering encouragement), applicable at different levels of the hierarchy. Attention is paid to metacognition, meta-affect and meta-motivation covering the capability of both the learner and the educational system to understand, reason about and regulate cognition, affect and motivation. Finally, research questions and areas of further work are identified in theory development, the role of the meta levels, and design considerations.","[{'authorId': '1738475', 'name': 'John Benedict du Boulay'}, {'authorId': '1715292', 'name': 'K. Avramides'}, {'authorId': '1804082', 'name': 'R. Luckin'}, {'authorId': '1403924770', 'name': 'Erika Martínez-Mirón'}, {'authorId': '1400900451', 'name': 'G. Rebolledo-Méndez'}, {'authorId': '31951571', 'name': 'Amanda Carr'}]",74.0,"{'bibtex': '@Article{Boulay2010TowardsST,\n author = {John Benedict du Boulay and K. Avramides and R. Luckin and Erika Martínez-Mirón and G. Rebolledo-Méndez and Amanda Carr},\n journal = {Int. J. Artif. Intell. Educ.},\n pages = {197-229},\n title = {Towards Systems That Care: A Conceptual Framework based on Motivation, Metacognition and Affect},\n volume = {20},\n year = {2010}\n}\n'}",,"{'volume': '20', 'pages': '197-229', 'name': 'Int. J. Artif. Intell. Educ.'}",117.0,"Towards Systems That Care: A Conceptual Framework based on Motivation, Metacognition and Affect",2010.0
1537,8159f3f1bc1155ff56bed7fea38d052c4524108b,"We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.","[{'authorId': '50227009', 'name': 'Uttaran Bhattacharya'}, {'authorId': '10172108', 'name': 'Nicholas Rewkowski'}, {'authorId': '39373885', 'name': 'A. Banerjee'}, {'authorId': '6325349', 'name': 'P. Guhan'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",75.0,"{'bibtex': '@Article{Bhattacharya2021Text2GesturesAT,\n author = {Uttaran Bhattacharya and Nicholas Rewkowski and A. Banerjee and P. Guhan and Aniket Bera and Dinesh Manocha},\n journal = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},\n pages = {1-10},\n title = {Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents**This work has been supported in part by ARO Grants W911NF1910069 and W911NF1910315, and Intel. Code and additional materials available at: https://gamma.umd.edu/t2g},\n year = {2021}\n}\n'}",,"{'pages': '1-10', 'name': '2021 IEEE Virtual Reality and 3D User Interfaces (VR)'}",72.0,"Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents**This work has been supported in part by ARO Grants W911NF1910069 and W911NF1910315, and Intel. Code and additional materials available at: https://gamma.umd.edu/t2g",2021.0
1538,815c464c53d29e62c476470aa0eb8eb7d47d0923,"Purpose: People with high-functioning autism (HFA) have qualitative differences in facial expression and prosody production, which are rarely systematically quantified. The authors’ goals were to qualitatively and quantitatively analyze prosody and facial expression productions in children and adolescents with HFA. Method: Participants were 22 male children and adolescents with HFA and 18 typically developing (TD) controls (17 males, 1 female). The authors used a story retelling task to elicit emotionally laden narratives, which were analyzed through the use of acoustic measures and perceptual codes. Naı¨ve listeners coded all productions for emotion type, degree of expressiveness, and awkwardness. Results: The group with HFA was not significantly different in accuracy or expressiveness of facial productions, but was significantly more awkward than the TD group. Participants with HFA were significantly more expressive in their vocal productions, with a trend for greater awkwardness. Severity of social communication impairment, as captured by the Autism Diagnostic Observation Schedule (ADOS; Lord, Rutter, DiLavore, & Risi, 1999), was correlated with greater vocal and facial awkwardness. Conclusions: Facial and vocal expressions of participants with HFA were as recognizable as those of their TD peers but were qualitatively different, particularly when listeners coded samples with intact dynamic properties. These preliminary data show qualitative differences in nonverbal communication that may have significant negative impact on the social communication success of children and adolescents with HFA.","[{'authorId': '2112121541', 'name': 'Ruth B. Grossman'}, {'authorId': '5644208', 'name': 'Lisa R. Edelson'}, {'authorId': '1404511422', 'name': 'H. Tager-Flusberg'}, {'authorId': '2251496803', 'name': 'Janna Oetting'}, {'authorId': '2251468363', 'name': 'Nina Capone-Singleton'}]",89.0,"{'bibtex': '@Misc{None,\n author = {Ruth B. Grossman and Lisa R. Edelson and H. Tager-Flusberg and Janna Oetting and Nina Capone-Singleton},\n title = {Emotional Facial and Vocal Expressions During Story Retelling by Children and Adolescents With High-Functioning Autism}\n}\n'}",,,32.0,Emotional Facial and Vocal Expressions During Story Retelling by Children and Adolescents With High-Functioning Autism,
1539,8164a5c811c641ce9f7cb6a73cdc3ca02fc789d6,"Key Points Question What is the burden of depression symptoms among US adults during the coronavirus disease 2019 (COVID-19) pandemic compared with before COVID-19, and what are the risk factors associated with depression symptoms? Findings In this survey study that included 1441 respondents from during the COVID-19 pandemic and 5065 respondents from before the pandemic, depression symptom prevalence was more than 3-fold higher during the COVID-19 pandemic than before. Lower income, having less than $5000 in savings, and having exposure to more stressors were associated with greater risk of depression symptoms during COVID-19. Meaning These findings suggest that there is a high burden of depression symptoms in the US associated with the COVID-19 pandemic and that this burden falls disproportionately on individuals who are already at increased risk.","[{'authorId': '26428407', 'name': 'Catherine K. Ettman'}, {'authorId': '145247865', 'name': 'S. Abdalla'}, {'authorId': '7778047', 'name': 'Gregory H. Cohen'}, {'authorId': '3568083', 'name': 'Laura A Sampson'}, {'authorId': '39961865', 'name': 'P. Vivier'}, {'authorId': '1822602', 'name': 'S. Galea'}]",1417.0,"{'bibtex': '@Article{Ettman2020PrevalenceOD,\n author = {Catherine K. Ettman and S. Abdalla and Gregory H. Cohen and Laura A Sampson and P. Vivier and S. Galea},\n journal = {JAMA Network Open},\n title = {Prevalence of Depression Symptoms in US Adults Before and During the COVID-19 Pandemic},\n volume = {3},\n year = {2020}\n}\n'}",,"{'volume': '3', 'name': 'JAMA Network Open'}",30.0,Prevalence of Depression Symptoms in US Adults Before and During the COVID-19 Pandemic,2020.0
1540,817078a19b41c435f95cd0eb3bc0d8b73f3adf76,"Current approaches to the development of natural language dialogue systems are discussed, and it is claimed that they do not sufficiently consider the unique qualities of man-machine interaction as distinct from general human discourse. It is concluded that empirical studies of this unique communication situation are required for the development of user-friendly interactive systems. One way of achieving this is through the use of so-called Wizard of Oz studies. The focus of the work described in the paper is on the practical execution of the studies and the methodological conclusions drawn on the basis of the authors' experience. While the focus is on natural language interfaces, the methods used and the conclusions drawn from the results obtained are of relevance also to other kinds of intelligent interfaces.","[{'authorId': '2633619', 'name': 'Nils Dahlbäck'}, {'authorId': '144349151', 'name': 'Arne Jönsson'}, {'authorId': '1742056', 'name': 'Lars Ahrenberg'}]",1276.0,"{'bibtex': '@Article{Dahlbäck1993WizardOO,\n author = {Nils Dahlbäck and Arne Jönsson and Lars Ahrenberg},\n journal = {Knowl. Based Syst.},\n pages = {258-266},\n title = {Wizard of Oz studies: why and how},\n volume = {6},\n year = {1993}\n}\n'}",,"{'volume': '6', 'pages': '258-266', 'name': 'Knowl. Based Syst.'}",28.0,Wizard of Oz studies: why and how,1993.0
1541,818cfcf6afabe5bd5f44c49686d0f20ba0b2c3d2,"Whereas the perception of emotion from facial expression has been extensively studied cross-culturally, little is known about judges’ ability to infer emotion from vocal cues. This article reports the results from a study conducted in nine countries in Europe, the United States, and Asia on vocal emotion portrayals of anger, sadness, fear, joy, and neutral voice as produced by professional German actors. Data show an overall accuracy of 66% across all emotions and countries. Although accuracy was substantially better than chance, there were sizable differences ranging from 74% in Germany to 52% in Indonesia. However, patterns of confusion were very similar across all countries. These data suggest the existence of similar inference rules from vocal expression across cultures. Generally, accuracy decreased with increasing language dissimilarity from German in spite of the use of language-free speech samples. It is concluded that culture- and language-specific paralinguistic patterns may influence the decoding process.","[{'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '47161796', 'name': 'R. Banse'}, {'authorId': '4874112', 'name': 'H. Wallbott'}]",710.0,"{'bibtex': '@Article{Scherer2001EmotionIF,\n author = {K. Scherer and R. Banse and H. Wallbott},\n journal = {Journal of Cross-Cultural Psychology},\n pages = {76 - 92},\n title = {Emotion Inferences from Vocal Expression Correlate Across Languages and Cultures},\n volume = {32},\n year = {2001}\n}\n'}",,"{'volume': '32', 'pages': '76 - 92', 'name': 'Journal of Cross-Cultural Psychology'}",46.0,Emotion Inferences from Vocal Expression Correlate Across Languages and Cultures,2001.0
1542,819d3d76362925246bb735c05aaf6b3b4b4cc685,"Pedagogical agents (PAs) have the ability to scaffold and regulate students' learning about complex topics while using intelligent tutoring systems (ITSs). Research on ITSs predominantly focuses on the impact that these systems have on overall learning, while the specific components of human-ITS interaction, such as student-PA dialogue within the system, are given little attention. One hundred undergraduate students interacted with MetaTutor, a multiagent hypermedia ITS, to learn about the human circulatory system. Data from these interactions were drawn from questionnaires and log-files to determine the extent to which a specific agent from MetaTutor, Sam the Strategizer, impacted students' overall emotions while using the system. Results indicated that Sam negatively impacted students' experiences of enjoyment, in relation to the other agents of MetaTutor, and the frequency of Sam's interactions with students significantly predicted their reports of boredom while using the system. Implications for the design of affect-sensitive multiagent ITSs are discussed.","[{'authorId': '3408438', 'name': 'Nicholas V. Mudrick'}, {'authorId': '145394858', 'name': 'R. Azevedo'}, {'authorId': '37057683', 'name': 'M. Taub'}, {'authorId': '40845119', 'name': 'François Bouchet'}]",3.0,"{'bibtex': ""@Article{Mudrick2015DoesTF,\n author = {Nicholas V. Mudrick and R. Azevedo and M. Taub and François Bouchet},\n journal = {Cognitive Science},\n pages = {1661-1666},\n title = {Does the Frequency of Pedagogical Agent Intervention Relate to Learners' Self-Reported Boredom while using Multiagent Intelligent Tutoring Systems?},\n year = {2015}\n}\n""}",,"{'volume': '', 'pages': '1661-1666', 'name': 'Cognitive Science'}",28.0,Does the Frequency of Pedagogical Agent Intervention Relate to Learners' Self-Reported Boredom while using Multiagent Intelligent Tutoring Systems?,2015.0
1543,81aba593a542b0188c0a4823b4f43628b5824c80,,"[{'authorId': '50787260', 'name': 'Deborah Goren'}, {'authorId': '32733837', 'name': 'H. Wilson'}]",113.0,"{'bibtex': '@Article{Goren2006QuantifyingFE,\n author = {Deborah Goren and H. Wilson},\n journal = {Vision Research},\n pages = {1253-1262},\n title = {Quantifying facial expression recognition across viewing conditions},\n volume = {46},\n year = {2006}\n}\n'}",,"{'volume': '46', 'pages': '1253-1262', 'name': 'Vision Research'}",45.0,Quantifying facial expression recognition across viewing conditions,2006.0
1544,81b2098fc1454700ebe531d97d20409a017241a8,"Whole body expressions are among the main visual stimulus categories that are naturally associated with faces and the neuroscientific investigation of how body expressions are processed has entered the research agenda this last decade. Here we describe the stimulus set of whole body expressions termed bodily expressive action stimulus test (BEAST), and we provide validation data for use of these materials by the community of emotion researchers. The database was composed of 254 whole body expressions from 46 actors expressing 4 emotions (anger, fear, happiness, and sadness). In all pictures the face of the actor was blurred and participants were asked to categorize the emotions expressed in the stimuli in a four alternative-forced-choice task. The results show that all emotions are well recognized, with sadness being the easiest, followed by fear, whereas happiness was the most difficult. The BEAST appears a valuable addition to currently available tools for assessing recognition of affective signals. It can be used in explicit recognition tasks as well as in matching tasks and in implicit tasks, combined either with facial expressions, with affective prosody, or presented with affective pictures as context in healthy subjects as well as in clinical populations.","[{'authorId': '4628064', 'name': 'B. de Gelder'}, {'authorId': '7202556', 'name': 'J. van den Stock'}]",131.0,"{'bibtex': '@Article{Gelder2011TheBE,\n author = {B. de Gelder and J. van den Stock},\n journal = {Frontiers in Psychology},\n title = {The Bodily Expressive Action Stimulus Test (BEAST). Construction and Validation of a Stimulus Basis for Measuring Perception of Whole Body Expression of Emotions},\n volume = {2},\n year = {2011}\n}\n'}",,"{'volume': '2', 'name': 'Frontiers in Psychology'}",50.0,The Bodily Expressive Action Stimulus Test (BEAST). Construction and Validation of a Stimulus Basis for Measuring Perception of Whole Body Expression of Emotions,2011.0
1545,81cc7979787490762e43e2048f670ffdf46bd267,,"[{'authorId': '113331537', 'name': 'T. Ballmer'}, {'authorId': '66607572', 'name': 'W. Brennenstuhl'}]",61.0,"{'bibtex': '@Inproceedings{Ballmer1980SpeechAC,\n author = {T. Ballmer and W. Brennenstuhl},\n title = {Speech act classification},\n year = {1980}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Speech act classification,1980.0
1546,81ed6bbbd73fb8aba8fd2a594e2a0e6422dd175c,"This paper describes and discusses the preliminary results of a behavioural study on robot social acceptability, which was carried out during a public demonstration in South Korea. Data was collected by means of direct observation of people behaviour during interaction with robots. The most interesting result to emerge is that of young people: they tended to react to the robots presence with extreme curiosity and, quite often, to treat them aggressively. In this paper, the word bullying is used to describe any kind of improper and violent behaviour, intended to cause damages or impede the robot operation. It is the authors' opinion that if not tackled appropriately, abuses towards robots may become a serious hindrance to their future deployment, and safety. Hence, the necessity to tackle this issue with dedicated solutions during the early phases of design.","[{'authorId': '153723511', 'name': 'P. Salvini'}, {'authorId': '3141568', 'name': 'G. Ciaravella'}, {'authorId': '2607701', 'name': 'Wonpil Yu'}, {'authorId': '145565425', 'name': 'G. Ferri'}, {'authorId': '15511033', 'name': 'A. Manzi'}, {'authorId': '3345187', 'name': 'B. Mazzolai'}, {'authorId': '1736793', 'name': 'C. Laschi'}, {'authorId': '3082789', 'name': 'Sang-Rok Oh'}, {'authorId': '145745294', 'name': 'P. Dario'}]",104.0,"{'bibtex': '@Article{Salvini2010HowSA,\n author = {P. Salvini and G. Ciaravella and Wonpil Yu and G. Ferri and A. Manzi and B. Mazzolai and C. Laschi and Sang-Rok Oh and P. Dario},\n journal = {19th International Symposium in Robot and Human Interactive Communication},\n pages = {1-7},\n title = {How safe are service robots in urban environments? Bullying a robot},\n year = {2010}\n}\n'}",,"{'pages': '1-7', 'name': '19th International Symposium in Robot and Human Interactive Communication'}",34.0,How safe are service robots in urban environments? Bullying a robot,2010.0
1547,8202d55f2229757c0f8eac880e4269cdbce3c9ac,,"[{'authorId': '2254103', 'name': 'O. John'}]",278.0,"{'bibtex': '@Inproceedings{John1989TowardsAT,\n author = {O. John},\n pages = {261-271},\n title = {Towards a Taxonomy of Personality Descriptors},\n year = {1989}\n}\n'}",,"{'volume': '', 'pages': '261-271', 'name': ''}",22.0,Towards a Taxonomy of Personality Descriptors,1989.0
1548,820a7b066de4d236fd8f10374043c9c784bd6801,"We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.","[{'authorId': '2297177', 'name': 'Thammathip Piumsomboon'}, {'authorId': '48534381', 'name': 'Gun A. Lee'}, {'authorId': '153077675', 'name': 'J. Hart'}, {'authorId': '2502367', 'name': 'Barrett Ens'}, {'authorId': '1719686', 'name': 'R. Lindeman'}, {'authorId': '143885004', 'name': 'B. Thomas'}, {'authorId': '1684805', 'name': 'M. Billinghurst'}]",262.0,"{'bibtex': '@Article{Piumsomboon2018MiniMeAA,\n author = {Thammathip Piumsomboon and Gun A. Lee and J. Hart and Barrett Ens and R. Lindeman and B. Thomas and M. Billinghurst},\n journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},\n title = {Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems'},53.0,Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration,2018.0
1549,82181fedda0d56822e825b35af3f7f8131ee6952,"Answers to the question ""What are human emotions for?"" have stimulated highly productive programs of research on emotional phenomena in psychology and neuroscience in the past decade. Although a variety of functions have been proposed and examined at different levels of abstraction, what is undeniable is that when emotional processing is compromised, most things social go awry. In this review we survey the research findings documenting the functions of emotion and link these to new discoveries about how emotion is accurately processed and transmitted. We focus specifically on emotion processing in dyads and groups, which reflects the current scientific trend. Within dyads, emotional expressions and learning and understanding through vicarious emotion are the phenomena of interest. Behavioral and brain mechanisms supporting their successful occurrence are evaluated. At the group level, group emotions and group-based emotions, two very different phenomena, are discussed, and mechanistic accounts are reviewed.","[{'authorId': '1986858', 'name': 'P. Niedenthal'}, {'authorId': '40529687', 'name': 'M. Brauer'}]",296.0,"{'bibtex': '@Article{Niedenthal2012SocialFO,\n author = {P. Niedenthal and M. Brauer},\n journal = {Annual review of psychology},\n pages = {\n          259-85\n        },\n title = {Social functionality of human emotion.},\n volume = {63},\n year = {2012}\n}\n'}",,"{'volume': '63', 'pages': '\n          259-85\n        ', 'name': 'Annual review of psychology'}",221.0,Social functionality of human emotion.,2012.0
1550,82284e968c67325c614034b03e22bc5505379605,"People were asked to observe a person with whom they lived, to report when they noticed that person experiencing an emotion, and to report what cues they used to detect the emotion. In Phase 1, observers were told to ""list the cues they used""; in Phase 2, they were told to ""describe how they could tell"""" that the target person was experiencing an emotion. Results were similar in both phases. Only 5 of the 182 respondents reported using a single cue whereas 10 reported using at least a dozen cues. Two out of three respondents reported using vocal cues; over a half reported using facial, indirect verbal, and context cues; nearly a half reported using body and activity cues; about a quarter of the respondents reported using physiological, trait, and other cues; and fewer than a tenth reported using direct verbal cues. Roughly the same number of cues and the same distribution of cue categories was found regardless of the emotion being observed, the sex of the person observing, the sex of the person being obse...","[{'authorId': '13712093', 'name': 'S. Planalp'}]",111.0,"{'bibtex': '@Article{Planalp1996VarietiesOC,\n author = {S. Planalp},\n journal = {Cognition & Emotion},\n pages = {137-153},\n title = {Varieties of Cues to Emotion in Naturally Occurring Situations},\n volume = {10},\n year = {1996}\n}\n'}",,"{'volume': '10', 'pages': '137-153', 'name': 'Cognition & Emotion'}",0.0,Varieties of Cues to Emotion in Naturally Occurring Situations,1996.0
1551,8236182aa4b26e1d2109e4d1ab392d1972d16b63,,"[{'authorId': '1403827243', 'name': 'C. Becker-Asano'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",199.0,"{'bibtex': '@Article{Becker-Asano2009AffectiveCW,\n author = {C. Becker-Asano and I. Wachsmuth},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {32-49},\n title = {Affective computing with primary and secondary emotions in a virtual human},\n volume = {20},\n year = {2009}\n}\n'}",,"{'volume': '20', 'pages': '32-49', 'name': 'Autonomous Agents and Multi-Agent Systems'}",48.0,Affective computing with primary and secondary emotions in a virtual human,2009.0
1554,824195fcb3a60157f038ad4c2a702d5c2d6f1ee0,,"[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '3023920', 'name': 'Jillian Gerten'}, {'authorId': '2432742', 'name': 'Edward Fast'}, {'authorId': '2091935839', 'name': 'Robin Duffy'}]",395.0,"{'bibtex': '@Inproceedings{Gratch2007CreatingRW,\n author = {J. Gratch and Ning Wang and Jillian Gerten and Edward Fast and Robin Duffy},\n pages = {125-138},\n title = {Creating Rapport with Virtual Agents},\n year = {2007}\n}\n'}",,{'pages': '125-138'},35.0,Creating Rapport with Virtual Agents,2007.0
1558,82554b56858be7b1b1ef0209f839141670782264,"ABSTRACT Virtual agents are systems that add a social dimension to computing, often featuring not only natural language input but also an embodiment or avatar. This allows them to take on a more social role and leverage the use of nonverbal communication (NVC). In humans, NVC is used for many purposes, including communicating intent, directing attention, and conveying emotion. As a result, researchers have developed agents that emulate these behaviors. However, challenges pervade the design and development of NVC in agents. Some articles reveal inconsistencies in the benefits of agent NVC; others show signs of difficulties in the process of analyzing and implementing behaviors. Thus, it is unclear what the specific outcomes and effects of incorporating NVC in agents and what outstanding challenges underlie development. This survey seeks to review the uses, outcomes, and development of NVC in virtual agents to identify challenges and themes to improve and motivate the design of future virtual agents.","[{'authorId': '10693895', 'name': 'Isaac Wang'}, {'authorId': '151062472', 'name': 'J. Ruiz'}]",27.0,"{'bibtex': '@Article{Wang2021ExaminingTU,\n author = {Isaac Wang and J. Ruiz},\n booktitle = {International journal of human computer interactions},\n journal = {International Journal of Human–Computer Interaction},\n pages = {1648 - 1673},\n title = {Examining the Use of Nonverbal Communication in Virtual Agents},\n volume = {37},\n year = {2021}\n}\n'}","[{'paperId': '4f4d620c928c4e1717bb5f2b562732f53c07a6ec', 'title': 'Psychological insights into the research and practice of embodied conversational agents, chatbots and social assistive robots: a systematic meta-review'}, {'paperId': 'e2b2d6a691f97d06901910a2bef5a9bf97adf847', 'title': 'AMBY: A development environment for youth to create conversational agents'}, {'paperId': 'f706f48972d4d722da460decda0bf4d0ee4de1d5', 'title': 'CNAMD Corpus: A Chinese Natural Audiovisual Multimodal Database of Conversations for Social Interactive Agents'}, {'paperId': 'd4d17ec93816df707a43e5afe166715cac3cecdb', 'title': 'Designing and Evaluating an Emotionally Responsive Virtual Patient Simulation.'}, {'paperId': '472ffbe74d57203eee39d39d1c1c9ce393bc10b5', 'title': 'Exploring the Emotional Functions of Co-Speech Hand Gesture\xa0in Language and Communication.'}, {'paperId': '293a91518292938951e244b6056a90885659a36e', 'title': '3D Dynamic Image Modeling Based on Machine Learning in Film and Television Animation'}, {'paperId': 'c88de7b177c0706936e791bfd7eeb7afe2412fb2', 'title': 'ASAP: Endowing Adaptation Capability to Agent in Human-Agent Interaction'}, {'paperId': '7fc361bba54d48433277f8decb7618ee1acb58cb', 'title': 'Intertwining the social and the cognitive loops: socially enactive cognition for human-compatible interactive systems'}, {'paperId': '54c2ea048956190343cd2e99f7ef26eb62e21653', 'title': 'Are we ready yet for digital transformation? Virtual versus on-campus OSCE as assessment tools in pharmacy education. A randomized controlled head-to-head comparative assessment'}, {'paperId': 'cec406a353b2179452a0a7ff731a3f7051e6c5a5', 'title': 'The impact of virtual agents’ multimodal communication on brain activity and cognitive load in Virtual Reality'}, {'paperId': '483f2f436d144237e68a0e7e92763ed9cfea73db', 'title': 'Understanding Interviewees’ Perceptions and Behaviour towards Verbally and Non-verbally Expressive Virtual Interviewing Agents'}, {'paperId': 'a585300d7369839ba73999835f4a40a412f1ee9f', 'title': 'Towards Emotionally Expressive Virtual Human Agents to Foster L2 Production: Insights from a Preliminary Woz Experiment'}, {'paperId': 'b64c1f66d8b6ee96b9f5eb7ceaa2cd86604f5cdc', 'title': 'Android Robots vs Virtual Agents: which system differently aged users prefer?'}, {'paperId': '8aae9e8de4a419fa2848ab291c69f8b5352d9ce1', 'title': 'An Exploration of Cognitive and Emotional Factors for Types of Virtual Tutors and Design Methods in Virtual Reality'}, {'paperId': '655eda5abfb976af2a9821a2517bb9f1a6791888', 'title': 'Microexpressions in digital humans: perceived affect, sincerity, and trustworthiness'}, {'paperId': '8c8dd87d470febbc7f911b4af3f98e93270db843', 'title': 'Embodied Virtual Patients as a Simulation-Based Framework for Training Clinician-Patient Communication Skills: An Overview of Their Use in Psychiatric and Geriatric Care'}, {'paperId': '2be2ede53faabb27d4746c5ebf6b2e0bf047a05a', 'title': 'Evaluating Interactional Synchrony in Full-Body Interaction with Autistic Children'}, {'paperId': '9d6a73583794b10d91c8cb389bd2b2351e29298b', 'title': 'Humans in (Digital) Space: Representing Humans in Virtual Environments'}, {'paperId': 'f449258538cbee7d250099af4b41dc753c666086', 'title': 'An overview of mock interviews as a training tool for interviewers of children.'}, {'paperId': '978ed95f05367797a3db4c73a6762ca250445c86', 'title': 'Effects on Co-Presence of a Virtual Human: A Comparison of Display and Interaction Types'}, {'paperId': '9c1943cb91114074074019d28df93c8fdcd537c9', 'title': ""Do Prosody and Embodiment Influence the Perceived Naturalness of Conversational Agents' Speech?""}, {'paperId': '7bc13208989bed8cb302ad29066c7dc69d0ffe37', 'title': 'Erfolgreicher Wissensaustausch in virtuellen Teams – Wie wichtig ist soziale Präsenz?'}, {'paperId': '914bf76d748c0fcbca38c77bc92ed69ab4bbc76f', 'title': 'Towards Designing Enthusiastic AI Agents'}, {'paperId': '5a4bdeb5e76a7002f1faa2f12e8fa201934d2768', 'title': 'Smile mimicry smoothens human-virtual human interactions'}, {'paperId': '0f8aace1f40dcb2e5d32666a1513d01facf88a68', 'title': 'Attach That There: Investigating 3D Virtual Assembly Assistants That Point Into the Real World'}, {'paperId': '8f5e09c19900deeaaddac67fd87e098ed7382dac', 'title': 'Doctoral Consortium: Verbal Interactions with Embodied Conversational Agents'}, {'paperId': 'aa2868950c556ec56f42187be226148b349ef70c', 'title': 'A New Good Listener, the Digital Human: A comparative research analysis of conversational virtual agents and robots'}]","{'name': 'International Journal of Human–Computer Interaction', 'pages': '1648 - 1673', 'volume': '37'}",212.0,Examining the Use of Nonverbal Communication in Virtual Agents,2021.0
1559,8272bd76f36d195023f245735e23e6b5c8b19afd,"cussed. As an illustration of these concepts, a study is reported of 56 white, middle-class infants, 49-51 weeks of age, in a strange situation. The presence of the mother was found to encourage exploratory behavior, her absence to depress exploration and to heighten attachment behaviors. In separation episodes such behaviors as crying and search increased. In reunion episodes proximity-seeking and contact-maintaining behaviors were heightened. In a substantial proportion of Ss, contact-resisting behaviors were also heightened in the reunion episodes, usually in conjunction with contactmaintaining behaviors, thus suggesting ambivalence. Some Ss also displayed proximity-avoiding behavior in relation to the mother in the reunion episodes. These findings are discussed in the context of relevant observational, clinical, and experimental studies of human and nonhuman primates, including studies of mother-child separation. In conclusion, it is urged that the concepts of attachment and attachment behavior be kept broad enough to comprehend the spectrum of the findings of this range of studies.","[{'authorId': '144607047', 'name': 'M. Ainsworth'}, {'authorId': '152945687', 'name': 'S. M. Bell'}]",1505.0,"{'bibtex': '@Article{Ainsworth1970AttachmentEA,\n author = {M. Ainsworth and S. M. Bell},\n journal = {Child development},\n pages = {\n          49-67\n        },\n title = {Attachment, exploration, and separation: illustrated by the behavior of one-year-olds in a strange situation.},\n volume = {41 1},\n year = {1970}\n}\n'}",,"{'volume': '41 1', 'pages': '\n          49-67\n        ', 'name': 'Child development'}",36.0,"Attachment, exploration, and separation: illustrated by the behavior of one-year-olds in a strange situation.",1970.0
1560,8273427401842f4110a7b0db8b116aa3dc5412a6,,"[{'authorId': '2816755', 'name': 'J. Bavelas'}, {'authorId': '7679798', 'name': 'Nicole Chovil'}]",56.0,"{'bibtex': '@Inproceedings{Bavelas2006NonverbalAV,\n author = {J. Bavelas and Nicole Chovil},\n title = {Nonverbal and Verbal Communication: Hand Gestures and Facial Displays as Part of Language Use in Face-to-face Dialogue.},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Nonverbal and Verbal Communication: Hand Gestures and Facial Displays as Part of Language Use in Face-to-face Dialogue.,2006.0
1561,8281d76e5c61b3ded3c397cfe8f81b042ab0f06a,"We build on a neuroanatomical model of how empathic states can motivate caregiving behavior, via empathy circuit-driven activation of regions in the hypothalamus and amygdala, which in turn stimulate a mesolimbic–ventral pallidum pathway, by integrating findings related to the perception of pain in self and others. On this basis, we propose a network to capture states of personal distress and (weak and strong forms of) empathic concern, which are particularly relevant for psychotherapists conducting attachment-based interventions. This model is then extended for the case of self-attachment therapy, in which conceptualized components of the self serve as both the source of and target for empathic resonance. In particular, we consider how states of empathic concern involving an other that is perceived as being closely related to the self might enhance the motivation for self-directed bonding (which in turn is proposed to lead the individual toward more compassionate states) in terms of medial prefrontal cortex–mediated activation of these caregiving pathways. We simulate our model computationally and discuss the interplay between the bonding and empathy protocols of the therapy.","[{'authorId': '2585368', 'name': 'David Cittern'}, {'authorId': '1694989', 'name': 'A. Edalat'}]",10.0,"{'bibtex': '@Article{Cittern2017ANM,\n author = {David Cittern and A. Edalat},\n journal = {Computational Psychiatry (Cambridge, Mass.)},\n pages = {132 - 167},\n title = {A Neural Model of Empathic States in Attachment-Based Psychotherapy},\n volume = {1},\n year = {2017}\n}\n'}",,"{'volume': '1', 'pages': '132 - 167', 'name': 'Computational Psychiatry (Cambridge, Mass.)'}",179.0,A Neural Model of Empathic States in Attachment-Based Psychotherapy,2017.0
1563,82a367e5d46e07259ab76dce6adc15c21e7accf0,"Large dense crowds show aggregate behavior with reduced individual freedom of movement. We present a novel, scalable approach for simulating such crowds, using a dual representation both as discrete agents and as a single continuous system. In the continuous setting, we introduce a novel variational constraint called unilateral incompressibility, to model the large-scale behavior of the crowd, and accelerate inter-agent collision avoidance in dense scenarios. This approach makes it possible to simulate very large, dense crowds composed of up to a hundred thousand agents at near-interactive rates on desktop computers.","[{'authorId': '40319117', 'name': 'Rahul Narain'}, {'authorId': '10153422', 'name': 'Abhinav Golas'}, {'authorId': '144564863', 'name': 'Sean Curtis'}, {'authorId': '144247566', 'name': 'M. Lin'}]",379.0,"{'bibtex': '@Article{Narain2009AggregateDF,\n author = {Rahul Narain and Abhinav Golas and Sean Curtis and M. Lin},\n journal = {ACM SIGGRAPH Asia 2009 papers},\n title = {Aggregate dynamics for dense crowd simulation},\n year = {2009}\n}\n'}",,{'name': 'ACM SIGGRAPH Asia 2009 papers'},53.0,Aggregate dynamics for dense crowd simulation,2009.0
1564,82aab0e0ca0414d333ce6fac255d1773593301c2,,"[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '152990295', 'name': 'Q. Wei'}]",103.0,"{'bibtex': '@Article{Kim2011TheIO,\n author = {Yanghee Kim and Q. Wei},\n journal = {Comput. Educ.},\n pages = {505-514},\n title = {The impact of learner attributes and learner choice in an agent-based environment},\n volume = {56},\n year = {2011}\n}\n'}",,"{'volume': '56', 'pages': '505-514', 'name': 'Comput. Educ.'}",119.0,The impact of learner attributes and learner choice in an agent-based environment,2011.0
1565,82ab7355f4c10428081cddd99271db1de7ecaaa9,"To resolve several problems of ACT-R’s declarative memory (DM), Schultheis, Barkowsky, and Bertel (2006) developed a new long-term memory (LTM) component, called LTM . In this paper we present two ACT-R interfaces which integrate LTM into ACT-R. Such integrating LTM makes it easily accessible to ACT-R modelers and allows more thoroughly evaluating it in its interplay with other components of a cognitive architecture. By considering four different memory phenomena we show that ACT-R with LTM is superior to ACT-R employing only DM and, thus, (a) LTM ’s benefits are not impaired when integrating it into a cognitive architecture and (b) using the newly developed interfaces improves ACT-R. In particular, integrating LTM into ACT-R allows computationally exploring memory conceptions which cannot be modeled with ACT-R utilizing only DM.","[{'authorId': '2652839', 'name': 'Holger Schultheis'}, {'authorId': '72620112', 'name': 'S. Lile'}]",4.0,"{'bibtex': '@Inproceedings{Schultheis2007ExtendingA,\n author = {Holger Schultheis and S. Lile},\n title = {Extending ACT-R ’ s Memory Capabilities},\n year = {2007}\n}\n'}",,,16.0,Extending ACT-R ’ s Memory Capabilities,2007.0
1566,82ec3a47fa559719fa6f66b92fa5695d30ee6e9c,,"[{'authorId': '1954903', 'name': 'R. Zajonc'}]",395.0,"{'bibtex': '@Inproceedings{Zajonc2000FeelingAT,\n author = {R. Zajonc},\n title = {Feeling and thinking: Closing the debate over the independence of affect.},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Feeling and thinking: Closing the debate over the independence of affect.,2000.0
1567,82ee73dc12d35a69ef063d8769e434317d687381,"ABSTRACT Objective: To present an overview of studies that investigated associations between social cognition functions (social cue perception, empathy, understanding intentions) and social behaviour or social outcome following traumatic brain injury (TBI). Methods: The literature search was conducted in the Medline, PsycInfo, Cochrane Library and Web of Science databases. Main criteria for selection were that the participants were adult persons with TBI, social cognition as well as social behaviour or social outcome post-TBI was assessed and correlations between social cognition and social behaviour or outcome were reported. Average correlations were calculated based on weighted summation of the correlations from the individual studies. Results: Of the 511 publications identified in the search, 13 were selected. Ten of these assessed emotion recognition, six assessed understanding of intentions and two assessed empathy. Average correlations between social cognition performance and post-injury social behaviour or outcome were significant for each of the three social cognition functions; poorer performance was associated with poorer outcome. Effect sizes varied from small to moderate (understanding intentions) to moderate (emotion recognition) to large (empathy). Conclusions: The associations were in line with models of social cognition and proposals that impairments in social cognition may underlie social behaviour difficulties and poor social outcome following TBI.","[{'authorId': '3748173', 'name': 'M. Milders'}]",48.0,"{'bibtex': '@Article{Milders2018RelationshipBS,\n author = {M. Milders},\n journal = {Brain Injury},\n pages = {62 - 68},\n title = {Relationship between social cognition and social behaviour following traumatic brain injury},\n volume = {33},\n year = {2018}\n}\n'}",,"{'volume': '33', 'pages': '62 - 68', 'name': 'Brain Injury'}",58.0,Relationship between social cognition and social behaviour following traumatic brain injury,2018.0
1568,83015591fb75285e89a2660fc7a403ff6d2434fb,"This paper describes the Thinking-Talking Head; an interdisciplinary project that sits between and draws upon engineering/computer science and behavioural/cognitive science; research and performance; implementation and evaluation. The project involves collaboration between computer scientists, engineers, language technologists and cognitive scientists, and its aim is twofold (a) to create a 3-D computer animation of a human head that will interact in real time with human agents, and (b) to serve as a research platform to drive research in the contributing disciplines, and in talking head research in general. The thinkingtalking head will emulate elements of face-to-face conversation through speech (including intonation), gaze and gesture. So it must have an active sensorium that accurately reflects the properties of its immediate environment, and must be able to generate appropriate communicative signals to feedback to the interlocutor. Here we describe the current implementation and outline how we are tackling issues concerning both the outputs (synthetic voice, visual speech, facial expressiveness and naturalness) from and inputs (auditory-visual speech recognition, emotion recognition, auditory-visual speaker localization) to the head. We describe how these head functions will be tuned and evaluated using various paradigms, including an imitation paradigm.","[{'authorId': '46427362', 'name': 'C. Davis'}, {'authorId': '1734469', 'name': 'Jeesun Kim'}, {'authorId': '1747304', 'name': 'T. Kuratate'}, {'authorId': '2108264905', 'name': 'Johnson Chen'}, {'authorId': '1751704', 'name': 'Stelarc'}, {'authorId': '144701668', 'name': 'D. Burnham'}]",10.0,"{'bibtex': '@Inproceedings{Davis2007MakingAT,\n author = {C. Davis and Jeesun Kim and T. Kuratate and Johnson Chen and Stelarc and D. Burnham},\n pages = {8},\n title = {Making a thinking-talking head},\n year = {2007}\n}\n'}",,{'pages': '8'},22.0,Making a thinking-talking head,2007.0
1569,8323fd773b7b2eba2f38a416d1e43395fafe7f23,,"[{'authorId': '3098701', 'name': 'Peter A. M. Ruijten'}, {'authorId': '2291357', 'name': 'Y. D. Kort'}, {'authorId': '3292087', 'name': 'P. Kosnar'}]",6.0,"{'bibtex': '@Inproceedings{Ruijten2012BridgingTG,\n author = {Peter A. M. Ruijten and Y. D. Kort and P. Kosnar},\n pages = {251-255},\n title = {Bridging the Gap between the Home and the Lab: A Qualitative Study of Acceptance of an Avatar Feedback System},\n year = {2012}\n}\n'}",,{'pages': '251-255'},4.0,Bridging the Gap between the Home and the Lab: A Qualitative Study of Acceptance of an Avatar Feedback System,2012.0
1570,836cbc9abe2aeea8b25a2166fda925dbfdfc8af0,"We present a novel and generic framework for the recognition of body expressions using human postures. Motivated by the state of the art from the domain of psychology, our approach recognizes expression by analyzing sequence of pose. Features proposed in this article are computationally simple and intuitive to understand. They are based on visual cues and provide in-depth understanding of body postures required to recognize body expressions. We have evaluated our approach on different databases with heterogeneous movements and body expressions. Our recognition results exceeds state of the art for some database and for others we obtain results at par with state of the art.","[{'authorId': '8726114', 'name': 'Arthur Crenn'}, {'authorId': '2267311525', 'name': 'Rizwan Ahmed Khan'}, {'authorId': '144176077', 'name': 'Alexandre Meyer'}, {'authorId': '1768560', 'name': 'S. Bouakaz'}]",29.0,"{'bibtex': '@Article{Crenn2016BodyER,\n author = {Arthur Crenn and Rizwan Ahmed Khan and Alexandre Meyer and S. Bouakaz},\n journal = {2016 International Conference on 3D Imaging (IC3D)},\n pages = {1-7},\n title = {Body expression recognition from animated 3D skeleton},\n year = {2016}\n}\n'}",,"{'pages': '1-7', 'name': '2016 International Conference on 3D Imaging (IC3D)'}",26.0,Body expression recognition from animated 3D skeleton,2016.0
1571,8370c5bb4037333fb0fd7949d736469e70186980,"This study examined whether or not embodied-agent-based learning would help middle-grade females have more positive mathematics learning experiences. The study used an explanatory mixed methods research design. First, a classroom-based experiment was conducted with one hundred twenty 9th graders learning introductory algebra (53% male and 47% female; 51% Caucasian and 49% Latino). The results revealed that learner gender was a significant factor in the learners’ evaluations of their agent (η² = .07), the learners’ task-specific attitudes (η² = .05), and their task-specific self-efficacy (η² = .06). In-depth interviews were then conducted with 22 students selected from the experiment participants. The interviews revealed that Latina and Caucasian females built a different type of relationship with their agent and reported more positive learning experiences as compared with Caucasian males. The females’ favorable view of the agent-based learning was largely influenced by their everyday classroom experiences, implying that students’ learning experience in real and virtual spaces was interconnected.","[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '145752919', 'name': 'J. Lim'}]",24.0,"{'bibtex': '@Article{Kim2013GenderedSW,\n author = {Yanghee Kim and J. Lim},\n journal = {Journal of Educational Psychology},\n pages = {1164-1174},\n title = {Gendered socialization with an embodied agent: Creating a social and affable mathematics learning environment for middle-grade females},\n volume = {105},\n year = {2013}\n}\n'}",,"{'volume': '105', 'pages': '1164-1174', 'name': 'Journal of Educational Psychology'}",61.0,Gendered socialization with an embodied agent: Creating a social and affable mathematics learning environment for middle-grade females,2013.0
1572,8386839cf792836efe9e3ff0e9e7891d8a479c00,,"[{'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '145813496', 'name': 'C. Martinho'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",681.0,"{'bibtex': '@Article{Leite2013SocialRF,\n author = {Iolanda Leite and C. Martinho and Ana Paiva},\n journal = {International Journal of Social Robotics},\n pages = {291-308},\n title = {Social Robots for Long-Term Interaction: A Survey},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '291-308', 'name': 'International Journal of Social Robotics'}",87.0,Social Robots for Long-Term Interaction: A Survey,2013.0
1573,83e701b8d8797c764af2d3fb5c43e32109f755e6,,"[{'authorId': '2066584531', 'name': 'M. Mirzaei'}, {'authorId': '3031660', 'name': 'S. Ghorshi'}, {'authorId': '145285466', 'name': 'Mohammad Mortazavi'}]",22.0,"{'bibtex': '@Article{Mirzaei2014AudiovisualSR,\n author = {M. Mirzaei and S. Ghorshi and Mohammad Mortazavi},\n journal = {The Visual Computer},\n pages = {245-257},\n title = {Audio-visual speech recognition techniques in augmented reality environments},\n volume = {30},\n year = {2014}\n}\n'}",,"{'volume': '30', 'pages': '245-257', 'name': 'The Visual Computer'}",33.0,Audio-visual speech recognition techniques in augmented reality environments,2014.0
1574,840826772e34d7eab002df2932522cd816e8d237,"Sight distance is a key element in highway geometric design. Existing models for evaluating sight distance are applicable only to two-dimensional (213), separate horizontal, and vertical alignments or simple elements of these separate alignments (vertical curve, horizontal curve). A new model using global positioning system (GPS) data is presented for determining the available sight distance on 3D combined horizontal and vertical alignments. Piecewise parametric equations in the form of cubic B-splines are used to represent the highway surface and sight obstructions, including tangents (grades), horizontal curves, and vertical curves. The available sight distance is found analytically by examining the intersection between the sight line and the elements representing the highway surface and sight obstructions. A profile of available sight distance can be established and used to evaluate sight-distance deficiency. Application of the new model is illustrated using actual GPS data for highway K-177 in Kansas (United States). The model has been tested and verified on most of the highways in Kansas. Software has been developed and can be used for determining the available sight distance on any highway for which GPS data are available.","[{'authorId': '97519585', 'name': 'Girish Nehate'}, {'authorId': '35054724', 'name': 'M. Rys'}]",48.0,"{'bibtex': '@Article{Nehate20063DCO,\n author = {Girish Nehate and M. Rys},\n journal = {Journal of Transportation Engineering-asce},\n pages = {691-698},\n title = {3D Calculation of Stopping-Sight Distance from GPS Data},\n volume = {132},\n year = {2006}\n}\n'}",,"{'volume': '132', 'pages': '691-698', 'name': 'Journal of Transportation Engineering-asce'}",8.0,3D Calculation of Stopping-Sight Distance from GPS Data,2006.0
1575,8408e30b80a5480eae6e3febd97ff256ec3cd9af,,"[{'authorId': '2067039', 'name': 'J. Elhai'}, {'authorId': '2253158295', 'name': 'Lucas de Francisco Carvalho'}, {'authorId': '2059921983', 'name': 'F. Miguel'}, {'authorId': '6976005', 'name': 'P. Palmieri'}, {'authorId': '2076129712', 'name': 'Ricardo Primi'}, {'authorId': '7233725', 'name': 'B. Christopher Frueh'}]",105.0,"{'bibtex': '@Article{Elhai2011TestingWP,\n author = {J. Elhai and Lucas de Francisco Carvalho and F. Miguel and P. Palmieri and Ricardo Primi and B. Christopher Frueh},\n journal = {Journal of anxiety disorders},\n pages = {\n          404-10\n        },\n title = {Testing whether posttraumatic stress disorder and major depressive disorder are similar or unique constructs.},\n volume = {25 3},\n year = {2011}\n}\n'}",,"{'volume': '25 3', 'pages': '\n          404-10\n        ', 'name': 'Journal of anxiety disorders'}",38.0,Testing whether posttraumatic stress disorder and major depressive disorder are similar or unique constructs.,2011.0
1576,840acc3bf20d515551772190800994707b4041fb,,"[{'authorId': '1696756', 'name': 'M. Jemni'}, {'authorId': '2116027', 'name': 'Oussama El Ghoul'}]",57.0,"{'bibtex': '@Inproceedings{Jemni2008AST,\n author = {M. Jemni and Oussama El Ghoul},\n pages = {670-677},\n title = {A System to Make Signs Using Collaborative Approach},\n year = {2008}\n}\n'}",,{'pages': '670-677'},6.0,A System to Make Signs Using Collaborative Approach,2008.0
1577,841f9486696e2cb4acc4805149f420f1e20380c9,"Virtual agents are being increasingly used in the areas of healthcare, treatments, and therapy. Virtual agents with emotional intelligence have shown the potential to be applicable to deliver mental health intervention and facilitate therapies for children on the autism spectrum. To build an emotionally intelligent agent, automatic recognition of emotions is an essential building block. For mental health therapies and treatments, detecting rapidly changing emotions of individuals can be useful to know whether the patients are showing appropriate emotional response or not. To this end, we present a new data-driven approach to identify the perceived emotions of individuals based on their walking styles. We extract an individual’s walking gait in the form of a sequence of 3D poses given an RGB video of him/her walking. We leverage the gait features to classify the perceived emotional state of the individual into one of four categories: happy, sad, angry, or neutral. First, we use an LSTM network to extract deep features of the gait using labeled emotion datasets. Next, we compute the affective features of the gaits using posture and movement cues. We combine these affective features with the deep features and classify them using a Random Forest Classifier. We observe that this approach provides an accuracy of 80:07% in identifying the perceived emotions. Additionally, we present a new dataset consisting of videos of walking individuals, their extracted gaits, and perceived emotion labels associated with each gait. We refer to this dataset as the ‘EWalk (Emotion Walk)” dataset.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '50227009', 'name': 'Uttaran Bhattacharya'}, {'authorId': '80905955', 'name': 'Kyra Kapsaskis'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",11.0,"{'bibtex': '@Article{Randhavane2019LearningPE,\n author = {Tanmay Randhavane and Uttaran Bhattacharya and Kyra Kapsaskis and Kurt Gray and Aniket Bera and Dinesh Manocha},\n booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\n journal = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\n pages = {395-399},\n title = {Learning Perceived Emotion Using Affective and Deep Features for Mental Health Applications},\n year = {2019}\n}\n'}","[{'paperId': 'fd6cefc927c20adfb15d0385b68ddd4ac322b693', 'title': 'Learning to Simplify Spatial-Temporal Graphs in Gait Analysis'}, {'paperId': '081f883401a573bfedb37586d7ebd735daaa7d3c', 'title': 'GaitPT: Skeletons Are All You Need For Gait Recognition'}, {'paperId': '7fafb25a69b6f4201bdf07bbdbbb2f15cd4423ae', 'title': 'Multi-modal speech emotion detection using optimised deep neural network classifier'}, {'paperId': '83a4e2977b63dc09753e834f88d243679416d450', 'title': 'Learning Gait Emotions Using Affective and Deep Features'}, {'paperId': 'b2d70e6d43f000b168749f2840af68943ab53471', 'title': 'Integrated Equipment for Parkinson’s Disease Early Detection Using Graph Convolution Network'}, {'paperId': '44cd043d4fcf4ab9a4e304efc8a86605b751688c', 'title': ""Who's a Good Boy? Reinforcing Canine Behavior in Real-Time using Machine Learning""}, {'paperId': '446a7fdc90b2bc1e764c31e21cb6a38893a94281', 'title': 'EWareNet: Emotion-Aware Pedestrian Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation'}, {'paperId': '02989aa1e7ad920f944441fc1f6a1492d21b9a58', 'title': 'EWareNet: Emotion Aware Human Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation'}, {'paperId': 'e6e86665ad3a08c568577c4918b16b2c14f28ad7', 'title': 'Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion'}, {'paperId': 'e04bcb5b159b4f8444e14228e9ab32c3979ed55a', 'title': 'Review of Different Combinations of Facial Expression Recognition System'}, {'paperId': '852c4a92512d0c13411011ef45f58111be2ccece', 'title': 'ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation'}]","{'name': '2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)', 'pages': '395-399'}",35.0,Learning Perceived Emotion Using Affective and Deep Features for Mental Health Applications,2019.0
1578,842254774243a56313626ba04582360f1659cb06,"Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection.","[{'authorId': '2265542260', 'name': 'Samarth Tripathi'}, {'authorId': '2265542262', 'name': 'Sarthak Tripathi'}, {'authorId': '144396601', 'name': 'H. Beigi'}]",118.0,"{'bibtex': '@Inproceedings{Tripathi2018MultiModalER,\n author = {Samarth Tripathi and Sarthak Tripathi and H. Beigi},\n title = {Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning},\n year = {2018}\n}\n'}",,,15.0,Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning,2018.0
1580,8432aa15730df7432e1ca82ce48f0a90da2a86ec,,"[{'authorId': '4528719', 'name': 'Kristopher J Preacher'}, {'authorId': '23662196', 'name': 'A. Hayes'}]",15585.0,"{'bibtex': '@Article{Preacher2004SPSSAS,\n author = {Kristopher J Preacher and A. Hayes},\n journal = {Behavior Research Methods, Instruments, & Computers},\n pages = {717-731},\n title = {SPSS and SAS procedures for estimating indirect effects in simple mediation models},\n volume = {36},\n year = {2004}\n}\n'}",,"{'volume': '36', 'pages': '717-731', 'name': 'Behavior Research Methods, Instruments, & Computers'}",41.0,SPSS and SAS procedures for estimating indirect effects in simple mediation models,2004.0
1581,843305cd48f9b43f0e7bb5414e6fdb3e78740e3b,"Despite the fact that the facial expressions of emotions are naturally dynamic social signals, their communicative value has typically been studied using static photographs. In this paper, we focus on the perception of emotions from naturally occurring, dynamic facial displays produced in social interactions. In describing their impressions of 200 video records of spontaneous emotional expressions produced during a face-to-face emotional sharing task, observers were found to agree on five emotional factors: enjoyment, hostility, embarrassment, surprise, and sadness. FACS coding and sequential analysis using a pattern detection algorithm showed that recordings rated high on one emotional factor were characterized by unique sequences of facial actions coordinated with eye and/or gaze actions. Our results suggest that the dynamic unfolding of facial displays and their combination with additional nonverbal signals may play an important and still under-investigated role in emotion perception in face-to-face interactions.","[{'authorId': '10539662', 'name': 'S. Kaiser'}]",19.0,"{'bibtex': '@Article{Kaiser2011SequentialPO,\n author = {S. Kaiser},\n journal = {Swiss Journal of Psychology},\n pages = {241-252},\n title = {Sequential Patterning of Facial Actions in the Production and Perception of Emotional Expressions},\n volume = {70},\n year = {2011}\n}\n'}",,"{'volume': '70', 'pages': '241-252', 'name': 'Swiss Journal of Psychology'}",44.0,Sequential Patterning of Facial Actions in the Production and Perception of Emotional Expressions,2011.0
1582,844ef462c0f75deb10ff748a3ee1fb93018146c5,,"[{'authorId': '1719487', 'name': 'Dolores Cañamero'}]",89.0,"{'bibtex': '@Inproceedings{Cañamero2003DesigningEF,\n author = {Dolores Cañamero},\n title = {Designing emotions for activity selection in autonomous agents},\n year = {2003}\n}\n'}",,,0.0,Designing emotions for activity selection in autonomous agents,2003.0
1584,846aedd869a00c09b40f1f1f35673cb22bc87490,,"[{'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '1885349', 'name': 'Aja Huang'}, {'authorId': '2772217', 'name': 'Chris J. Maddison'}, {'authorId': '35099444', 'name': 'A. Guez'}, {'authorId': '2175946', 'name': 'L. Sifre'}, {'authorId': '47568983', 'name': 'George van den Driessche'}, {'authorId': '4337102', 'name': 'Julian Schrittwieser'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '2749418', 'name': 'Vedavyas Panneershelvam'}, {'authorId': '1975889', 'name': 'Marc Lanctot'}, {'authorId': '48373216', 'name': 'S. Dieleman'}, {'authorId': '2401609', 'name': 'Dominik Grewe'}, {'authorId': '4111313', 'name': 'John Nham'}, {'authorId': '2583391', 'name': 'Nal Kalchbrenner'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '40662181', 'name': 'M. Leach'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '1686971', 'name': 'T. Graepel'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",14369.0,"{'bibtex': '@Article{Silver2016MasteringTG,\n author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},\n journal = {Nature},\n pages = {484-489},\n title = {Mastering the game of Go with deep neural networks and tree search},\n volume = {529},\n year = {2016}\n}\n'}",,"{'volume': '529', 'pages': '484-489', 'name': 'Nature'}",72.0,Mastering the game of Go with deep neural networks and tree search,2016.0
1585,84986b977962f7f491b9243a2a52626771457542,,"[{'authorId': '1715144', 'name': 'K. Karpouzis'}, {'authorId': '2001300', 'name': 'G. Caridakis'}, {'authorId': '2358968', 'name': 'Stavroula-Evita Fotinea'}, {'authorId': '1893648', 'name': 'E. Efthimiou'}]",86.0,"{'bibtex': '@Article{Karpouzis2007EducationalRA,\n author = {K. Karpouzis and G. Caridakis and Stavroula-Evita Fotinea and E. Efthimiou},\n journal = {Comput. Educ.},\n pages = {54-74},\n title = {Educational resources and implementation of a Greek sign language synthesis architecture},\n volume = {49},\n year = {2007}\n}\n'}",,"{'volume': '49', 'pages': '54-74', 'name': 'Comput. Educ.'}",36.0,Educational resources and implementation of a Greek sign language synthesis architecture,2007.0
1586,849bb402af1dd335e32a1bd253d1f5803942e9fe,"Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.","[{'authorId': '8281265', 'name': 'Yiping Song'}, {'authorId': '144539156', 'name': 'Rui Yan'}, {'authorId': '2144439440', 'name': 'Xiang Li'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '47474380', 'name': 'Ming Zhang'}]",103.0,"{'bibtex': '@Article{Song2016TwoAB,\n author = {Yiping Song and Rui Yan and Xiang Li and Dongyan Zhao and Ming Zhang},\n journal = {ArXiv},\n title = {Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems},\n volume = {abs/1610.07149},\n year = {2016}\n}\n'}",,"{'volume': 'abs/1610.07149', 'name': 'ArXiv'}",31.0,Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems,2016.0
1587,84d6423fb7764a8cbda4b3ef75a953d256d86298,"In this paper, we describe user experiences with a system equipped with cognitive vision that interacts with the user in the context of personal assistance in the office. A cognitive vision computer can see the user and user responses and react to situations that happen in the environment, crossing the boundary between the virtual and the physical world. How should such a seeing computer interact with its users? Three different interface styles -- a traditional GUI, a cartoon-like embodied agent and a realistic embodied agent -- are tested in two tasks where users are actively observed by a (simulated) cognitive vision system. The system assists them in problem solving. Both the non-embodied and the embodied interaction styles offer the user certain advantages and the pros and cons based on the experiment results are discussed in terms of performance, intelligence, trust, comfort, and social presence.","[{'authorId': '3002173', 'name': 'A. Geven'}, {'authorId': '2701197', 'name': 'Johann Schrammel'}, {'authorId': '1751253', 'name': 'M. Tscheligi'}]",19.0,"{'bibtex': '@Inproceedings{Geven2006InteractingWE,\n author = {A. Geven and Johann Schrammel and M. Tscheligi},\n pages = {135-144},\n title = {Interacting with embodied agents that can see: how vision-enabled agents can assist in spatial tasks},\n year = {2006}\n}\n'}",,{'pages': '135-144'},25.0,Interacting with embodied agents that can see: how vision-enabled agents can assist in spatial tasks,2006.0
1590,84fbdd5923781dba16b8343b2ab3bfd418180862,"In this chapter we outline the requirements for a systematic corpus of actor portrayals and describe the development, recording, editing, and validating of a major new corpus, the Geneva Multimodal Emotion Portrayal (GEMEP). This corpus consists of more than 7,000 audio-video emotion portrayals, representing 18 emotions (including rarely studied subtle emotions), portrayed by 10 professional actors who were coached by a professional director. The portrayals are recorded with optimal digital quality in multiple modalities, using both pseudo linguistic utterances and affect bursts. In addition, the corpus includes stimuli with systematically varied intensity levels, as well as instances of masked expressions. From the total corpus, 1,260 portrayals were selected and submitted to a first rating procedure in different modalities to establish validity in terms of inter-judge reliability and recognition accuracy. The results show that the portrayed expressions are recognized by lay judges with an accuracy level that, in the case of all emotions, largely exceeded chance and that compares very favorably with published tests of emotion recognition that use highly selected stimulus sets. The portrayals also reach very satisfactory levels of inter-rater reliability for category judgments and ratings of believability and intensity of the portrayals. The validity of the corpus is further confirmed by replicating results in earlier work on the role of expression modality and the corresponding communication channel for cue utilization in emotion recognition. We show that, as expected, the highest accuracy results if both auditory and visual information (voice, face, and gestures) is available, but that sizeable accuracy is achieved even when only one modality is available. The video modality is slightly superior to the audio modality, probably reflecting the fact that facial and gestural cues are more discrete and iconic than vocal cues. However, there are important interactions between emotion and modality, as particular emotions seem to be preferentially communicated by visual or audio cues. The results also raise important issues concerning the relationships","[{'authorId': '2162242', 'name': 'T. Bänziger'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",198.0,"{'bibtex': '@Inproceedings{Bänziger2010IntroducingTG,\n author = {T. Bänziger and K. Scherer},\n title = {Introducing the Geneva Multimodal Emotion Portrayal (GEMEP) corpus},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",40.0,Introducing the Geneva Multimodal Emotion Portrayal (GEMEP) corpus,2010.0
1592,850899a151dfb8fb35c756ea9284249db01e496d,"This study investigated the claim that humans will readily form team relationships with computers . Drawing from the group dynamic literature in human – human interactions , a laboratory experiment ( n (cid:53) 56) manipulated identity and interdependence to create team af ﬁliation in a human – computer interaction . The data show that subjects who are told they are interdependent with the computer af ﬁliate with the computer as a team . The data also show that the ef fects of being in a team with a computer are the same as the ef fects of being in a team with another human : subjects in the interdependence conditions perceived the computer to be more similar to themselves , saw themselves as more cooperative , were more open to inﬂuence from the computer , thought the information from the computer was of higher quality , found the information from the computer friendlier , and conformed more to the computer’s information . Subjects in the identity conditions showed neither team af ﬁliation nor the ef fects of team af ﬁliation .","[{'authorId': '123022889', 'name': 'Lifford'}, {'authorId': '69876593', 'name': 'Ass'}, {'authorId': '97612131', 'name': 'F. B.J.'}, {'authorId': '114718563', 'name': 'Ogg'}, {'authorId': '2105450908', 'name': 'Oungme'}, {'authorId': '2071008837', 'name': 'Oon'}]",289.0,"{'bibtex': '@Inproceedings{Lifford1996CanCB,\n author = {Lifford and Ass and F. B.J. and Ogg and Oungme and Oon},\n title = {Can computers be teammates?},\n year = {1996}\n}\n'}",,,35.0,Can computers be teammates?,1996.0
1593,854eca61a57d2c1ea1019663caf022bc8fd0b909,,"[{'authorId': '2053486178', 'name': 'Pauli Virtanen'}, {'authorId': '3912995', 'name': 'R. Gommers'}, {'authorId': '35525979', 'name': 'T. Oliphant'}, {'authorId': '2065380893', 'name': 'Matt Haberland'}, {'authorId': '144896751', 'name': 'Tyler Reddy'}, {'authorId': '3084321', 'name': 'D. Cournapeau'}, {'authorId': '2143743195', 'name': 'Evgeni Burovski'}, {'authorId': '143825690', 'name': 'Pearu Peterson'}, {'authorId': '2214143', 'name': 'Warren Weckesser'}, {'authorId': '2065294575', 'name': 'Jonathan Bright'}, {'authorId': '2066718598', 'name': 'Stéfan J. van der Walt'}, {'authorId': '144082394', 'name': 'M. Brett'}, {'authorId': '2115849250', 'name': 'Joshua Wilson'}, {'authorId': '2061249', 'name': 'K. Millman'}, {'authorId': '101711386', 'name': 'N. Mayorov'}, {'authorId': '2072898697', 'name': 'Andrew R. J. Nelson'}, {'authorId': '48376304', 'name': 'E. Jones'}, {'authorId': '2066375342', 'name': 'Robert Kern'}, {'authorId': '144752199', 'name': 'Eric Larson'}, {'authorId': '144873258', 'name': 'C. Carey'}, {'authorId': '49005409', 'name': 'İlhan Polat'}, {'authorId': '2150672343', 'name': 'Yu Feng'}, {'authorId': '2054115424', 'name': 'Eric W. Moore'}, {'authorId': '2081469', 'name': 'J. Vanderplas'}, {'authorId': '98592399', 'name': 'D. Laxalde'}, {'authorId': '15571182', 'name': 'Josef Perktold'}, {'authorId': '2772998', 'name': 'R. Cimrman'}, {'authorId': '35265702', 'name': 'Ian Henriksen'}, {'authorId': '153037053', 'name': 'E. Quintero'}, {'authorId': '2065073027', 'name': 'Charles R. Harris'}, {'authorId': '6402888', 'name': 'A. Archibald'}, {'authorId': '19235619', 'name': 'Antônio H. Ribeiro'}, {'authorId': '2570016', 'name': 'Fabian Pedregosa'}, {'authorId': '1491359454', 'name': 'P. van Mulbregt'}, {'authorId': '2064378280', 'name': 'Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco'}, {'authorId': '1491360442', 'name': 'A. Vijaykumar'}, {'authorId': '46473152', 'name': 'Alessandro Pietro Bardelli'}, {'authorId': '13044073', 'name': 'Alex Rothberg'}, {'authorId': '5301477', 'name': 'A. Hilboll'}, {'authorId': '117221049', 'name': 'Andre Kloeckner'}, {'authorId': '2860725', 'name': 'A. Scopatz'}, {'authorId': '2116599554', 'name': 'Antony Lee'}, {'authorId': '2842990', 'name': 'A. Rokem'}, {'authorId': '144291907', 'name': 'C. N. Woods'}, {'authorId': '80845765', 'name': 'Chad Fulton'}, {'authorId': '122327721', 'name': 'Charles Masson'}, {'authorId': '1491360726', 'name': 'C. Häggström'}, {'authorId': '73014178', 'name': 'Clark Fitzgerald'}, {'authorId': '46347313', 'name': 'D. Nicholson'}, {'authorId': '1491359271', 'name': 'David R. Hagen'}, {'authorId': '1721034', 'name': 'D. Pasechnik'}, {'authorId': '1759500', 'name': 'E. Olivetti'}, {'authorId': '2151071148', 'name': 'Eric Martin'}, {'authorId': '34422202', 'name': 'Eric Wieser'}, {'authorId': '2110243690', 'name': 'Fabrice Silva'}, {'authorId': '15628537', 'name': 'F. Lenders'}, {'authorId': '2052594289', 'name': 'Florian Wilhelm'}, {'authorId': '113071069', 'name': 'G. Young'}, {'authorId': '2058961413', 'name': 'Gavin A. Price'}, {'authorId': '40657747', 'name': 'G. Ingold'}, {'authorId': '2059529796', 'name': 'Gregory E. Allen'}, {'authorId': '87747838', 'name': 'Gregory R. Lee'}, {'authorId': '40936461', 'name': 'H. Audren'}, {'authorId': '35148114', 'name': 'I. Probst'}, {'authorId': '144455209', 'name': 'J. Dietrich'}, {'authorId': '2669435', 'name': 'J. Silterra'}, {'authorId': '38847103', 'name': 'James T. Webber'}, {'authorId': '31387499', 'name': 'J. Slavič'}, {'authorId': '3083916', 'name': 'J. Nothman'}, {'authorId': '2151427', 'name': 'J. Buchner'}, {'authorId': '2562288', 'name': 'Johannes Kulick'}, {'authorId': '3010882', 'name': 'Johannes L. Schönberger'}, {'authorId': '1491360335', 'name': 'J. V. De Miranda Cardoso'}, {'authorId': '145935219', 'name': 'J. Reimer'}, {'authorId': '2068517007', 'name': 'J. Harrington'}, {'authorId': '152794754', 'name': 'Juan Rodríguez'}, {'authorId': '1398851518', 'name': 'Juan Nunez-Iglesias'}, {'authorId': '48736174', 'name': 'Justin Kuczynski'}, {'authorId': '50159131', 'name': 'K. Tritz'}, {'authorId': '47049820', 'name': 'M. Thoma'}, {'authorId': '144620446', 'name': 'M. Newville'}, {'authorId': '2997408', 'name': 'Matthias Kümmerer'}, {'authorId': '48393751', 'name': 'Maximilian Bolingbroke'}, {'authorId': '2980014', 'name': 'Michael Tartre'}, {'authorId': '36917288', 'name': 'M. Pak'}, {'authorId': '2116828326', 'name': 'Nathaniel J. Smith'}, {'authorId': '28955794', 'name': 'N. Nowaczyk'}, {'authorId': '1491361121', 'name': 'Nikolay Shebanov'}, {'authorId': '144208188', 'name': 'O. Pavlyk'}, {'authorId': '96695635', 'name': 'P. A. Brodtkorb'}, {'authorId': '2111215482', 'name': 'Perry Lee'}, {'authorId': '144431879', 'name': 'R. McGibbon'}, {'authorId': '3449704', 'name': 'Roman Feldbauer'}, {'authorId': '2112242667', 'name': 'Sam Lewis'}, {'authorId': '152330021', 'name': 'S. Tygier'}, {'authorId': '34953991', 'name': 'Scott Sievert'}, {'authorId': '1737624', 'name': 'S. Vigna'}, {'authorId': '2053615130', 'name': 'Stefan Peterson'}, {'authorId': '5891171', 'name': 'S. More'}, {'authorId': '92169827', 'name': 'Tadeusz Pudlik'}, {'authorId': '66273392', 'name': 'T. Oshima'}, {'authorId': '1915727', 'name': 'T. Pingel'}, {'authorId': '144512158', 'name': 'T. Robitaille'}, {'authorId': '3419085', 'name': 'Thomas Spura'}, {'authorId': '2646100', 'name': 'T. Jones'}, {'authorId': '90587764', 'name': 'T. Cera'}, {'authorId': '1491358355', 'name': 'Tim Leslie'}, {'authorId': '2207047', 'name': 'Tiziano Zito'}, {'authorId': '1491360459', 'name': 'Tom Krauss'}, {'authorId': '10515643', 'name': 'U. Upadhyay'}, {'authorId': '1722413', 'name': 'Y. Halchenko'}, {'authorId': '1398440330', 'name': 'Y. Vázquez-Baeza'}]",15305.0,"{'bibtex': '@Article{Virtanen2019SciPy1F,\n author = {Pauli Virtanen and R. Gommers and T. Oliphant and Matt Haberland and Tyler Reddy and D. Cournapeau and Evgeni Burovski and Pearu Peterson and Warren Weckesser and Jonathan Bright and Stéfan J. van der Walt and M. Brett and Joshua Wilson and K. Millman and N. Mayorov and Andrew R. J. Nelson and E. Jones and Robert Kern and Eric Larson and C. Carey and İlhan Polat and Yu Feng and Eric W. Moore and J. Vanderplas and D. Laxalde and Josef Perktold and R. Cimrman and Ian Henriksen and E. Quintero and Charles R. Harris and A. Archibald and Antônio H. Ribeiro and Fabian Pedregosa and P. van Mulbregt and Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco and A. Vijaykumar and Alessandro Pietro Bardelli and Alex Rothberg and A. Hilboll and Andre Kloeckner and A. Scopatz and Antony Lee and A. Rokem and C. N. Woods and Chad Fulton and Charles Masson and C. Häggström and Clark Fitzgerald and D. Nicholson and David R. Hagen and D. Pasechnik and E. Olivetti and Eric Martin and Eric Wieser and Fabrice Silva and F. Lenders and Florian Wilhelm and G. Young and Gavin A. Price and G. Ingold and Gregory E. Allen and Gregory R. Lee and H. Audren and I. Probst and J. Dietrich and J. Silterra and James T. Webber and J. Slavič and J. Nothman and J. Buchner and Johannes Kulick and Johannes L. Schönberger and J. V. De Miranda Cardoso and J. Reimer and J. Harrington and Juan Rodríguez and Juan Nunez-Iglesias and Justin Kuczynski and K. Tritz and M. Thoma and M. Newville and Matthias Kümmerer and Maximilian Bolingbroke and Michael Tartre and M. Pak and Nathaniel J. Smith and N. Nowaczyk and Nikolay Shebanov and O. Pavlyk and P. A. Brodtkorb and Perry Lee and R. McGibbon and Roman Feldbauer and Sam Lewis and S. Tygier and Scott Sievert and S. Vigna and Stefan Peterson and S. More and Tadeusz Pudlik and T. Oshima and T. Pingel and T. Robitaille and Thomas Spura and T. Jones and T. Cera and Tim Leslie and Tiziano Zito and Tom Krauss and U. Upadhyay and Y. Halchenko and Y. Vázquez-Baeza},\n journal = {Nature Methods},\n pages = {261 - 272},\n title = {SciPy 1.0: fundamental algorithms for scientific computing in Python},\n volume = {17},\n year = {2019}\n}\n'}",,"{'volume': '17', 'pages': '261 - 272', 'name': 'Nature Methods'}",144.0,SciPy 1.0: fundamental algorithms for scientific computing in Python,2019.0
1595,855f193ba056a80a4bd0d925fa4add46b9b03f4b,,"[{'authorId': '2060600', 'name': 'Tomoko Koda'}, {'authorId': '143807934', 'name': 'T. Ishida'}, {'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '1742930', 'name': 'E. André'}]",48.0,"{'bibtex': '@Article{Koda2009AvatarCC,\n author = {Tomoko Koda and T. Ishida and M. Rehm and E. André},\n journal = {AI & SOCIETY},\n pages = {237-250},\n title = {Avatar culture: cross-cultural evaluations of avatar facial expressions},\n volume = {24},\n year = {2009}\n}\n'}",,"{'volume': '24', 'pages': '237-250', 'name': 'AI & SOCIETY'}",30.0,Avatar culture: cross-cultural evaluations of avatar facial expressions,2009.0
1596,8571611db04df42d9ddcca39b1a3c23c11d51b6d,"SARA (Socially-Aware Robot Assistant) is an embodied intelligent personal assistant that analyses the user’s visual (head and face movement), vocal (acoustic features) and verbal (conversational strategies) behaviours to estimate its rapport level with the user, and uses its own appropriate visual, vocal and verbal behaviors to achieve task and social goals. The presented agent aids conference attendees by eliciting their preferences through building rapport, and then making informed personalized recommendations about sessions to attend and people to meet.","[{'authorId': '2026985', 'name': 'Yoichi Matsuyama'}, {'authorId': '2061543783', 'name': 'Arjun Bhardwaj'}, {'authorId': '2114012102', 'name': 'Ran Zhao'}, {'authorId': '7903321', 'name': 'Oscar Romeo'}, {'authorId': '51200544', 'name': 'Sushma A. Akoju'}, {'authorId': '145431806', 'name': 'Justine Cassell'}]",78.0,"{'bibtex': '@Inproceedings{Matsuyama2016SociallyAwareAI,\n author = {Yoichi Matsuyama and Arjun Bhardwaj and Ran Zhao and Oscar Romeo and Sushma A. Akoju and Justine Cassell},\n pages = {224-227},\n title = {Socially-Aware Animated Intelligent Personal Assistant Agent},\n year = {2016}\n}\n'}",,{'pages': '224-227'},12.0,Socially-Aware Animated Intelligent Personal Assistant Agent,2016.0
1597,8577dec8a9c980cabfecfa73c8cf237bd123bed3,"People make determinations about the social characteristics of an agent (e.g., robot or virtual agent) by interpreting social cues displayed by the agent, such as facial expressions. Although a considerable amount of research has been conducted investigating age-related differences in emotion recognition of human faces (e.g., Sullivan, & Ruffman, 2004), the effect of age on emotion identification of virtual agent facial expressions has been largely unexplored. Age-related differences in emotion recognition of facial expressions are an important factor to consider in the design of agents that may assist older adults in a recreational or healthcare setting. The purpose of the current research was to investigate whether age-related differences in facial emotion recognition can extend to emotion-expressive virtual agents. Younger and older adults performed a recognition task with a virtual agent expressing six basic emotions. Larger age-related differences were expected for virtual agents displaying negative emotions, such as anger, sadness, and fear. In fact, the results indicated that older adults showed a decrease in emotion recognition accuracy for a virtual agent's emotions of anger, fear, and happiness.","[{'authorId': '1809740', 'name': 'Jenay M. Beer'}, {'authorId': '1689705', 'name': 'A. D. Fisk'}, {'authorId': '145912604', 'name': 'W. Rogers'}]",18.0,"{'bibtex': '@Article{Beer2009EmotionRO,\n author = {Jenay M. Beer and A. D. Fisk and W. Rogers},\n journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},\n pages = {131 - 135},\n title = {Emotion Recognition of Virtual Agents Facial Expressions: The Effects of Age and Emotion Intensity},\n volume = {53},\n year = {2009}\n}\n'}",,"{'volume': '53', 'pages': '131 - 135', 'name': 'Proceedings of the Human Factors and Ergonomics Society Annual Meeting'}",6.0,Emotion Recognition of Virtual Agents Facial Expressions: The Effects of Age and Emotion Intensity,2009.0
1601,859262670d678f20756639228f8a7eae5d65b9b6,"In this paper, we seek to review the broad landscape of research in computational emotions and cognition. We begin by classifying and organizing an enumeration of recent models and systems and then discuss some of the landmark models from the literature, such as EMA and WASABI. We then discuss open problems with the current state of research. These issues are standardizing criteria for evaluation of models, the complexity and breadth of the domain, and the need to implement a working system which addresses integration with more of the rich history of AI research. We also provide suggestions for future research, particularly standardization to facilitate community collaboration.","[{'authorId': '2110344685', 'name': 'Jerry Lin'}, {'authorId': '3161354', 'name': 'Marc Spraragen'}, {'authorId': '51371300', 'name': 'M. Zyda'}]",53.0,"{'bibtex': '@Inproceedings{Lin2012ComputationalMO,\n author = {Jerry Lin and Marc Spraragen and M. Zyda},\n pages = {59-76},\n title = {Computational Models of Emotion and Cognition},\n volume = {2},\n year = {2012}\n}\n'}",,"{'volume': '2', 'pages': '59-76', 'name': ''}",60.0,Computational Models of Emotion and Cognition,2012.0
1602,86130e5a513d1d1b69f0cccee70e2c4598c0692f,"In this paper, Maxine, a powerful engine to develop applications with embodied animated agents is presented. The engine, based on the use of opensource libraries, enables multimodal real-time interaction with the user: via text, voice, images and gestures. Maxine virtual agents can establish emotional communication with the user through their facial expressions, the modulation of the voice and expressing the answers of the agents according to the information gathered by the system: noise level in the room, observer’s position, emotional state of the observer, etc. Moreover, the user’s emotions are considered and captured through images. For the moment, Maxine virtual agents have been used as virtual presenters and, a specific application, MaxinePPT, has been developed to allow non-programmers to create 3D presentations easily from classical PowerPoint presentations. Nevertheless, other applications are also envisaged.","[{'authorId': '144046205', 'name': 'E. Cerezo'}, {'authorId': '1787072', 'name': 'S. Baldassarri'}, {'authorId': '2123508', 'name': 'F. Serón'}]",21.0,"{'bibtex': '@Inproceedings{Cerezo2007INTERACTIVEAF,\n author = {E. Cerezo and S. Baldassarri and F. Serón},\n title = {INTERACTIVE AGENTS FOR MULTIMODAL EMOTIONAL USER INTERACTION},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",18.0,INTERACTIVE AGENTS FOR MULTIMODAL EMOTIONAL USER INTERACTION,2007.0
1603,86584dd53bc7a753b9502c564020c3f094b4d673,"Digman, The Curious History of the Five-Factor Model. Saucier, Goldberg, The Language of Personality: Lexical Perspectives on the Five-Factor Model. McCrae, Costa, Toward a New Generation of Personality Theories: Theoretical Contexts for the Five-Factor Model. Wiggins, Trapnell, A Dyadic-Interactional Perspective on the Five-Factor Model. Hogan, A Socioanalytic Perspective on the Five-Factor Model. Buss, Social Adaptation and Five Major Theories of Personality.","[{'authorId': '31743834', 'name': 'J. Wiggins'}]",1018.0,"{'bibtex': '@Inproceedings{Wiggins1996TheFM,\n author = {J. Wiggins},\n title = {The five-factor model of personality : theoretical perspectives},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The five-factor model of personality : theoretical perspectives,1996.0
1604,8662a17f17be821cb7a32e7206382e9f1ea6d946,"Summary 
 
Impression formation was examined as a function of interpersonal physical distance in an interview. It was predicted that a confederate would be rated less socially active as the distance between him and the subject increased. The hypothesis was supported by a significant negative linear trend in the composite ratings of friendliness, aggressiveness, extraversion, and dominance A variation in this trend, indicating that confederates seated closest to the subject were seen as less socially active, was explained in terms of compensatory behaviors minimizing the effect of close physical proximity","[{'authorId': '2521228', 'name': 'M. Patterson'}, {'authorId': '79781700', 'name': 'L. Sechrest'}]",85.0,"{'bibtex': '@Article{Patterson1970InterpersonalDA,\n author = {M. Patterson and L. Sechrest},\n journal = {Journal of personality},\n pages = {\n          161-6\n        },\n title = {Interpersonal distance and impression formation.},\n volume = {38 2},\n year = {1970}\n}\n'}",,"{'volume': '38 2', 'pages': '\n          161-6\n        ', 'name': 'Journal of personality'}",7.0,Interpersonal distance and impression formation.,1970.0
1605,866a3ee2a8630cf06739f02207d385e93c46744e,"An important part of creating a new and immersive interactive experience for a user is to have the perception that the virtual character is 'paying attention' to the user during the interaction. Such perceived attention could be derived from subtle movements such as the eye glancing towards the user or with larger movements such as turning its face or even its body towards the user. This understanding forms the basis for existing research models.
 Our work describes a user study that evaluates the subject's perception of dancing with a virtual character. We attempt to investigate the relationship and relevance of the facial expressions and eye gazing contact animations on a virtual character which moves vigorously as compared to the idle one (i.e. without facial expression). For this experiment, a virtual character is shown on the screen using stereoscopic projection, while the participant wears shutter glasses at all times during the user study. The observations of the subject about the realism and perceived interactivity of the character is tabulated through the data collected of the perceived opinions during the experiments. Other perceived factors such as the enjoyment of the experience and attractiveness of the virtual character are also considered.","[{'authorId': '2900145', 'name': 'M. Loke'}, {'authorId': '37704348', 'name': 'K. Tang'}, {'authorId': '1765531', 'name': 'G. G. Chua'}, {'authorId': '21743127', 'name': 'O. Tan'}, {'authorId': '2020444', 'name': 'F. Farbiz'}]",1.0,"{'bibtex': '@Inproceedings{Loke2009TheEO,\n author = {M. Loke and K. Tang and G. G. Chua and O. Tan and F. Farbiz},\n pages = {107-112},\n title = {The effect of facial animation on a dancing character},\n year = {2009}\n}\n'}",,{'pages': '107-112'},14.0,The effect of facial animation on a dancing character,2009.0
1606,8674042a8d3e8939d3e3446a15dc3e4202173fa9,"Systematic studies of infants with autism have not been previously carried out. Taking advantage of a new prospective screening instrument for autism in infancy (S. Baron-Cohen et al., 1996), the present study found that, compared with developmentally delayed and normally developing children, 20-month-old children with autism were specifically impaired on some aspects of empathy, joint attention, and imitation. Infants with autism failed to use social gaze in the empathy and joint attention tasks. Both the infants with autism and the infants with developmental delay demonstrated functional play, but very few participants in either group produced spontaneous pretend play. In the developmental delay group, but not the autism group, pretend play was shown following prompting. The implications of these findings for developmental accounts of autism and for the early diagnosis of the disorder are discussed.","[{'authorId': '3692247', 'name': 'T. Charman'}, {'authorId': '2498385', 'name': 'J. Swettenham'}, {'authorId': '2179290575', 'name': 'S. Baron-Cohen'}, {'authorId': '144851134', 'name': 'A. Cox'}, {'authorId': '2244046', 'name': 'G. Baird'}, {'authorId': '117539417', 'name': 'Auriol Drew'}]",785.0,"{'bibtex': '@Article{Charman1997InfantsWA,\n author = {T. Charman and J. Swettenham and S. Baron-Cohen and A. Cox and G. Baird and Auriol Drew},\n journal = {Developmental psychology},\n pages = {\n          781-9\n        },\n title = {Infants with autism: an investigation of empathy, pretend play, joint attention, and imitation.},\n volume = {33 5},\n year = {1997}\n}\n'}",,"{'volume': '33 5', 'pages': '\n          781-9\n        ', 'name': 'Developmental psychology'}",52.0,"Infants with autism: an investigation of empathy, pretend play, joint attention, and imitation.",1997.0
1607,86872ec55179b6cc03764b5af20529a5367ec6a2,,"[{'authorId': '3136443', 'name': 'M. Kazemifard'}, {'authorId': '9312093', 'name': 'N. Ghasem-Aghaee'}, {'authorId': '2064368', 'name': 'T. Ören'}]",29.0,"{'bibtex': '@Article{Kazemifard2011DesignAI,\n author = {M. Kazemifard and N. Ghasem-Aghaee and T. Ören},\n journal = {Expert Syst. Appl.},\n pages = {2640-2652},\n title = {Design and implementation of GEmA: A generic emotional agent},\n volume = {38},\n year = {2011}\n}\n'}",,"{'volume': '38', 'pages': '2640-2652', 'name': 'Expert Syst. Appl.'}",48.0,Design and implementation of GEmA: A generic emotional agent,2011.0
1610,868fca5d103fd5901926005a2282a8e4916571b2,,"[{'authorId': '46270580', 'name': 'Z. Liu'}]",6.0,"{'bibtex': ""@Inproceedings{Liu2007SimulationOV,\n author = {Z. Liu},\n pages = {284-291},\n title = {Simulation of Virtual Human's Mental State in Behavior Animation},\n year = {2007}\n}\n""}",,{'pages': '284-291'},11.0,Simulation of Virtual Human's Mental State in Behavior Animation,2007.0
1611,86a10ae36ecb3b4fac560a5b03fc6ca6cf7ef7e9,"Social Stories™ have gained Wide acceptance as an intervention for children With autism spectrum disorders (ASD), yet extant research provides little empirical evidence in support of their efficacy. This study examines the use of Social Stories to target repetitive tapping behavior displayed by a child With ASD, moderate intellectual disability, and associated language impairment. Over an extended period there Was evidence of a decrease in the target behavior. Further, this decrease Was associated With increased comprehension of the Social Story. The findings suggest that it is appropriate to consider language skills When evaluating the suitability of this intervention for students With moderate intellectual disabilities and to monitor comprehension.","[{'authorId': '5446795', 'name': 'Georgina Reynhout'}, {'authorId': '144951154', 'name': 'M. Carter'}]",61.0,"{'bibtex': '@Article{Reynhout2007SocialSE,\n author = {Georgina Reynhout and M. Carter},\n journal = {Focus on Autism and Other Developmental Disabilities},\n pages = {173 - 181},\n title = {Social Story™ Efficacy With a Child With Autism Spectrum Disorder and Moderate Intellectual Disability},\n volume = {22},\n year = {2007}\n}\n'}",,"{'volume': '22', 'pages': '173 - 181', 'name': 'Focus on Autism and Other Developmental Disabilities'}",40.0,Social Story™ Efficacy With a Child With Autism Spectrum Disorder and Moderate Intellectual Disability,2007.0
1612,86dc90b7957c39c696158befc0a881b0fdfa02ac,"Crowd simulation for virtual environments offers many challenges centered on the trade‐offs between rich behavior, control and computational cost. In this paper we present a new approach to controlling the behavior of agents in a crowd. Our method is scalable in the sense that increasingly complex crowd behaviors can be created without a corresponding increase in the complexity of the agents. Our approach is also more authorable; users can dynamically specify which crowd behaviors happen in various parts of an environment. Finally, the character motion produced by our system is visually convincing. We achieve our aims with a situation‐based control structure. Basic agents have very limited behaviors. As they enter new situations, additional, situation‐specific behaviors are composed on the fly to enable agents to respond appropriately. The composition is done using a probabilistic mechanism. We demonstrate our system with three environments including a city street and a theater.","[{'authorId': '40461728', 'name': 'Mankyu Sung'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}, {'authorId': '1687162', 'name': 'Stephen Chenney'}]",266.0,"{'bibtex': '@Article{Sung2004ScalableBF,\n author = {Mankyu Sung and Michael Gleicher and Stephen Chenney},\n journal = {Computer Graphics Forum},\n title = {Scalable behaviors for crowd simulation},\n volume = {23},\n year = {2004}\n}\n'}",,"{'volume': '23', 'name': 'Computer Graphics Forum'}",29.0,Scalable behaviors for crowd simulation,2004.0
1613,86e18f3a7f9311827d96970474d82ac92a017c15,,"[{'authorId': '2327911', 'name': 'F. Verberne'}, {'authorId': '145960497', 'name': 'Jaap Ham'}, {'authorId': '2064022036', 'name': 'Aditya Ponnada'}, {'authorId': '3026039', 'name': 'C. Midden'}]",50.0,"{'bibtex': '@Inproceedings{Verberne2013TrustingDC,\n author = {F. Verberne and Jaap Ham and Aditya Ponnada and C. Midden},\n pages = {234-245},\n title = {Trusting Digital Chameleons: The Effect of Mimicry by a Virtual Social Agent on User Trust},\n year = {2013}\n}\n'}",,{'pages': '234-245'},26.0,Trusting Digital Chameleons: The Effect of Mimicry by a Virtual Social Agent on User Trust,2013.0
1614,86f81c8a46a84cd969ed80e88395ba5de709794f,"Whether you’re aware of it or not, when you interact with others, you’re continuously giving and receiving wordless signals. All of your nonverbal behaviors—the gestures you make, your posture, your tone of voice, how much eye contact you make—send strong messages. They can put people at ease, build trust, and draw others towards you, or they can offend, confuse, and undermine what you’re trying to convey. These messages don’t stop when you stop speaking either. Even when you’re silent, you’re still communicating nonverbally.","[{'authorId': '2896960', 'name': 'J. Burgoon'}]",1161.0,"{'bibtex': '@Article{Burgoon1990NonverbalC,\n author = {J. Burgoon},\n journal = {Deutsche Krankenpflegezeitschrift},\n pages = {\n          751-4\n        },\n title = {[Nonverbal communication].},\n volume = {43 10},\n year = {1990}\n}\n'}",,"{'volume': '43 10', 'pages': '\n          751-4\n        ', 'name': 'Deutsche Krankenpflegezeitschrift'}",10.0,[Nonverbal communication].,1990.0
1616,86fbf1f45e6ef9b5624feeb03cf75b986c7aca0f,,"[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",48.0,"{'bibtex': '@Inproceedings{Melo2010TheIO,\n author = {C. D. Melo and P. Carnevale and J. Gratch},\n pages = {357-370},\n title = {The Influence of Emotions in Embodied Agents on Human Decision-Making},\n year = {2010}\n}\n'}",,{'pages': '357-370'},47.0,The Influence of Emotions in Embodied Agents on Human Decision-Making,2010.0
1618,870b8904dadb6a2f40290dddddd5c989f3656e5e,,"[{'authorId': '2149200216', 'name': 'Yan Lin'}, {'authorId': '8404908', 'name': 'Pei-yan Shan'}, {'authorId': '49408848', 'name': 'Wenjing Jiang'}, {'authorId': '38692969', 'name': 'Can Sheng'}, {'authorId': '2115502649', 'name': 'Lin Ma'}]",52.0,"{'bibtex': '@Article{Lin2018SubjectiveCD,\n author = {Yan Lin and Pei-yan Shan and Wenjing Jiang and Can Sheng and Lin Ma},\n journal = {Neurological Sciences},\n pages = {41-49},\n title = {Subjective cognitive decline: preclinical manifestation of Alzheimer’s disease},\n volume = {40},\n year = {2018}\n}\n'}",,"{'volume': '40', 'pages': '41-49', 'name': 'Neurological Sciences'}",66.0,Subjective cognitive decline: preclinical manifestation of Alzheimer’s disease,2018.0
1619,875aaba134119e8118ff735f092db3266f5f97b0,"How do emotions affect the opponent's behavior in a negotiation? Two experiments explored the interpersonal effects of anger and happiness. In Study 1 participants received information about the emotion (anger vs. happiness vs. no emotion) of their (fake) opponent. Participants with an angry opponent made lower demands and larger concessions than did participants with a happy opponent, those with a non-emotional opponent falling in between. Furthermore, the opponent's emotions induced similar emotions in the participants (i.e., ""emotional contagion""), and participants with a happy opponent evaluated the opponent and the negotiation more favorably than did participants with an angry opponent. In Study 2 participants received information about both the opponent's experienced and communicated emotions. As predicted, angry communications (unlike happy ones) induced fear and thereby mitigated the effect of the opponent's experienced emotion.","[{'authorId': '74691001', 'name': 'G. A. Kleef'}, {'authorId': '2893927', 'name': 'C. Dreu'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",64.0,"{'bibtex': '@Inproceedings{Kleef2003TheIE,\n author = {G. A. Kleef and C. Dreu and A. Manstead},\n title = {The Interpersonal Effects of Anger and Happiness on Negotiation Behavior and Outcomes},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",57.0,The Interpersonal Effects of Anger and Happiness on Negotiation Behavior and Outcomes,2003.0
1620,875df1fa9dc777ee1339b564065563c1a89a48bd,"Book synopsis: The first ever comprehensive review of the field of face perception, providing a much needed reference work for students and researchers in the brain sciences 
Includes cutting edge chapters, written and edited by an international team of top researchers 
Illustrated in full colour throughout 
The human face is unique among social stimuli in conveying such a variety of different characteristics. A person's identity, sex, race, age, emotional state, focus of attention, facial speech patterns, and attractiveness are all detected and interpreted with relative ease from the face. Humans also display a surprising degree of consistency in the extent to which personality traits, such as trustworthiness and likeability, are attributed to faces. In the past thirty years, face perception has become an area of major interest within psychology, with a rapidly expanding research base. Yet until now, there has been no comprehensive reference work bringing together this ever growing body of research. 
 
The Oxford Handbook of Face Perception is the most comprehensive and commanding review of the field ever published. It looks at the functional and neural mechanisms underlying the perception, representation, and interpretation of facial characteristics, such as identity, expression, eye gaze, attractiveness, personality, and race. It examines the development of these processes, their neural correlates in both human and non-human primates, congenital and acquired disorders resulting from their breakdown, and the theoretical and computational frameworks for their underlying mechanisms. With chapters by an international team of leading authorities from the brain sciences, the book is a landmark publication on face perception.","[{'authorId': '2825775', 'name': 'A. Calder'}, {'authorId': '2411445', 'name': 'G. Rhodes'}, {'authorId': '145546235', 'name': 'Mark H. Johnson'}, {'authorId': '2327323', 'name': 'J. Haxby'}]",411.0,"{'bibtex': '@Inproceedings{Calder2011OxfordHO,\n author = {A. Calder and G. Rhodes and Mark H. Johnson and J. Haxby},\n title = {Oxford Handbook of Face Perception},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Oxford Handbook of Face Perception,2011.0
1621,87cacc03f51582e64a6d0e3d7d068230f0e03a80,,"[{'authorId': '38124822', 'name': 'R. Todd'}, {'authorId': '2792268', 'name': 'William A. Cunningham'}, {'authorId': '5040426', 'name': 'A. Anderson'}, {'authorId': '48529647', 'name': 'Evan Thompson'}]",284.0,"{'bibtex': '@Article{Todd2012AffectbiasedAA,\n author = {R. Todd and William A. Cunningham and A. Anderson and Evan Thompson},\n journal = {Trends in Cognitive Sciences},\n pages = {365-372},\n title = {Affect-biased attention as emotion regulation},\n volume = {16},\n year = {2012}\n}\n'}",,"{'volume': '16', 'pages': '365-372', 'name': 'Trends in Cognitive Sciences'}",72.0,Affect-biased attention as emotion regulation,2012.0
1622,87e3b99195442cb37b3807c4d097b886b46a1458,"In recent years, there has been an increasing interest in designing social robots to interact with people to provide therapy and companionship. Most social robots currently being used are light-weight and much smaller in size compared to people. In this work, we investigate designing interactions for larger and more physically capable robots as they have more potential to assist people physically. A modified version of Baxter robot was used, by sitting Baxter on top of an electronic wheelchair. Two experiments were designed for studying the role of facial expressions and body movements in establishing trust with the user and for expressing attitudes. Our results suggest that the robot is capable of expressing fine and distinguishable attitudes (proud vs. relaxed) using its body language, and the coupling between body movements and speech is essential for the robot to be viewed as a person.","[{'authorId': '33432486', 'name': 'Mei Si'}, {'authorId': '2068949697', 'name': 'Joseph Dean McDaniel'}]",4.0,"{'bibtex': '@Article{Si2016EstablishTA,\n author = {Mei Si and Joseph Dean McDaniel},\n journal = {Cognitive Science},\n title = {Establish Trust and Express Attitude for a Non-Humanoid Robot},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': 'Cognitive Science'}",19.0,Establish Trust and Express Attitude for a Non-Humanoid Robot,2016.0
1623,87f40e6f3022adbc1f1905e3e506abad05a9964f,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. 
 
An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","[{'authorId': '2047446108', 'name': 'Tomas Mikolov'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2118440152', 'name': 'Kai Chen'}, {'authorId': '32131713', 'name': 'G. Corrado'}, {'authorId': '49959210', 'name': 'J. Dean'}]",30689.0,"{'bibtex': '@Inproceedings{Mikolov2013DistributedRO,\n author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and G. Corrado and J. Dean},\n pages = {3111-3119},\n title = {Distributed Representations of Words and Phrases and their Compositionality},\n year = {2013}\n}\n'}",,{'pages': '3111-3119'},24.0,Distributed Representations of Words and Phrases and their Compositionality,2013.0
1624,88420722e8afff17c970b361944476249bafb402,"In spatial dialog like in direction giving humans make frequent use of speechaccompanying gestures. Some gestures 
convey largely the same information as speech while others complement speech. This paper reports a study on how speakers distribute meaning across speech and gesture, 
and depending on what factors. Utterance meaning and the wider dialog context were tested by statistically analyzing 
a corpus of direction-giving dialogs. Problems of speech production (as indicated by discourse markers and disfluencies), the communicative goals, and the information 
status were found to be influential, while feedback signals by the addressee do not have any influence.","[{'authorId': '2025591', 'name': 'K. Bergmann'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",38.0,"{'bibtex': '@Inproceedings{Bergmann2006VerbalOV,\n author = {K. Bergmann and S. Kopp},\n title = {Verbal or Visual? How Information is Distributed across Speech and Gesture in Spatial Dialog},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",20.0,Verbal or Visual? How Information is Distributed across Speech and Gesture in Spatial Dialog,2006.0
1625,8867f29ae3275d2fa6448cd35bf2e705e7cfcd6b,"The eye-tracking method was used to assess attentional orienting to and engagement on emotional visual scenes. In Experiment 1, unpleasant, neutral, or pleasant target pictures were presented simultaneously with neutral control pictures in peripheral vision under instruction to compare pleasantness of the pictures. The probability of first fixating an emotional picture, and the frequency of subsequent fixations, were greater than those for neutral pictures. In Experiment 2, participants were instructed to avoid looking at the emotional pictures, but these were still more likely to be fixated first and gazed longer during the first-pass viewing than neutral pictures. Low-level visual features cannot explain the results. It is concluded that overt visual attention is captured by both unpleasant and pleasant emotional content.","[{'authorId': '2036051', 'name': 'L. Nummenmaa'}, {'authorId': '2464572', 'name': 'J. Hyönä'}, {'authorId': '144866673', 'name': 'M. Calvo'}]",413.0,"{'bibtex': '@Article{Nummenmaa2006EyeMA,\n author = {L. Nummenmaa and J. Hyönä and M. Calvo},\n journal = {Emotion},\n pages = {\n          257-68\n        },\n title = {Eye movement assessment of selective attentional capture by emotional pictures.},\n volume = {6 2},\n year = {2006}\n}\n'}",,"{'volume': '6 2', 'pages': '\n          257-68\n        ', 'name': 'Emotion'}",49.0,Eye movement assessment of selective attentional capture by emotional pictures.,2006.0
1626,889ce7b8ee0e11d8729e0bd010eaa60275553b46,,"[{'authorId': '39910563', 'name': 'Chris Ingraham'}]",38.0,"{'bibtex': '@Article{Ingraham2023ToAT,\n author = {Chris Ingraham},\n journal = {Capacious: Journal of Emerging Affect Inquiry},\n title = {To Affect Theory},\n year = {2023}\n}\n'}",,{'name': 'Capacious: Journal of Emerging Affect Inquiry'},20.0,To Affect Theory,2023.0
1627,88b05cd40dd40ab4cfc9b33bbff106710fb98605,,"[{'authorId': '2125976962', 'name': 'Wu He'}, {'authorId': '1421235202', 'name': 'Jim X. Chen'}, {'authorId': '2154840326', 'name': 'Weihua Zhang'}]",4.0,"{'bibtex': '@Article{He2016CrowdSU,\n author = {Wu He and Jim X. Chen and Weihua Zhang},\n journal = {Multimedia Tools and Applications},\n pages = {5981-5998},\n title = {Crowd simulation using DC model and density information},\n volume = {75},\n year = {2016}\n}\n'}",,"{'volume': '75', 'pages': '5981-5998', 'name': 'Multimedia Tools and Applications'}",36.0,Crowd simulation using DC model and density information,2016.0
1628,88daafe9bffb7257d7a9675b72dccb06ed634831,"Highly believable anthropomorphic agents endanger electronic consumers. Because of concerning tendencies in human-agent interaction arising from agents’ anthropomorphic qualities, consumers may unwittingly treat agents as competent, trustworthy, living counterparts. This paper concludes that developers must focus agent design on consumer welfare, not technical virtuosity, if legal and ethical perils are to be avoided.","[{'authorId': '145950072', 'name': 'Carey Heckman'}, {'authorId': '1796045', 'name': 'J. Wobbrock'}]",43.0,"{'bibtex': '@Inproceedings{Heckman2000PutYB,\n author = {Carey Heckman and J. Wobbrock},\n pages = {435-442},\n title = {Put your best face forward: anthropomorphic agents, e-commerce consumers, and the law},\n year = {2000}\n}\n'}",,{'pages': '435-442'},31.0,"Put your best face forward: anthropomorphic agents, e-commerce consumers, and the law",2000.0
1629,88eb68c5181577c484526aa553b60e77460008e1,,"[{'authorId': '2116782180', 'name': 'Yutaka Suzuki'}, {'authorId': '47824497', 'name': 'Lisa Galli'}, {'authorId': '47665056', 'name': 'Ayaka Ikeda'}, {'authorId': '3003945', 'name': 'S. Itakura'}, {'authorId': '1725699', 'name': 'M. Kitazaki'}]",117.0,"{'bibtex': '@Article{Suzuki2015MeasuringEF,\n author = {Yutaka Suzuki and Lisa Galli and Ayaka Ikeda and S. Itakura and M. Kitazaki},\n journal = {Scientific Reports},\n title = {Measuring empathy for human and robot hand pain using electroencephalography},\n volume = {5},\n year = {2015}\n}\n'}",,"{'volume': '5', 'name': 'Scientific Reports'}",51.0,Measuring empathy for human and robot hand pain using electroencephalography,2015.0
1630,88eda1b2fd61036c2ca682d4abc34e700ec8ba7e,"In this paper we explore the factors and methodologies from a range of disciplines used to investigate trust in human-robot interaction (HRI). Our investigation highlights a growing field, which recognises the importance of understanding the deployment of robots in real-world settings, but where a lack of common definitions and experimental clarity impedes the development of a comprehensive framework for 
investigation. As a result, we propose a bottom-up approach that emphasises context and user perspective as the foundation for future investigations into trust in HRI.","[{'authorId': '145971457', 'name': 'David Cameron'}, {'authorId': '40035385', 'name': 'J. Aitken'}, {'authorId': '2082714', 'name': 'Emily C. Collins'}, {'authorId': '2147696', 'name': 'L. Boorman'}, {'authorId': '35321396', 'name': 'A. Chua'}, {'authorId': '2164679', 'name': 'Samuel Fernando'}, {'authorId': '2226533', 'name': 'O. McAree'}, {'authorId': '2071395783', 'name': 'U. M. Hernandez'}, {'authorId': '145887557', 'name': 'J. Law'}]",48.0,"{'bibtex': '@Inproceedings{Cameron2015FramingFT,\n author = {David Cameron and J. Aitken and Emily C. Collins and L. Boorman and A. Chua and Samuel Fernando and O. McAree and U. M. Hernandez and J. Law},\n title = {Framing Factors: The Importance of Context and the Individual in Understanding Trust in Human-Robot Interaction},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",40.0,Framing Factors: The Importance of Context and the Individual in Understanding Trust in Human-Robot Interaction,2015.0
1631,88f08edabf757803c4b47b072bce56c6bdabb269,"Contingencies between objects and people can be mechanical or intentional-social in nature. In this fMRI study we used simplified stimuli to investigate brain regions involved in the detection of mechanical and intentional contingencies. Using a factorial design we manipulated the 'animacy' and 'contingency' of stimulus movement, and the subject's attention to the contingencies. The detection of mechanical contingency between shapes whose movement was inanimate engaged the middle temporal gyrus and right intraparietal sulcus. The detection of intentional contingency between shapes whose movement was animate activated superior parietal networks bilaterally. These activations were unaffected by attention to contingency. Additional regions, the right middle frontal gyrus and left superior temporal sulcus, became activated by the animate-contingent stimuli when subjects specifically attended to the contingent nature of the stimuli. Our results help to clarify neural networks previously associated with 'theory of mind' and agency detection. In particular, the results suggest that low-level perception of agency in terms of objects reacting to other objects at a distance is processed by parietal networks. In contrast, the activation of brain regions traditionally associated with theory of mind tasks appears to require attention to be directed towards agency and contingency.","[{'authorId': '2255585142', 'name': 'Sarah-Jayne Blakemore'}, {'authorId': '2255466533', 'name': 'Pascal Boyer'}, {'authorId': '1402207980', 'name': 'M. Pachot-Clouard'}, {'authorId': '2255611758', 'name': 'Andrew N. Meltzoff'}, {'authorId': '2255341971', 'name': 'Christoph Segebarth'}, {'authorId': '2243749813', 'name': 'Jean Decety'}]",245.0,"{'bibtex': '@Article{Blakemore2003TheDO,\n author = {Sarah-Jayne Blakemore and Pascal Boyer and M. Pachot-Clouard and Andrew N. Meltzoff and Christoph Segebarth and Jean Decety},\n journal = {Cerebral cortex},\n pages = {\n          837-44\n        },\n title = {The detection of contingency and animacy from simple animations in the human brain.},\n volume = {13 8},\n year = {2003}\n}\n'}",,"{'volume': '13 8', 'pages': '\n          837-44\n        ', 'name': 'Cerebral cortex'}",34.0,The detection of contingency and animacy from simple animations in the human brain.,2003.0
1632,8904946785a548a9fc0f86aac452c787c409cf7c,"This study examined the patterns of avoidance and recognition in pedestrians as they passed a confederate. Specifically, the effects of condition (avoid, look, and look plus smile) and sex of confederate on passing pedestrians were examined in a field study on over 600 participants. A log-linear analysis of the results showed support for the hypotheses of greater glancing toward the female confederates and greater glancing when the confederates looked and smiled. A Sex of Confederate x Condition interaction qualified these main effects, however, with the female confederates receiving a much higher proportion of glances than men in the look-only condition. Analyses of additional pedestrian responses among those who did glance at the confederates indicated that the look and smile condition produced higher levels of smiling, nodding, and greetings than did the other two conditions. The apparent processes underlying these subtle, brief exchanges are discussed, and the ecology of passing encounters is considered.","[{'authorId': '2521228', 'name': 'M. Patterson'}, {'authorId': '104571806', 'name': 'Anthony Webb'}, {'authorId': '50859999', 'name': 'Warren E. Schwartz'}]",30.0,"{'bibtex': '@Article{Patterson2002PassingEP,\n author = {M. Patterson and Anthony Webb and Warren E. Schwartz},\n journal = {Basic and Applied Social Psychology},\n pages = {57 - 66},\n title = {Passing Encounters: Patterns of Recognition and Avoidance in Pedestrians},\n volume = {24},\n year = {2002}\n}\n'}",,"{'volume': '24', 'pages': '57 - 66', 'name': 'Basic and Applied Social Psychology'}",20.0,Passing Encounters: Patterns of Recognition and Avoidance in Pedestrians,2002.0
1633,890b82514abb9be893fd6054d3520956449b612e,"Foreword Introduction The study of spontaneous facial expression in psychology I: BASIC RESEARCH ON EMOTION 1. Is the startle reaction an emotion? 2. The asymmetry of facial actions is inconsisten with models of hemispheric specialization 3. Coherence between expressive and experiential systmes in emotion 4. Will the real relationship between facial expression and affective experience please stand up: The case of exhilaration 5. Extraversion, alcohol, and enjoyment 6. Signs of appeasement: Evidence for the distinct displays of embarrassment, amusement, and shame 7. Genuine, suppressed, and faked facial behavior during exacerbation of chronic low back pain 8. The consistency of facial expressions of pain: a comparison across modalities 9. Smiles when lying 10. Behavioral markers and recognizability of the smile of enjoyment 11. Components and recognition of facial expression in the communication of emotion by actors 12. Differentiating emotion elicited and deliberate emotional facial expressions 13. Japanese and American infants' responses to arm restraint 14. Differential facial responses to four basic tests in newborns II: APPLIED RESEARCH 15. Facial expressions in affective disorders 16. Emotional experience and epxression in schizophrenia and depression 17. Interaction regulations used by schizophrenic and psychosomatic patients: Studies on facial behavior in dyadic interactions 18. Nonverbal expression of psychological states in psychiatric patients 19. Depression and suicide faces 20. Prototypical affective microsequences in psychotherapeutic interaction 21. Facial expressions of emotion and psychopathology in adolescent boys 22. Type A behavior pattern: Facial behavior and speech components Conclusion What we have learned by measuring the face Index","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '4935859', 'name': 'E. Rosenberg'}]",1971.0,"{'bibtex': '@Inproceedings{Ekman2005WhatTF,\n author = {P. Ekman and E. Rosenberg},\n title = {What the face reveals : basic and applied studies of spontaneous expression using the facial action coding system (FACS)},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,What the face reveals : basic and applied studies of spontaneous expression using the facial action coding system (FACS),2005.0
1636,894d95d3a34647b2930dc38bbd0cfdd5b61f896c,"The power asymmetry hypothesis claims that individuals should have distinct signals of appeasement/affiliation and play when status difference is high, whereas these signals should overlap in egalitarian interactions. Naturalistic observations were conducted on humans interacting in groups that differed in terms of age composition (and presumably social status). Three affiliative behaviours were recorded by focal sampling: spontaneous smiles, deliberate smiles and laughter. Interestingly, young men showed significantly higher proportions of deliberate smiles in comparison to laughter when interacting with people of a different age class than when interacting in same-age groups. The pattern of affiliative behaviours in women remained unaffected by the age composition of groups. This partly supports the power asymmetry hypothesis and suggests that in men, deliberate smiles could play a role in the regulation of hierarchical relationships.","[{'authorId': '48898927', 'name': 'M. Méhu'}, {'authorId': '144610599', 'name': 'Robin I. M. Dunbar'}]",60.0,"{'bibtex': '@Article{Méhu2008RelationshipBS,\n author = {M. Méhu and Robin I. M. Dunbar},\n journal = {Folia Primatologica},\n pages = {269 - 280},\n title = {Relationship between Smiling and Laughter in Humans (Homo sapiens): Testing the Power Asymmetry Hypothesis},\n volume = {79},\n year = {2008}\n}\n'}",,"{'volume': '79', 'pages': '269 - 280', 'name': 'Folia Primatologica'}",57.0,Relationship between Smiling and Laughter in Humans (Homo sapiens): Testing the Power Asymmetry Hypothesis,2008.0
1637,89583fc4293d6b7bc3b8460833df7e90440d27ab,": Artificial entities, such as virtual agents, have become more pervasive. Their long-term presence among humans requires the virtual agent’s ability to express appropriate emotions to elicit the necessary empathy from the users. Affective empathy involves behavioral mimicry, a synchronized co-movement between dyadic pairs. However, the characteristics of such synchrony between humans and virtual agents remain unclear in empathic interactions. Our study evaluates the participant’s behavioral synchronization when a virtual agent exhibits an emotional expression congruent with the emotional context through facial expressions, behavioral gestures, and voice. Participants viewed an emotion-eliciting video stimulus (negative or positive) with a virtual agent. The participants then conversed with the virtual agent about the video, such as how the participant felt about the content. The virtual agent expressed emotions congruent with the video or neutral emotion during the dialog. The participants’ facial expressions, such as the facial expressive intensity and facial muscle movement, were measured during the dialog using a camera. The results showed the participants’ significant behavioral synchronization (i.e., cosine similarity ≥ .05) in both the negative and positive emotion conditions, evident in the participant’s facial mimicry with the virtual agent. Additionally, the participants’ facial expressions, both movement and intensity, were significantly stronger in the emotional virtual agent than in the neutral virtual agent. In particular, we found that the facial muscle intensity of AU45 (Blink) is an effective index to assess the participant’s synchronization that differs by the individual’s empathic capability (low, mid, high). Based on the results, we suggest an appraisal criterion to provide empirical conditions to validate empathic interaction based on the facial expression measures.","[{'authorId': '2007043751', 'name': 'Sung Park'}, {'authorId': '2149246721', 'name': 'Seongeon Park'}, {'authorId': '38593989', 'name': 'Mincheol Whang'}]",0.0,"{'bibtex': '@Article{Park2022EmpathicRO,\n author = {Sung Park and Seongeon Park and Mincheol Whang},\n booktitle = {Computers Materials & Continua},\n journal = {Computers, Materials & Continua},\n title = {Empathic Responses of Behavioral-Synchronization in Human-Agent Interaction},\n year = {2022}\n}\n'}",[],"{'name': 'Computers, Materials & Continua'}",78.0,Empathic Responses of Behavioral-Synchronization in Human-Agent Interaction,2022.0
1638,8958ae7e240a6ce932c44ce5b67a14047fe90702,"Background Persecutory delusions may be unfounded threat beliefs maintained by safety-seeking behaviours that prevent disconfirmatory evidence being successfully processed. Use of virtual reality could facilitate new learning. Aims To test the hypothesis that enabling patients to test the threat predictions of persecutory delusions in virtual reality social environments with the dropping of safety-seeking behaviours (virtual reality cognitive therapy) would lead to greater delusion reduction than exposure alone (virtual reality exposure). Method Conviction in delusions and distress in a real-world situation were assessed in 30 patients with persecutory delusions. Patients were then randomised to virtual reality cognitive therapy or virtual reality exposure, both with 30 min in graded virtual reality social environments. Delusion conviction and real-world distress were then reassessed. Results In comparison with exposure, virtual reality cognitive therapy led to large reductions in delusional conviction (reduction 22.0%, P = 0.024, Cohen's d = 1.3) and real-world distress (reduction 19.6%, P = 0.020, Cohen's d = 0.8). Conclusion Cognitive therapy using virtual reality could prove highly effective in treating delusions.","[{'authorId': '145331574', 'name': 'D. Freeman'}, {'authorId': '2086592052', 'name': 'Jonathan Bradley'}, {'authorId': '1705895', 'name': 'Angus Antley'}, {'authorId': '48176898', 'name': 'Emilie Bourke'}, {'authorId': '3558924', 'name': 'Natalie DeWeever'}, {'authorId': '46682899', 'name': 'Nicole Evans'}, {'authorId': '5152415', 'name': 'Emma Černis'}, {'authorId': '1883625', 'name': 'Bryony Sheaves'}, {'authorId': '144328468', 'name': 'Felicity Waite'}, {'authorId': '145301048', 'name': 'G. Dunn'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '144809882', 'name': 'D. Clark'}]",189.0,"{'bibtex': '@Article{Freeman2016VirtualRI,\n author = {D. Freeman and Jonathan Bradley and Angus Antley and Emilie Bourke and Natalie DeWeever and Nicole Evans and Emma Černis and Bryony Sheaves and Felicity Waite and G. Dunn and M. Slater and D. Clark},\n journal = {The British Journal of Psychiatry},\n pages = {62 - 67},\n title = {Virtual reality in the treatment of persecutory delusions: randomised controlled experimental study testing how to reduce delusional conviction},\n volume = {209},\n year = {2016}\n}\n'}",,"{'volume': '209', 'pages': '62 - 67', 'name': 'The British Journal of Psychiatry'}",24.0,Virtual reality in the treatment of persecutory delusions: randomised controlled experimental study testing how to reduce delusional conviction,2016.0
1640,89624aae669fecb85fa7fb69a7e00d55c27428ff,,"[{'authorId': '39944436', 'name': 'Miya R. Asato'}, {'authorId': '143606531', 'name': 'J. Sweeney'}, {'authorId': '143953194', 'name': 'B. Luna'}]",145.0,"{'bibtex': '@Article{Asato2006CognitivePI,\n author = {Miya R. Asato and J. Sweeney and B. Luna},\n journal = {Neuropsychologia},\n pages = {2259-2269},\n title = {Cognitive processes in the development of TOL performance},\n volume = {44},\n year = {2006}\n}\n'}",,"{'volume': '44', 'pages': '2259-2269', 'name': 'Neuropsychologia'}",70.0,Cognitive processes in the development of TOL performance,2006.0
1641,897fbf837c26f54bfbe9c0c0abb788cda3b9e14a,,"[{'authorId': '1786701', 'name': 'M. Hertzum'}, {'authorId': '1699653', 'name': 'Hans H. K. Andersen'}, {'authorId': '143924209', 'name': 'V. Andersen'}, {'authorId': '2052328887', 'name': 'C. B. Hansen'}]",164.0,"{'bibtex': '@Article{Hertzum2002TrustII,\n author = {M. Hertzum and Hans H. K. Andersen and V. Andersen and C. B. Hansen},\n journal = {Interact. Comput.},\n pages = {575-599},\n title = {Trust in information sources: seeking information from people, documents, and virtual agents},\n volume = {14},\n year = {2002}\n}\n'}",,"{'volume': '14', 'pages': '575-599', 'name': 'Interact. Comput.'}",54.0,"Trust in information sources: seeking information from people, documents, and virtual agents",2002.0
1643,89b610b08200429b85e3bd69cbb86bb86ed1d222,,"[{'authorId': '38497468', 'name': 'Moran Cerf'}, {'authorId': '2598615', 'name': 'Nikhil R. Thiruvengadam'}, {'authorId': '1908459', 'name': 'F. Mormann'}, {'authorId': '2120469', 'name': 'A. Kraskov'}, {'authorId': '144936828', 'name': 'R. Quiroga'}, {'authorId': '145624227', 'name': 'C. Koch'}, {'authorId': '1774949', 'name': 'I. Fried'}]",165.0,"{'bibtex': '@Article{Cerf2010OnlineVC,\n author = {Moran Cerf and Nikhil R. Thiruvengadam and F. Mormann and A. Kraskov and R. Quiroga and C. Koch and I. Fried},\n journal = {Nature},\n pages = {1104-1108},\n title = {On-line, voluntary control of human temporal lobe neurons},\n volume = {467},\n year = {2010}\n}\n'}",,"{'volume': '467', 'pages': '1104-1108', 'name': 'Nature'}",25.0,"On-line, voluntary control of human temporal lobe neurons",2010.0
1644,89c8dc163149de20e9518c7ce155f4a11b678d07,"Human observers readily recognize emotions expressed in body movement. Their perceptual judgments are based on simple movement features, such as overall speed, but also on more intricate posture and dynamic cues. The systematic analysis of such features is complicated due to the difficulty of considering the large number of potentially relevant kinematic and dynamic parameters. To identify emotion-specific features we motion-captured the neutral and emotionally expressive (anger, happiness, sadness, fear) gaits of 25 individuals. Body posture was characterized by average flexion angles, and a low-dimensional parameterization of the spatio-temporal structure of joint trajectories was obtained by approximation with a nonlinear mixture model. Applying sparse regression, we extracted critical emotion-specific posture and movement features, which typically depended only on a small number of joints. The features we extracted from the motor behavior closely resembled features that were critical for the perception of emotion from gait, determined by a statistical analysis of classification and rating judgments of 21 observers presented with avatars animated with the recorded movements. The perceptual relevance of these features was further supported by another experiment showing that artificial walkers containing only the critical features induced high-level after-effects matching those induced by adaptation with natural emotional walkers.","[{'authorId': '5273793', 'name': 'Claire L. Roether'}, {'authorId': '1682667', 'name': 'Lars Omlor'}, {'authorId': '33424509', 'name': 'A. Christensen'}, {'authorId': '33365402', 'name': 'M. Giese'}]",344.0,"{'bibtex': '@Article{Roether2009CriticalFF,\n author = {Claire L. Roether and Lars Omlor and A. Christensen and M. Giese},\n journal = {Journal of vision},\n pages = {\n          15.1-32\n        },\n title = {Critical features for the perception of emotion from gait.},\n volume = {9 6},\n year = {2009}\n}\n'}",,"{'volume': '9 6', 'pages': '\n          15.1-32\n        ', 'name': 'Journal of vision'}",122.0,Critical features for the perception of emotion from gait.,2009.0
1645,89cc72f63dfce6c1374515e050f788812895a4bb,"Massively multiplayer online computer games are played in complex, persistent virtual worlds. Over time, the landscape of these worlds evolves and changes as players create and personalise their own virtual property. In contrast, many non-player characters that populate virtual game worlds possess a fixed set of pre-programmed behaviours and lack the ability to adapt and evolve in time with their surroundings. This paper presents motivated reinforcement learning agents as a means of creating non-player characters that can both evolve and adapt. Motivated reinforcement learning agents explore their environment and learn new behaviours in response to interesting experiences, allowing them to display progressively evolving behavioural patterns. In dynamic worlds, environmental changes provide an additional source of interesting experiences triggering further learning and allowing the agents to adapt their existing behavioural patterns in time with their surroundings.","[{'authorId': '1758755', 'name': 'K. Merrick'}, {'authorId': '145345893', 'name': 'M. Maher'}]",66.0,"{'bibtex': '@Inproceedings{Merrick2006MotivatedRL,\n author = {K. Merrick and M. Maher},\n pages = {3},\n title = {Motivated reinforcement learning for non-player characters in persistent computer game worlds},\n year = {2006}\n}\n'}",,{'pages': '3'},21.0,Motivated reinforcement learning for non-player characters in persistent computer game worlds,2006.0
1646,8a3097249401d91b5cc59eb5ea9e9e631e24fd69,"저자 Louis Cozolino가 ‘Building and Rebuilding the Human Brain’이라는 부제를 가지고 처음 이 책을 출간했을 때, 많은 이들이 신경과학의 시선으로 정신치료를 설명하는 이 책의 등장에 많은 찬사를 보냈다. 사실 처음 신경과학이 대 두될 때 극단적인 환원주의들은 저마다 설익은 담론을 쏟아 내기 시작했고, 이에 대해 우려하는 사람들은 ‘어떤 사람이 무언가를 보고 있다고 해서 대뇌 후두엽이 무언가를 보고 있 다고 말하는 것은 웃기는 일’이라며 반박하기도 했다. 그도 그럴 것이, 초창기에는 신경과학과 심리학 사이에는 다소간 에 이분법적인 대립이 있었고 서로에 대한 몰이해 속에서 그 저 자신의 학문이 더 우월하다는 힘겨루기의 분위기가 있었 기 때문이리라 짐작해본다. 하지만 이 책에서 저자는 심리학 자로서의 자신의 다년간의 경험을 잘 녹여내어 신경과학과 정신치료의 틈을 좁히려는 노력을 시도하고 있다. 이번에 새 로 나온 그의 개정증보판은 ‘Healing the Social Brain’이라는 부제를 가지고 있다. 정신치료자이자 정신과 의사로서 환자 를 대하는 입장에서, 이 책에서 어떠한 내용을 소개하고 있고 어떠한 의미가 있는지를 살펴보고자 한다.","[{'authorId': '2152473740', 'name': 'Tae Young Lee'}]",153.0,"{'bibtex': '@Inproceedings{Lee2014TheNO,\n author = {Tae Young Lee},\n pages = {83-86},\n title = {The Neuroscience of Psychotherapy: Healing the Social Brain},\n volume = {25},\n year = {2014}\n}\n'}",,"{'volume': '25', 'pages': '83-86', 'name': ''}",0.0,The Neuroscience of Psychotherapy: Healing the Social Brain,2014.0
1647,8a388f43b366c4366d3d402f610b92c5422b3aff,"Purpose 
 
 
 
 
The paper aims to explain the limitations of existing cognitive architectures and affective models, and propose a new cognitive-affective architecture that can be integrated in real intelligent agents to make them more realistic and believable. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The paper evaluates the state of the art, and describes the design and implementation of the cognitive-affective architecture in an agent. A brief evaluation of the agent is provided. 
 
 
 
 
Findings 
 
 
 
 
The paper clearly states that it is possible to use cognitive architectures to help, but there is a lack of architectures that address the problem of combining cognition and emotion in agents in a unified, simplified way. A cognitive-affective architecture is useful to make believable intelligent agents in an easier way. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The paper does not explore a lot of possible future work that can be done to extend the emotional expressions of the agent, as well as including direct emotional-sensing capabilities in real time. 
 
 
 
 
Practical implications 
 
 
 
 
The paper argues about the need to include cognitive-affective architectures in modern intelligent agents. The architecture allows to influence and modify the behavior of the agent in real time, to achieve a more realistic and believable interaction with the user. 
 
 
 
 
Social implications 
 
 
 
 
The paper remarks the importance of a cognitive-affective architecture that makes intelligent agents able to help the users in different tasks and environments. 
 
 
 
 
Originality/value 
 
 
 
 
The paper describes a new cognitive-affective architecture and its utility for modern intelligent agents. This is proven by including it in a previous agent, which boosts its behavior and emotional expression possibilities and thus improves user experience.","[{'authorId': '20950778', 'name': 'J. P. Marco'}, {'authorId': '2123508', 'name': 'F. Serón'}, {'authorId': '2082630073', 'name': 'Eva Cerezo Bagdasari'}]",5.0,"{'bibtex': '@Article{Marco2017CombiningCA,\n author = {J. P. Marco and F. Serón and Eva Cerezo Bagdasari},\n booktitle = {Kybernetes},\n journal = {Kybernetes},\n pages = {933-946},\n title = {Combining cognition and emotion in virtual agents},\n volume = {46},\n year = {2017}\n}\n'}","[{'paperId': 'c06f2d6deab517b2dafc51130ac28d8a068bcad3', 'title': 'Arquitetura Pedagógica para Novos Objetos Digitais de Aprendizagem visando a Computação Afetiva'}, {'paperId': '2fdb2aaaeb0e11a36715c8cc4ee41f90478aeb95', 'title': 'Towards the construction of computational models of emotions from the perspective of a software system'}, {'paperId': 'a94d2b825acc0deeba7157128262d7adbf2916f7', 'title': 'Knowledge dynamics: a thermodynamics approach'}, {'paperId': '985c4d7d51a5dc0286fea81dfc12d435df8b4041', 'title': 'The Theory of Knowledge Fields: A Thermodynamics Approach'}, {'paperId': '17a760676293cf740da2e5d2ee09df35b39ee3ba', 'title': 'Identifying contemplative intensity in cognitive architectures for virtual-agent minds'}]","{'name': 'Kybernetes', 'pages': '933-946', 'volume': '46'}",47.0,Combining cognition and emotion in virtual agents,2017.0
1648,8a40b6c75dd6392ee0d3af73cdfc46f59337efa9,"In this paper, we report our experiments on feature-based facial expression recognition within an architecture based on a two-layer perceptron. We investigate the use of two types of features extracted from face images: the geometric positions of a set of fiducial points on a face, and a set of multiscale and multiorientation Gabor wavelet coefficients at these points. They can be used either independently or jointly. The recognition performance with different types of features has been compared, which shows that Gabor wavelet coefficients are much more powerful than geometric positions. Furthermore, since the first layer of the perceptron actually performs a nonlinear reduction of the dimensionality of the feature space, we have also studied the desired number of hidden units, i.e. the appropriate dimension to represent a facial expression in order to achieve a good recognition rate. It turns out that five to seven hidden units are probably enough to represent the space of facial expressions. Then, we have investigated the importance of each individual fiducial point to facial expression recognition. Sensitivity analysis reveals that points on cheeks and on forehead carry little useful information. After discarding them, not only the computational efficiency increases, but also the generalization performance slightly improves. Finally, we have studied the significance of image scales. Experiments show that facial expression recognition is mainly a low frequency process, and a spatial resolution of 64 pixels × 64 pixels is probably enough.","[{'authorId': '51064498', 'name': 'Zhengyou Zhang'}]",173.0,"{'bibtex': '@Article{Zhang1999FeatureBasedFE,\n author = {Zhengyou Zhang},\n journal = {Int. J. Pattern Recognit. Artif. Intell.},\n pages = {893-911},\n title = {Feature-Based Facial Expression Recognition: Sensitivity Analysis and Experiments with A Multilayer Perceptron},\n volume = {13},\n year = {1999}\n}\n'}",,"{'volume': '13', 'pages': '893-911', 'name': 'Int. J. Pattern Recognit. Artif. Intell.'}",29.0,Feature-Based Facial Expression Recognition: Sensitivity Analysis and Experiments with A Multilayer Perceptron,1999.0
1649,8a4d5c488a6d462a170c9f57df049dcf1f19928c,"Expressive facial animation synthesis of human like characters has had many approaches with good results. MPEG‐4 standard has functioned as the basis of many of those approaches. In this paper we would like to lay out the knowledge of some of those approaches inside an ontology in order to support the modeling of emotional facial animation in virtual humans (VH). Inside this ontology we will present MPEG‐4 facial animation concepts and its relationship with emotion through expression profiles that utilize psychological models of emotions. The ontology allows storing, indexing and retrieving prerecorded synthetic facial animations that can express a given emotion. Also this ontology can be used a refined knowledge base in regards to the emotional facial animation creation. This ontology is made using Web Ontology Language and the results are presented as answered queries. Copyright © 2006 John Wiley & Sons, Ltd.","[{'authorId': '66173005', 'name': 'Alejandra García-Rojas'}, {'authorId': '1699529', 'name': 'F. Vexo'}, {'authorId': '2223622395', 'name': 'D. Thalmann'}, {'authorId': '3346592', 'name': 'A. Raouzaiou'}, {'authorId': '1715144', 'name': 'K. Karpouzis'}, {'authorId': '1707243', 'name': 'S. Kollias'}, {'authorId': '2437352', 'name': 'L. Moccozet'}, {'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}]",40.0,"{'bibtex': '@Article{García-Rojas2006EmotionalFE,\n author = {Alejandra García-Rojas and F. Vexo and D. Thalmann and A. Raouzaiou and K. Karpouzis and S. Kollias and L. Moccozet and N. Magnenat-Thalmann},\n journal = {Computer Animation and Virtual Worlds},\n title = {Emotional face expression profiles supported by virtual human ontology},\n volume = {17},\n year = {2006}\n}\n'}",,"{'volume': '17', 'name': 'Computer Animation and Virtual Worlds'}",19.0,Emotional face expression profiles supported by virtual human ontology,2006.0
1650,8a4eb837dcebf0e761f026c7b267ce8c7b47a21e,"We present a new method, Social Groups and Navigation (SGN), to simulate the walking behavior of small pedestrian groups in virtual environments. SGN is the first method to simulate group behavior on both global and local levels. We define quantitative metrics to measure the coherence and the sociality of a group based on existing empirical data of real crowds. SGN is designed to let groups stay in coherent and social formations without explicitly modeling such formations. For groups of four, SGN generates between 13% and 53% more socially-friendly behavior than existing methods, measured over the lifetime of a group in the simulation. For groups of three, the gain is between 15% and 31%, and for groups of two, the gain is between 1% and 4%. SGN can be used with any existing global path planner and any existing path following method. Experiments show that SGN enables the simulation of thousands of agents in real time. Due to its flexible design, it can be easily integrated into a larger crowd simulation framework.","[{'authorId': '2099596', 'name': 'N. Jaklin'}, {'authorId': '2796536', 'name': 'A. Kremyzas'}, {'authorId': '1766759', 'name': 'Roland Geraerts'}]",16.0,"{'bibtex': '@Article{Jaklin2015AddingST,\n author = {N. Jaklin and A. Kremyzas and Roland Geraerts},\n journal = {Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology},\n title = {Adding sociality to virtual pedestrian groups},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology'},32.0,Adding sociality to virtual pedestrian groups,2015.0
1651,8a92831841307b513ca9c1b2c65499b08eb4c156,"
 
 Instagram is a relatively new form of communication where users can easily share their updates by taking photos and tweaking them using filters. It has seen rapid growth in the number of users as well as uploads since it was launched in October 2010. In spite of the fact that it is the most popular photo capturing and sharing application, it has attracted relatively less attention from the research community. In this paper, we present both qualitative and quantitative analysis on Instagram. We use computer vision techniques to examine the photo content. Based on that, we identify the different types of active users on Instagram using clustering. Our results reveal several insights about Instagram which were never studied before, that include: 1) Eight popular photos categories, 2) Five distinct types of Instagram users in terms of their posted photos, and 3) A user's audience (number of followers) is independent of his/her shared photos on Instagram. To our knowledge, this is the first in-depth study of content and users on Instagram.
 
","[{'authorId': '2731506', 'name': 'Yuheng Hu'}, {'authorId': '2630752', 'name': 'L. Manikonda'}, {'authorId': '1740315', 'name': 'S. Kambhampati'}]",688.0,"{'bibtex': '@Article{Hu2014WhatWI,\n author = {Yuheng Hu and L. Manikonda and S. Kambhampati},\n journal = {Proceedings of the International AAAI Conference on Web and Social Media},\n title = {What We Instagram: A First Analysis of Instagram Photo Content and User Types},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the International AAAI Conference on Web and Social Media'},10.0,What We Instagram: A First Analysis of Instagram Photo Content and User Types,2014.0
1652,8a93f26ccfff647e2da299f8870fbd1a2d9cb3cd,"Objective: To investigate the neuropsychological characteristics of patients diagnosed with mild cognitive impairment (MCI) with and without apathy. Methods: A cohort of 245 MCI patients (mean age = 72 ± 5.5 years; mean MMSE = 27.5 ± 1.3) was divided into two subgroups according to their Apathy Inventory score and underwent an extensive neuropsychological battery. Results: There were 94 (38.4%) patients with and 151 (61.6%) patients without apathy. At baseline the apathetic subgroup had a significantly lower total score on the free and cued selective reminding test (FCSR). Furthermore, the apathetic subgroup showed a significant deterioration in FCSR total recall score between baseline and the 1-year assessment. In conclusion, the presence of apathy in MCI patients is not associated with frontal task performance but with a higher degree of memory impairment.","[{'authorId': '2282672', 'name': 'P. Robert'}, {'authorId': '6158356', 'name': 'C. Berr'}, {'authorId': '5893688', 'name': 'M. Volteau'}, {'authorId': '2080746895', 'name': 'C. Bertogliati'}, {'authorId': '46866294', 'name': 'M. Benoit'}, {'authorId': '4310818', 'name': 'F. Mahieux'}, {'authorId': '6715113', 'name': 'S. Legrain'}, {'authorId': '145400912', 'name': 'B. Dubois'}]",79.0,"{'bibtex': '@Article{Robert2006NeuropsychologicalPI,\n author = {P. Robert and C. Berr and M. Volteau and C. Bertogliati and M. Benoit and F. Mahieux and S. Legrain and B. Dubois},\n journal = {Dementia and Geriatric Cognitive Disorders},\n pages = {192 - 197},\n title = {Neuropsychological Performance in Mild Cognitive Impairment with and without Apathy},\n volume = {21},\n year = {2006}\n}\n'}",,"{'volume': '21', 'pages': '192 - 197', 'name': 'Dementia and Geriatric Cognitive Disorders'}",32.0,Neuropsychological Performance in Mild Cognitive Impairment with and without Apathy,2006.0
1653,8ab11ccfddb7886c280725e46b89de1c14a47725,,"[{'authorId': '2117177936', 'name': 'Seungho Lee'}, {'authorId': '30441083', 'name': 'T. Adams'}, {'authorId': '2920923', 'name': 'B. Ryoo'}]",40.0,"{'bibtex': '@Article{Lee1997AFN,\n author = {Seungho Lee and T. Adams and B. Ryoo},\n journal = {Automation in Construction},\n pages = {97-107},\n title = {A fuzzy navigation system for mobile construction robots},\n volume = {6},\n year = {1997}\n}\n'}",,"{'volume': '6', 'pages': '97-107', 'name': 'Automation in Construction'}",14.0,A fuzzy navigation system for mobile construction robots,1997.0
1654,8abee3db6616015964d99c2aa7b59d617e46b582,,"[{'authorId': '1688528', 'name': 'Hsiu-Mei Huang'}, {'authorId': '48067182', 'name': 'Ulrich Rauch'}, {'authorId': '34874569', 'name': 'S. Liaw'}]",669.0,"{'bibtex': ""@Article{Huang2010InvestigatingLA,\n author = {Hsiu-Mei Huang and Ulrich Rauch and S. Liaw},\n journal = {Comput. Educ.},\n pages = {1171-1182},\n title = {Investigating learners' attitudes toward virtual reality learning environments: Based on a constructivist approach},\n volume = {55},\n year = {2010}\n}\n""}",,"{'volume': '55', 'pages': '1171-1182', 'name': 'Comput. Educ.'}",70.0,Investigating learners' attitudes toward virtual reality learning environments: Based on a constructivist approach,2010.0
1655,8ad0cdfe7bcca856bc2e12a26fbd8e736bb58164,"Surely since the Enlightenment, if not before, the study of mind has centered principally on how man achieves a ""true"" knowledge of the world. Emphasis in this pursuit has varied, of course: empiricists have concentrated on the mind's interplay with an external world of nature, hoping to find the key in the association of sensations and ideas, while rationalists have looked inward to the powers of mind itself for the principles of right reason. The objective, in either case, has been to discover how we achieve ""reality,"" that is to say, how we get a reliable fix on the world, a world that is, as it were, assumed to be immutable and, as it were, ""there to be observed."" This quest has, of course, had a profound effect on the development of psychology, and the empiricist and rationalist traditions have dominated our conceptions of how the mind grows and how it gets its grasp on the ""real world."" Indeed, at midcentury Gestalt theory represented the rationalist wing of this enterprise and American learning theory the empiricist. Both gave accounts of mental development as proceeding in some more or less linear and uniform fashion from an initial incompetence in grasping reality to a final competence, in one case attributing it to the working out of internal processes or mental organization, and in the other to some unspecified principle of reflection by which—whether through reinforcement, association, or conditioning—we came to respond to the world ""as it is."" There have always been dissidents who","[{'authorId': '1678200', 'name': 'J. Bruner'}]",4201.0,"{'bibtex': '@Article{Bruner1991TheNC,\n author = {J. Bruner},\n journal = {Critical Inquiry},\n pages = {1 - 21},\n title = {The Narrative Construction of Reality},\n volume = {18},\n year = {1991}\n}\n'}",,"{'volume': '18', 'pages': '1 - 21', 'name': 'Critical Inquiry'}",5.0,The Narrative Construction of Reality,1991.0
1656,8ad286cb66dd0151cdada2be4cdf3c3cc1f34056,"In the context of the Carezheimer project, the authors propose to personalize a narrative simulation to improve daily living abilities of people caring for relative with Alzheimer's disease. This personalization engages learners in a story offering adapted action possibilities. In this paper, after a brief description and definition of personalization in training simulations, we describe the dimensions of personalization for a narrative-based serious game and explain how these dimensions impact the dynamic generation of narrative.","[{'authorId': '50478764', 'name': 'Lucie A. Chauveau'}, {'authorId': '1691377', 'name': 'N. Szilas'}, {'authorId': '51111105', 'name': 'Anna Laura Luiu'}, {'authorId': '1720898', 'name': 'Frédéric Ehrler'}]",7.0,"{'bibtex': ""@Article{Chauveau2018DimensionsOP,\n author = {Lucie A. Chauveau and N. Szilas and Anna Laura Luiu and Frédéric Ehrler},\n journal = {2018 IEEE 6th International Conference on Serious Games and Applications for Health (SeGAH)},\n pages = {1-8},\n title = {Dimensions of personalization in a narrative pedagogical simulation for Alzheimer's caregivers},\n year = {2018}\n}\n""}",,"{'pages': '1-8', 'name': '2018 IEEE 6th International Conference on Serious Games and Applications for Health (SeGAH)'}",39.0,Dimensions of personalization in a narrative pedagogical simulation for Alzheimer's caregivers,2018.0
1657,8aedd5026b05dc0e87616d1306902812c1b7b5ab,"We are exploring the use of virtual reality for training people how to perform tasks, such as operating and maintaining complex equipment. This video describes Steve, an agent we are developing that assists in the training. Steve is an autonomous, animated agent that cohabits the virtual world with students. Steve continuously monitors the state of the virtual world, periodically manipulating it through virtual motor actions. His objective is to help students learn to perform physical, procedural tasks. He can demonstrate tasks, explaining his actions, as well as monitor students performing tasks, providing help when they need it. In addition to teaching students individual tasks, he can also help them learn to perform multi-person team tasks: he can serve as a tutor for a student learning a particular role in the team, and he can play the role of a teammate when a human teammate is unavailable. By integrating previous work in agent architectures, intelligent tutoring systems, and computer graphics, Steve illustrates a new breed of computer tutor: a human-like agent that can interact with students in a virtual world to help them learn.","[{'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '144995564', 'name': 'M. Rey'}]",32.0,"{'bibtex': '@Inproceedings{Rickel1998STEVES,\n author = {J. Rickel and M. Rey},\n pages = {332-333},\n title = {STEVE (video session): a pedagogical agent for virtual reality},\n year = {1998}\n}\n'}",,{'pages': '332-333'},8.0,STEVE (video session): a pedagogical agent for virtual reality,1998.0
1658,8af78f6f4fe2bdcd3c9d3bafc424b04a22d4e5ea,"Adaptive control of thought-rational (ACT-R; J. R. Anderson & C. Lebiere, 1998) has evolved into a theory that consists of multiple modules but also explains how these modules are integrated to produce coherent cognition. The perceptual-motor modules, the goal module, and the declarative memory module are presented as examples of specialized systems in ACT-R. These modules are associated with distinct cortical regions. These modules place chunks in buffers where they can be detected by a production system that responds to patterns of information in the buffers. At any point in time, a single production rule is selected to respond to the current pattern. Subsymbolic processes serve to guide the selection of rules to fire as well as the internal operations of some modules. Much of learning involves tuning of these subsymbolic processes. A number of simple and complex empirical examples are described to illustrate how these modules function singly and in concert.","[{'authorId': '2158597261', 'name': 'John R. Anderson'}, {'authorId': '3240261', 'name': 'Daniel Bothell'}, {'authorId': '1731836', 'name': 'M. Byrne'}, {'authorId': '2067498', 'name': 'Scott Douglass'}, {'authorId': '1749342', 'name': 'C. Lebiere'}, {'authorId': '39669695', 'name': 'Yulin Qin'}]",2953.0,"{'bibtex': '@Article{Anderson2004AnIT,\n author = {John R. Anderson and Daniel Bothell and M. Byrne and Scott Douglass and C. Lebiere and Yulin Qin},\n journal = {Psychological review},\n pages = {\n          1036-60\n        },\n title = {An integrated theory of the mind.},\n volume = {111 4},\n year = {2004}\n}\n'}",,"{'volume': '111 4', 'pages': '\n          1036-60\n        ', 'name': 'Psychological review'}",189.0,An integrated theory of the mind.,2004.0
1660,8b245ec3a85a96b6eacff3887b64cc8299dab80e,,"[{'authorId': '47397964', 'name': 'E. S. Knowles'}]",14.0,"{'bibtex': '@Article{Knowles1972BoundariesAS,\n author = {E. S. Knowles},\n journal = {Environment and Behavior},\n title = {Boundaries around social space: Dyadic responses to an invader.},\n year = {1972}\n}\n'}",,"{'volume': '', 'name': 'Environment and Behavior'}",0.0,Boundaries around social space: Dyadic responses to an invader.,1972.0
1661,8b28d45e96fd2f8fb2dd07f4518dda0b992ef30d,,"[{'authorId': '2242798922', 'name': 'K. Williams'}, {'authorId': '20719550', 'name': 'Blair J. Jarvis'}]",775.0,"{'bibtex': '@Article{Williams2006CyberballAP,\n author = {K. Williams and Blair J. Jarvis},\n journal = {Behavior Research Methods},\n pages = {174-180},\n title = {Cyberball: A program for use in research on interpersonal ostracism and acceptance},\n volume = {38},\n year = {2006}\n}\n'}",,"{'volume': '38', 'pages': '174-180', 'name': 'Behavior Research Methods'}",15.0,Cyberball: A program for use in research on interpersonal ostracism and acceptance,2006.0
1662,8b2a70839a3d68292aaa907b15b78c22cf20a579,,"[{'authorId': '30441535', 'name': 'Zixiang Fei'}, {'authorId': '48553748', 'name': 'Erfu Yang'}, {'authorId': '2145163577', 'name': 'Day-Uei Li'}, {'authorId': '2060677585', 'name': 'Stephen Butler'}, {'authorId': '2793317', 'name': 'W. Ijomah'}, {'authorId': '2109496899', 'name': 'Xia Li'}, {'authorId': '46544755', 'name': 'Huiyu Zhou'}]",83.0,"{'bibtex': '@Article{Fei2020DeepCN,\n author = {Zixiang Fei and Erfu Yang and Day-Uei Li and Stephen Butler and W. Ijomah and Xia Li and Huiyu Zhou},\n journal = {Neurocomputing},\n pages = {212-227},\n title = {Deep convolution network based emotion analysis towards mental health care},\n volume = {388},\n year = {2020}\n}\n'}",,"{'volume': '388', 'pages': '212-227', 'name': 'Neurocomputing'}",54.0,Deep convolution network based emotion analysis towards mental health care,2020.0
1663,8b2e9d7176144bcb9a0f1e3cf09fabf675085ed4,"In this mind-expanding book, scientific pioneer Marvin Minsky continues his groundbreaking research, offering a fascinating new model for how our minds work. He argues persuasively that emotions, intuitions, and feelings are not distinct things, but different ways of thinking.By examining these different forms of mind activity, Minsky says, we can explain why our thought sometimes takes the form of carefully reasoned analysis and at other times turns to emotion. He shows how our minds progress from simple, instinctive kinds of thought to more complex forms, such as consciousness or self-awareness. And he argues that because we tend to see our thinking as fragmented, we fail to appreciate what powerful thinkers we really are. Indeed, says Minsky, if thinking can be understood as the step-by-step process that it is, then we can build machines -- artificial intelligences -- that not only can assist with our thinking by thinking as we do but have the potential to be as conscious as we are.Eloquently written, The Emotion Machine is an intriguing look into a future where more powerful artificial intelligences await.","[{'authorId': '1847175', 'name': 'M. Minsky'}]",613.0,"{'bibtex': '@Inproceedings{Minsky2006TheEM,\n author = {M. Minsky},\n title = {The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",14.0,"The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind",2006.0
1664,8b3468284a883eff59a05bab957b2840a1ad96cc,,"[{'authorId': '2250619', 'name': 'S. Yoo'}, {'authorId': '1816334', 'name': 'N. Gujar'}, {'authorId': '2067770790', 'name': 'Peter Hu'}, {'authorId': '1603168041', 'name': 'F. Jolesz'}, {'authorId': '2341798', 'name': 'M. Walker'}]",1049.0,"{'bibtex': '@Article{Yoo2007TheHE,\n author = {S. Yoo and N. Gujar and Peter Hu and F. Jolesz and M. Walker},\n journal = {Current Biology},\n pages = {R877-R878},\n title = {The human emotional brain without sleep — a prefrontal amygdala disconnect},\n volume = {17},\n year = {2007}\n}\n'}",,"{'volume': '17', 'pages': 'R877-R878', 'name': 'Current Biology'}",9.0,The human emotional brain without sleep — a prefrontal amygdala disconnect,2007.0
1665,8b3fff6b4f2d21b217dc5cdbe7487cd7303e847a,"In an era in which robots take a part in our lives in daily living activities, humans have to trust robots in home environments. We aim to create guidelines that allow humans to trust robots to be able to look after their well-being by adopting human-like behaviours. We want to study a Human-Robot Interaction (HRI) to assess whether a certain degree of transparency in the robots actions, the use of social behaviours and natural communications can affect humans' sense of trust and companionship towards the robots. However, trust can change over time due to different factors, e.g. due to perceiving erroneous robot behaviors. We believe that the magnitude and the timing of the error during an interaction may have different impacts resulting in different scales of loss of trust and of restoring lost trust.","[{'authorId': '48369504', 'name': 'Alessandra Rossi'}, {'authorId': '1724361', 'name': 'K. Dautenhahn'}, {'authorId': '1749179', 'name': 'K. Koay'}, {'authorId': '49163673', 'name': 'J. Saunders'}]",38.0,"{'bibtex': '@Article{Rossi2017InvestigatingHP,\n author = {Alessandra Rossi and K. Dautenhahn and K. Koay and J. Saunders},\n journal = {Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},\n title = {Investigating Human Perceptions of Trust in Robots for Safe HRI in Home Environments},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction'},15.0,Investigating Human Perceptions of Trust in Robots for Safe HRI in Home Environments,2017.0
1666,8b43a0a905745e7c1ae24c10129b6c51e1c676d6,"In this article, we propose an approach of nonverbal interaction with virtual agents to control agents' behavioral expressivity by extracting and combining acoustic and gestural features. The goal for this approach is twofold, (i) expressing individual features like situated arousal and personal style and (ii) transmitting this information in an immersive 3D environment by suitable means.","[{'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '30169286', 'name': 'Thurid Vogt'}, {'authorId': '2754538', 'name': 'M. Wissner'}, {'authorId': '1790555', 'name': 'Nikolaus Bee'}]",8.0,"{'bibtex': '@Inproceedings{Rehm2008DancingTN,\n author = {M. Rehm and Thurid Vogt and M. Wissner and Nikolaus Bee},\n pages = {1249-1252},\n title = {Dancing the night away: controlling a virtual karaoke dancer by multimodal expressive cues},\n year = {2008}\n}\n'}",,{'pages': '1249-1252'},15.0,Dancing the night away: controlling a virtual karaoke dancer by multimodal expressive cues,2008.0
1667,8bb51f40236fd06406f22b31fcacb381539c3bf9,"The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. Theoretical formalizations of such agents and their implementations have proceeded in parallel with little or no connection between them. Tkis paper explores a particular type of rational agent, a BeliefDesire-Intention (BDI) agent. The primary aim of this paper is to integrate (a) the theoretical foundations of BDI agents from both a quantitative decision-theoretic perspective and a symbolic reasoning perspective; (b) the implementations of BDI agents from an ideal theoretical perspective and a more practical perspective; and (c) the building of large-scale applications based on BDI agents. In particular, an air-trafflc management application will be described from both a theoretical and an implementation perspective.","[{'authorId': '145946928', 'name': 'Anand Srinivasa Rao'}, {'authorId': '1694809', 'name': 'M. Georgeff'}]",3415.0,"{'bibtex': '@Inproceedings{Rao1995BDIAF,\n author = {Anand Srinivasa Rao and M. Georgeff},\n pages = {312-319},\n title = {BDI Agents: From Theory to Practice},\n year = {1995}\n}\n'}",,{'pages': '312-319'},40.0,BDI Agents: From Theory to Practice,1995.0
1670,8bcd1232bb971194e5a37cfbad696f393d9a0e2a,,"[{'authorId': '1684604', 'name': 'A. Mühlberger'}, {'authorId': '34439843', 'name': 'M. Wieser'}, {'authorId': '1709679', 'name': 'M. Herrmann'}, {'authorId': '152592651', 'name': 'P. Weyers'}, {'authorId': '29601294', 'name': 'Christian Tröger'}, {'authorId': '145825010', 'name': 'P. Pauli'}]",206.0,"{'bibtex': '@Article{Mühlberger2009EarlyCP,\n author = {A. Mühlberger and M. Wieser and M. Herrmann and P. Weyers and Christian Tröger and P. Pauli},\n journal = {Journal of Neural Transmission},\n pages = {735-746},\n title = {Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons},\n volume = {116},\n year = {2009}\n}\n'}",,"{'volume': '116', 'pages': '735-746', 'name': 'Journal of Neural Transmission'}",81.0,Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons,2009.0
1671,8c0dc29f2f97fb00ac8800f59036da005023dd30,"This study tested the binding hypothesis: that emotional reactions trigger binding mechanisms that link an emotional event to salient contextual features such as event location, a frequently recalled aspect of naturally occurring flashbulb memories. Our emotional events were taboo words in a Stroop color-naming task, and event location was manipulated by presenting the words in different task-irrelevant screen locations. Seventy-two participants named the font color of taboo and neutral words, with instructions to ignore word meaning; in one condition, several words were location consistent (i.e., always occupied the same screen location), whereas in another condition, several colors were location consistent. Then, in a surprise recognition memory test, participants recalled the locations of location-consistent words or colors. Although attention enhanced overall location memory for colors (the attended dimension during color naming), emotion (taboo vs. neutral words) enhanced location memory for words but not colors. These results support the binding hypothesis but contradict the hypothesis that emotional events induce imagelike memories more often than nonemotional events.","[{'authorId': '143917492', 'name': 'D. G. MacKay'}, {'authorId': '4069327', 'name': 'Marat V. Ahmetzanov'}]",164.0,"{'bibtex': '@Article{MacKay2005EmotionMA,\n author = {D. G. MacKay and Marat V. Ahmetzanov},\n journal = {Psychological Science},\n pages = {25 - 32},\n title = {Emotion, Memory, and Attention in the Taboo Stroop Paradigm},\n volume = {16},\n year = {2005}\n}\n'}",,"{'volume': '16', 'pages': '25 - 32', 'name': 'Psychological Science'}",36.0,"Emotion, Memory, and Attention in the Taboo Stroop Paradigm",2005.0
1672,8c1e46c901ae51434897138344c437ec6d7a55fc,"In this paper, an alternative training approach to the EEM-based training method is presented and a fuzzy reactive navigation architecture is described. The new training method is 270 times faster in learning speed; and is only 4% of the learning cost of the EEM method. It also has very reliable convergence of learning; very high number of learned rules (98.8%); and high adaptability. Using the rule base learned from the new method, the proposed fuzzy reactive navigator fuses the obstacle avoidance behaviour and goal seeking behaviour to determine its control actions, where adaptability is achieved with the aid of an environment evaluator. A comparison of this navigator using the rule bases obtained from the new training method and the EEM method, shows that the new navigator guarantees a solution and its solution is more acceptable.","[{'authorId': '2251398230', 'name': 'Nelson H C Yung'}, {'authorId': '1694177', 'name': 'C. Ye'}]",90.0,"{'bibtex': '@Article{Yung1999AnIM,\n author = {Nelson H C Yung and C. Ye},\n journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society},\n pages = {\n          314-21\n        },\n title = {An intelligent mobile vehicle navigator based on fuzzy logic and reinforcement learning},\n volume = {29 2},\n year = {1999}\n}\n'}",,"{'volume': '29 2', 'pages': '\n          314-21\n        ', 'name': 'IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society'}",20.0,An intelligent mobile vehicle navigator based on fuzzy logic and reinforcement learning,1999.0
1673,8c53674d18328f453e3131ad9229e1ab51d4e977,,"[{'authorId': '1450274569', 'name': 'Anne-Mei'}]",1.0,"{'bibtex': '@Article{Anne-Mei2016OudW,\n author = {Anne-Mei},\n journal = {TvV Tijdschrift voor Verzorgenden},\n pages = {35},\n title = {Oud worden},\n volume = {48},\n year = {2016}\n}\n'}",,"{'volume': '48', 'pages': '35', 'name': 'TvV Tijdschrift voor Verzorgenden'}",0.0,Oud worden,2016.0
1674,8c62297d4c299f201be3c6b877fe81f1bd4f5e3d,,"[{'authorId': '2060460', 'name': 'E. Blanchard'}, {'authorId': '1398931204', 'name': 'Jacqueline Jones-Alexander'}, {'authorId': '4751979', 'name': 'T. C. Buckley'}, {'authorId': '6574143', 'name': 'C. Forneris'}]",3849.0,"{'bibtex': '@Article{Blanchard1996PsychometricPO,\n author = {E. Blanchard and Jacqueline Jones-Alexander and T. C. Buckley and C. Forneris},\n journal = {Behaviour research and therapy},\n pages = {\n          669-73\n        },\n title = {Psychometric properties of the PTSD Checklist (PCL).},\n volume = {34 8},\n year = {1996}\n}\n'}",,"{'volume': '34 8', 'pages': '\n          669-73\n        ', 'name': 'Behaviour research and therapy'}",4.0,Psychometric properties of the PTSD Checklist (PCL).,1996.0
1675,8c783aaa01a756f50ea8c2fe6d13795398a6c043,,"[{'authorId': '2056813456', 'name': 'Li Gong'}, {'authorId': '3853032', 'name': 'J. Lai'}]",40.0,"{'bibtex': '@Article{Gong2003ToMO,\n author = {Li Gong and J. Lai},\n journal = {International Journal of Speech Technology},\n pages = {123-131},\n title = {To Mix or Not to Mix Synthetic Speech and Human Speech? Contrasting Impact on Judge-Rated Task Performance versus Self-Rated Performance and Attitudinal Responses},\n volume = {6},\n year = {2003}\n}\n'}",,"{'volume': '6', 'pages': '123-131', 'name': 'International Journal of Speech Technology'}",17.0,To Mix or Not to Mix Synthetic Speech and Human Speech? Contrasting Impact on Judge-Rated Task Performance versus Self-Rated Performance and Attitudinal Responses,2003.0
1676,8c81135d3faf02d0d5483fb4dc254695406646d9,,"[{'authorId': '1728894', 'name': 'C. Bartneck'}]",92.0,"{'bibtex': ""@Article{Bartneck2001HowCI,\n author = {C. Bartneck},\n journal = {User Modeling and User-Adapted Interaction},\n pages = {279-295},\n title = {How Convincing is Mr. Data's Smile: Affective Expressions of Machines},\n volume = {11},\n year = {2001}\n}\n""}",,"{'volume': '11', 'pages': '279-295', 'name': 'User Modeling and User-Adapted Interaction'}",39.0,How Convincing is Mr. Data's Smile: Affective Expressions of Machines,2001.0
1677,8cea6f07d18b8c2eae13a7127bb43848a4c41af6,,"[{'authorId': '50401747', 'name': 'Jonathan S. Blum'}, {'authorId': '2060176227', 'name': 'R. Booth'}]",543.0,"{'bibtex': ""@Inproceedings{Blum2005ThePD,\n author = {Jonathan S. Blum and R. Booth},\n title = {The Prisoner's Dilemma},\n year = {2005}\n}\n""}",,"{'volume': '', 'name': ''}",0.0,The Prisoner's Dilemma,2005.0
1678,8cf0841ca5b89f51fc48f5e5b3cd2d6af956f1ff,"We introduce and investigate by numerical simulations a number of models of emotional agents at the square lattice. Our models describe the most general features of emotions such as the spontaneous emotional arousal, emotional relaxation, and transfers of emotions between different agents. Group emotions in the considered models are periodically fluctuating between two opposite valency levels and as result the mean value of such group emotions is zero. The oscillations amplitude depends strongly on probability ps of the individual spontaneous arousal. For small values of relaxation times tau we observed a stochastic resonance, i.e. the signal to noise ratio SNR is maximal for a non-zero ps parameter. The amplitude increases with the probability p of local affective interactions while the mean oscillations period increases with the relaxation time tau and is only weakly dependent on other system parameters. Presence of emotional antenna can enhance positive or negative emotions and for the optimal transition probability the antenna can change agents emotions at longer distances. The stochastic resonance was also observed for the influence of emotions on task execution efficiency.","[{'authorId': '1708031', 'name': 'Agnieszka Czaplicka'}, {'authorId': '143788693', 'name': 'A. Chmiel'}, {'authorId': '35433346', 'name': 'J. Hołyst'}]",12.0,"{'bibtex': '@Article{Czaplicka2010EmotionalAA,\n author = {Agnieszka Czaplicka and A. Chmiel and J. Hołyst},\n journal = {arXiv: Physics and Society},\n title = {Emotional agents at the square lattice},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': 'arXiv: Physics and Society'}",26.0,Emotional agents at the square lattice,2010.0
1679,8cf0c072eeaec310aff806bdb7a7931b81e9ed99,"Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human–computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a convolutional neural network (CNN) to extract features from the speech, while for the visual modality a deep residual network of 50 layers is used. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, long short-term memory networks are utilized. The system is then trained in an end-to-end fashion where—by also taking advantage of the correlations of each of the streams—we manage to significantly outperform, in terms of concordance correlation coefficient, traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.","[{'authorId': '2829366', 'name': 'Panagiotis Tzirakis'}, {'authorId': '2814229', 'name': 'George Trigeorgis'}, {'authorId': '1752913', 'name': 'M. Nicolaou'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1776444', 'name': 'S. Zafeiriou'}]",449.0,"{'bibtex': '@Article{Tzirakis2017EndtoEndME,\n author = {Panagiotis Tzirakis and George Trigeorgis and M. Nicolaou and Björn Schuller and S. Zafeiriou},\n journal = {IEEE Journal of Selected Topics in Signal Processing},\n pages = {1301-1309},\n title = {End-to-End Multimodal Emotion Recognition Using Deep Neural Networks},\n volume = {11},\n year = {2017}\n}\n'}",,"{'volume': '11', 'pages': '1301-1309', 'name': 'IEEE Journal of Selected Topics in Signal Processing'}",53.0,End-to-End Multimodal Emotion Recognition Using Deep Neural Networks,2017.0
1680,8d11a0d6f0477c8f176ffe84addf524060559b19,,"[{'authorId': '145417478', 'name': 'Brent Lance'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",28.0,"{'bibtex': '@Article{Lance2009GlancesGA,\n author = {Brent Lance and S. Marsella},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {50-69},\n title = {Glances, glares, and glowering: how should a virtual human express emotion through gaze?},\n volume = {20},\n year = {2009}\n}\n'}",,"{'volume': '20', 'pages': '50-69', 'name': 'Autonomous Agents and Multi-Agent Systems'}",32.0,"Glances, glares, and glowering: how should a virtual human express emotion through gaze?",2009.0
1681,8d28d9c1b8efdc804f16c595027d23084f4eb675,Contents: Facts and Theories of Adult Development. A Trait Approach to Personality. Measuring Personality. The Search for Growth or Decline in Personality. Cross-Cultural Perspectives on Personality and Aging. The Course of Personality Development in the Individual. Stability Reconsidered: Qualifications and Rival Hypotheses. A Different View: Ego Psychologies and Projective Methods. Adult Development as Seen through the Personal Interview. A Five-Factor Theory of Personality. The Influences of Personality on the Life Course.,"[{'authorId': '6206591', 'name': 'R. McCrae'}, {'authorId': '2281038', 'name': 'P. Costa'}]",1965.0,"{'bibtex': '@Inproceedings{McCrae2005PersonalityIA,\n author = {R. McCrae and P. Costa},\n title = {Personality in Adulthood: A Five-Factor Theory Perspective},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Personality in Adulthood: A Five-Factor Theory Perspective,2005.0
1682,8d46cd0d24c19d9bc47846f20d8faca9377b37cc,"In this paper we suggest an action selection and behavior arbitration scheme for autonomous robots, called AASBA (for ""affective action selection and behavior arbitration""), which uses affective states to select the robot's behavior at any given time and pass control from the currently active behavior to the newly selected one. AASBA views action selection and behavior arbitration as an integral part of the agent architecture. The major strength of the proposed scheme is that it can be employed in a variety of agent architectures and that it allows for modifications and extensions of the agent architecture without the need of restructuring the overall control system. Preliminary evaluations of the AASBA scheme are performed with a sample implementation of a two-layered architecture (based on the AASBA scheme) on an autonomous robot.","[{'authorId': '1793014', 'name': 'Matthias Scheutz'}]",19.0,"{'bibtex': '@Inproceedings{Scheutz2002AffectiveAS,\n author = {Matthias Scheutz},\n pages = {334-340},\n title = {Affective Action Selection and Behavior Arbitration for autonomous Robots},\n year = {2002}\n}\n'}",,{'pages': '334-340'},14.0,Affective Action Selection and Behavior Arbitration for autonomous Robots,2002.0
1683,8d4b356db6eaf8bff66de9a3be87e5f0ed6da67e,,"[{'authorId': '2226198', 'name': 'M. Nyström'}, {'authorId': '3165327', 'name': 'K. Holmqvist'}]",528.0,"{'bibtex': '@Article{Nyström2010AnAA,\n author = {M. Nyström and K. Holmqvist},\n journal = {Behavior Research Methods},\n pages = {188-204},\n title = {An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data},\n volume = {42},\n year = {2010}\n}\n'}",,"{'volume': '42', 'pages': '188-204', 'name': 'Behavior Research Methods'}",21.0,"An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data",2010.0
1684,8d4b84a0931b9916b9e19aec6b667f3a2c139fd8,"We interact daily with computers that appear and behave like humans. Some researchers propose that people apply the same social norms to computers as they do to humans, suggesting that social psychological knowledge can be applied to our interactions with computers. In contrast, theories of human–automation interaction postulate that humans respond to machines in unique and specific ways. We believe that anthropomorphism—the degree to which an agent exhibits human characteristics—is the critical variable that may resolve this apparent contradiction across the formation, violation, and repair stages of trust. Three experiments were designed to examine these opposing viewpoints by varying the appearance and behavior of automated agents. Participants received advice that deteriorated gradually in reliability from a computer, avatar, or human agent. Our results showed (a) that anthropomorphic agents were associated with greater trust resilience, a higher resistance to breakdowns in trust; (b) that these effects were magnified by greater uncertainty; and c) that incorporating human-like trust repair behavior largely erased differences between the agents. Automation anthropomorphism is therefore a critical variable that should be carefully incorporated into any general theory of human–agent trust as well as novel automation design.","[{'authorId': '113889522', 'name': 'E. D. de Visser'}, {'authorId': '34330513', 'name': 'Samuel S. Monfort'}, {'authorId': '2402701', 'name': 'Ryan McKendrick'}, {'authorId': '1410179766', 'name': 'Melissa A. B. Smith'}, {'authorId': '2095598398', 'name': 'Patrick McKnight'}, {'authorId': '144714297', 'name': 'F. Krueger'}, {'authorId': '3264674', 'name': 'R. Parasuraman'}]",327.0,"{'bibtex': '@Article{Visser2016AlmostHA,\n author = {E. D. de Visser and Samuel S. Monfort and Ryan McKendrick and Melissa A. B. Smith and Patrick McKnight and F. Krueger and R. Parasuraman},\n journal = {Journal of experimental psychology. Applied},\n pages = {\n          331-49\n        },\n title = {Almost human: Anthropomorphism increases trust resilience in cognitive agents.},\n volume = {22 3},\n year = {2016}\n}\n'}",,"{'volume': '22 3', 'pages': '\n          331-49\n        ', 'name': 'Journal of experimental psychology. Applied'}",132.0,Almost human: Anthropomorphism increases trust resilience in cognitive agents.,2016.0
1685,8d4ec47bf579424079edb60b4662df82648a04eb,,"[{'authorId': '2559167', 'name': 'C. Battaglino'}]",1.0,"{'bibtex': '@Inproceedings{Battaglino2015MoralEA,\n author = {C. Battaglino},\n title = {Moral Emotional Agents: Linking Emotions with Moral Values and Deliberation},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Moral Emotional Agents: Linking Emotions with Moral Values and Deliberation,2015.0
1686,8d5013258fccc78615e47b8ab5f81812b989ac4a,,"[{'authorId': '34641476', 'name': 'R. Srikant'}, {'authorId': '144947410', 'name': 'R. Agrawal'}]",3177.0,"{'bibtex': '@Inproceedings{Srikant1996MiningSP,\n author = {R. Srikant and R. Agrawal},\n pages = {3-17},\n title = {Mining Sequential Patterns: Generalizations and Performance Improvements},\n year = {1996}\n}\n'}",,{'pages': '3-17'},14.0,Mining Sequential Patterns: Generalizations and Performance Improvements,1996.0
1687,8d71646a3d2658c78a7a7982f7bbbcff7981fc45,,"[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",802.0,"{'bibtex': '@Article{Gratch2004ADF,\n author = {J. Gratch and S. Marsella},\n journal = {Cognitive Systems Research},\n pages = {269-306},\n title = {A domain-independent framework for modeling emotion},\n volume = {5},\n year = {2004}\n}\n'}",,"{'volume': '5', 'pages': '269-306', 'name': 'Cognitive Systems Research'}",122.0,A domain-independent framework for modeling emotion,2004.0
1698,8d7aacba92dcf2be1f5e711bc1238ab1eb6fc390,"Conversational partners influence each others' emotions and topics. Using a large dataset of Twitter conversations and an unsupervised machine learning technique, we discover patterns of emotion influence in naturally occurring conversations. We describe our computational framework for automatically classifying emotions, analyzing the emotional transitions, and discovering emotion influence patterns. We found that conversational partners usually express the same emotion (emotion contagion), but when they do not, one of the conversational partners tends to respond with a positive emotion. Also, tweets containing sympathy, apology, and complaint are significant emotion influencers. One of the interesting findings is that expressing a desired emotion is the best strategy to alter partner's emotion.","[{'authorId': '2844832', 'name': 'Suin Kim'}, {'authorId': '72761736', 'name': 'Jinyeong Bak'}, {'authorId': '2463290', 'name': 'Alice H. Oh'}]",10.0,"{'bibtex': '@Article{Kim2012DiscoveringEI,\n author = {Suin Kim and Jinyeong Bak and Alice H. Oh},\n journal = {ACM SIGWEB Newsletter},\n pages = {1 - 6},\n title = {""Discovering emotion influence patterns in online social network conversations"" by Suin Kim, JinYeong Bak, and Alice Oh, with Ching-man Au Yeung as coordinator},\n volume = {2012},\n year = {2012}\n}\n'}",,"{'volume': '2012', 'pages': '1 - 6', 'name': 'ACM SIGWEB Newsletter'}",11.0,"""Discovering emotion influence patterns in online social network conversations"" by Suin Kim, JinYeong Bak, and Alice Oh, with Ching-man Au Yeung as coordinator",2012.0
1699,8d7b4c6e3193abe833418f52094cf5f27eb3da78,"Storytelling accompanies the evolution of the human race, and bears witness to both its identity and culture. For a long time, it has represented a sort of “oral encyclopedia” which, through narrative, has passed down from generation to generation traditions, customs and knowledge (Halverson, 1992). The role that storytelling played, even before the discovery of writing, in the diffusion of knowledge, in the construction of interpersonal relationships and in the birth of new societies, is well known. Without doubt writing is the earliest “technology” invented by man, through which culture and knowledge have been passed down over time, after having been transmitted orally for a long time. According to Barthes and Duisit (1975), in fact, storytelling begins with the same story of the human race, almost as if to represent an ontogenetically distinctive trait of the species, which unites homo sapiens with the homo digitalis of today’s society (Montag & Diefenbach, 2018). Other authors (Kenyon et al., 1996) argue that men and women not only have stories to tell, but that they are the stories they tell. Storytelling, therefore, always reinterprets in a new way and with different tools a natural propensity of human beings to tell about themselves, to build memories capable of projecting the past into the future.","[{'authorId': '100833814', 'name': 'Davide Capperucci'}]",2.0,"{'bibtex': '@Inproceedings{Capperucci2020STORYTELLINGFI,\n author = {Davide Capperucci},\n title = {STORYTELLING FOR INCLUSION},\n year = {2020}\n}\n'}",,,28.0,STORYTELLING FOR INCLUSION,2020.0
1700,8d9ae00aad621e9f12b9c5c0ede8496ab3bad80b,"Personality is the coherent patterning of affect, behavior, cognition, and desires (goals) over time and space. Just as a full blown emotion represents an integration of feeling, action, appraisal and wants at a particular time and location so does personality represent integration over time and space of these components (Ortony et al., 2005). A helpful analogy is to consider that personality is to emotion as climate is to weather. That is, what one expects is personality, what one observes at any particular moment is emotion. To understand the personality-affect link it is necessary to consider the ways in which personality may be described. Since Theophrastus’ discussion of characters and Galen’s theory of temperament (Stelmack & Stalikas, 1991), dimensional models of individual differences in personality have consistently identified three (the Giant Three, e.g., Eysenck & Eysenck (1985)) to five (the Big Five, e.g., Digman (1990)) broad dimensions of personality. Two of these dimensions, in particular, Extraversion (E) and Neuroticism (N, sometimes referred to by the other end of the dimension as Emotional Stability) have been associated with individual differences in affective level and environmental responsivity (Corr, 2008; Revelle, 1995). Ever since antiquity, starting with Galen’s classification of the four different humors, it has been assumed that individuals differ in their predisposition to experience certain emotions. Extrapolating from animal studies, E and N have been associated with the Behavioral Activation System (BAS) and Behavioral Inhibition System (BIS) respectively, while distinctions between trait fear and trait anxiety have been associated with the Fight/Freeze/Flight System (FFFS) (Corr, 2008; Gray & McNaughton, 2000). Indeed, the basic assumptions of Reinforcement Sensitivity Theory (Corr, 2008), perhaps better labeled as Three Systems Theory, are that the stable personality traits reflect individual differences in reactivity to emotional and affectively valenced environmental cues. Descriptively, there is much literature on hysteric, neurotic, or anhedonic personalities (Kellerman, 1990), or, in more recent terminology, on trait anger, trait anxiety, or trait positive-negative affect (Spielberger et al., 1999; Tellegen et al., 1999) . These trait differences in emotionality increase the odds of experiencing trait-congruent emotions. In other words, individuals high on trait anxiety run an increased risk of experiencing anxiety bouts, individuals high on trait anger get irritated more often, and so forth. Thus, in a quasi-representative survey of everyday emotion experiences Scherer et al. (2004) showed that the emotionality dispositions may significantly increase the risk to experience certain emotions. Concretely, the more frequently respondents habitually experienced a particular kind of emotion (trait emotionality), the more likely they experienced an exemplar of that emotion category yesterday. Thus, respondents high on trait anxiety were almost three times as likely to have experienced anxiety yesterday compared to those who are low on this trait. In the case of trait sadness and trait despair, the likelihood is about two times higher. Respondents high on trait irritation are about 1.5 times more likely to have experienced anger yesterday. Similarly, respondents reporting frequent habitual pleasure, surprise, or pride experiences are also 1.5 times more likely to have experienced joy or happiness. Because some emotions occur less frequently than expected for respondents with certain habitual emotion dispositions, some types of trait emotionality might inoculate, or shield, against particular emotions. The results seem to indicate that trait pleasure may reduce the risk of despair, and that trait surprise may reduce the risk of anxiety. These results do not just reflect common responses to questionnaires, but rather reflect basic neural processes. Using functional brain mapping (e.g., fMRI), trait extraversion and neuroticism were","[{'authorId': '2443754', 'name': 'W. Revelle'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",155.0,"{'bibtex': '@Inproceedings{Revelle2009PersonalityAE,\n author = {W. Revelle and K. Scherer},\n pages = {304-305},\n title = {Personality and emotion},\n year = {2009}\n}\n'}",,"{'volume': '', 'pages': '304-305', 'name': ''}",27.0,Personality and emotion,2009.0
1701,8db3f00721e99b7bd44d7089d39641ae4b971bf3,"Allan Schore has for some 15 years written about the processes underlying affect regulation in normal and abnormal self-development and attachment. His best known book entitled Affect Regulation and the Origin of the Self: The Neurobiology of Emotional Development, published in 1994, was the first coherent attempt to integrate the then recent findings of neurobiology with clinical observations in children and adults and brought down barriers that had impeded the understanding of the self and its disorders. In the present volume, Schore incorporates the vast amount of data from neurosciences since 1994 and presents the reader with a truly compelling theoretical synthesis of this literature. 
 
In part I, there are 4 chapters on developmental affective neuroscience. They deal with the contribution experience expectant vs. experience dependent phenomena make to the development of affect regulation. The former are primarily gene dependent (e.g., the CNS of a newborn is equipped to function well in infants who live within a reasonably safe environment and are exposed to gradual rather than violent changes) while the latter are dependent on the care taking practices the child is exposed to. One example cited is the affective transmissions in mutual gaze transactions between infants and their mothers. These affective parental responses are the first means by which mothers can provide a model of affect modulation to their infants (e.g., mother senses when her baby is becoming overstimulated and will respond by decreasing her own stimulation, leading to calming the infant). Such soothing behaviors will secondarily effect the maturation of the orbitofrontal cortex and strengthen its regulatory abilities. Attachment behavior is likewise based on the reciprocal activation of the couple’s endogenous opiate systems but also regulates the dopamine levels in the infant’s brain. Schore brings these and other interdisciplinary findings together by citing the available evidence and at times even presenting colored PET or fMRI scans to make his point. 
 
In part II, 5 chapters deal with developmental neuropsychiatric data and their relevance on development of the right brain, secure attachment relationships and on symptoms of PTSD, borderline and antisocial personality disorders. Here again, Schore cites studies that explain important psychological processes through neuropsychiatric data. For example, he cites evidence that in the context of face-to-face interactions, mothers trigger production of corticotropin releasing factor (CRF) in their infants. The CRF, in turn raises the concentration of noradrenaline, increasing general energy metabolism but also controls endorphin and ACTH. production, leading to an elated state in the infant. 
 
When it comes to PTSD and other well-defined psychiatric disorders, the overall picture becomes more complicated. For example, Schore claims that PTSD is related to the inability of the right prefrontal cortex to sufficiently modulate amygdala (i.e., aggressive) functions. The fact that this also occurs in children with a disorganized disoriented insecure attachment pattern is then seen as proof that this particular early maternal caretaking pattern contributes to later dissociative psychopathology. One could counter that proposition by pointing out that elevated cortisol levels are important for overall stress management – but that prenatally elevated levels, especially in the third trimester of pregnancy, have been found to be especially pathogenic as they effect the developing brain at its most critical time. However, high cortisol levels can be caused by a variety of conditions. Intra-uterine growth retardation (IUGR) is a common condition associated in children born with ‘small gestational age’ (SGA). It is innately stressful for the infant, hence associated with elevated prenatal cortisol levels that are not associated to later maternal attachment patterns. Aggressive behavior disorders are also described as a consequence of a right brain system impaired for regulating aggressive affective states. Here it is said to be the low arousal state characteristic for antisocial and aggressive individuals that such individuals try to increase back to optimal or normal levels by seeking stimulation. While this may be one pathway leading to aggressive behavior disorders, there are authors such as Tremblay and colleagues in Montreal who claim that all young children are highly aggressive and must “unlearn” this behavior in the process of development. Those who do not or cannot do so will make up our clinical population. 
 
In summary, the present volume of Allan Schore provides the reader with a provocative and stimulating theoretical synthesis of multi-disciplinary work that relates affect regulation to the development of the self and its deviations. Schore’s writing style is almost poetic and transforms potentially dry data into an exciting story of discovery and multidisciplinary dependency. He also suggests, at least indirectly, preventive measures that can address the problem of violence and other dysfunctions of the developing self in children through optimal early social-emotional experiences. I highly recommend this volume to researchers and clinicians.","[{'authorId': '48017979', 'name': 'K. Minde'}]",550.0,"{'bibtex': ""@Article{Minde2006AffectDA,\n author = {K. Minde},\n journal = {Journal de l'Académie canadienne de psychiatrie de l'enfant et de l'adolescent},\n pages = {100-101},\n title = {Affect Dysregulation and Disorders of the Self},\n volume = {15},\n year = {2006}\n}\n""}",,"{'volume': '15', 'pages': '100-101', 'name': ""Journal de l'Académie canadienne de psychiatrie de l'enfant et de l'adolescent""}",0.0,Affect Dysregulation and Disorders of the Self,2006.0
1702,8e071b1a900a9edbba3577b88e600f5e6a5ec3e5,"In this paper we address an often overlooked problem in defeasible argumentation: how do we deal with arguments that are on their own defeated, but together remain undefeated? Pollock (1991) finds this accrual of arguments a na tural supposition, but t hen surprisingly denies its existence. We think that arguments do a ccrue. To hand le the accrual of arguments, we introduce compound defeat of arguments. We call the defeat of arguments compound, if groups of arguments can b e defeated b y other groups of arguments. The formalism presented in this paper is based on this notion o f compound defeat. It adequately handles the accrual of arguments.","[{'authorId': '1697177', 'name': 'Bart Verheij'}]",40.0,"{'bibtex': '@Inproceedings{Verheij1999AccrualOA,\n author = {Bart Verheij},\n title = {Accrual of arguments in defeasible argumentation},\n year = {1999}\n}\n'}",,"{'volume': '', 'name': ''}",17.0,Accrual of arguments in defeasible argumentation,1999.0
1703,8e1f18632ae699ef049559a1f22c21813b27dc56,,"[{'authorId': '145171812', 'name': 'P. Olivier'}]",4.0,"{'bibtex': '@Inproceedings{Olivier2004GestureSI,\n author = {P. Olivier},\n pages = {319-322},\n title = {Gesture Synthesis in a Real-World ECA},\n year = {2004}\n}\n'}",,{'pages': '319-322'},5.0,Gesture Synthesis in a Real-World ECA,2004.0
1704,8e218630f62acb0178a250aa16aa3b33593d9e72,"Empathy accounts for the naturally occurring subjective experience of similarity between the feelings expressed by self and others without loosing sight of whose feelings belong to whom. Empathy involves not only the affective experience of the other person's actual or inferred emotional state but also some minimal recognition and understanding of another's emotional state. In light of multiple levels of analysis ranging from developmental psychology, social psychology, cognitive neuroscience, and clinical neuropsychology, this article proposes a model of empathy that involves parallel and distributed processing in a number of dissociable computational mechanisms. Shared neural representations, self-awareness, mental flexibility, and emotion regulation constitute the basic macrocomponents of empathy, which are underpinned by specific neural systems. This functional model may be used to make specific predictions about the various empathy deficits that can be encountered in different forms of social and neurological disorders.","[{'authorId': '3235030', 'name': 'J. Decety'}, {'authorId': '6079112', 'name': 'P. Jackson'}]",2513.0,"{'bibtex': '@Article{Decety2004TheFA,\n author = {J. Decety and P. Jackson},\n journal = {Behavioral and cognitive neuroscience reviews},\n pages = {\n          71-100\n        },\n title = {The functional architecture of human empathy.},\n volume = {3 2},\n year = {2004}\n}\n'}",,"{'volume': '3 2', 'pages': '\n          71-100\n        ', 'name': 'Behavioral and cognitive neuroscience reviews'}",307.0,The functional architecture of human empathy.,2004.0
1705,8e5d7405ece81f0ac9fcb4574e53bf0c827e9ad7,"Contemporary emotion regulation research emphasizes intrapersonal processes such as cognitive reappraisal and expressive suppression, but people experiencing affect commonly choose not to go it alone. Instead, individuals often turn to others for help in shaping their affective lives. How and under what circumstances does such interpersonal regulation modulate emotional experience? Although scientists have examined allied phenomena such as social sharing, empathy, social support, and prosocial behavior for decades, there have been surprisingly few attempts to integrate these data into a single conceptual framework of interpersonal regulation. Here we propose such a framework. We first map a ""space"" differentiating classes of interpersonal regulation according to whether an individual uses an interpersonal regulatory episode to alter their own or another person's emotion. We then identify 2 types of processes--response-dependent and response-independent--that could support interpersonal regulation. This framework classifies an array of processes through which interpersonal contact fulfills regulatory goals. More broadly, it organizes diffuse, heretofore independent data on ""pieces"" of interpersonal regulation, and identifies growth points for this young and exciting research domain.","[{'authorId': '2268731', 'name': 'Jamil Zaki'}, {'authorId': '2055993791', 'name': 'W. C. Williams'}]",672.0,"{'bibtex': '@Article{Zaki2013InterpersonalER,\n author = {Jamil Zaki and W. C. Williams},\n journal = {Emotion},\n pages = {\n          803-10\n        },\n title = {Interpersonal emotion regulation.},\n volume = {13 5},\n year = {2013}\n}\n'}",,"{'volume': '13 5', 'pages': '\n          803-10\n        ', 'name': 'Emotion'}",105.0,Interpersonal emotion regulation.,2013.0
1706,8e9f3511c1b9bb9bc8d9573df1085c402de87ab6,"This paper promotes socially intelligent animated agents for the pedagogical task of English conversation training for native speakers of Japanese. As a novel feature, social role awareness is introduced to animated conversational agents, that are by non-strong affective reasoners, but otherwise often lack the social competence observed in humans. In particular, humans may easily adjust their behavior depending on their respective role in a social setting, whereas their synthetic pendants tend to be driven mostly by emotions and personality. Our main contribution is the incorporation of a ""social filter program"" to mental models of animated agents. This program may qualify an agent's expression of its emotional state by the social contest, thereby enhancing the agent's believability as a conversational partner. Our implemented system is web-based and demonstrates socially aware animated agents in a virtual coffee shop environment. An experiment with our conversation system shows that users consider socially aware agents as more natural than agents that violate conventional practices.","[{'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}]",57.0,"{'bibtex': ""@Article{Prendinger2001LetsTS,\n author = {H. Prendinger and M. Ishizuka},\n journal = {IEEE Trans. Syst. Man Cybern. Part A},\n pages = {465-471},\n title = {Let's talk! Socially intelligent agents for language conversation training},\n volume = {31},\n year = {2001}\n}\n""}",,"{'volume': '31', 'pages': '465-471', 'name': 'IEEE Trans. Syst. Man Cybern. Part A'}",52.0,Let's talk! Socially intelligent agents for language conversation training,2001.0
1707,8ea8846223e29a6b327d9a188390c1735fbe0928,,"[{'authorId': '3213354', 'name': 'B. Gelder'}, {'authorId': '40022981', 'name': 'J. V. D. Stock'}, {'authorId': '6702631', 'name': 'H. Meeren'}, {'authorId': '7318717', 'name': 'C. Sinke'}, {'authorId': '2512383', 'name': 'M. Kret'}, {'authorId': '3005853', 'name': 'M. Tamietto'}]",269.0,"{'bibtex': '@Article{Gelder2010StandingUF,\n author = {B. Gelder and J. V. D. Stock and H. Meeren and C. Sinke and M. Kret and M. Tamietto},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {513-527},\n title = {Standing up for the body. Recent progress in uncovering the networks involved in the perception of bodies and bodily expressions},\n volume = {34},\n year = {2010}\n}\n'}",,"{'volume': '34', 'pages': '513-527', 'name': 'Neuroscience & Biobehavioral Reviews'}",110.0,Standing up for the body. Recent progress in uncovering the networks involved in the perception of bodies and bodily expressions,2010.0
1708,8ea96dd631a74655461b3c95580c8a18c11daf0c,"What is the relation between gestures and speech? In terms of symbolic forms, of course, the spontaneous and unwitting gestures we make while talking differ sharply from spoken language itself. Whereas spoken language is linear, segmented, standardized, and arbitrary, gestures are global, synthetic, idiosyncratic, and imagistic. In Hand and Mind, David McNeill presents a bold theory of the essential unity of speech and the gestures that accompany it. This long-awaited, provocative study argues that the unity of gestures and language far exceeds the surface level of speech noted by previous researchers and in fact also includes the semantic and pragmatic levels of language. In effect, the whole concept of language must be altered to take into account the nonsegmented, instantaneous, and holistic images conveyed by gestures. McNeill and his colleagues carefully devised a standard methodology for examining the speech and gesture behavior of individuals engaged in narrative discourse. A research subject is shown a cartoon like the 1950 Canary Row--a classic Sylvester and Tweedy Bird caper that features Sylvester climbing up a downspout, swallowing a bowling ball and slamming into a brick wall. After watching the cartoon, the subject is videotaped recounting the story from memory to a listener who has not seen the cartoon. Painstaking analysis of the videotapes revealed that although the research subjects--children as well as adults, some neurologically impaired--represented a wide variety of linguistic groupings, the gestures of people speaking English and a half dozen other languages manifest the same principles. Relying on data from more than ten years of research, McNeill shows thatgestures do not simply form a part of what is said and meant but have an impact on thought itself. He persuasively argues that because gestures directly transfer mental images to visible forms, conveying ideas that language cannot always express, we must examine language and gesture","[{'authorId': '145493778', 'name': 'D. McNeill'}]",2038.0,"{'bibtex': '@Inproceedings{McNeill1992HandAM,\n author = {D. McNeill},\n title = {Hand and Mind: What Gestures Reveal about Thought},\n year = {1992}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Hand and Mind: What Gestures Reveal about Thought,1992.0
1709,8eed8a4154239d39cd58afef67673d3488c9c2e9,"The goal of this review was to provide a brief overview of recent developments in the domain of emotional mimicry research. We argue that emotional signals are intrinsically meaningful within a social relationship, which is crucial for understanding the functionality and boundary conditions of emotional mimicry. On the basis of a review of the literature on facial mimicry of emotion displays, we conclude that the classic matched motor hypothesis does not hold for emotional mimicry. We alternatively propose a contextual view of emotional mimicry, which states that emotional mimicry depends on the social context: we only mimic emotional signals that are interpreted to promote affiliation goals and not necessarily what we see. As a further consequence, we are less likely to mimic strangers and we do not mimic people we do not like nor emotions that signal antagonism.","[{'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '7444483', 'name': 'A. Fischer'}]",234.0,"{'bibtex': '@Article{Hess2014EmotionalMW,\n author = {U. Hess and A. Fischer},\n journal = {Social and Personality Psychology Compass},\n pages = {45-57},\n title = {Emotional mimicry: why and when we mimic emotions},\n volume = {8},\n year = {2014}\n}\n'}",,"{'volume': '8', 'pages': '45-57', 'name': 'Social and Personality Psychology Compass'}",72.0,Emotional mimicry: why and when we mimic emotions,2014.0
1710,8eef62972d27bd54e9744cda44aa3938a732221a,,"[{'authorId': '1693653', 'name': 'H. Kober'}, {'authorId': '1731779', 'name': 'L. F. Barrett'}, {'authorId': '2052441460', 'name': 'Joshua W. Joseph'}, {'authorId': '1398003816', 'name': 'E. Bliss-Moreau'}, {'authorId': '2593387', 'name': 'Kristen A. Lindquist'}, {'authorId': '2549424', 'name': 'T. Wager'}]",1019.0,"{'bibtex': '@Article{Kober2008FunctionalGA,\n author = {H. Kober and L. F. Barrett and Joshua W. Joseph and E. Bliss-Moreau and Kristen A. Lindquist and T. Wager},\n journal = {NeuroImage},\n pages = {998-1031},\n title = {Functional grouping and cortical–subcortical interactions in emotion: A meta-analysis of neuroimaging studies},\n volume = {42},\n year = {2008}\n}\n'}",,"{'volume': '42', 'pages': '998-1031', 'name': 'NeuroImage'}",370.0,Functional grouping and cortical–subcortical interactions in emotion: A meta-analysis of neuroimaging studies,2008.0
1711,8ef763cb4f13f8dc9d4376b5ad8f377331e8572d,,"[{'authorId': '1784851', 'name': 'Tara N. Sainath'}, {'authorId': '144707379', 'name': 'Brian Kingsbury'}, {'authorId': '1698208', 'name': 'G. Saon'}, {'authorId': '38940652', 'name': 'H. Soltau'}, {'authorId': '40360972', 'name': 'Abdel-rahman Mohamed'}, {'authorId': '35188630', 'name': 'George E. Dahl'}, {'authorId': '1720857', 'name': 'B. Ramabhadran'}]",444.0,"{'bibtex': '@Article{Sainath2015DeepCN,\n author = {Tara N. Sainath and Brian Kingsbury and G. Saon and H. Soltau and Abdel-rahman Mohamed and George E. Dahl and B. Ramabhadran},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          39-48\n        },\n title = {Deep Convolutional Neural Networks for Large-scale Speech Tasks},\n volume = {64},\n year = {2015}\n}\n'}",,"{'volume': '64', 'pages': '\n          39-48\n        ', 'name': 'Neural networks : the official journal of the International Neural Network Society'}",33.0,Deep Convolutional Neural Networks for Large-scale Speech Tasks,2015.0
1712,8f089b968032e11b2d50c1b3c7404433241227e6,"There has been increasing evidence to suggest that the root cause of much mental illness lies in a sub-optimal capacity for affect regulation. Cognition and emotion are intricately linked and cognitive deficits, which are characteristic of many psychiatric conditions, are often driven by affect dysregulation, which itself can usually be traced back to sub-optimal childhood development as supported by Attachment Theory. Individuals with insecure attachment types in their childhoods are prone to a variety of mental illness, whereas a secure attachment type in childhood provides a secure base in life. We therefore propose a holistic approach to tackle chronic anxiety and depression, typical of Axis II clinical disorders, which is informed by the development of the infant brain in social interaction with its primary care-givers. We formulate, in a self-administrable way, the protocols governing the interaction of a securely attached child with its primary care-givers that produce the capacity for affect regulation in the child. We posit that these protocols construct, by neuroplasticity and long term potentiation, new optimal neural pathways in the brains of adults with insecure childhood attachment that suffer from mental disorder. This procedure is called self-attachment and aims to help the individuals to create their own attachment objects in the form of their adult self looking after their inner child.","[{'authorId': '1694989', 'name': 'A. Edalat'}]",6.0,"{'bibtex': '@Inproceedings{Edalat2017SelfattachmentA,\n author = {A. Edalat},\n title = {Self-attachment : A self-administrable intervention for chronic anxiety and depression ∗},\n year = {2017}\n}\n'}",,,34.0,Self-attachment : A self-administrable intervention for chronic anxiety and depression ∗,2017.0
1713,8f12d0e8f154501e43932cb249446a922a3d5cd4,"It has been shown that humans are sensitive to the portrayal of emotions for virtual characters. However, previous work in this area has often examined this sensitivity using extreme examples of facial or body animation. Less is known about how attuned people are at recognizing emotions as they are expressed during conversational communication. In order to determine whether body or facial motion is a better indicator for emotional expression for game characters, we conduct a perceptual experiment using synchronized full-body and facial motion-capture data. We find that people can recognize emotions from either modality alone, but combining facial and body motion is preferable in order to create more expressive characters.","[{'authorId': '31894925', 'name': 'Cathy Ennis'}, {'authorId': '1869571', 'name': 'Ludovic Hoyet'}, {'authorId': '2479558', 'name': 'A. Egges'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",31.0,"{'bibtex': '@Article{Ennis2013EmotionCE,\n author = {Cathy Ennis and Ludovic Hoyet and A. Egges and R. Mcdonnell},\n journal = {Proceedings of Motion on Games},\n title = {Emotion Capture: Emotionally Expressive Characters for Games},\n year = {2013}\n}\n'}",,{'name': 'Proceedings of Motion on Games'},25.0,Emotion Capture: Emotionally Expressive Characters for Games,2013.0
1714,8f14c00079c601a95b7b0403e42653dedbc4e6c2,,"[{'authorId': '2065597068', 'name': 'K. Sreelakshmi'}, {'authorId': '9328378', 'name': 'P. C. Rafeeque'}, {'authorId': '2102687511', 'name': 'S. Sreetha'}, {'authorId': '2082825798', 'name': 'E. S. Gayathri'}]",24.0,"{'bibtex': '@Article{Sreelakshmi2018DeepBL,\n author = {K. Sreelakshmi and P. C. Rafeeque and S. Sreetha and E. S. Gayathri},\n journal = {Procedia Computer Science},\n pages = {939-946},\n title = {Deep Bi-Directional LSTM Network for Query Intent Detection},\n volume = {143},\n year = {2018}\n}\n'}",,"{'volume': '143', 'pages': '939-946', 'name': 'Procedia Computer Science'}",2.0,Deep Bi-Directional LSTM Network for Query Intent Detection,2018.0
1715,8f311992038db87b7fe2f750f272c948d00d35ce,,"[{'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '2539387', 'name': 'Virginie Demeure'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",38.0,"{'bibtex': '@Inproceedings{Niewiadomski2010WarmthCB,\n author = {Radoslaw Niewiadomski and Virginie Demeure and C. Pelachaud},\n pages = {272-285},\n title = {Warmth, Competence, Believability and Virtual Agents},\n year = {2010}\n}\n'}",,{'pages': '272-285'},25.0,"Warmth, Competence, Believability and Virtual Agents",2010.0
1716,8f454cd100429ee2c44bfb237448e365420dcdd1,"Recent findings in neuroscience suggest an overlap between brain regions involved in the execution of movement and perception of another’s movement. This so-called “action-perception coupling” is supposed to serve our ability to automatically infer the goals and intentions of others by internal simulation of their actions. A consequence of this coupling is motor interference (MI), the effect of movement observation on the trajectory of one’s own movement. Previous studies emphasized that various features of the observed agent determine the degree of MI, but could not clarify how human-like an agent has to be for its movements to elicit MI and, more importantly, what ‘human-like’ means in the context of MI. Thus, we investigated in several experiments how different aspects of appearance and motility of the observed agent influence motor interference (MI). Participants performed arm movements in horizontal and vertical directions while observing videos of a human, a humanoid robot, or an industrial robot arm with either artificial (industrial) or human-like joint configurations. Our results show that, given a human-like joint configuration, MI was elicited by observing arm movements of both humanoid and industrial robots. However, if the joint configuration of the robot did not resemble that of the human arm, MI could longer be demonstrated. Our findings present evidence for the importance of human-like joint configuration rather than other human-like features for perception-action coupling when observing inanimate agents.","[{'authorId': '48790377', 'name': 'Aleksandra Kupferberg'}, {'authorId': '50647134', 'name': 'Markus Huber'}, {'authorId': '6752359', 'name': 'B. Helfer'}, {'authorId': '40508886', 'name': 'Claus Lenz'}, {'authorId': '143873832', 'name': 'A. Knoll'}, {'authorId': '1829139', 'name': 'S. Glasauer'}]",39.0,"{'bibtex': '@Article{Kupferberg2012MovingJL,\n author = {Aleksandra Kupferberg and Markus Huber and B. Helfer and Claus Lenz and A. Knoll and S. Glasauer},\n journal = {PLoS ONE},\n title = {Moving Just Like You: Motor Interference Depends on Similar Motility of Agent and Observer},\n volume = {7},\n year = {2012}\n}\n'}",,"{'volume': '7', 'name': 'PLoS ONE'}",41.0,Moving Just Like You: Motor Interference Depends on Similar Motility of Agent and Observer,2012.0
1717,8f7c3e8dc7aef0a7b9ad70911ffc4d181349273e,"Appraisal theories of emotion hold that it is the way a person interprets a situation--rather than the situation itself--that gives rise to one emotion rather than another emotion (or no emotion at all). Unfortunately, most prior tests of this foundational hypothesis have simultaneously varied situations and appraisals, making an evaluation of this assumption difficult. In the present study, participants responded to a standardized laboratory situation with a variety of different emotions. Appraisals predicted the intensity of individual emotions across participants. In addition, subgroups of participants with similar emotional response profiles made comparable appraisals. Together, these findings suggest that appraisals may be necessary and sufficient to determine different emotional reactions toward a particular situation.","[{'authorId': '4125928', 'name': 'M. Siemer'}, {'authorId': '2172628', 'name': 'I. Mauss'}, {'authorId': '1775321', 'name': 'J. Gross'}]",333.0,"{'bibtex': '@Article{Siemer2007SameSE,\n author = {M. Siemer and I. Mauss and J. Gross},\n journal = {Emotion},\n pages = {\n          592-600\n        },\n title = {Same situation--different emotions: how appraisals shape our emotions.},\n volume = {7 3},\n year = {2007}\n}\n'}",,"{'volume': '7 3', 'pages': '\n          592-600\n        ', 'name': 'Emotion'}",45.0,Same situation--different emotions: how appraisals shape our emotions.,2007.0
1718,8fa322274600d7500c235ef4b96e9e553c0293ee,,"[{'authorId': '2059085302', 'name': 'Liu Zhen'}, {'authorId': '2111523862', 'name': 'Jin Wei'}, {'authorId': '2140198344', 'name': 'Huang Peng'}, {'authorId': '104114672', 'name': 'Chai Yanjie'}]",17.0,"{'bibtex': '@Article{Zhen2013AnEC,\n author = {Liu Zhen and Jin Wei and Huang Peng and Chai Yanjie},\n journal = {Journal of Computer Research and Development},\n pages = {2578},\n title = {An Emotion Contagion Simulation Model for Crowd Events},\n volume = {50},\n year = {2013}\n}\n'}",,"{'volume': '50', 'pages': '2578', 'name': 'Journal of Computer Research and Development'}",0.0,An Emotion Contagion Simulation Model for Crowd Events,2013.0
1719,90171eabf908b7517b668158d116405acb574a77,,"[{'authorId': '5586879', 'name': 'T. Sharot'}, {'authorId': '2471431', 'name': 'E. Phelps'}]",342.0,"{'bibtex': '@Article{Sharot2004HowAM,\n author = {T. Sharot and E. Phelps},\n journal = {Cognitive, Affective, & Behavioral Neuroscience},\n pages = {294-306},\n title = {How arousal modulates memory: Disentangling the effects of attention and retention},\n volume = {4},\n year = {2004}\n}\n'}",,"{'volume': '4', 'pages': '294-306', 'name': 'Cognitive, Affective, & Behavioral Neuroscience'}",72.0,How arousal modulates memory: Disentangling the effects of attention and retention,2004.0
1720,902b5d8882ed6f817a109ff3a0a6692fb1abbc28,"This chapter pleads for more inspiration from human nature, in agent-based modeling. As an illustration of an effort in that direction, it summarizes and discusses an agent-based model of the build-up and adaptation of trust between multiple producers and suppliers. The central question is whether, and under what conditions, trust and loyalty are viable in markets. While the model incorporates some well known behavioural phenomena from the trust literature, more extended modeling of human nature is called for. The chapter explores a line of further research on the basis of notions of mental framing and frame switching on the basis of relational signaling, derived from social psychology.","[{'authorId': '2143128', 'name': 'B. Nooteboom'}]",7.0,"{'bibtex': '@Article{Nooteboom2006HumanNI,\n author = {B. Nooteboom},\n journal = {Behavioral & Experimental Economics},\n title = {Human Nature in the Adaptation of Trust},\n year = {2006}\n}\n'}",,{'name': 'Behavioral & Experimental Economics'},48.0,Human Nature in the Adaptation of Trust,2006.0
1721,903a510ac39dde78df59f37247ab91e04d7c50e0,,"[{'authorId': '46549484', 'name': 'S. Redfern'}, {'authorId': None, 'name': 'M. Colhoun'}, {'authorId': '2111086828', 'name': 'Jordi Hernàndez'}, {'authorId': '72492302', 'name': 'N. Naughton'}, {'authorId': '122603678', 'name': 'D. Noonan'}]",2.0,"{'bibtex': '@Inproceedings{Redfern2006DistributedVR,\n author = {S. Redfern and M. Colhoun and Jordi Hernàndez and N. Naughton and D. Noonan},\n pages = {241-255},\n title = {Distributed Virtual Reality Learning Environments},\n year = {2006}\n}\n'}",,"{'volume': '', 'pages': '241-255', 'name': ''}",0.0,Distributed Virtual Reality Learning Environments,2006.0
1722,903ec87f1eb8759520feecaf4a719608fa761e58,"Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively","[{'authorId': '46178179', 'name': 'Chih-Wen Goo'}, {'authorId': '30725803', 'name': 'Guang-Lai Gao'}, {'authorId': '3176770', 'name': 'Yun-Kai Hsu'}, {'authorId': '2818470', 'name': 'Chih-Li Huo'}, {'authorId': '2110505230', 'name': 'Tsung-Chieh Chen'}, {'authorId': '31294636', 'name': 'Keng-Wei Hsu'}, {'authorId': '1725643', 'name': 'Yun-Nung (Vivian) Chen'}]",393.0,"{'bibtex': '@Inproceedings{Goo2018SlotGatedMF,\n author = {Chih-Wen Goo and Guang-Lai Gao and Yun-Kai Hsu and Chih-Li Huo and Tsung-Chieh Chen and Keng-Wei Hsu and Yun-Nung (Vivian) Chen},\n pages = {753-757},\n title = {Slot-Gated Modeling for Joint Slot Filling and Intent Prediction},\n year = {2018}\n}\n'}",,{'pages': '753-757'},13.0,Slot-Gated Modeling for Joint Slot Filling and Intent Prediction,2018.0
1723,9057e20e9806f9b14a1d29d57cf7553f7cd56838,,"[{'authorId': '1759817', 'name': 'A. Schirmer'}, {'authorId': '46306086', 'name': 'R. Adolphs'}]",250.0,"{'bibtex': '@Article{Schirmer2017EmotionPF,\n author = {A. Schirmer and R. Adolphs},\n journal = {Trends in Cognitive Sciences},\n pages = {216-228},\n title = {Emotion Perception from Face, Voice, and Touch: Comparisons and Convergence},\n volume = {21},\n year = {2017}\n}\n'}",,"{'volume': '21', 'pages': '216-228', 'name': 'Trends in Cognitive Sciences'}",112.0,"Emotion Perception from Face, Voice, and Touch: Comparisons and Convergence",2017.0
1724,90782440650fddd909a8168f63d596c8cd23832d,"Currently, most cognitive scientists view the brain as a general-purpose computer and the processes of mind as software algorithms running on this neural architecture. From this perspective, conscious feelings, like pleasure, play no functional role in controlling human behaviour. This paper proposes that such computational theories are based on a false premise; namely, that the external world is full of light, sounds, smells, and tastes that can be detected through our senses. An alternative viewpoint, evolutionary functionalism, considers the world to be composed of energy/matter and views conscious experiences, like pleasure, as evolved emergent properties of biological tissue. From this perspective, natural selection has favoured conscious experiences that serve as evaluations of (feelings), or discriminations among (sensations) those aspects of the physical and social world that are biologically relevant. Over generations, it is the functional usefulness of these emergent properties that has shaped the neural architecture that underlies them.","[{'authorId': '49668269', 'name': 'V. Johnston'}]",150.0,"{'bibtex': '@Article{Johnston2003TheOA,\n author = {V. Johnston},\n journal = {Cognition and Emotion},\n pages = {167 - 179},\n title = {The origin and function of pleasure},\n volume = {17},\n year = {2003}\n}\n'}",,"{'volume': '17', 'pages': '167 - 179', 'name': 'Cognition and Emotion'}",16.0,The origin and function of pleasure,2003.0
1725,90a250ccd19c68ff4b141762ee978fe8752975b5,,"[{'authorId': '2422863', 'name': 'Z. Sabeur'}, {'authorId': '120205775', 'name': 'N. Doulamis'}, {'authorId': '48970420', 'name': 'L. Middleton'}, {'authorId': '1403994536', 'name': 'Banafshe Arbab-Zavar'}, {'authorId': '3132568', 'name': 'Gianluca Correndo'}, {'authorId': '2046146', 'name': 'A. Amditis'}]",10.0,"{'bibtex': '@Inproceedings{Sabeur2015MultimodalCV,\n author = {Z. Sabeur and N. Doulamis and L. Middleton and Banafshe Arbab-Zavar and Gianluca Correndo and A. Amditis},\n pages = {162-173},\n title = {Multi-modal Computer Vision for the Detection of Multi-scale Crowd Physical Motions and Behavior in Confined Spaces},\n year = {2015}\n}\n'}",,{'pages': '162-173'},19.0,Multi-modal Computer Vision for the Detection of Multi-scale Crowd Physical Motions and Behavior in Confined Spaces,2015.0
1726,90db6006ddcb063af6fd42c70c7e7f91bbe61301,"For members of a group negatively stereotyped in a domain, making mistakes can aggravate the influence of stereotype threat because negative stereotypes often blame target individuals and attribute the outcome to their lack of ability. Virtual agents offering real-time error feedback may influence performance under stereotype threat by shaping the performers' attributional perception of errors they commit. We explored this possibility with female drivers, considering the prevalence of the ""women-are-bad-drivers"" stereotype. Specifically, we investigated how in-vehicle voice agents offering error feedback based on responsibility attribution (internal vs. external) and outcome attribution (ability vs. effort) influence female drivers' performance under stereotype threat. In addressing this question, we conducted an experiment in a virtual driving simulation environment that provided moment-to-moment error feedback messages. Participants performed a challenging driving task and made mistakes preprogrammed to occur. Results showed that the agent's error feedback with outcome attribution moderated the stereotype threat effect on driving performance. Participants under stereotype threat had a smaller number of collisions when the errors were attributed to effort than to ability. In addition, outcome attribution feedback moderated the effect of responsibility attribution on driving performance. Implications of these findings are discussed.","[{'authorId': '3136128', 'name': 'Yeon Kyoung Joo'}, {'authorId': '1401571040', 'name': 'Roselyn J. Lee-Won'}]",3.0,"{'bibtex': ""@Article{Joo2016AnAI,\n author = {Yeon Kyoung Joo and Roselyn J. Lee-Won},\n journal = {Cyberpsychology, behavior and social networking},\n pages = {\n          615-620\n        },\n title = {An Agent-Based Intervention to Assist Drivers Under Stereotype Threat: Effects of In-Vehicle Agents' Attributional Error Feedback},\n volume = {19 10},\n year = {2016}\n}\n""}",,"{'volume': '19 10', 'pages': '\n          615-620\n        ', 'name': 'Cyberpsychology, behavior and social networking'}",41.0,An Agent-Based Intervention to Assist Drivers Under Stereotype Threat: Effects of In-Vehicle Agents' Attributional Error Feedback,2016.0
1727,90edb80e8687b1ee18883c8795aa1e5767afe581,"This special track will serve as a forum to unite researchers from the interdisciplinary arena that encompasses computer science, engineering, HCI, psychology, and education to exchange ideas, frameworks, methods, and tools relating to affective computing. Although the last decade has been ripe with theory and applications relevant to AC, these advances are accompanied by a new set of challenges. By providing a framework to discuss and evaluate novel research, we hope to leverage recent advances to speed-up future research in this area.","[{'authorId': '2200019872', 'name': ""Sidney K. D'Mello""}, {'authorId': '2264152559', 'name': 'Rafael A. Calvo'}]",1498.0,"{'bibtex': ""@Inproceedings{D'Mello2011SpecialTO,\n author = {Sidney K. D'Mello and Rafael A. Calvo},\n title = {Special Track on Affective Computing},\n year = {2011}\n}\n""}",,"{'volume': '', 'name': ''}",0.0,Special Track on Affective Computing,2011.0
1728,910bfa42d0c43117735b2307356d1145957a0f99,,"[{'authorId': '2081134241', 'name': 'C. Lallemand'}, {'authorId': '2073604354', 'name': 'Vincent Koenig'}, {'authorId': '2034260', 'name': 'G. Gronier'}, {'authorId': '46776514', 'name': 'Romain Martin'}]",77.0,"{'bibtex': '@Article{Lallemand2015CréationEV,\n author = {C. Lallemand and Vincent Koenig and G. Gronier and Romain Martin},\n journal = {European Review of Applied Psychology-revue Europeenne De Psychologie Appliquee},\n pages = {239-252},\n title = {Création et validation d’une version française du questionnaire AttrakDiff pour l’évaluation de l’expérience utilisateur des systèmes interactifs},\n volume = {65},\n year = {2015}\n}\n'}",,"{'volume': '65', 'pages': '239-252', 'name': 'European Review of Applied Psychology-revue Europeenne De Psychologie Appliquee'}",68.0,Création et validation d’une version française du questionnaire AttrakDiff pour l’évaluation de l’expérience utilisateur des systèmes interactifs,2015.0
1729,9125fc9a8e2748a4b92db8f133238a599de5fbb3,"The Uncanny Valley Hypothesis (Mori, 1970) predicts that perceptual difficulty distinguishing between a humanlike object (e.g., lifelike prosthetic hand, mannequin) and its human counterpart evokes negative affect. Research has focused on affect, with inconsistent results, but little is known about how objects along the hypothesis’ dimension of human likeness (DHL) are actually perceived. This study used morph continua based on human and highly realistic computer-generated (avatar) faces to represent the DHL. Total number and dwell time of fixations to facial features were recorded while participants (N = 60) judged avatar versus human category membership of the faces in a forced choice categorization task. Fixation and dwell data confirmed the face feature hierarchy (eyes, nose, and mouth in this order of importance) across the DHL. There were no further findings for fixation. A change in the relative importance of these features was found for dwell time, with greater preferential processing of eyes and mouth of categorically ambiguous faces compared with unambiguous avatar faces. There were no significant differences between ambiguous and human faces. These findings applied for men and women, though women generally dwelled more on the eyes to the disadvantage of the nose. The mouth was unaffected by gender. In summary, the relative importance of facial features changed on the DHL’s non-human side as a function of categorization ambiguity. This change was indicated by dwell time only, suggesting greater depth of perceptual processing of the eyes and mouth of ambiguous faces compared with these features in unambiguous avatar faces.","[{'authorId': '3442607', 'name': 'M. Cheetham'}, {'authorId': '144678539', 'name': 'I. Pavlović'}, {'authorId': '2058668434', 'name': 'Nicola Jordan'}, {'authorId': '40529088', 'name': 'Pascal Suter'}, {'authorId': '5703561', 'name': 'L. Jancke'}]",61.0,"{'bibtex': '@Article{Cheetham2013CategoryPA,\n author = {M. Cheetham and I. Pavlović and Nicola Jordan and Pascal Suter and L. Jancke},\n journal = {Frontiers in Psychology},\n title = {Category Processing and the human likeness dimension of the Uncanny Valley Hypothesis: Eye-Tracking Data},\n volume = {4},\n year = {2013}\n}\n'}",,"{'volume': '4', 'name': 'Frontiers in Psychology'}",116.0,Category Processing and the human likeness dimension of the Uncanny Valley Hypothesis: Eye-Tracking Data,2013.0
1730,913c210a76ebd42f32f1587ffedf58a4d4e589b7,"There is disagreement in the literature about the exact nature of the phenomenon of empathy. There are emotional, cognitive, and conditioning views, applying in varying degrees across species. An adequate description of the ultimate and proximate mechanism can integrate these views. Proximately, the perception of an object's state activates the subject's corresponding representations, which in turn activate somatic and autonomic responses. This mechanism supports basic behaviors (e.g., alarm, social facilitation, vicariousness of emotions, mother-infant responsiveness, and the modeling of competitors and predators) that are crucial for the reproductive success of animals living in groups. The Perception-Action Model (PAM), together with an understanding of how representations change with experience, can explain the major empirical effects in the literature (similarity, familiarity, past experience, explicit teaching, and salience). It can also predict a variety of empathy disorders. The interaction between the PAM and prefrontal functioning can also explain different levels of empathy across species and age groups. This view can advance our evolutionary understanding of empathy beyond inclusive fitness and reciprocal altruism and can explain different levels of empathy across individuals, species, stages of development, and situations.","[{'authorId': '5011255', 'name': 'S. Preston'}, {'authorId': '3533326', 'name': 'F. D. de Waal'}]",3384.0,"{'bibtex': '@Article{Preston2001EmpathyIU,\n author = {S. Preston and F. D. de Waal},\n journal = {The Behavioral and brain sciences},\n pages = {\n          1-20; discussion 20-71\n        },\n title = {Empathy: Its ultimate and proximate bases.},\n volume = {25 1},\n year = {2001}\n}\n'}",,"{'volume': '25 1', 'pages': '\n          1-20; discussion 20-71\n        ', 'name': 'The Behavioral and brain sciences'}",680.0,Empathy: Its ultimate and proximate bases.,2001.0
1732,913ea1c8295893d0a54e3703ed5571122afeb445,"Researches in Biometrics (B. M.) and Kansei engineering (K. E.) commonly utilize biological or physiological data as their major means. Therefore, it seems plausible to expect meaningful cooperation between B. M. and K. E. Aiming at exploring a possibility to materialize such an expectation, in the 1st part of the paper, we first discuss some fundamental issues of K. E. including the characteristic of K.E. as well as the definition of the term Kansei itself. In the 2nd part of the paper, the issue of human internal information such as emotions which reside the inside of human body, and hence, are unmeasurable from the outside are discussed as a basic problem lying over both B. M. and K.E. In particular, problems how to acquire human internal information are discussed through several examples.","[{'authorId': '3163017', 'name': 'T. Nagashima'}]",2.0,"{'bibtex': '@Article{Nagashima2013TheFO,\n author = {T. Nagashima},\n journal = {2013 International Conference on Biometrics and Kansei Engineering},\n pages = {332-335},\n title = {The Fundamentals of Kansei Engineering and a Methodological Problem: Human Internal Information and Empathy},\n year = {2013}\n}\n'}",,"{'pages': '332-335', 'name': '2013 International Conference on Biometrics and Kansei Engineering'}",12.0,The Fundamentals of Kansei Engineering and a Methodological Problem: Human Internal Information and Empathy,2013.0
1733,9147fffbcdd4c0f0a651c91e038d9b3e7df4fc21,"Cross-lagged panel analysis is an analytical strategy used to describe reciprocal relationships, or directional influences, between variables over time. Cross-lagged panel models (CLPM), also referred to as cross-lagged path models and cross-lagged regression models, are estimated using panel data, or longitudinal data where each observation or person is recorded at multiple points in time. The models are considered ""crossed"" because they estimate relationships from one variable to another and vice-versa. They are considered ""lagged"" because they estimate relationships between variables across different time points. Taken together, cross-lagged panel models estimate the directional influence variables have on each other over time. The primary goal of cross-lagged panel models is to examine the causal influences between variables. In essence, cross-lagged panel analysis compares the relationship between variable X at Time 1 and variable Y at Time 2 with the relationship between variable Y at Time 1 and X at Time 2. It is widely used to examine the stability and relationships between variables over time to better understand how variables influence each other over time. This entry discusses cross-lagged panel analysis, an analytical strategy used in longitudinal communication research. It describes its rationales and origins in research. It also describes modern path-analytic approaches to cross-lagged panel analysis. Finally, this entry discusses some important assumptions and issues with cross-lagged panel analysis.","[{'authorId': '2011035', 'name': 'M. Kearney'}]",165.0,"{'bibtex': '@Inproceedings{Kearney2016CrossLaggedPA,\n author = {M. Kearney},\n title = {Cross-Lagged Panel Analysis},\n year = {2016}\n}\n'}",,,5.0,Cross-Lagged Panel Analysis,2016.0
1734,916f556cae3d8262cfd9abf268b61832210b536d,"Crowd simulation has been an active and important area of research in the field of interactive 3D graphics for several decades. However, only recently has there been an increased focus on evaluating the fidelity of the results with respect to real-world situations. The focus to date has been on analyzing the properties of low-level features such as pedestrian trajectories, or global features such as crowd densities. We propose a new approach based on finding latent Path Patterns in both real and simulated data in order to analyze and compare them. Unsupervised clustering by non-parametric Bayesian inference is used to learn the patterns, which themselves provide a rich visualization of the crowd's behaviour. To this end, we present a new Stochastic Variational Dual Hierarchical Dirichlet Process (SV-DHDP) model. The fidelity of the patterns is then computed with respect to a reference, thus allowing the outputs of different algorithms to be compared with each other and/or with real data accordingly.","[{'authorId': '2149698569', 'name': 'He Wang'}, {'authorId': '2050262', 'name': 'Jan Ondřej'}, {'authorId': '1404017833', 'name': ""C. O'Sullivan""}]",33.0,"{'bibtex': ""@Article{Wang2016PathPA,\n author = {He Wang and Jan Ondřej and C. O'Sullivan},\n journal = {Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},\n title = {Path patterns: analyzing and comparing real and simulated crowds},\n year = {2016}\n}\n""}",,{'name': 'Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games'},54.0,Path patterns: analyzing and comparing real and simulated crowds,2016.0
1735,9174c318999aeea2b1dd30ebfbef0b600ac80d69,"Background Self-criticism is a ubiquitous feature of psychopathology and can be combatted by increasing levels of self-compassion. However, some patients are resistant to self-compassion. Aims To investigate whether the effects of self-identification with virtual bodies within immersive virtual reality could be exploited to increase self-compassion in patients with depression. Method We developed an 8-minute scenario in which 15 patients practised delivering compassion in one virtual body and then experienced receiving it from themselves in another virtual body. Results In an open trial, three repetitions of this scenario led to significant reductions in depression severity and self-criticism, as well as to a significant increase in self-compassion, from baseline to 4-week follow-up. Four patients showed clinically significant improvement. Conclusions The results indicate that interventions using immersive virtual reality may have considerable clinical potential and that further development of these methods preparatory to a controlled trial is now warranted. Declaration of interest None. Copyright and usage © The Royal College of Psychiatrists 2016. This is an open access article distributed under the terms of the Creative Commons Attribution (CC BY) licence.","[{'authorId': '36318701', 'name': 'Caroline J. Falconer'}, {'authorId': '2338435', 'name': 'Aitor Rovira'}, {'authorId': '32572883', 'name': 'John A. King'}, {'authorId': '145962984', 'name': 'P. Gilbert'}, {'authorId': '1705895', 'name': 'Angus Antley'}, {'authorId': '5188198', 'name': 'P. Fearon'}, {'authorId': '38454124', 'name': 'N. Ralph'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '5126407', 'name': 'C. Brewin'}]",185.0,"{'bibtex': '@Article{Falconer2016EmbodyingSW,\n author = {Caroline J. Falconer and Aitor Rovira and John A. King and P. Gilbert and Angus Antley and P. Fearon and N. Ralph and M. Slater and C. Brewin},\n journal = {BJPsych open},\n pages = {74 - 80},\n title = {Embodying self-compassion within virtual reality and its effects on patients with depression},\n volume = {2},\n year = {2016}\n}\n'}",,"{'volume': '2', 'pages': '74 - 80', 'name': 'BJPsych open'}",40.0,Embodying self-compassion within virtual reality and its effects on patients with depression,2016.0
1737,919eebb0308f0cbcc2f30a5f925f404ba6c0baa6,,"[{'authorId': '3329640', 'name': 'A. Elkins'}, {'authorId': '3286035', 'name': 'D. Derrick'}]",93.0,"{'bibtex': '@Article{Elkins2013TheSO,\n author = {A. Elkins and D. Derrick},\n journal = {Group Decision and Negotiation},\n pages = {897-913},\n title = {The Sound of Trust: Voice as a Measurement of Trust During Interactions with Embodied Conversational Agents},\n volume = {22},\n year = {2013}\n}\n'}",,"{'volume': '22', 'pages': '897-913', 'name': 'Group Decision and Negotiation'}",50.0,The Sound of Trust: Voice as a Measurement of Trust During Interactions with Embodied Conversational Agents,2013.0
1738,920d960c7ff897fce29f8e828537bf440b3b4f6e,"Complex Systems are made up of numerous interacting sub-components. Non-linear interactions of these components or agents give rise to emergent behavior observable at the global scale. Agent-based modeling and simulation is a proven paradigm which has previously been used for effective computational modeling of complex systems in various domains. Because of its popular use across different scientific domains, research in agent-based modeling has primarily been vertical in nature. The goal of this manuscript is to provide a single hands-on guide to developing cognitive agent-based models for the exploration of emergence across various types of complex systems. We present practical ideas and examples for researchers and practitioners for the building of agent-based models using a horizontal approach - applications are demonstrated in a number of exciting domains as diverse as wireless sensors networks, peer-to-peer networks, complex social systems, research networks, epidemiological HIV","[{'authorId': '1795560', 'name': 'M. Niazi'}, {'authorId': '144664815', 'name': 'A. Hussain'}]",45.0,"{'bibtex': '@Inproceedings{Niazi2012CognitiveAC,\n author = {M. Niazi and A. Hussain},\n title = {Cognitive Agent-based Computing-I: A Unified Framework for Modeling Complex Adaptive Systems using Agent-based & Complex Network-based Methods},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Cognitive Agent-based Computing-I: A Unified Framework for Modeling Complex Adaptive Systems using Agent-based & Complex Network-based Methods,2012.0
1739,92777f9a04ca5d55bfbdc409132d94be57d42417,,"[{'authorId': '2997504', 'name': 'D. Reidsma'}, {'authorId': '3251916', 'name': 'H. V. Welbergen'}, {'authorId': '1754666', 'name': 'R. Poppe'}, {'authorId': '2074422254', 'name': 'Pieter Bos'}, {'authorId': '144483472', 'name': 'A. Nijholt'}]",37.0,"{'bibtex': '@Inproceedings{Reidsma2006TowardsBD,\n author = {D. Reidsma and H. V. Welbergen and R. Poppe and Pieter Bos and A. Nijholt},\n pages = {1-12},\n title = {Towards Bi-directional Dancing Interaction},\n year = {2006}\n}\n'}",,{'pages': '1-12'},20.0,Towards Bi-directional Dancing Interaction,2006.0
1740,9295d40353cbe4c56bbcf813abefb028b4a5ed65,"Speech intelligibility can be improved by adding lip image and facial image to speech signal. Thus the lip image synthesis plays an important role to realize a natural human-like face of computer agents. Moreover the synthesized lip movement images can compensate lack of auditory information for hearing impaired people. We propose a novel lip movement synthesis method based on mapping from input speech based on Hidden Markov Model (HMM). This paper compares the HMM-based method and a conventional method using vector quantization (VQ). In the experiment, error and time differential error between synthesized lip movement images and original ones are used for evaluation. The result shows that the error of the HMM based method is 8.7% smaller than that of the VQ-based method. Moreover, the HMM-based method reduces time differential error by 32% than the VQ's. The result also shows that the errors are mostly caused by phoneme /h/ and /Q/. Since lip shapes of those phonemes are strongly dependent on succeeding phoneme, the context dependent synthesis on the HMM-based method is applied to reduce the error. The improved HMM-based method realizes reduction of the error (differential error) by 10.5% (11%) compared with the original HMM-based method.","[{'authorId': '2290237', 'name': 'E. Yamamoto'}, {'authorId': '145223960', 'name': 'Satoshi Nakamura'}, {'authorId': '9243990', 'name': 'K. Shikano'}]",136.0,"{'bibtex': '@Article{Yamamoto1998LipMS,\n author = {E. Yamamoto and Satoshi Nakamura and K. Shikano},\n journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},\n pages = {154-159},\n title = {Lip movement synthesis from speech based on hidden Markov models},\n year = {1998}\n}\n'}",,"{'pages': '154-159', 'name': 'Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition'}",23.0,Lip movement synthesis from speech based on hidden Markov models,1998.0
1741,92b648eabe5a9d95fa2389b4a93472322d43309d,,"[{'authorId': '153068102', 'name': 'Joanna Hale'}, {'authorId': '145213359', 'name': 'A. Hamilton'}]",49.0,"{'bibtex': '@Article{Hale2016TestingTR,\n author = {Joanna Hale and A. Hamilton},\n journal = {Scientific Reports},\n title = {Testing the relationship between mimicry, trust and rapport in virtual reality conversations},\n volume = {6},\n year = {2016}\n}\n'}",,"{'volume': '6', 'name': 'Scientific Reports'}",68.0,"Testing the relationship between mimicry, trust and rapport in virtual reality conversations",2016.0
1742,92b701bec2a86d0c3f5655e8e9116e87608f5341,"More than another friendly face, Rea knows how to have a conversation with living, breathing human users with a wink, a nod, and a sidelong glance. A nimals and humans all manifest social qualities and skills. Dogs recognize dominance and submission , stand corrected by their superiors, demonstrate consistent personalities, and so forth. On the other hand, only humans communicate through language and carry on conversations with one another. The skills involved in human conversation have developed in such a way as to exploit all the special characteristics of the human body. We make complex repre-sentational gestures with our prehensile hands, gaze away and toward one another out of the corners of our centrally set eyes, and use the pitch and melody of our flexible voices to emphasize and clarify what we are saying. Perhaps because conversation is so defining of humanness and human interaction, the metaphor of face-to-face conversation has been applied to human-computer interface design for quite some time. One of the early arguments for the utility of this metaphor pointed to the application of the features of face-to-face conversation in human-computer interaction, including mixed initiative, nonverbal communication , sense of presence, and the rules involved in transferring control [9]. However, although these features have gained widespread recognition, human-computer conversation has only recently become more than a metaphor. That is, only recently have human-computer interface designers taken the metaphor seriously enough to attempt to design a computer that could hold up its end of the conversation with a human user. Here, I describe some of the features of human-human conversation being implemented in this new genre of embodied conversational agent, exploring a notable embodied conversational agent—named Rea—based on these features. Because conversation is such a primary skill for humans and learned so early in life (practiced, in fact, between infants and their mothers taking turns cooing and burbling EMBODIED CONVERSATIONAL INTERFACE AGENTS","[{'authorId': '145431806', 'name': 'Justine Cassell'}]",358.0,"{'bibtex': '@Article{Cassell2000EmbodiedCI,\n author = {Justine Cassell},\n journal = {Commun. ACM},\n pages = {70-78},\n title = {Embodied conversational interface agents},\n volume = {43},\n year = {2000}\n}\n'}",,"{'volume': '43', 'pages': '70-78', 'name': 'Commun. ACM'}",16.0,Embodied conversational interface agents,2000.0
1743,92bed1ffb5d5eb17a5d45d0e8f784c46caaa693c,,"[{'authorId': '1769251', 'name': 'A. Graesser'}, {'authorId': '2772086', 'name': 'Shulan Lu'}, {'authorId': '144874959', 'name': 'G. T. Jackson'}, {'authorId': '40490729', 'name': 'Heather H. Mitchell'}, {'authorId': '2056465739', 'name': 'M. Ventura'}, {'authorId': '1731622', 'name': 'A. Olney'}, {'authorId': '2073332', 'name': 'M. Louwerse'}]",477.0,"{'bibtex': '@Article{Graesser2004AutoTutorAT,\n author = {A. Graesser and Shulan Lu and G. T. Jackson and Heather H. Mitchell and M. Ventura and A. Olney and M. Louwerse},\n journal = {Behavior Research Methods, Instruments, & Computers},\n pages = {180-192},\n title = {AutoTutor: A tutor with dialogue in natural language},\n volume = {36},\n year = {2004}\n}\n'}",,"{'volume': '36', 'pages': '180-192', 'name': 'Behavior Research Methods, Instruments, & Computers'}",83.0,AutoTutor: A tutor with dialogue in natural language,2004.0
1744,92dfe7e03f007a4cbfb1af57b79e31660ad7525f,,"[{'authorId': '2153514729', 'name': 'Yan Dong'}, {'authorId': '2950532', 'name': 'Yongna Li'}, {'authorId': '2113205902', 'name': 'Tingting Sun'}]",12.0,"{'bibtex': ""@Article{Dong2014HappyFC,\n author = {Yan Dong and Yongna Li and Tingting Sun},\n journal = {Comput. Secur.},\n pages = {85-93},\n title = {Happy faces considered trustworthy irrespective of perceiver's mood: Challenges to the mood congruency effect},\n volume = {47},\n year = {2014}\n}\n""}",,"{'volume': '47', 'pages': '85-93', 'name': 'Comput. Secur.'}",48.0,Happy faces considered trustworthy irrespective of perceiver's mood: Challenges to the mood congruency effect,2014.0
1746,92ed769ba5558b7ff4acb2531bdb764b347790ba,"Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.","[{'authorId': '1703046', 'name': 'R. Passonneau'}]",166.0,"{'bibtex': '@Inproceedings{Passonneau2006MeasuringAO,\n author = {R. Passonneau},\n pages = {831-836},\n title = {Measuring Agreement on Set-valued Items (MASI) for Semantic and Pragmatic Annotation},\n year = {2006}\n}\n'}",,{'pages': '831-836'},16.0,Measuring Agreement on Set-valued Items (MASI) for Semantic and Pragmatic Annotation,2006.0
1747,92f33180db3a2c98cc2bde77d3bfa8b329be4692,"This paper explores new authoring paradigms and computer-assisted authoring tools for free-form interactive narratives. We present a new design formalism, Interactive Behavior Trees (IBT's), which decouples the monitoring of user input, the narrative, and how the user may influence the story outcome. We introduce automation tools for IBT's, to help the author detect and automatically resolve inconsistencies in the authored narrative, or conflicting user interactions that may hinder story progression. We compare IBT's to traditional story graph representations and show that our formalism better scales with the number of story arcs, and the degree and granularity of user input. The authoring time is further reduced with the help of automation, and errors are completely avoided. Our approach enables content creators to easily author complex, branching narratives with multiple story arcs in a modular, extensible fashion while empowering players with the agency to freely interact with the characters in the story and the world they inhabit.","[{'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '144973887', 'name': 'Jessica Falk'}, {'authorId': '3071164', 'name': 'Fabio Zünd'}, {'authorId': '145658471', 'name': 'Marcel Marti'}, {'authorId': '1693475', 'name': 'R. Sumner'}, {'authorId': '2257153235', 'name': 'M. Gross'}]",50.0,"{'bibtex': '@Article{Kapadia2015ComputerassistedAO,\n author = {Mubbasir Kapadia and Jessica Falk and Fabio Zünd and Marcel Marti and R. Sumner and M. Gross},\n journal = {Proceedings of the 19th Symposium on Interactive 3D Graphics and Games},\n title = {Computer-assisted authoring of interactive narratives},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 19th Symposium on Interactive 3D Graphics and Games'},27.0,Computer-assisted authoring of interactive narratives,2015.0
1748,931ec58dfb5b1928622274fdc5041c71f0e16d00,"Historically, at least 3 methodological problems have dogged experimental social psychology: the experimental control-mundane realism trade-off, lack of replication, and unrepresentative sampling. We argue that immersive virtual environment technology (IVET) can help ameliorate, if not solve, these methodological problems and, thus, holds promise as a new social psychological research tool. In this article, we first present an overview of IVET and review IVET-based research within psychology and other fields. Next, we propose a general model of social influence within immersive virtual environments and present some preliminary findings regarding its utility for social psychology. Finally, we present a new paradigm for experimental social psychology that may enable researchers to unravel the very fabric of social interaction.","[{'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '2386187', 'name': 'J. Loomis'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2396884', 'name': 'Kimberly R. Swinth'}, {'authorId': '32047738', 'name': 'Crystal L. Hoyt'}, {'authorId': '1737161', 'name': 'J. Bailenson'}]",919.0,"{'bibtex': '@Article{Blascovich2002TARGETAI,\n author = {J. Blascovich and J. Loomis and A. Beall and Kimberly R. Swinth and Crystal L. Hoyt and J. Bailenson},\n journal = {Psychological Inquiry},\n pages = {103 - 124},\n title = {TARGET ARTICLE: Immersive Virtual Environment Technology as a Methodological Tool for Social Psychology},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '103 - 124', 'name': 'Psychological Inquiry'}",103.0,TARGET ARTICLE: Immersive Virtual Environment Technology as a Methodological Tool for Social Psychology,2002.0
1751,932b227ec4baade553307e8ed2f07f058c6b2b5a,"Joint attention (JA) and spontaneous facial mimicry (SFM) are fundamental processes in social interactions, and they are closely related to empathic abilities. When tested independently, both of these processes have been usually observed to be atypical in individuals with autism spectrum conditions (ASC). However, it is not known how these processes interact with each other in relation to autistic traits. This study addresses this question by testing the impact of JA on SFM of happy faces using a truly interactive paradigm. Sixty‐two neurotypical participants engaged in gaze‐based social interaction with an anthropomorphic, gaze‐contingent virtual agent. The agent either established JA by initiating eye contact or looked away, before looking at an object and expressing happiness or disgust. Eye tracking was used to make the agent's gaze behavior and facial actions contingent to the participants' gaze. SFM of happy expressions was measured by Electromyography (EMG) recording over the Zygomaticus Major muscle. Results showed that JA augments SFM in individuals with low compared with high autistic traits. These findings are in line with reports of reduced impact of JA on action imitation in individuals with ASC. Moreover, they suggest that investigating atypical interactions between empathic processes, instead of testing these processes individually, might be crucial to understanding the nature of social deficits in autism. Autism Res 2016, 9: 781–789. © 2015 The Authors Autism Research published by Wiley Periodicals, Inc. on behalf of International Society for Autism Research","[{'authorId': '50377075', 'name': 'J. Neufeld'}, {'authorId': '3093585', 'name': 'C. Ioannou'}, {'authorId': '15684638', 'name': 'S. Korb'}, {'authorId': '2127424', 'name': 'L. Schilbach'}, {'authorId': '3102450', 'name': 'B. Chakrabarti'}]",27.0,"{'bibtex': '@Article{Neufeld2015SpontaneousFM,\n author = {J. Neufeld and C. Ioannou and S. Korb and L. Schilbach and B. Chakrabarti},\n journal = {Autism Research},\n pages = {781 - 789},\n title = {Spontaneous Facial Mimicry is Modulated by Joint Attention and Autistic Traits},\n volume = {9},\n year = {2015}\n}\n'}",,"{'volume': '9', 'pages': '781 - 789', 'name': 'Autism Research'}",67.0,Spontaneous Facial Mimicry is Modulated by Joint Attention and Autistic Traits,2015.0
1752,938658b6d86b74be6a22741a1ca24fded5c2bbf4,"Discusses some of the key issues that must be addressed in creating virtual humans, or androids. As a first step, we overview the issues and available tools in three key areas of virtual human research: face-to-face conversation, emotions and personality, and human figure animation. Assembling a virtual human is still a daunting task, but the building blocks are getting bigger and better every day.","[{'authorId': '69014762', 'name': 'J. Gratch'}, {'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '2920582', 'name': 'E. Petajan'}, {'authorId': '1699200', 'name': 'N. Badler'}]",423.0,"{'bibtex': '@Article{Gratch2002CreatingIV,\n author = {J. Gratch and J. Rickel and E. André and Justine Cassell and E. Petajan and N. Badler},\n journal = {IEEE Intell. Syst.},\n pages = {54-63},\n title = {Creating Interactive Virtual Humans: Some Assembly Required},\n volume = {17},\n year = {2002}\n}\n'}",,"{'volume': '17', 'pages': '54-63', 'name': 'IEEE Intell. Syst.'}",53.0,Creating Interactive Virtual Humans: Some Assembly Required,2002.0
1753,93b0dc0311845a5b649c9f9764a3f55392291204,,"[{'authorId': '49884103', 'name': 'M. Javaid'}, {'authorId': '38272234', 'name': 'Abid Haleem'}]",85.0,"{'bibtex': '@Article{Javaid2020VirtualRA,\n author = {M. Javaid and Abid Haleem},\n journal = {Clinical Epidemiology and Global Health},\n title = {Virtual reality applications toward medical field},\n year = {2020}\n}\n'}",,{'name': 'Clinical Epidemiology and Global Health'},82.0,Virtual reality applications toward medical field,2020.0
1754,93dcf21519bf58e135e66fef5376376d6e2af371,,"[{'authorId': '153068102', 'name': 'Joanna Hale'}, {'authorId': '145213359', 'name': 'A. Hamilton'}]",51.0,"{'bibtex': '@Article{Hale2016CognitiveMF,\n author = {Joanna Hale and A. Hamilton},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {106-123},\n title = {Cognitive mechanisms for responding to mimicry from others},\n volume = {63},\n year = {2016}\n}\n'}",,"{'volume': '63', 'pages': '106-123', 'name': 'Neuroscience & Biobehavioral Reviews'}",187.0,Cognitive mechanisms for responding to mimicry from others,2016.0
1755,9401a600bf069852d9df1c2fb4d9f969b5e8df0e,"We explored the association between users' social anxiety and the interactional fidelity of an agent (also referred to as a virtual human), specifically addressing whether the contingency of agents' nonverbal feedback affects the relationship between users' social anxiety and their feelings of rapport, performance, or judgment on interaction partners. This subject was examined across four experimental conditions where participants interacted with three different types of agents and a real human. The three types of agents included the Non-Contingent Agent, the Responsive Agent (opposite to the Non-Contingent Agent), and the Mediated Agent (controlled by a real human). The results indicated that people having greater social anxiety would feel less rapport and show worse performance while feeling more embarrassment if they experience the untimely feedback of the Non-Contingent Agent. The results also showed people having more anxiety would trust real humans less as their interaction partners. We discuss the implication of this relationship between social anxiety in a human subject and the interactional fidelity of an agent on the design of virtual characters for social skills training and therapy.","[{'authorId': '34728215', 'name': 'Sin-Hwa Kang'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '24279134', 'name': 'J. Watt'}]",49.0,"{'bibtex': ""@Inproceedings{Kang2008DoesTC,\n author = {Sin-Hwa Kang and J. Gratch and Ning Wang and J. Watt},\n pages = {120-127},\n title = {Does the contingency of agents' nonverbal feedback affect users' social anxiety?},\n year = {2008}\n}\n""}",,{'pages': '120-127'},29.0,Does the contingency of agents' nonverbal feedback affect users' social anxiety?,2008.0
1756,9435fa2f5fa49d5193e775ff46dfcf4dd081756a,,"[{'authorId': '40123315', 'name': 'Angelo Cafaro'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}, {'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '1678537', 'name': 'D. Heylen'}, {'authorId': '145538307', 'name': 'K. R. Jóhannsdóttir'}, {'authorId': '2242296', 'name': 'Gunnar Steinn Valgarðsson'}]",96.0,"{'bibtex': ""@Inproceedings{Cafaro2012FirstIU,\n author = {Angelo Cafaro and H. Vilhjálmsson and T. Bickmore and D. Heylen and K. R. Jóhannsdóttir and Gunnar Steinn Valgarðsson},\n pages = {67-80},\n title = {First Impressions: Users' Judgments of Virtual Agents' Personality and Interpersonal Attitude in First Encounters},\n year = {2012}\n}\n""}",,{'pages': '67-80'},41.0,First Impressions: Users' Judgments of Virtual Agents' Personality and Interpersonal Attitude in First Encounters,2012.0
1759,9447b40e63152e8951cd32e22ee09dd0b838a5b3,"When people interact with communication robots in daily life, their attitudes and emotions toward the robots affect their behavior. From the perspective of robotics design, we need to investigate the influences of these attitudes and emotions on human-robot interaction. This paper reports our empirical study on the relationships between people's attitudes and emotions, and their behavior toward a robot. In particular, we focused on negative attitudes, anxiety, and communication avoidance behavior, which have important implications for robotics design. For this purpose, we used two psychological scales that we had developed: negative attitudes toward robots scale (NARS) and robot anxiety scale (RAS). In the experiment, subjects and a humanoid robot are engaged in simple interactions including scenes of meeting, greeting, self-disclosure, and physical contact. Experimental results indicated that there is a relationship between negative attitudes and emotions, and communication avoidance behavior. A gender effect was also suggested.","[{'authorId': '1768404', 'name': 'T. Nomura'}, {'authorId': '48309591', 'name': 'T. Kanda'}, {'authorId': '2108797118', 'name': 'Tomohiro Suzuki'}, {'authorId': '2110051673', 'name': 'Kiyotaka Kato'}]",297.0,"{'bibtex': '@Article{Nomura2008PredictionOH,\n author = {T. Nomura and T. Kanda and Tomohiro Suzuki and Kiyotaka Kato},\n journal = {IEEE Transactions on Robotics},\n pages = {442-451},\n title = {Prediction of Human Behavior in Human--Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes Toward Robots},\n volume = {24},\n year = {2008}\n}\n'}",,"{'volume': '24', 'pages': '442-451', 'name': 'IEEE Transactions on Robotics'}",30.0,Prediction of Human Behavior in Human--Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes Toward Robots,2008.0
1760,9471f2a81d707028bcfc83d3d1c97f89e4fc8051,"A narrative relies on the imperfect knowledge of the user to create interactions between the characters that are ultimately used as a plot device to drive the narrative. This motivates our exploration of ways to encode this information, provides means for a user to both query and influence the knowledge, and guides the user based on a model of their experience. We developed PICA: a proactive intelligent conversational agent for interactive narratives that can guide users through such experiences. The underlying knowledge base is designed using a sub-symbolic architecture, which encodes belief models for multiple users and autonomous agents in addition to the actual story knowledge. We also developed a discourse module using Behavior Trees to intuitively design the proactive and reactive capabilities of PICA. We compare our approach to neural networks and symbolic knowledge bases and demonstrate its functionality.","[{'authorId': '144973887', 'name': 'Jessica Falk'}, {'authorId': '2634383', 'name': 'Steven Poulakos'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '1693475', 'name': 'R. Sumner'}]",10.0,"{'bibtex': '@Article{Falk2018PICAPI,\n author = {Jessica Falk and Steven Poulakos and Mubbasir Kapadia and R. Sumner},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {PICA: Proactive Intelligent Conversational Agent for Interactive Narratives},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},40.0,PICA: Proactive Intelligent Conversational Agent for Interactive Narratives,2018.0
1761,9481b33f73c41e74da91197a5057cf1c0d5d087b,"Parallel imaging in the form of multiband radiofrequency excitation, together with reduced k‐space coverage in the phase‐encode direction, was applied to human gradient echo functional MRI at 7 T for increased volumetric coverage and concurrent high spatial and temporal resolution. Echo planar imaging with simultaneous acquisition of four coronal slices separated by 44mm and simultaneous 4‐fold phase‐encoding undersampling, resulting in 16‐fold acceleration and up to 16‐fold maximal aliasing, was investigated. Task/stimulus‐induced signal changes and temporal signal behavior under basal conditions were comparable for multiband and standard single‐band excitation and longer pulse repetition times. Robust, whole‐brain functional mapping at 7 T, with 2 × 2 × 2mm3 (pulse repetition time 1.25 sec) and 1 × 1 × 2mm3 (pulse repetition time 1.5 sec) resolutions, covering fields of view of 256 × 256 × 176mm3 and 192 × 172 × 176mm3, respectively, was demonstrated with current gradient performance. Magn Reson Med 63:1144–1153, 2010. © 2010 Wiley‐Liss, Inc.","[{'authorId': '3292782', 'name': 'S. Moeller'}, {'authorId': '1761064', 'name': 'E. Yacoub'}, {'authorId': '2063170', 'name': 'C. Olman'}, {'authorId': '2176539', 'name': 'E. Auerbach'}, {'authorId': '48666414', 'name': 'J. Strupp'}, {'authorId': '1782015', 'name': 'N. Harel'}, {'authorId': '1804349', 'name': 'K. Uğurbil'}]",1288.0,"{'bibtex': '@Article{Moeller2010MultibandMG,\n author = {S. Moeller and E. Yacoub and C. Olman and E. Auerbach and J. Strupp and N. Harel and K. Uğurbil},\n journal = {Magnetic Resonance in Medicine},\n title = {Multiband multislice GE‐EPI at 7 tesla, with 16‐fold acceleration using partial parallel imaging with application to high spatial and temporal whole‐brain fMRI},\n volume = {63},\n year = {2010}\n}\n'}",,"{'volume': '63', 'name': 'Magnetic Resonance in Medicine'}",31.0,"Multiband multislice GE‐EPI at 7 tesla, with 16‐fold acceleration using partial parallel imaging with application to high spatial and temporal whole‐brain fMRI",2010.0
1762,94a93babe070e6f84e5728ba64a43ce003104cfa,"More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this paper, we describe FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, FAtiMA Toolkit's library based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.","[{'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '28004507', 'name': 'Manuel Guimarães'}, {'authorId': '145255182', 'name': 'P. A. Santos'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",8.0,"{'bibtex': '@Article{Mascarenhas2021FAtiMAT,\n author = {S. Mascarenhas and Manuel Guimarães and P. A. Santos and João Dias and R. Prada and Ana Paiva},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {FAtiMA Toolkit - Toward an effective and accessible tool for the development of intelligent virtual agents and social robots},\n volume = {abs/2103.03020},\n year = {2021}\n}\n'}","[{'paperId': '5f1774f1aca24046de5f6d9eef53dd354725149f', 'title': 'Non-player character decision-making in computer games'}, {'paperId': '08dea8c4a53dfbc053abdfd1d776447336aec098', 'title': 'Multimodal adaptive empathic agent architecture'}, {'paperId': '15e4491004211facd607c3c431c0c160d667bdf4', 'title': 'Impact of adaptive multimodal empathic behavior on the user interaction'}, {'paperId': 'c62e33d4b9896f0f90aa8af743254baf5f3dd8aa', 'title': 'Towards Explainable Social Agent Authoring tools: A case study on FAtiMA-Toolkit'}, {'paperId': 'df7c6ac457b9f88562bb0f2774fa164b51cfe3e0', 'title': 'EEG Model: Emotional Episode Generation for Social Sharing of Emotions'}, {'paperId': '361a7d5369aec4bbf63f2b3d45b5f1410e9fdb9e', 'title': 'Affective Communication for Socially Assistive Robots (SARs) for Children with Autism Spectrum Disorder: A Systematic Review'}, {'paperId': '3553ec56064a32874c7323333fbbbfe5643e998b', 'title': 'Separable convolutional neural networks for facial expressions recognition'}, {'paperId': '497ace38840d6e31033d536c703408b079d5ea87', 'title': 'Measuring Emotion Intensity: Evaluating Resemblance in Neural Network Facial Animation Controllers and Facial Emotion Corpora'}]","{'name': 'ArXiv', 'volume': 'abs/2103.03020'}",54.0,FAtiMA Toolkit - Toward an effective and accessible tool for the development of intelligent virtual agents and social robots,2021.0
1763,94ba7821609310682ed5a84c50c377d29f197c58,"There is a great need in the Joint Forces to have human to human interpersonal training for skills such as negotiation, leadership, interviewing and cultural training. Virtual environments can be incredible training tools if used properly and used for the correct training application. Virtual environments have already been very successful in training Warfighters how to operate vehicles and weapons systems. At the Institute for Creative Technologies (ICT) we have been exploring a new question: can virtual environments be used to train Warfighters in interpersonal skills such as negotiation, tactical questioning and leadership that are so critical for success in the contemporary operating environment? Using embodied conversational agents to create this type of training system has been one of the goals of the Virtual Humans project at the institute. ICT has a great deal of experience building complex, integrated and immersive training systems that address the human factor needs for training experiences. This paper will address the research, technology and value of developing virtual humans for training environments. This research includes speech recognition, natural language understanding & generation, dialogue management, cognitive agents, emotion modeling, question response managers, speech generation and non-verbal behavior. Also addressed will be the diverse set of training environments we have developed for the system, from single computer laptops to multi-computer immersive displays to real and virtual integrated environments. This paper will also discuss the problems, issues and solutions we encountered while building these systems. The paper will recount subject testing we have performed in these environments and results we have obtained from users. Finally the future of this type of Virtual Humans technology and training applications will be discussed.","[{'authorId': '3181776', 'name': 'Patrick G. Kenny'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1684040', 'name': 'W. Swartout'}, {'authorId': '2251500479', 'name': 'David Traum'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '1794931', 'name': 'D. Piepol'}]",138.0,"{'bibtex': '@Inproceedings{Kenny2007BuildingIV,\n author = {Patrick G. Kenny and Arno Hartholt and J. Gratch and W. Swartout and David Traum and S. Marsella and D. Piepol},\n title = {Building Interactive Virtual Humans for Training Environments},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",38.0,Building Interactive Virtual Humans for Training Environments,2007.0
1764,94d47826a77ff1e40bcb26f71163c353061d1a63,"Did you like that or not? Did the system’s choice of adaptation aggravate you more, or did it bring about an expression of gratefulness? Does this interest you or bore you? Recognition of the effects an action has on a user is a key part of adapting successfully to users; how can machines be enabled to recognize affective expressions such as frustration, interest, anger, or joy? And, what are guidelines for designing their response, especially given that recognition is likely to not be perfect? This talk will present new technologies under development for sensing and responding appropriately to human affective expressions. Current applications include usability feedback, health behavior change, learning companions, and human-robot interaction.","[{'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",289.0,"{'bibtex': '@Article{Picard2000TowardCT,\n author = {Rosalind W. Picard},\n journal = {IBM Syst. J.},\n pages = {705-719},\n title = {Toward computers that recognize and respond to user emotion},\n volume = {39},\n year = {2000}\n}\n'}",,"{'volume': '39', 'pages': '705-719', 'name': 'IBM Syst. J.'}",30.0,Toward computers that recognize and respond to user emotion,2000.0
1765,94f65e53740b20f4527043dfee1cac606002a246,"Do hand gestures play a role in spatial cognition? This paper reviews literature addressing the roles of gestures in (1) expressing spatial information, (2) communicating about spatial information, and (3) thinking about spatial information. Speakers tend to produce gestures when they produce linguistic units that contain spatial information, and they gesture more when talking about spatial topics than when talking about abstract or verbal ones. Thus, gestures are commonly used to express spatial information. Speakers use gestures more in situations when those gestures could contribute to communication, suggesting that they intend those gestures to communicate. Further, gestures influence addressees' comprehension of the speech they accompany, and addressees also detect information that is conveyed uniquely in gestures. Thus, gestures contribute to effective communication of spatial information. Gestures also play multiple roles in thinking about spatial information. There is evidence that gestures activate lexical and spatial representations, promote a focus on spatial information, and facilitate the packaging of spatial information in speech. Finally, some of the observed variation across tasks in gesture production is associated with task differences in demands on spatial cognitive processes, and individual differences in gesture production are associated with individual differences in spatial and verbal abilities. In sum, gestures appear to play multiple roles in spatial cognition. Central challenges for future research include: (1) better specification of the mental representations that give rise to gestures, (2) deeper understanding of the mechanisms by which gestures play a role in spatial thinking, and (3) greater knowledge of the sources of task and individual differences in gesture production.","[{'authorId': '3177547', 'name': 'M. Alibali'}]",296.0,"{'bibtex': '@Article{Alibali2005GestureIS,\n author = {M. Alibali},\n journal = {Spatial Cognition & Computation},\n pages = {307 - 331},\n title = {Gesture in Spatial Cognition: Expressing, Communicating, and Thinking About Spatial Information},\n volume = {5},\n year = {2005}\n}\n'}",,"{'volume': '5', 'pages': '307 - 331', 'name': 'Spatial Cognition & Computation'}",63.0,"Gesture in Spatial Cognition: Expressing, Communicating, and Thinking About Spatial Information",2005.0
1766,94f9869ec0b2294449755d87b20ccda67d789a82,,"[{'authorId': '2955636', 'name': 'Tsung-Yu Liu'}, {'authorId': '7239678', 'name': 'Yu-Ling Chu'}]",526.0,"{'bibtex': '@Article{Liu2010UsingUG,\n author = {Tsung-Yu Liu and Yu-Ling Chu},\n journal = {Comput. Educ.},\n pages = {630-643},\n title = {Using ubiquitous games in an English listening and speaking course: Impact on learning outcomes and motivation},\n volume = {55},\n year = {2010}\n}\n'}",,"{'volume': '55', 'pages': '630-643', 'name': 'Comput. Educ.'}",44.0,Using ubiquitous games in an English listening and speaking course: Impact on learning outcomes and motivation,2010.0
1767,956382ee75f05f1b1403f60d5ae1ebe8f407ebee,"Empathy can be defined as the ability to perceive and understand others' emotional states. Neuropsychological evidence has shown that humans empathize with each other to different degrees depending on factors such as their mood, personality, and social relationships. Although artificial agents have been endowed with features such as affect, personality, and the ability to build social relationships, little attention has been devoted to the role of such features as factors that can modulate their empathic behavior. In this paper, we present and discuss the results of an empirical evaluation of a computational model of empathy which allows a virtual human to exhibit different degrees of empathy. Our model is supported by psychological models of empathy and is applied and evaluated in the context of a conversational agent scenario.","[{'authorId': '3262504', 'name': 'Hana Boukricha'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}, {'authorId': '35078906', 'name': 'M. N. Carminati'}, {'authorId': '7267206', 'name': 'P. Knoeferle'}]",31.0,"{'bibtex': '@Article{Boukricha2013ACM,\n author = {Hana Boukricha and I. Wachsmuth and M. N. Carminati and P. Knoeferle},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {1-6},\n title = {A Computational Model of Empathy: Empirical Evaluation},\n year = {2013}\n}\n'}",,"{'pages': '1-6', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",16.0,A Computational Model of Empathy: Empirical Evaluation,2013.0
1768,959291d5fddcf6d445bca68c1c726a3c7a381f17,,"[{'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1766725', 'name': 'D. Sadek'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",63.0,"{'bibtex': '@Article{Ochs2012AFM,\n author = {M. Ochs and D. Sadek and C. Pelachaud},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {410-440},\n title = {A formal model of emotions for an empathic rational dialog agent},\n volume = {24},\n year = {2012}\n}\n'}",,"{'volume': '24', 'pages': '410-440', 'name': 'Autonomous Agents and Multi-Agent Systems'}",67.0,A formal model of emotions for an empathic rational dialog agent,2012.0
1769,95a6c00a852b56783e9aba6a1c36381e6b643f7c,"Since Darwin’s seminal works, the universality of facial expressions of emotion has remained one of the longest standing debates in the biological and social sciences. Briefly stated, the universality hypothesis claims that all humans communicate six basic internal emotional states (happy, surprise, fear, disgust, anger, and sad) using the same facial movements by virtue of their biological and evolutionary origins [Susskind JM, et al. (2008) Nat Neurosci 11:843–850]. Here, we refute this assumed universality. Using a unique computer graphics platform that combines generative grammars [Chomsky N (1965) MIT Press, Cambridge, MA] with visual perception, we accessed the mind’s eye of 30 Western and Eastern culture individuals and reconstructed their mental representations of the six basic facial expressions of emotion. Cross-cultural comparisons of the mental representations challenge universality on two separate counts. First, whereas Westerners represent each of the six basic emotions with a distinct set of facial movements common to the group, Easterners do not. Second, Easterners represent emotional intensity with distinctive dynamic eye activity. By refuting the long-standing universality hypothesis, our data highlight the powerful influence of culture on shaping basic behaviors once considered biologically hardwired. Consequently, our data open a unique nature–nurture debate across broad fields from evolutionary psychology and social neuroscience to social networking via digital avatars.","[{'authorId': '2143019', 'name': 'Rachael E. Jack'}, {'authorId': '48522841', 'name': 'Oliver G. B. Garrod'}, {'authorId': '2118681985', 'name': 'Hui Yu'}, {'authorId': '3489043', 'name': 'R. Caldara'}, {'authorId': '2287417', 'name': 'P. Schyns'}]",599.0,"{'bibtex': '@Article{Jack2012FacialEO,\n author = {Rachael E. Jack and Oliver G. B. Garrod and Hui Yu and R. Caldara and P. Schyns},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {7241 - 7244},\n title = {Facial expressions of emotion are not culturally universal},\n volume = {109},\n year = {2012}\n}\n'}",,"{'volume': '109', 'pages': '7241 - 7244', 'name': 'Proceedings of the National Academy of Sciences'}",42.0,Facial expressions of emotion are not culturally universal,2012.0
1770,95b803d07c37e8349bd7b1318367d8237c76cbc0,"We present a machine learning technique for driving 3D facial animation by audio input in real time and with low latency. Our deep neural network learns a mapping from input waveforms to the 3D vertex coordinates of a face model, and simultaneously discovers a compact, latent code that disambiguates the variations in facial expression that cannot be explained by the audio alone. During inference, the latent code can be used as an intuitive control for the emotional state of the face puppet. We train our network with 3--5 minutes of high-quality animation data obtained using traditional, vision-based performance capture methods. Even though our primary goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language, as we demonstrate with a user study. The results are applicable to in-game dialogue, low-cost localization, virtual reality avatars, and telepresence.","[{'authorId': '2976930', 'name': 'Tero Karras'}, {'authorId': '1761103', 'name': 'Timo Aila'}, {'authorId': '36436218', 'name': 'S. Laine'}, {'authorId': '3468872', 'name': 'Antti Herva'}, {'authorId': '49244945', 'name': 'J. Lehtinen'}]",305.0,"{'bibtex': '@Article{Karras2017AudiodrivenFA,\n author = {Tero Karras and Timo Aila and S. Laine and Antti Herva and J. Lehtinen},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 12},\n title = {Audio-driven facial animation by joint end-to-end learning of pose and emotion},\n volume = {36},\n year = {2017}\n}\n'}",,"{'volume': '36', 'pages': '1 - 12', 'name': 'ACM Transactions on Graphics (TOG)'}",53.0,Audio-driven facial animation by joint end-to-end learning of pose and emotion,2017.0
1771,962cf4ead989b6d5620497a018988d5776158cc5,"Communication robots are now getting popular. In particular, partner robots, which can perform personal services, are in high demand. However, they can be prohibitively expensive. Therefore, we considered a multi-user robot with a virtual agent service which could satisfy user demands. But, several issues need to be solved in order to achieve this purpose. Firstly, there is no general service platform for such robots. Secondly, even if we use the multi-user robot by executing the virtual agent service, the physical shape, and other characteristics of the multi-user robot sometimes creates a strong impression on users. Therefore, we proposed a virtual agent service platform, and the robot features modification for a multi-user robot. The robot can autonomously adjust its position according to each user’s physiological signals, which based on emotion in real-time. We presented a preliminary evaluation to determine whether the proposed method could improve users’ robot experience even for the users who are not familiar with the robot at all.","[{'authorId': '1995826585', 'name': 'Shoudai Suzuki'}, {'authorId': '2679060', 'name': 'M. N. Anuardi'}, {'authorId': '2042016', 'name': 'Peeraya Sripian'}, {'authorId': '3299510', 'name': 'N. Matsuhira'}, {'authorId': '145094033', 'name': 'Midori Sugaya'}]",0.0,"{'bibtex': '@Article{Suzuki2020MultiuserRI,\n author = {Shoudai Suzuki and M. N. Anuardi and Peeraya Sripian and N. Matsuhira and Midori Sugaya},\n booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},\n journal = {2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},\n pages = {1006-1012},\n title = {Multi-user Robot Impression with a Virtual Agent and Features Modification According to Real-time Emotion from Physiological Signals},\n year = {2020}\n}\n'}",[],"{'name': '2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)', 'pages': '1006-1012'}",16.0,Multi-user Robot Impression with a Virtual Agent and Features Modification According to Real-time Emotion from Physiological Signals,2020.0
1772,96421c506cd1027552d8040b858948bc351bc973,,"[{'authorId': '90935194', 'name': 'V. Sevinç'}, {'authorId': '75109453', 'name': 'M. Berkman'}]",98.0,"{'bibtex': '@Article{Sevinç2019PsychometricEO,\n author = {V. Sevinç and M. Berkman},\n journal = {Applied ergonomics},\n pages = {\n          102958\n        },\n title = {Psychometric evaluation of Simulator Sickness Questionnaire and its variants as a measure of cybersickness in consumer virtual environments.},\n volume = {82},\n year = {2019}\n}\n'}",,"{'volume': '82', 'pages': '\n          102958\n        ', 'name': 'Applied ergonomics'}",71.0,Psychometric evaluation of Simulator Sickness Questionnaire and its variants as a measure of cybersickness in consumer virtual environments.,2019.0
1773,9645b941e3db4bc017cc2d80ee6315fe95650dc1,"The notion of Emotional contagion is a phenomenon in which a human emotion infects others. Various studies have been done to cause emotional contagion between a human and a robot or an anthropomorphic agent in HRI research fields. However, few studies have been done to compare different kinds of agents in order to find important properties in emotional contagion in human-agent interaction (HAI). In this research, we conducted an experiment to determine which properties cause this phenomenon between anthropomorphic agents and users. We prepared two kinds of agents. One is a cartoon-like agent, and the other is a concrete agent. The cartoon-like agent smiled exaggeratedly, and the concrete agent smiled modestly. As a result, we found that the concrete agent was more effective than the cartoon-like agent at emotional contagion. This result suggests a model for designing more trustworthy and familiar agents and robots.","[{'authorId': '49201495', 'name': 'T. Matsui'}, {'authorId': '1679243', 'name': 'S. Yamada'}]",4.0,"{'bibtex': '@Article{Matsui2016EmotionalCB,\n author = {T. Matsui and S. Yamada},\n booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},\n journal = {2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {1172-1176},\n title = {Emotional contagion between user and product recommendation virtual agent},\n year = {2016}\n}\n'}","[{'paperId': 'd1329fa6cdc62cb1ccdbedfc5f9bafbf5d192666', 'title': 'Emotional Virtual Characters for Improving Motivation and Performance in VR Exergames'}, {'paperId': 'a03ab390f8aa1ca2d104fa7532b75291648a22cd', 'title': 'Effect of Group Identity on Emotional Contagion in Dyadic Human Agent Interaction'}, {'paperId': 'b363aee20ff410d869168f7362d06893d8451810', 'title': 'Alexa Feels Blue And so Do I? Conversational Agents Displaying Emotions via Light Modalities'}, {'paperId': '8d5760d42ec16a0c7c213a1bc6cf1aa278e6f225', 'title': 'Exploring the influence of a human-like dancing virtual character on the evocation of human emotion'}]","{'name': '2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)', 'pages': '1172-1176'}",16.0,Emotional contagion between user and product recommendation virtual agent,2016.0
1774,967f32841955b72f358190436baa5510839d9ab3,.,"[{'authorId': '2223654620', 'name': 'Sagl B Needle'}, {'authorId': '2223656870', 'name': 'Asd Christus'}, {'authorId': '2223654618', 'name': 'D. Wuksch'}]",5557.0,"{'bibtex': '@Misc{None,\n author = {Sagl B Needle and Asd Christus and D. Wuksch},\n title = {A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins}\n}\n'}",,,9.0,A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins,
1775,96a762cdead5b85474f4287bc96e8f2dc52fc873,,"[{'authorId': '50544160', 'name': 'M. Malkawi'}, {'authorId': '5806627', 'name': 'Omayya Murad'}]",56.0,"{'bibtex': '@Article{Malkawi2012ArtificialNF,\n author = {M. Malkawi and Omayya Murad},\n journal = {Human-centric Computing and Information Sciences},\n pages = {1-13},\n title = {Artificial neuro fuzzy logic system for detecting human emotions},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'pages': '1-13', 'name': 'Human-centric Computing and Information Sciences'}",41.0,Artificial neuro fuzzy logic system for detecting human emotions,2012.0
1776,96c1401e7b1fe0b539f8a2fe4ba4183f9e9812f8,"Agent designers typically specify how an agent should interpret stimuli it receives. Designers also provide the agent with a list of actions from which the agent chooses a response based on the stimulation. We present a system where the agent's response to new stimuli is based solely on its memory and emotional state with no predefined response suggested other than for unconditioned stimuli like getting burned or, in this case, hearing a scary noise. We then let unspecified emotion emerge within the agent for newly encountered stimuli by association with these unconditioned stimuli. We use our agent to recreate a virtual version of the Little Albert Experiment, and were able to reproduce a form of conditioned emotion response without directing the agent's perception, appraisal or response.","[{'authorId': '2060687559', 'name': 'Alexander Patrick'}, {'authorId': '2813638', 'name': 'Curtis L Gittens'}, {'authorId': '1793961', 'name': 'M. Katchabaw'}]",2.0,"{'bibtex': '@Article{Patrick2015TheVL,\n author = {Alexander Patrick and Curtis L Gittens and M. Katchabaw},\n booktitle = {IEEE Games Entertainment Media Conference},\n journal = {2015 IEEE Games Entertainment Media Conference (GEM)},\n pages = {1-8},\n title = {The virtual little albert experiment: Creating conditioned emotion response in virtual agents},\n year = {2015}\n}\n'}","[{'paperId': 'f26e7022271e843bd3a5c3fb13fceaec298ddce2', 'title': 'Relationship between risk aversion, risky investment intention, investment choices: Impact of personality traits and emotion'}, {'paperId': 'f9f6ee937ef3abeae5eb62d6b37f9e8492a97508', 'title': 'Learned Behavior: Enabling Believable Virtual Characters through Reinforcement'}]","{'name': '2015 IEEE Games Entertainment Media Conference (GEM)', 'pages': '1-8'}",24.0,The virtual little albert experiment: Creating conditioned emotion response in virtual agents,2015.0
1777,96d04974f796ee1b80d4988ff5cd3b25efeaab4d,,"[{'authorId': '2462740', 'name': 'K. Scherer'}]",1635.0,"{'bibtex': '@Inproceedings{Scherer2001AppraisalCA,\n author = {K. Scherer},\n title = {Appraisal considered as a process of multilevel sequential checking.},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Appraisal considered as a process of multilevel sequential checking.,2001.0
1781,96ee1e388a823aa0ef5bd98032e7443d69dc5115,,"[{'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",14.0,"{'bibtex': '@Article{Johnson2015FacetoFaceIW,\n author = {W. Johnson and James C. Lester},\n journal = {International Journal of Artificial Intelligence in Education},\n pages = {25 - 36},\n title = {Face-to-Face Interaction with Pedagogical Agents, Twenty Years Later},\n volume = {26},\n year = {2015}\n}\n'}",,"{'volume': '26', 'pages': '25 - 36', 'name': 'International Journal of Artificial Intelligence in Education'}",0.0,"Face-to-Face Interaction with Pedagogical Agents, Twenty Years Later",2015.0
1782,970aa7c680bd34cf9c0abc8df4dcbd31b24465a3,,"[{'authorId': '2201191', 'name': 'H. Kawamichi'}, {'authorId': '25569909', 'name': 'Sho K. Sugawara'}, {'authorId': '47509042', 'name': 'Yuki H. Hamano'}, {'authorId': '3236294', 'name': 'R. Kitada'}, {'authorId': '38381724', 'name': 'E. Nakagawa'}, {'authorId': '1757663', 'name': 'T. Kochiyama'}, {'authorId': '1843699', 'name': 'N. Sadato'}]",20.0,"{'bibtex': '@Article{Kawamichi2018NeuralCU,\n author = {H. Kawamichi and Sho K. Sugawara and Yuki H. Hamano and R. Kitada and E. Nakagawa and T. Kochiyama and N. Sadato},\n journal = {Scientific Reports},\n title = {Neural correlates underlying change in state self-esteem},\n volume = {8},\n year = {2018}\n}\n'}",,"{'volume': '8', 'name': 'Scientific Reports'}",64.0,Neural correlates underlying change in state self-esteem,2018.0
1783,97510e2048af0c6c510aed405091514946c4eb13,"In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators.","[{'authorId': '114028501', 'name': 'R. M. Baron'}, {'authorId': '2060895', 'name': 'D. Kenny'}]",83748.0,"{'bibtex': '@Article{Baron1986TheMV,\n author = {R. M. Baron and D. Kenny},\n journal = {Journal of personality and social psychology},\n pages = {\n          1173-82\n        },\n title = {The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations.},\n volume = {51 6},\n year = {1986}\n}\n'}",,"{'volume': '51 6', 'pages': '\n          1173-82\n        ', 'name': 'Journal of personality and social psychology'}",54.0,"The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations.",1986.0
1784,97947d10a2e4d79a34c5996bf9f8e2b1fc47f427,"Purpose – The purpose of this article is to outline ways in which the large body of empirical work on creativity can meaningfully inform negotiation. In doing so, two general streams of creativity research and their implications for negotiation theory and empirical analysis are considered. Negotiation pundits advise that negotiators should engage in creative problem-solving to craft integrative agreements, and it is widely believed by both negotiation theorists and practitioners that “out-of-the-box” thinking and creative idea generation are necessary for win–win negotiation. Although practitioners have strongly encouraged parties to engage in creative problem-solving, there are remarkably few empirical investigations of creative thinking, brainstorming and other idea-generation methods in negotiation. Design/methodology/approach – First, creativity as a trait is considered and the relationship between individual differences in creativity and negotiation performance is examined. Then, creative thinking as...","[{'authorId': '145495014', 'name': 'E. Wilson'}, {'authorId': '50405579', 'name': 'Leigh Thompson'}]",15.0,"{'bibtex': '@Article{Wilson2014CreativityAN,\n author = {E. Wilson and Leigh Thompson},\n journal = {International Journal of Conflict Management},\n pages = {359-386},\n title = {Creativity and negotiation research: the integrative potential},\n volume = {25},\n year = {2014}\n}\n'}",,"{'volume': '25', 'pages': '359-386', 'name': 'International Journal of Conflict Management'}",133.0,Creativity and negotiation research: the integrative potential,2014.0
1785,97bcea32979ed602fd404448a4e4cedad4171d79,"To realize natural-looking virtual agents, one key technical challenge is to automatically generate nonverbal behaviors from spoken language. Since nonverbal behavior varies depending on personality, it is important to generate these nonverbal behaviors to match the expected personality of a virtual agent. In this work, we study how personality traits relate to the process of generating individual nonverbal behaviors from the whole body, including the head, eye gaze, arms, and posture. To study this, we first created a dialogue corpus including transcripts, a broad range of labelled nonverbal behaviors, and the Big Five personality scores of participants in dyad interactions. We constructed models that can predict each nonverbal behavior label given as an input language representation from the participants' spoken sentences. Our experimental results show that personality can help improve the prediction of nonverbal behaviors.","[{'authorId': '1736448', 'name': 'Ryo Ishii'}, {'authorId': '118242121', 'name': 'Chaitanya Ahuja'}, {'authorId': '1718158', 'name': 'Y. Nakano'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",9.0,"{'bibtex': '@Article{Ishii2020ImpactOP,\n author = {Ryo Ishii and Chaitanya Ahuja and Y. Nakano and Louis-Philippe Morency},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Impact of Personality on Nonverbal Behavior Generation},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},61.0,Impact of Personality on Nonverbal Behavior Generation,2020.0
1786,97cf5e8e117f758e08c81fbe2af17b9fe8e1f408,"In this paper we present a new version of the APML (Affective Presentation Markup Language, [?]) representation language, called FML-APML. This new version encompasses the tags of APML as well as other tags related, for example, to world references and emotional state. The presented language has been developed in the Greta framework [?]. Greta is an ECA (Embodied Conversational Agent) that starting from a representation of its communicative intention, plans the verbal (speech) and nonverbal signals (facial expressions, head movements, gestures) in order to convey it. We use the FML-APML language to model the agent’s communicative intention.","[{'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",20.0,"{'bibtex': '@Inproceedings{Mancini2008TheFL,\n author = {M. Mancini and C. Pelachaud},\n title = {The FML-APML language},\n year = {2008}\n}\n'}",,,0.0,The FML-APML language,2008.0
1787,97d665a781947ad74f10f9e05888ba931df5b820,"Research has largely neglected the effects of gaze direction cues on the perception of facial expressions of emotion. It was hypothesized that when gaze direction matches the underlying behavioral intent (approach-avoidance) communicated by an emotional expression, the perception of that emotion would be enhanced (i.e., shared signal hypothesis). Specifically, the authors expected that (a) direct gaze would enhance the perception of approach-oriented emotions (anger and joy) and (b) averted eye gaze would enhance the perception of avoidance-oriented emotions (fear and sadness). Three studies supported this hypothesis. Study 1 examined emotional trait attributions made to neutral faces. Study 2 examined ratings of ambiguous facial blends of anger and fear. Study 3 examined the influence of gaze on the perception of highly prototypical expressions.","[{'authorId': '2075454382', 'name': 'R. B. Adams'}, {'authorId': '4170904', 'name': 'R. Kleck'}]",566.0,"{'bibtex': '@Article{Adams2005EffectsOD,\n author = {R. B. Adams and R. Kleck},\n journal = {Emotion},\n pages = {\n          3-11\n        },\n title = {Effects of direct and averted gaze on the perception of facially communicated emotion.},\n volume = {5 1},\n year = {2005}\n}\n'}",,"{'volume': '5 1', 'pages': '\n          3-11\n        ', 'name': 'Emotion'}",65.0,Effects of direct and averted gaze on the perception of facially communicated emotion.,2005.0
1788,97dd654e0985e35ad4be14841bcb70fde7300f46,"Problems demanding globally optimal solutions are ubiquitous, yet many are intractable when they involve constrained functions having many local optima and interacting, mixed-type variables.The differential evolution (DE) algorithm is a practical approach to global numerical optimization which is easy to understand, simple to implement, reliable, and fast. Packed with illustrations, computer code, new insights, and practical advice, this volume explores DE in both principle and practice. It is a valuable resource for professionals needing a proven optimizer and for students wanting an evolutionary perspective on global numerical optimization.","[{'authorId': '30516941', 'name': 'K. Price'}, {'authorId': '2389722', 'name': 'R. Storn'}, {'authorId': '1680702', 'name': 'J. Lampinen'}]",6120.0,"{'bibtex': '@Inproceedings{Price2014DifferentialEA,\n author = {K. Price and R. Storn and J. Lampinen},\n title = {Differential Evolution: A Practical Approach to Global Optimization},\n year = {2014}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Differential Evolution: A Practical Approach to Global Optimization,2014.0
1789,97fc82faf465f28db5ca3fca58315a3e98f47130,,"[{'authorId': '2081091091', 'name': 'C. Julmi'}]",3.0,"{'bibtex': '@Article{Julmi2018ATO,\n author = {C. Julmi},\n journal = {Human Studies},\n pages = {623-641},\n title = {A Theory of Affective Communication: On the Phenomenological Foundations of Perspective Taking},\n volume = {41},\n year = {2018}\n}\n'}",,"{'volume': '41', 'pages': '623-641', 'name': 'Human Studies'}",56.0,A Theory of Affective Communication: On the Phenomenological Foundations of Perspective Taking,2018.0
1790,97fc86fd169215c1c0dea5274eaabba56d537736,"We examined age differences in attention to and memory for faces expressing sadness, anger, and happiness. Participants saw a pair of faces, one emotional and one neutral, and then a dot probe that appeared in the location of one of the faces. In two experiments, older adults responded faster to the dot if it was presented on the same side as a neutral face than if it was presented on the same side as a negative face. Younger adults did not exhibit this attentional bias. Interactions of age and valence were also found for memory for the faces, with older adults remembering positive better than negative faces. These findings reveal that in their initial attention, older adults avoid negative information. This attentional bias is consistent with older adults' generally better emotional well-being and their tendency to remember negative less well than positive information.","[{'authorId': '2145954', 'name': 'M. Mather'}, {'authorId': '5994816', 'name': 'L. Carstensen'}]",771.0,"{'bibtex': '@Article{Mather2003AgingAA,\n author = {M. Mather and L. Carstensen},\n journal = {Psychological Science},\n pages = {409 - 415},\n title = {Aging and Attentional Biases for Emotional Faces},\n volume = {14},\n year = {2003}\n}\n'}",,"{'volume': '14', 'pages': '409 - 415', 'name': 'Psychological Science'}",37.0,Aging and Attentional Biases for Emotional Faces,2003.0
1791,980dc62e4a1724b7f5dbc649dda8e08011e6bac1,"Conversational agents are getting increasingly popular and find applications in health and customer services. Conversations in these fields are often emotionally charged. It is, therefore, necessary to handle the conversation with some degree of empathy to be effective. In this work, we leverage advances in the field of natural language processing to create a dialogue system that can convincingly generate empathic responses to text-based messages. To improve the system's ability to converse with empathy, we train the language model on empathic conversations and inject additional emotional information in the response generation. We propose two chatbots: a benchmark bot and an empathic bot. Additionally, we implement an emotion classifier that allows us to predict the emotional state of text-based messages. We evaluate both chatbots in quantitative studies and compare them with human responses in qualitative studies involving human judges. Our evaluation shows that our empathic chatbot outperforms the benchmark bot and even the human-generated responses in terms of perceived empathy. Additionally, we achieve state-of-the-art results in terms of response quality using transformer-based language models. Finally we report that we can double the initial performance of the emotion classifier using undersampling techniques, yielding a final F1-score of 0.81 in six basic emotions.","[{'authorId': '144558519', 'name': 'Jacky Casas'}, {'authorId': '50817194', 'name': 'Timo Spring'}, {'authorId': '1396363912', 'name': 'Karl Daher'}, {'authorId': '1802011', 'name': 'E. Mugellini'}, {'authorId': '72699451', 'name': 'Omar Abou Khaled'}, {'authorId': '1393644275', 'name': 'P. Cudré-Mauroux'}]",11.0,"{'bibtex': '@Article{Casas2021EnhancingCA,\n author = {Jacky Casas and Timo Spring and Karl Daher and E. Mugellini and Omar Abou Khaled and P. Cudré-Mauroux},\n journal = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},\n title = {Enhancing Conversational Agents with Empathic Abilities},\n year = {2021}\n}\n'}",,{'name': 'Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents'},31.0,Enhancing Conversational Agents with Empathic Abilities,2021.0
1792,9819b600a828a57e1cde047bbe710d3446b30da5,"A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition","[{'authorId': '2047446108', 'name': 'Tomas Mikolov'}, {'authorId': '2245567', 'name': 'M. Karafiát'}, {'authorId': '1816892', 'name': 'L. Burget'}, {'authorId': '1899242', 'name': 'J. Černocký'}, {'authorId': '2803071', 'name': 'S. Khudanpur'}]",5549.0,"{'bibtex': '@Inproceedings{Mikolov2010RecurrentNN,\n author = {Tomas Mikolov and M. Karafiát and L. Burget and J. Černocký and S. Khudanpur},\n pages = {1045-1048},\n title = {Recurrent neural network based language model},\n year = {2010}\n}\n'}",,{'pages': '1045-1048'},17.0,Recurrent neural network based language model,2010.0
1793,982a837cd433d5c23950122bde9a3fa95561bf65,,"[{'authorId': '49403663', 'name': 'W. Morris'}]",140.0,"{'bibtex': '@Inproceedings{Morris1992AFA,\n author = {W. Morris},\n title = {A functional analysis of the role of mood in affective systems.},\n year = {1992}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,A functional analysis of the role of mood in affective systems.,1992.0
1794,9850a6de42fcfaecac73f189318c4c3be49ae707,,"[{'authorId': '51118766', 'name': 'Yiriko Someya'}, {'authorId': '3187057', 'name': 'Y. Tobe'}, {'authorId': '46784763', 'name': 'Reiji Yoshida'}, {'authorId': '3299510', 'name': 'N. Matsuhira'}, {'authorId': '145094033', 'name': 'Midori Sugaya'}]",2.0,"{'bibtex': '@Inproceedings{Someya2018HumanRobotPS,\n author = {Yiriko Someya and Y. Tobe and Reiji Yoshida and N. Matsuhira and Midori Sugaya},\n pages = {157-167},\n title = {Human-Robot Personal Space Evaluated with Biological Information Emotion Estimation Method},\n year = {2018}\n}\n'}",,{'pages': '157-167'},0.0,Human-Robot Personal Space Evaluated with Biological Information Emotion Estimation Method,2018.0
1795,9853f122750431a4eda76af110747cb179b8f163,,"[{'authorId': '2650933', 'name': 'Steffi Heidig'}, {'authorId': '1795235', 'name': 'G. Clarebout'}]",237.0,"{'bibtex': '@Article{Heidig2011DoPA,\n author = {Steffi Heidig and G. Clarebout},\n journal = {Educational Research Review},\n pages = {27-54},\n title = {Do pedagogical agents make a difference to student motivation and learning},\n volume = {6},\n year = {2011}\n}\n'}",,"{'volume': '6', 'pages': '27-54', 'name': 'Educational Research Review'}",84.0,Do pedagogical agents make a difference to student motivation and learning,2011.0
1796,985f050927d62e329c865526636fbc85421672e5,"Regional soil quality issues arising from rapid urbanization have received extensive attention. The riverbank that runs through a city is representative of urbanization gradient transformation. Thirty soil samples in the Yangtze River Delta urban agglomeration were collected and analyzed for the concentrations of seven analytes. Correlation, principle component analysis, cluster analysis and GeoDetector models suggested that the four groups (Cr-Ni-Cu, Cu-Zn-As-Sb, Cd and Pb) shared the same sources in the core urban region; five groups (Cr-Ni-Cu-Zn, As, Cd, Sb and Pb) in the suburbs and three groups (Cr-Ni, Cu-Zn-Cd-Sb-Pb and As) in the exurbs. GeoDetector methods not only validated the results of the three other methods, but also provided more possible impact factors. Besides the direct influences, the interaction effects among factors were quantified. Interactive combination with strong nonlinear increment changed from between-two-weak factors in the central region to between-strong-and-weak factors in the suburbs. In the exurbs, the stronger interaction effects were observed between strong and weak factors. Therefore, the GeoDetector model, which provided more detailed information of artificial sources could be used as a tool for identifying the potential factors of toxic elements and offering scientific basis for the development of subsequent pollution reduction strategies.","[{'authorId': '38033299', 'name': 'S. Zuo'}, {'authorId': '41031443', 'name': 'Shaoqing Dai'}, {'authorId': '4516308', 'name': 'Yaying Li'}, {'authorId': '46741007', 'name': 'Jianfeng Tang'}, {'authorId': '102246858', 'name': 'Y. Ren'}]",29.0,"{'bibtex': '@Article{Zuo2018AnalysisOH,\n author = {S. Zuo and Shaoqing Dai and Yaying Li and Jianfeng Tang and Y. Ren},\n journal = {International Journal of Environmental Research and Public Health},\n title = {Analysis of Heavy Metal Sources in the Soil of Riverbanks Across an Urbanization Gradient},\n volume = {15},\n year = {2018}\n}\n'}",,"{'volume': '15', 'name': 'International Journal of Environmental Research and Public Health'}",53.0,Analysis of Heavy Metal Sources in the Soil of Riverbanks Across an Urbanization Gradient,2018.0
1797,986972cd62940e95aeba1cb1ccd6d33d124348de,,"[{'authorId': '7234960', 'name': 'A. Cowell'}, {'authorId': '1701555', 'name': 'K. Stanney'}]",45.0,"{'bibtex': '@Inproceedings{Cowell2003EmbodimentAI,\n author = {A. Cowell and K. Stanney},\n pages = {301-309},\n title = {Embodiment and Interaction Guidelines for Designing Credible, Trustworthy Embodied Conversational Agents},\n year = {2003}\n}\n'}",,{'pages': '301-309'},63.0,"Embodiment and Interaction Guidelines for Designing Credible, Trustworthy Embodied Conversational Agents",2003.0
1798,98844c17c22a93e7a6b40a06433427b7be8f68e2,"Background Face processing, amongst many basic visual skills, is thought to be invariant across all humans. From as early as 1965, studies of eye movements have consistently revealed a systematic triangular sequence of fixations over the eyes and the mouth, suggesting that faces elicit a universal, biologically-determined information extraction pattern. Methodology/Principal Findings Here we monitored the eye movements of Western Caucasian and East Asian observers while they learned, recognized, and categorized by race Western Caucasian and East Asian faces. Western Caucasian observers reproduced a scattered triangular pattern of fixations for faces of both races and across tasks. Contrary to intuition, East Asian observers focused more on the central region of the face. Conclusions/Significance These results demonstrate that face processing can no longer be considered as arising from a universal series of perceptual events. The strategy employed to extract visual information from faces differs across cultures.","[{'authorId': '2215584', 'name': 'C. Blais'}, {'authorId': '2143019', 'name': 'Rachael E. Jack'}, {'authorId': '143959339', 'name': 'Christoph Scheepers'}, {'authorId': '2798560', 'name': 'D. Fiset'}, {'authorId': '3489043', 'name': 'R. Caldara'}]",518.0,"{'bibtex': '@Article{Blais2008CultureSH,\n author = {C. Blais and Rachael E. Jack and Christoph Scheepers and D. Fiset and R. Caldara},\n journal = {PLoS ONE},\n title = {Culture Shapes How We Look at Faces},\n volume = {3},\n year = {2008}\n}\n'}",,"{'volume': '3', 'name': 'PLoS ONE'}",28.0,Culture Shapes How We Look at Faces,2008.0
1799,98ab45160269f7c1545f7924f989d5da1895e9a5,"More than 40 years ago, Masahiro Mori, a robotics professor at the Tokyo Institute of Technology, wrote an essay [1] on how he envisioned people's reactions to robots that looked and acted almost like a human. In particular, he hypothesized that a person's response to a humanlike robot would abruptly shift from empathy to revulsion as it approached, but failed to attain, a lifelike appearance. This descent into eeriness is known as the uncanny valley. The essay appeared in an obscure Japanese journal called Energy in 1970, and in subsequent years, it received almost no attention. However, more recently, the concept of the uncanny valley has rapidly attracted interest in robotics and other scientific circles as well as in popular culture. Some researchers have explored its implications for human-robot interaction and computer-graphics animation, whereas others have investigated its biological and social roots. Now interest in the uncanny valley should only intensify, as technology evolves and researchers build robots that look human. Although copies of Mori's essay have circulated among researchers, a complete version hasn't been widely available. The following is the first publication of an English translation that has been authorized and reviewed by Mori. (See “Turning Point” in this issue for an interview with Mori.).","[{'authorId': '46861004', 'name': 'M. Mori'}, {'authorId': '1690354', 'name': 'K. Macdorman'}, {'authorId': '1956705', 'name': 'Norri Kageki'}]",1528.0,"{'bibtex': '@Article{Mori2012TheUV,\n author = {M. Mori and K. Macdorman and Norri Kageki},\n journal = {IEEE Robotics Autom. Mag.},\n pages = {98-100},\n title = {The Uncanny Valley [From the Field]},\n volume = {19},\n year = {2012}\n}\n'}",,"{'volume': '19', 'pages': '98-100', 'name': 'IEEE Robotics Autom. Mag.'}",0.0,The Uncanny Valley [From the Field],2012.0
1802,98d22de9fccc4c945e7787675003dc88aeec9f8a,"An investigator who plans to conduct an experiment with multiple independent variables must decide whether to use a complete or reduced factorial design. This article advocates a resource management perspective on making this decision, in which the investigator seeks a strategic balance between service to scientific objectives and economy. Considerations in making design decisions include whether research questions are framed as main effects or simple effects; whether and which effects are aliased (confounded) in a particular design; the number of experimental conditions that must be implemented in a particular design and the number of experimental subjects the design requires to maintain the desired level of statistical power; and the costs associated with implementing experimental conditions and obtaining experimental subjects. In this article 4 design options are compared: complete factorial, individual experiments, single factor, and fractional factorial. Complete and fractional factorial designs and single-factor designs are generally more economical than conducting individual experiments on each factor. Although relatively unfamiliar to behavioral scientists, fractional factorial designs merit serious consideration because of their economy and versatility.","[{'authorId': '1807084', 'name': 'L. Collins'}, {'authorId': '3744578', 'name': 'John J. Dziak'}, {'authorId': '1812881', 'name': 'Runze Li'}]",335.0,"{'bibtex': '@Article{Collins2009DesignOE,\n author = {L. Collins and John J. Dziak and Runze Li},\n journal = {Psychological methods},\n pages = {\n          202-24\n        },\n title = {Design of experiments with multiple independent variables: a resource management perspective on complete and reduced factorial designs.},\n volume = {14 3},\n year = {2009}\n}\n'}",,"{'volume': '14 3', 'pages': '\n          202-24\n        ', 'name': 'Psychological methods'}",56.0,Design of experiments with multiple independent variables: a resource management perspective on complete and reduced factorial designs.,2009.0
1803,98e4c30c04572e66281dc87a361be3617cf1e2e7,"In this paper, we propose a new model to quantitatively compare global flow characteristics of two crowds. The proposed approach explores a 4‐D histogram that contains information on the local velocity (speed and orientation) of each spatial position, and the comparison is made using histogram distances. The 4‐D histogram also allows the comparison of specific characteristics, such as distribution of orientations only, speed only, relative spatial occupancy only, and combinations of such features. Experimental results indicate that the proposed quantitative metric correlates with visual inspection. Copyright © 2012 John Wiley & Sons, Ltd.","[{'authorId': '1679516', 'name': 'S. Musse'}, {'authorId': '2676680', 'name': 'V. Cassol'}, {'authorId': '2870402', 'name': 'C. Jung'}]",31.0,"{'bibtex': '@Article{Musse2012TowardsAQ,\n author = {S. Musse and V. Cassol and C. Jung},\n journal = {Computer Animation and Virtual Worlds},\n title = {Towards a quantitative approach for comparing crowds},\n volume = {23},\n year = {2012}\n}\n'}",,"{'volume': '23', 'name': 'Computer Animation and Virtual Worlds'}",27.0,Towards a quantitative approach for comparing crowds,2012.0
1804,98e95bff96a44995ac2fd2e5ac86945c04bce5ab,"This article proposes a unified framework for understanding creative problem solving, namely, the explicit-implicit interaction theory. This new theory of creative problem solving constitutes an attempt at providing a more unified explanation of relevant phenomena (in part by reinterpreting/integrating various fragmentary existing theories of incubation and insight). The explicit-implicit interaction theory relies mainly on 5 basic principles, namely, (a) the coexistence of and the difference between explicit and implicit knowledge, (b) the simultaneous involvement of implicit and explicit processes in most tasks, (c) the redundant representation of explicit and implicit knowledge, (d) the integration of the results of explicit and implicit processing, and (e) the iterative (and possibly bidirectional) processing. A computational implementation of the theory is developed based on the CLARION cognitive architecture and applied to the simulation of relevant human data. This work represents an initial step in the development of process-based theories of creativity encompassing incubation, insight, and various other related phenomena.","[{'authorId': '1772825', 'name': 'S. Hélie'}, {'authorId': '145966408', 'name': 'R. Sun'}]",325.0,"{'bibtex': '@Article{Hélie2010IncubationIA,\n author = {S. Hélie and R. Sun},\n journal = {Psychological review},\n pages = {\n          994-1024\n        },\n title = {Incubation, insight, and creative problem solving: a unified theory and a connectionist model.},\n volume = {117 3},\n year = {2010}\n}\n'}",,"{'volume': '117 3', 'pages': '\n          994-1024\n        ', 'name': 'Psychological review'}",144.0,"Incubation, insight, and creative problem solving: a unified theory and a connectionist model.",2010.0
1805,98efeaa4ff4dcef080006b945b5e075c4f2ec9cb,"Data on traffic accidents clearly point to road black spots, where the accident rate is always high. However, road safety research is still far from understanding why these particular places on a road are risky. The reason is the lack of sufficient knowledge on how pedestrians and drivers interact when facing a potentially dangerous traffic situation, and the lack of an integrated framework that relates the data on human behavior to real-world traffic situations. We attempt to tackle this problem by developing SAFEPED, a multi-agent microscopic three-dimensional (3D) simulation of vehicle and pedestrian dynamics at a black spot. SAFEPED is a test platform for evaluating experimentally estimated drivers’ and pedestrians’ behavioral rules, and estimating accident risks in different traffic situations. It aims to analyze the design of existing and future black spots and to assess alternative architectural and environmental solutions in order to identify maximally efficient safety countermeasures.","[{'authorId': '2104372', 'name': 'Gennady Waizman'}, {'authorId': '25700979', 'name': 'S. Shoval'}, {'authorId': '2174376', 'name': 'I. Benenson'}]",34.0,"{'bibtex': '@Article{Waizman2015MicroSimulationMF,\n author = {Gennady Waizman and S. Shoval and I. Benenson},\n journal = {Journal of Intelligent Transportation Systems},\n pages = {63 - 77},\n title = {Micro-Simulation Model for Assessing the Risk of Vehicle–Pedestrian Road Accidents},\n volume = {19},\n year = {2015}\n}\n'}",,"{'volume': '19', 'pages': '63 - 77', 'name': 'Journal of Intelligent Transportation Systems'}",44.0,Micro-Simulation Model for Assessing the Risk of Vehicle–Pedestrian Road Accidents,2015.0
1806,9944b7170b19b5c8ede595d657db260bccae8729,"Agent Based Models are very popular in a number of different areas. For example, they have been used in a range of domains ranging from modeling of tumor growth, immune systems, molecules to models of social networks, crowds and computer and mobile self-organizing networks. One reason for their success is their intuitiveness and similarity to human cognition. However, with this power of abstraction, in spite of being easily applicable to such a wide number of domains, it is hard to validate agent-based models. In addition, building valid and credible simulations is not just a challenging task but also a crucial exercise to ensure that what we are modeling is, at some level of abstraction, a model of our conceptual system; the system that we have in mind. In this paper, we address this important area of validation of agent based models by presenting a novel technique which has broad applicability and can be applied to all kinds of agent-based models. We present a framework, where a virtual overlay multi-agent system can be used to validate simulation models. In addition, since agent-based models have been typically growing, in parallel, in multiple domains, to cater for all of these, we present a new single validation technique applicable to all agent based models. Our technique, which allows for the validation of agent based simulations uses VOMAS: a Virtual Overlay Multi-agent System. This overlay multi-agent system can comprise various types of agents, which form an overlay on top of the agent based simulation model that needs to be validated. Other than being able to watch and log, each of these agents contains clearly defined constraints, which, if violated, can be logged in real time. To demonstrate its effectiveness, we show its broad applicability in a wide variety of simulation models ranging from social sciences to computer networks in spatial and non-spatial conceptual models.","[{'authorId': '1795560', 'name': 'M. Niazi'}, {'authorId': '144664815', 'name': 'A. Hussain'}, {'authorId': '1796567', 'name': 'M. Kolberg'}]",51.0,"{'bibtex': '@Article{Niazi2009VerificationV,\n author = {M. Niazi and A. Hussain and M. Kolberg},\n journal = {ArXiv},\n title = {Verification & Validation of Agent Based Simulations using the VOMAS (Virtual Overlay Multi-agent System) Approach},\n volume = {abs/1708.02361},\n year = {2009}\n}\n'}",,"{'volume': 'abs/1708.02361', 'name': 'ArXiv'}",31.0,Verification & Validation of Agent Based Simulations using the VOMAS (Virtual Overlay Multi-agent System) Approach,2009.0
1807,9997d0379f52c786a61e9306ca406c77f9bcfaf2,"Two studies investigated the importance of dynamic temporal characteristic information in facilitating the recognition of subtle expressions of emotion. In Experiment 1 there were three conditions, dynamic moving sequences that showed the expression emerging from neutral to a subtle emotion, a dynamic presentation containing nine static stills from the dynamic moving sequences (ran together to encapsulate a moving sequence) and a First–Last condition containing only the first (neutral) and last (subtle emotion) stills. The results showed recognition was significantly better for the dynamic moving sequences than both the Dynamic-9 and First–Last conditions. Experiments 2a and 2b then changed the dynamics of the moving sequences by speeding up, slowing down or disrupting the rhythm of the motion sequences. These manipulations significantly reduced recognition, and it was concluded that in addition to the perception of change, recognition is facilitated by the characteristic muscular movements associated with the portrayal of each emotion.","[{'authorId': '4195232', 'name': 'E. Bould'}, {'authorId': '47939572', 'name': 'Neil Morris'}, {'authorId': '30306572', 'name': 'B. Wink'}]",84.0,"{'bibtex': '@Article{Bould2008RecognisingSE,\n author = {E. Bould and Neil Morris and B. Wink},\n journal = {Cognition and Emotion},\n pages = {1569 - 1587},\n title = {Recognising subtle emotional expressions: The role of facial movements},\n volume = {22},\n year = {2008}\n}\n'}",,"{'volume': '22', 'pages': '1569 - 1587', 'name': 'Cognition and Emotion'}",20.0,Recognising subtle emotional expressions: The role of facial movements,2008.0
1808,999eb59966b35e6d16d882a63cdc4522cbad9a7d,"We present a novel framework for the recognition of body expressions using human postures. Proposed system is based on analyzing the spectral difference between an expressive and a neutral animation. Second problem that has been addressed in this paper is formalization of neutral animation. Formalization of neutral animation has not been tackled before and it can be very useful for the domain of synthesis of animation, recognition of expressions, etc. In this article, we proposed a cost function to synthesize a neutral motion from expressive motion. The cost function formalizes a neutral motion by computing the distance and by combining it with acceleration of each body joints during a motion. We have evaluated our approach on several databases with heterogeneous movements and body expressions. Our body expression recognition results exceeds state of the art on evaluated databases.","[{'authorId': '8726114', 'name': 'Arthur Crenn'}, {'authorId': '144176077', 'name': 'Alexandre Meyer'}, {'authorId': '144553570', 'name': 'Rizwan Ahmed Khan'}, {'authorId': '1971616', 'name': 'H. Konik'}, {'authorId': '1768560', 'name': 'S. Bouakaz'}]",11.0,"{'bibtex': '@Article{Crenn2017TowardAE,\n author = {Arthur Crenn and Alexandre Meyer and Rizwan Ahmed Khan and H. Konik and S. Bouakaz},\n journal = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},\n title = {Toward an efficient body expression recognition based on the synthesis of a neutral movement},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Multimodal Interaction'},33.0,Toward an efficient body expression recognition based on the synthesis of a neutral movement,2017.0
1810,99bf8ac8c131291d771923d861b188510194615e,"The Facial Action Coding System (FACS) is a widely used protocol for recognizing and labelling facial expression by describing the movement of muscles of the face. FACS is used to objectively measure the frequency and intensity of facial expressions without assigning any emotional meaning to those muscle movements. Instead FACS breaks down facial expressions into their smallest discriminable movements called Action Units. Each Action Unit creates a distinct change in facial appearance, such as an eyebrow lift or nose wrinkle. FACS coders can identify the Action Units which are present on the face when viewing still images or videos. Psychological research has used FACS to examine a variety of research questions including social-emotional development, neuropsychiatric disorders, and deception. In the course of this report we provide an overview of FACS and the Action Units, its reliability as a measure, and how it has been applied in some key areas of psychological research.","[{'authorId': '32173841', 'name': 'E. Prince'}, {'authorId': '39710065', 'name': 'Katherine B. Martin'}, {'authorId': '1874236', 'name': 'D. Messinger'}]",1509.0,"{'bibtex': '@Inproceedings{Prince2015FacialAC,\n author = {E. Prince and Katherine B. Martin and D. Messinger},\n title = {Facial Action Coding System},\n year = {2015}\n}\n'}",,,11.0,Facial Action Coding System,2015.0
1812,99d2dd2eecadaa926d9828bb6ec8049fa5a04f6e,"Traditionally, prejudice has been conceptualized as simple animosity. The stereotype content model (SCM) shows that some prejudice is worse. The SCM previously demonstrated separate stereotype dimensions of warmth (low-high) and competence (low-high), identifying four distinct out-group clusters. The SCM predicts that only extreme out-groups, groups that are both stereotypically hostile and stereotypically incompetent (low warmth, low competence), such as addicts and the homeless, will be dehumanized. Prior studies show that the medial prefrontal cortex (mPFC) is necessary for social cognition. Functional magnetic resonance imaging provided data for examining brain activations in 10 participants viewing 48 photographs of social groups and 12 participants viewing objects; each picture dependably represented one SCM quadrant. Analyses revealed mPFC activation to all social groups except extreme (low-low) out-groups, who especially activated insula and amygdala, a pattern consistent with disgust, the emotion predicted by the SCM. No objects, though rated with the same emotions, activated the mPFC. This neural evidence supports the prediction that extreme out-groups may be perceived as less than human, or dehumanized.","[{'authorId': '3705092', 'name': 'L. Harris'}, {'authorId': '1885803', 'name': 'S. Fiske'}]",915.0,"{'bibtex': '@Article{Harris2006DehumanizingTL,\n author = {L. Harris and S. Fiske},\n journal = {Psychological Science},\n pages = {847 - 853},\n title = {Dehumanizing the Lowest of the Low},\n volume = {17},\n year = {2006}\n}\n'}",,"{'volume': '17', 'pages': '847 - 853', 'name': 'Psychological Science'}",42.0,Dehumanizing the Lowest of the Low,2006.0
1813,99f037a157abc3aa7c75ba16be04bdf5208d1966,,"[{'authorId': '2022295313', 'name': 'Daniel Lambach'}, {'authorId': '145222753', 'name': 'M. Bayer'}, {'authorId': '2083410135', 'name': 'Felix S. Bethke'}, {'authorId': '121629413', 'name': 'Matteo Dressler'}, {'authorId': '103554374', 'name': 'Véronique Dudouet'}]",1846.0,"{'bibtex': '@Article{Lambach1934Theory,\n author = {Daniel Lambach and M. Bayer and Felix S. Bethke and Matteo Dressler and Véronique Dudouet},\n journal = {Music Educators Journal},\n pages = {50 - 50},\n title = {Theory},\n volume = {20},\n year = {1934}\n}\n'}",,"{'volume': '20', 'pages': '50 - 50', 'name': 'Music Educators Journal'}",286.0,Theory,1934.0
1814,9a043c08d8cfe58d26bf04cb01a60c069dbba9bd,,"[{'authorId': '3237926', 'name': 'M. Courgeon'}, {'authorId': '1742939', 'name': 'S. Buisine'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}]",59.0,"{'bibtex': ""@Inproceedings{Courgeon2009ImpactOE,\n author = {M. Courgeon and S. Buisine and Jean-Claude Martin},\n pages = {201-214},\n title = {Impact of Expressive Wrinkles on Perception of a Virtual Character's Facial Expressions of Emotions},\n year = {2009}\n}\n""}",,{'pages': '201-214'},46.0,Impact of Expressive Wrinkles on Perception of a Virtual Character's Facial Expressions of Emotions,2009.0
1816,9a2c9d67924271afc8c8f123decac1f98f151e1a,"This paper argues that thought is a necessary condition of emotion. It therefore opposes the •stance taken by Zajonc, which reflects two widespread misunderstandings about what is meant by cognitive processes in emotion: (a) that a cognitive appraisal of the significance of an encounter for one's well-being must occur in fixed stages through the information processing of initially meaningless inputs from the environment; and (b) that such an appraisal is necessarily deliberate, rational, and conscious. Some of the phylogenetic and ontogenetic implications of a cognitive theory of emotion are also discussed briefly. Recent years have seen a major change in the way psychologists view emotion—the rediscovery that emotions are products of cognitive processes. The emotional response is elicited by an evaluative perception in lower animals, and in humans by a complex 'cognitive appraisal of the significance of events for one's well-being. Although there are many other issues concerning the relations between emotion and cognition, my comments will focus on the role of thought in the emotional response. I will refer often to Zajonc's (1980) challenge to the assumption that cognition occurs prior to emotion. I use his views to illustrate widespread misunderstandings of what it means to speak of cognition as a causal antecedent of emotion; I also use his views as a point of departure for rny argument that cognitive activity is a necessary as well as sufficient condition of emotion. Do Emotions Require Cognitive Mediation? My own position on this question is a variant of a family of theories of emotion centered on the concept of cognitive appraisal. Campos and Sternberg (1981) state, for example, that ""The recent history of the study of emotion has been dominated by approaches stressing cognitive factors. In theories of adult emotional response, cognitive appraisal now functions as the central construct"" (p. 273). Its role is, to mediate the relationship between the person and the environment. The appraisal process gives rise to a particular emotion with greater or lesser intensity depending on how the relationship is evaluated with respect to the person's well-being. Cognitive appraisal means that the way one interprets one's plight at any given moment is crucial to the emotional response. Cognition and emotion are usually fused in nature (Folkman, Schaefer, & Lazarus, 1979), although they can be dissociated in certain unusual or abnormal states. For example, cognitive coping processes (cf. Lazarus, 1981) such as isolation and intellectualization (or detachment), which are aimed at regulating feelings, can create a dissociation between thoughts and feelings. Moreover, attack can occur without anger, and avoidance without fear. These latter conditions are also instances in which the usual link between thought and feeling has been loosened or broken. Yet such separations are less often a rule of living and more often a product of coping under special circumstances. The full experience of emotion (as opposed to sham rage, for example) normally includes three fused components: thoughts, action impulses, and somatic disturbances. When these components are dissociated we are left with something other than what we mean by a true emotional state. Our theories of emotion must reflect the normal fusion, and separating thoughts, action impulses, and so^ matic disturbances except under certain specifiable conditions (as was done in the old days of faculty psychology—which treated cognition, emotion, and motivation as independent entities) distorts rather than clarifies the structure of the mind (cf. Lazarus, Coyne, & Folkman, 1982). One bit of fallout from the above analysis is the implication, often derived from statements of cognitive theory, that cognitive appraisal is a necessary I wish to thank my research colleague, Susan Folkman, and my secretary, Carol Carr, for providing substantial editorial advice on this article. I appreciate their skill and judgment. Requests for reprints should be sent to Richard S. Lazarus, Department of Psychology, University of California, Berkeley, 4105 Tolman Hall, Berkeley, California 94720. Vol. 37, No. 9, 1019-1024 Copyright 1982 by the American Psychological Association, Inc. AMERICAN PSYCHOLOGIST • SEPTEMBER 1982 • 1019 0003-066X/82/3709-1019$00.75 I as well as sufficient condition of emotion. Such a position has been criticized trenchantly by Zajonc (1980). He writes that affect is erroneously regarded in contemporary psychological theory as postcognitive, occurring only after extensive cognitive operations have taken place, and that in actuality affective judgments are fairly independent of, and even precede, the perceptual and cognitive activities on which they are said to depend. Zajonc argues that not only can affect occur without extensive perceptual and cognitive encoding—and even before—but that affect and cognition are controlled by separate and partially independent neural systems (see also Tomkins, 1981). Zajonc thus seems to be saying two things contrary to what I have argued: first, that the proposed directionality in which cognition determines affect is wrorig and that the actual direction is affect to cognition; and second, that cognition and affect should be regarded as relatively independent subsystems rather than as fused and highly interdependent. Building his argument, Zajonc cites a stanza of poetry from e. e. cummings (1973): since feeling is first who pays any attention to the syntax of things will never wholly kiss you. (p. 160) He also cites Wundt's (1907) concept of affective primacy, and Bartlett (1932), Ittelson (1973), Osgood (1962), and Premack (1976) as having adopted the view that feelings come first. He states, for example: . < In fact, it is entirely possible that the very first stage of the organism's reaction to stimuli and the very first elements in retrieval are affective. It is further possible that we can like something or be afraid of it before we know precisely what it is and perhaps even without knowing what it is. (p. 154) The most serious mistake in Zajonc's analysis lies in his approach to cognition, which is characteristic of much of present-day cognitive psychology. In this approach information and meaning stem from the conception of mind as an analogue to a computer (Shannon & Weaver, 1962), a view illustrated also by the work of Newell and Simon (1961) and Weiner (I960). This conception has been rebutted by Dreyfus (1972), Polanyi (1958, 1966), and others, although the rebuttal has not affected the mainstream of cognitive psychology. The mainstream stance is that meanings for decision and action are built up from essentially meaningless stimulus display elements or bits and that systematic scanning of this display generates information. Thus, human cognition, like the operations of a computer, proceeds by serially receiving, registering, encoding, storing for the shortor longrun, and ""retrieving meaningless bits—a transformation to meaning that is called ""information processing."" Meanings and their associated emotions, or hot cognitions as Abelson (1963) referred to them, are built through such processing. As Erdelyi (1974) and others (e.g., Neisser, 1967) have suggested, however, emotion can influence the process at any of its stages. With this in mind, it is not surprising that Zajonc might be troubled by the implication that emotion lies at the end of a tortuous cognitive chain of information processing, and therefore find it necessary to suggest an independent system making possible rapid, nonreflective emotional reactions. As many have argued (Folkman et al., 1979; Wrubel, Benner, & Lazarus, 1981), humans are meaning-oriented, meaning-creating creatures who constantly evaluate events from the perspective of their well-being and react emotionally to some of these evaluations. Zajonc is therefore correct in asserting that meanings are immediately inherent in emotionally laden transactions without lengthy or sequential processing, but for the wrong reasons. In my view, the concept of meaning defined by the traditional information processing approach subscribed to by Zajonc has a perfectly reasonable-—and better—alternative. We do not always have to await revelation from information processing to unravel the environmental code. As was argued in the New Look movement in perception, personal factors such as beliefs, expectations, and motives or commitments influence attention and appraisal at the very outset of any encounter. Concern with individual differences leads inevitably to concern with personal meanings and to the factors that shape such meanings. We actively select and shape experience and in some degree mold it to our own requirements (see also Rychlak, 1981). Information processing as an exclusive model of cognition is insufficiently concerned with the person as a source of meaning. The history of debate about the phenomenon of subception is instructive (see Eriksen, 1956, 1960, 1962; Lazarus, 1956; Lazarus & McCleary, 1951). In a controversial experiment, McCleary and I showed that by associating a set of nonsense syllables to the threat of a painful electric shock, subjects would later react with a galvanic skin response selectively to the shock-associated syllables, even when they had misperceived and misreported, them. We referred to this phenomenon as ""autonomic discrimination without awareness,"" or ""sub1020 • SEPTEMBER 1982 • AMERICAN PSYCHOLOGIST ception,"" arguing that subjects somehow sensed the threat without consciously recognizing the syllables. The debate sparked by this interpretation touched on many complex issues, but it mainly centered on a claim by Bricker and Chapanis (1953) and Eriksen (1956, 1960, 1962) that even though the subjects had misreported what had been flashed on the screen, they probably had registered perceptually some of the structural elements of the syllables and had, in effect, reacted automatica","[{'authorId': '5628684', 'name': 'R. Lazarus'}]",1439.0,"{'bibtex': '@Article{Lazarus1982ThoughtsOT,\n author = {R. Lazarus},\n journal = {American Psychologist},\n pages = {1019-1024},\n title = {Thoughts on the relations between emotion and cognition.},\n volume = {37},\n year = {1982}\n}\n'}",,"{'volume': '37', 'pages': '1019-1024', 'name': 'American Psychologist'}",38.0,Thoughts on the relations between emotion and cognition.,1982.0
1818,9a3907ee9c0103f8773baec92d2c4b6596d1b825,,"[{'authorId': '31974606', 'name': 'F. Garwood'}]",72.0,"{'bibtex': '@Article{Garwood1947TheVO,\n author = {F. Garwood},\n journal = {Biometrika},\n pages = {\n          1-17\n        },\n title = {The variance of the overlap of geometrical figures with reference to a bombing problem.},\n volume = {34 1-2},\n year = {1947}\n}\n'}",,"{'volume': '34 1-2', 'pages': '\n          1-17\n        ', 'name': 'Biometrika'}",0.0,The variance of the overlap of geometrical figures with reference to a bombing problem.,1947.0
1819,9a4deea347aa995941f2f69e2d59936af19af42d,"Abstract Sigma (Σ) is a cognitive architecture and system whose development is driven by a combination of four desiderata: grand unification, generic cognition, functional elegance, and sufficient efficiency. Work towards these desiderata is guided by the graphical architecture hypothesis, that key to progress on them is combining what has been learned from over three decades’ worth of separate work on cognitive architectures and graphical models. In this article, these four desiderata are motivated and explained, and then combined with the graphical architecture hypothesis to yield a rationale for the development of Sigma. The current state of the cognitive architecture is then introduced in detail, along with the graphical architecture that sits below it and implements it. Progress in extending Sigma beyond these architectures and towards a full cognitive system is then detailed in terms of both a systematic set of higher level cognitive idioms that have been developed and several virtual humans that are built from combinations of these idioms. Sigma as a whole is then analyzed in terms of how well the progress to date satisfies the desiderata. This article thus provides the first full motivation, presentation and analysis of Sigma, along with a diversity of more specific results that have been generated during its development.","[{'authorId': '1749322', 'name': 'P. Rosenbloom'}, {'authorId': '2756886', 'name': 'A. Demski'}, {'authorId': '2345304', 'name': 'Volkan Ustun'}]",71.0,"{'bibtex': '@Article{Rosenbloom2016TheSC,\n author = {P. Rosenbloom and A. Demski and Volkan Ustun},\n journal = {Journal of Artificial General Intelligence},\n pages = {1 - 103},\n title = {The Sigma Cognitive Architecture and System: Towards Functionally Elegant Grand Unification},\n volume = {7},\n year = {2016}\n}\n'}",,"{'volume': '7', 'pages': '1 - 103', 'name': 'Journal of Artificial General Intelligence'}",119.0,The Sigma Cognitive Architecture and System: Towards Functionally Elegant Grand Unification,2016.0
1820,9a8661522f60b5b040eb08a54a0d26a81eaee2ec,"Virtual characters that appear almost photo-realistic have been shown to induce negative responses from viewers in traditional media, such as film and video games. This effect, described as the uncanny valley, is the reason why realism is often avoided when the aim is to create an appealing virtual character. In Virtual Reality, there have been few attempts to investigate this phenomenon and the implications of rendering virtual characters with high levels of realism on user enjoyment. In this paper, we conducted a large-scale experiment on over one thousand members of the public in order to gather information on how virtual characters are perceived in interactive virtual reality games. We were particularly interested in whether different render styles (realistic, cartoon, etc.) would directly influence appeal, or if a character's personality was the most important indicator of appeal. We used a number of perceptual metrics such as subjective ratings, proximity, and attribution bias in order to test our hypothesis. Our main result shows that affinity towards virtual characters is a complex interaction between the character's appearance and personality, and that realism is in fact a positive choice for virtual characters in virtual reality.","[{'authorId': '1710384', 'name': 'Katja Zibrek'}, {'authorId': '2540057', 'name': 'Elena Kokkinara'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",86.0,"{'bibtex': ""@Article{Zibrek2018TheEO,\n author = {Katja Zibrek and Elena Kokkinara and R. Mcdonnell},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {1681-1690},\n title = {The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character's Personality Play a Role?},\n volume = {24},\n year = {2018}\n}\n""}",,"{'volume': '24', 'pages': '1681-1690', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",47.0,The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character's Personality Play a Role?,2018.0
1821,9a9c83b55c1356a7eca08e22da7bf2c97e70772c,,"[{'authorId': '1742939', 'name': 'S. Buisine'}, {'authorId': '2094223', 'name': 'S. Abrilian'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}]",45.0,"{'bibtex': '@Inproceedings{Buisine2004EvaluationOM,\n author = {S. Buisine and S. Abrilian and Jean-Claude Martin},\n pages = {217-238},\n title = {Evaluation of Multimodal Behaviour of Embodied Agents},\n year = {2004}\n}\n'}",,{'pages': '217-238'},27.0,Evaluation of Multimodal Behaviour of Embodied Agents,2004.0
1822,9aadfa00377eea2b3c1a7607e372963efee248e6,"Nowadays, one of the important aspects of research on call/contact centre (CC) systems is how to automate their operations. Process automation is influenced by the continuous development in the implementation of virtual assistants. The effectiveness of virtual assistants depends on numerous factors. One of the most important is correctly recognizing the intent of clients conversing with the machine. Recognizing intentions is not an easy process, as often the client’s actual intentions can only be correctly identified after considering the client’s emotional state. When it comes to human–machine communication, the ability of a virtual assistant to recognize the client’s emotional state would greatly improve its effectiveness. This paper proposes a new method for recognizing interlocutors’ emotions dedicated directly to contact centre systems. The developed method provides opportunities to determine emotional states in text and voice channels. It provides opportunities to explore both the client’s and the agent’s emotional states. Information about agents’ emotions can be used to build their behavioural profiles, which is also applicable in contact centres. In addition, the paper explored the possibility of emotion assessment based on automatic transcriptions of recordings, which also positively affected emotion recognition performance in the voice channel. The research used actual conversations that took place during the operation of a large, commercial contact centre. The proposed solution makes it possible to recognize the emotions of customers contacting the hotline and agents handling these calls. Using this information in practical applications can increase the efficiency of agents’ work, efficiency of bots used in CC and increase customer satisfaction.","[{'authorId': '2176832114', 'name': 'Mirosław Płaza'}, {'authorId': '20702131', 'name': 'R. Kazala'}, {'authorId': '3487326', 'name': 'Z. Koruba'}, {'authorId': '30411283', 'name': 'Marcin Kozlowski'}, {'authorId': '1883034', 'name': 'Malgorzata Lucinska'}, {'authorId': '2189620851', 'name': 'Kamil Sitek'}, {'authorId': '2189646228', 'name': 'Jarosław Spyrka'}]",2.0,"{'bibtex': '@Article{Płaza2022EmotionRM,\n author = {Mirosław Płaza and R. Kazala and Z. Koruba and Marcin Kozlowski and Malgorzata Lucinska and Kamil Sitek and Jarosław Spyrka},\n booktitle = {Applied Sciences},\n journal = {Applied Sciences},\n title = {Emotion Recognition Method for Call/Contact Centre Systems},\n year = {2022}\n}\n'}","[{'paperId': '1e83ab09190afe862573f2484fdf34629920dec9', 'title': 'A review of natural language processing in contact centre automation'}, {'paperId': 'dda9641f4d2e099308b9dbe61a2e6880af5a8464', 'title': 'Role of Technology Innovation in Telemedicine: Focus on Sport Nutrition'}]",{'name': 'Applied Sciences'},0.0,Emotion Recognition Method for Call/Contact Centre Systems,2022.0
1823,9b035b1bdf28dd0c947dc8fcec715e5aca3360b7,,"[{'authorId': '4672023', 'name': 'L. Hayduk'}]",241.0,"{'bibtex': '@Article{Hayduk1978PersonalSA,\n author = {L. Hayduk},\n journal = {Psychological Bulletin},\n pages = {117-134},\n title = {Personal Space: An Evaluative and Orienting Overview.},\n volume = {85},\n year = {1978}\n}\n'}",,"{'volume': '85', 'pages': '117-134', 'name': 'Psychological Bulletin'}",143.0,Personal Space: An Evaluative and Orienting Overview.,1978.0
1825,9b77131efc055b70a1bc8352752c8a34939c1191,"A growing interest in the HCI community is the design and development of embodied agents in virtual environments. For virtual environments where social interaction is needed, an agent's facial expression may communicate emotive state to users both young and old. However, younger and older adults differ in how they label human facial expressions (Ruffman et al., 2008). Such possible age-related differences in labeling virtual agent expressions may impact the user's social experience in a virtual environment. The purpose of the current research was to investigate age-related differences in emotion recognition of several on-screen characters of varying degrees of human-likeness. Participants performed a recognition task with three characters demonstrating four basic emotions or neutral. The results indicated age-related differences for all character types. Older adults commonly mislabeled the human and synthetic human emotions of anger, fear, sadness, and neutral. For the virtual agent face, older adults commonly mislabeled the emotions of anger, fear, happiness, and neutral.","[{'authorId': '1809740', 'name': 'Jenay M. Beer'}, {'authorId': '1689705', 'name': 'A. D. Fisk'}, {'authorId': '145912604', 'name': 'W. Rogers'}]",10.0,"{'bibtex': '@Article{Beer2010RecognizingEI,\n author = {Jenay M. Beer and A. D. Fisk and W. Rogers},\n journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},\n pages = {2388 - 2392},\n title = {Recognizing Emotion in Virtual Agent, Synthetic Human, and Human Facial Expressions},\n volume = {54},\n year = {2010}\n}\n'}","[{'paperId': '45e2f7c6046a0d5880fd7ef6cb5e2efb1e426080', 'title': 'A Systematic Review of Human Factors Literature About Voice User Interfaces and Older Adults'}, {'paperId': 'c4252217a1e3f2583cb73b1e4d435c957ceec92c', 'title': 'Are facial emotion recognition tasks adequate for assessing social cognition in older people? A review of the literature.'}, {'paperId': '22198e76f30ebddca84ac06a12842bdb0882a367', 'title': 'Task characteristics influence facial emotion recognition age-effects: A meta-analytic review.'}, {'paperId': 'ca724971fc6253ae0e116cdb65c29eeb886c572e', 'title': 'Virtual eye region: development of a realistic model to convey emotion☆'}, {'paperId': 'aaff3d3da7d8b18cc5d9f85d5b1af63ec174fe84', 'title': 'The Perception of Emotion in Artificial Agents'}, {'paperId': '0aacb2390774249fd374ebfff1eb0a3a9d69dae0', 'title': 'Reconhecimento de emoções faciais no envelhecimento: uma revisão sistemática'}, {'paperId': 'c08b5a8338cdceae6d78b7108086e17d58c20b62', 'title': ""Younger and older users' recognition of virtual agent facial expressions""}, {'paperId': 'de4304f32502c1199a2ea46d67575cb3bcd40de5', 'title': 'Effects of Visual Fidelity on Biometric Cue Detection in Virtual Combat Profiling Training'}, {'paperId': '5ca1a4c07ff76c42c0e02df6dba13a58fa1bd95e', 'title': 'Emotion and motion: age-related differences in recognizing virtual agent facial expressions'}, {'paperId': '7186be4a04828e4e7436d95e4902d393b19f459e', 'title': 'Dynamic vs. Static'}]","{'name': 'Proceedings of the Human Factors and Ergonomics Society Annual Meeting', 'pages': '2388 - 2392', 'volume': '54'}",12.0,"Recognizing Emotion in Virtual Agent, Synthetic Human, and Human Facial Expressions",2010.0
1827,9b7a21e348b20bfd104087161549890e59b94cbc,"The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions. In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query $q$ will succeed when there is an argument ${\mathcal A}$ for $q$ that is warranted, i.e. the argument ${\mathcal A}$ that supports $q$ is found undefeated by a warrant procedure that implements a dialectical analysis. The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents.","[{'authorId': '2578572', 'name': 'A. García'}, {'authorId': '1790217', 'name': 'Guillermo R. Simari'}]",917.0,"{'bibtex': '@Article{García2003DefeasibleLP,\n author = {A. García and Guillermo R. Simari},\n journal = {Theory and Practice of Logic Programming},\n pages = {95 - 138},\n title = {Defeasible logic programming: an argumentative approach},\n volume = {4},\n year = {2003}\n}\n'}",,"{'volume': '4', 'pages': '95 - 138', 'name': 'Theory and Practice of Logic Programming'}",70.0,Defeasible logic programming: an argumentative approach,2003.0
1828,9bb28869d9808b12273c42229c2d1aa564e5bac8,,"[{'authorId': '46367714', 'name': 'J. Russell'}]",13178.0,"{'bibtex': '@Article{Russell1980ACM,\n author = {J. Russell},\n journal = {Journal of Personality and Social Psychology},\n pages = {1161-1178},\n title = {A circumplex model of affect.},\n volume = {39},\n year = {1980}\n}\n'}",,"{'volume': '39', 'pages': '1161-1178', 'name': 'Journal of Personality and Social Psychology'}",56.0,A circumplex model of affect.,1980.0
1834,9bc0d495ada08a5dddd0861a1f74dbd75122a5af,,"[{'authorId': '28226629', 'name': 'Bjoern Hartmann'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",247.0,"{'bibtex': '@Inproceedings{Hartmann2005ImplementingEG,\n author = {Bjoern Hartmann and M. Mancini and C. Pelachaud},\n pages = {188-199},\n title = {Implementing Expressive Gesture Synthesis for Embodied Conversational Agents},\n year = {2005}\n}\n'}",,{'pages': '188-199'},21.0,Implementing Expressive Gesture Synthesis for Embodied Conversational Agents,2005.0
1835,9bc780c0c9e8e4dba0533f10a0068f8476cf9b24,"The current framework of reinforcement learning is based on maximizing the expected returns based on scalar rewards. But in many real world situations, tradeoffs must be made among multiple objectives. Moreover, the agent's preferences between different objectives may vary with time. In this paper, we consider the problem of learning in the presence of time-varying preferences among multiple objectives, using numeric weights to represent their importance. We propose a method that allows us to store a finite number of policies, choose an appropriate policy for any weight vector and improve upon it. The idea is that although there are infinitely many weight vectors, they may be well-covered by a small number of optimal policies. We show this empirically in two domains: a version of the Buridan's ass problem and network routing.","[{'authorId': '145986014', 'name': 'Sriraam Natarajan'}, {'authorId': '1729906', 'name': 'Prasad Tadepalli'}]",141.0,"{'bibtex': '@Article{Natarajan2005DynamicPI,\n author = {Sriraam Natarajan and Prasad Tadepalli},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Dynamic preferences in multi-criteria reinforcement learning},\n year = {2005}\n}\n'}",,{'name': 'Proceedings of the 22nd international conference on Machine learning'},23.0,Dynamic preferences in multi-criteria reinforcement learning,2005.0
1836,9bd36d63aac878782184217d73e1fda9b2603bb5,"Masahiro Mori observed that as robots come to look more humanlike, they seem more familiar, until a point is reached at which subtle deviations from human norms cause them to look creepy. He referred to this dip in familiarity and corresponding surge in strangeness as the uncanny valley. The eerie sensation associated with a mismatch between human expectations and a robot’s behavior provides a useful source of feedback to improve the cognitive models implemented in the robot. Is the uncanny valley a necessary property of near-humanlike forms? This paper contributes to ongoing work in understanding the nature and causes of the uncanny valley by means of an experiment: 56 participants were asked to rate 13 robots and 1 human, shown in video clips, on a very mechanical (1) to very humanlike (9) scale, a very strange (1) to very familiar (9) scale, and a not eerie (0) to extremely eerie (10) scale. Contrary to earlier studies with morphs [MacDorman and Ishiguro, 2006], plots of average and median values for ratings on these scales do not reveal a single U-shaped valley as predicted by Mori’s uncanny valley hypothesis [1970], although his hypothesis allows for some variation owing to movement. Robots rated similarly on the mechanical versus humanlike scale can be rated quite differently on the strange versus familiar or the eeriness scales. The results indicate that the perceived human likeness of a robot is not the only factor determining the perceived familiarity, strangeness, or eeriness of the robot. This suggests that other factors could be manipulated to vary the familiarity, strangeness, or eeriness of a robot independently of its human likeness. Introduction To build robots that at least superficially approach human likeness is leading to insights in human perception and face-to-face interaction. These android robots possess the physical presence that simulated characters lack, yet can be more perfectly controlled than any human actor, to isolate the factor under study. Even in experiments in which the android’s responses are identical, we can observe how human responses vary according to their beliefs. For example, Japanese participants showed the same modesty with their eyes by averting gaze downward when interacting with an android as when interacting with a human interlocutor if they believed the android were under human control by telepresence [MacDorman et al., 2005]. In addition, androids provide an ideal testing ground for theories from the social and cognitive sciences because competing models can be implemented in an android and then tested by letting the android interact with industrial robot humanoid robot","[{'authorId': '1690354', 'name': 'K. Macdorman'}]",242.0,"{'bibtex': '@Inproceedings{Macdorman2006SubjectiveRO,\n author = {K. Macdorman},\n title = {Subjective Ratings of Robot Video Clips for Human Likeness , Familiarity , and Eeriness : An Exploration of the Uncanny Valley},\n year = {2006}\n}\n'}",,,8.0,"Subjective Ratings of Robot Video Clips for Human Likeness , Familiarity , and Eeriness : An Exploration of the Uncanny Valley",2006.0
1837,9bddf85953ae8276b728b70950f0cdc0503e5cd3,"Virtual humans are embodied software agents that should not only be realistic looking but also have natural and realistic behaviors. Traditional virtual human systems learn these interaction behaviors by observing how individuals respond in face-to-face situations (i.e., dir ect interaction). In contrast, this paper introduces a novel methodological approach called paras ocial consensus sampling (PCS) which allows multiple individuals to vicariously experience the same situation to gain insight on the typical (i.e., consensus view) of human responses in social interaction. This approach can help tease a part what is idiosyncratic from what is essential and help reveal the strength of cues that elicit social responses. Our PCS approach has several advantages over traditional methods: (1) it integrates data from multiple independent listeners interacting with the same speaker, (2) it associates probability of how likely feedback will be given over time, (3) it can be used as a prior to analyze and understand the face-to-face interaction data, (4) it facilitates much quicker and cheaper data collection. In this paper, we apply our PCS approach to learn a predictive model of listener backchannel feedback. Our experiments demonstrate that a virtual human driven by our PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data.","[{'authorId': '2110799090', 'name': 'Lixing Huang'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",59.0,"{'bibtex': '@Inproceedings{Huang2010ParasocialCS,\n author = {Lixing Huang and Louis-Philippe Morency and J. Gratch},\n pages = {1265-1272},\n title = {Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior},\n year = {2010}\n}\n'}",,{'pages': '1265-1272'},28.0,Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior,2010.0
1838,9be875c4202a52cd3fce2f20b9bb419989197fdd,"We report on an experiment that examined the influence of anthropomorphism and perceived agency on presence, copresence, and social presence in a virtual environment. The experiment varied the level of anthropomorphism of the image of interactants: high anthropomorphism, low anthropomorphism, or no image. Perceived agency was manipulated by telling the participants that the image was either an avatar controlled by a human, or an agent controlled by a computer. The results support the prediction that people respond socially to both human and computer-controlled entities, and that the existence of a virtual image increases tele-presence. Participants interacting with the less-anthropomorphic image reported more copresence and social presence than those interacting with partners represented by either no image at all or by a highly anthropomorphic image of the other, indicating that the more anthropomorphic images set up higher expectations that lead to reduced presence when these expectations were not met.","[{'authorId': '40372043', 'name': 'Kristine L. Nowak'}, {'authorId': '1726689', 'name': 'F. Biocca'}]",873.0,"{'bibtex': ""@Article{Nowak2003TheEO,\n author = {Kristine L. Nowak and F. Biocca},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {481-494},\n title = {The Effect of the Agency and Anthropomorphism on Users' Sense of Telepresence, Copresence, and Social Presence in Virtual Environments},\n volume = {12},\n year = {2003}\n}\n""}",,"{'volume': '12', 'pages': '481-494', 'name': 'Presence: Teleoperators & Virtual Environments'}",67.0,"The Effect of the Agency and Anthropomorphism on Users' Sense of Telepresence, Copresence, and Social Presence in Virtual Environments",2003.0
1840,9bfd7c5c87dcad98b17b16ba1f3b01e549b8b3d5,"A fresh assessment of what autonomous agents are or may aspire to be is presented in the paper. We dismiss the traditional dichotomy between reactive and symbolic architectures. A classiication in terms of the amount of knowledge embedded in the system is made, instead, running from regulatory to planning to adaptive agents. Regulatory agents are implementable as control systems or automata, and represent the ideal case in which the agent has all the knowledge it needs. We nd that each type of agent is best suited for a particular kind of behavior. The diierent kinds of behavior form a hierarchy in terms of frequency of occurrence and response time, and do not gracefully reduce to each other. This suggests implementing agents as hierarchies of simpler agents, each specialized in the kind of behavior it implements. In this scheme, agents at one level act through agents at the next lower level. We introduce the concept of drive to address the issue of where an agent's goals come from. A discussion is made of common pitfalls and suggestions for the design of planning agents. Finally, a parallel architecture that embodies many of the ideas presented is sketched and discussed. This architecture solves the two major problems present in Georgee's earlier architecture, PRS: strictly sequential execution, and inability to learn.","[{'authorId': '1763484', 'name': 'J. Brustoloni'}]",55.0,"{'bibtex': '@Inproceedings{Brustoloni1991AutonomousAC,\n author = {J. Brustoloni},\n title = {Autonomous Agents: Characterization and Requirements},\n year = {1991}\n}\n'}",,"{'volume': '', 'name': ''}",17.0,Autonomous Agents: Characterization and Requirements,1991.0
1841,9c0bedf98c4fa51676669cc509b0d97e4c7c5fd0,"In human dialogue, listener feedback is a pervasive phenomenon that serves important functions in the coordination of the conversation, both in regulating its flow, as well as in creating and ensuring understanding between interlocutors. This make feedback an interesting mechanism for conversational human-agent interaction. In this paper we describe computational models for an attentive speaker' agent is able to (1) interpret the feedback behaviour of its human interlocutors by probabilistically attributing listening-related mental states to them; (2) incrementally adapt its ongoing language and behaviour generation to their needs; and (3) elicit feedback from them when needed. We present a semi-autonomous interaction study, in which we compare such an attentive speaker agent with agents that either do not adapt their behaviour to their listeners' needs, or employ highly explicit ways of ensuring understanding. The results show that human interlocutors interacting with the attentive speaker agent provided significantly more listener feedback, felt that the agent was attentive to, and adaptive to their feedback, attested the agent a desire to be understood, and rated it more helpful in resolving difficulties in their understanding.","[{'authorId': '2849488', 'name': 'Hendrik Buschmeier'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",25.0,"{'bibtex': '@Inproceedings{Buschmeier2018CommunicativeLF,\n author = {Hendrik Buschmeier and S. Kopp},\n pages = {1213-1221},\n title = {Communicative Listener Feedback in Human-Agent Interaction: Artificial Speakers Need to Be Attentive and Adaptive},\n year = {2018}\n}\n'}",,{'pages': '1213-1221'},45.0,Communicative Listener Feedback in Human-Agent Interaction: Artificial Speakers Need to Be Attentive and Adaptive,2018.0
1842,9c14dc8a394f22c65775dbfe762c450a5bd7173f,"This article presents six ideas about the construction of emotion: (a) Emotions are more readily distinguished by the situations they signify than by patterns of bodily responses; (b) emotions emerge from, rather than cause, emotional thoughts, feelings, and expressions; (c) the impact of emotions is constrained by the nature of the situations they represent; (d) in the OCC account (the model proposed by Ortony, Clore, and Collins in 1988), appraisals are psychological aspects of situations that distinguish one emotion from another, rather than triggers that elicit emotions; (e) analyses of the affective lexicon indicate that emotion words refer to internal mental states focused on affect; (f) the modularity of emotion, long sought in biology and behavior, exists as mental schemas for interpreting human experience in story, song, drama, and conversation.","[{'authorId': '31458494', 'name': 'G. Clore'}, {'authorId': '1802934', 'name': 'A. Ortony'}]",172.0,"{'bibtex': '@Article{Clore2013PsychologicalCI,\n author = {G. Clore and A. Ortony},\n journal = {Emotion Review},\n pages = {335 - 343},\n title = {Psychological Construction in the OCC Model of Emotion},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '335 - 343', 'name': 'Emotion Review'}",40.0,Psychological Construction in the OCC Model of Emotion,2013.0
1843,9c1ddc2a368768fa73fa4d3e0dd02eadf9578a39,"Nonverbal behaviors play a crucial role in shaping outcomes in face-to-face clinical interactions. Experienced clinicians use nonverbals to foster rapport and ""read"" their clients to inform diagnoses. The rise of telemedicine and virtual health agents creates new opportunities, but it also strips away much of this nonverbal channel. Recent advances in low-cost computer vision and sensing technologies have the potential to address this challenge by learning to recognize nonverbal cues from large datasets of clinical interactions. These techniques can enhance both telemedicine and the emerging technology of virtual health agents. This article describes our current research in addressing these challenges in the domain of PTSD and depression screening for U.S. Veterans. We describe our general approach and report on our initial contribution: the creation of a large dataset of clinical interview data that facilitates the training of user-state sensing technology.","[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '6349590', 'name': 'Jill Boberg'}, {'authorId': '39471600', 'name': 'S. Koenig'}, {'authorId': '49651370', 'name': 'Todd Adamson'}, {'authorId': '29861580', 'name': 'A. Rizzo'}]",30.0,"{'bibtex': '@Article{Gratch2013UserStateSF,\n author = {J. Gratch and Louis-Philippe Morency and Stefan Scherer and Giota Stratou and Jill Boberg and S. Koenig and Todd Adamson and A. Rizzo},\n journal = {Studies in health technology and informatics},\n pages = {\n          151-7\n        },\n title = {User-State Sensing for Virtual Health Agents and TeleHealth Applications},\n volume = {184},\n year = {2013}\n}\n'}",,"{'volume': '184', 'pages': '\n          151-7\n        ', 'name': 'Studies in health technology and informatics'}",31.0,User-State Sensing for Virtual Health Agents and TeleHealth Applications,2013.0
1844,9c2aa9d9dd8ab8ee5d928bd12dad2627e10dba2a,"Facial muscular reactions to avatars' static (neutral, happy, angry) and dynamic (morphs developing from neutral to happy or angry) facial expressions, presented for 1 s each, were investigated in 48 participants. Dynamic expressions led to better recognition rates and higher intensity and realism ratings. Angry expressions were rated as more intense than happy expressions. EMG recordings indicated emotion-specific reactions to happy avatars as reflected in increased M. zygomaticus major and decreased M. corrugator supercilii tension, with stronger reactions to dynamic as compared to static expressions. Although rated as more intense, angry expressions elicited no significant M. corrugator supercilii activation. We conclude that facial reactions to angry and to happy facial expressions hold different functions in social interactions. Further research should vary dynamics in different ways and also include additional emotional expressions.","[{'authorId': '152592651', 'name': 'P. Weyers'}, {'authorId': '1684604', 'name': 'A. Mühlberger'}, {'authorId': '5599789', 'name': 'Carolin Hefele'}, {'authorId': '145825010', 'name': 'P. Pauli'}]",220.0,"{'bibtex': '@Article{Weyers2006ElectromyographicRT,\n author = {P. Weyers and A. Mühlberger and Carolin Hefele and P. Pauli},\n journal = {Psychophysiology},\n pages = {\n          450-3\n        },\n title = {Electromyographic responses to static and dynamic avatar emotional facial expressions.},\n volume = {43 5},\n year = {2006}\n}\n'}",,"{'volume': '43 5', 'pages': '\n          450-3\n        ', 'name': 'Psychophysiology'}",13.0,Electromyographic responses to static and dynamic avatar emotional facial expressions.,2006.0
1845,9c2f2fe4cbf7c30362f2818392b464e539b5aaca,,"[{'authorId': '5622454', 'name': 'Jessica L. Lakin'}, {'authorId': '5486548', 'name': 'V. Jefferis'}, {'authorId': '2193145799', 'name': 'C. M. Cheng'}, {'authorId': '6026289', 'name': 'T. Chartrand'}]",934.0,"{'bibtex': '@Article{Lakin2003TheCE,\n author = {Jessica L. Lakin and V. Jefferis and C. M. Cheng and T. Chartrand},\n journal = {Journal of Nonverbal Behavior},\n pages = {145-162},\n title = {The Chameleon Effect as Social Glue: Evidence for the Evolutionary Significance of Nonconscious Mimicry},\n volume = {27},\n year = {2003}\n}\n'}",,"{'volume': '27', 'pages': '145-162', 'name': 'Journal of Nonverbal Behavior'}",77.0,The Chameleon Effect as Social Glue: Evidence for the Evolutionary Significance of Nonconscious Mimicry,2003.0
1846,9c4230f57112ee254dfe80e4fb3b1eaa9a76aba6,"The realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop CVE. Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an emotibox that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '38811484', 'name': 'N. Yee'}, {'authorId': '3044182', 'name': 'D. Merget'}, {'authorId': '144194373', 'name': 'Ralph Schroeder'}]",349.0,"{'bibtex': '@Article{Bailenson2006TheEO,\n author = {J. Bailenson and N. Yee and D. Merget and Ralph Schroeder},\n journal = {PRESENCE: Teleoperators and Virtual Environments},\n pages = {359-372},\n title = {The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction},\n volume = {15},\n year = {2006}\n}\n'}",,"{'volume': '15', 'pages': '359-372', 'name': 'PRESENCE: Teleoperators and Virtual Environments'}",42.0,"The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction",2006.0
1847,9c6d9244be3f4628a4b7fa31def3452aa02f20d2,"Systematic observation and a questionnaire format were used to investigate the relationship between posture sharing and self-report indications of rapport in a group situation-college seminar classrooms. A pattern of significant positive correlations revealed that the greater the amount of mirroring and congruent postures evidenced by students vis-à-vis the teacher, the higher the ratings of involvement. Conversely, a significant negative relationship was found between amount of incongruent posture display and reports of interest. Implications of the findings for group dynamics and environmental design are discussed.","[{'authorId': '37544705', 'name': 'M. LaFrance'}, {'authorId': '113109009', 'name': 'Maida Broadbent'}]",220.0,"{'bibtex': '@Article{LaFrance1976GroupRP,\n author = {M. LaFrance and Maida Broadbent},\n journal = {Group & Organization Management},\n pages = {328 - 333},\n title = {Group Rapport: Posture Sharing as a Nonverbal Indicator},\n volume = {1},\n year = {1976}\n}\n'}",,"{'volume': '1', 'pages': '328 - 333', 'name': 'Group & Organization Management'}",11.0,Group Rapport: Posture Sharing as a Nonverbal Indicator,1976.0
1848,9c7a12d7b74f3ce349fdc70b73132a7ed1b7ce8f,"Current 3D capture and modeling technology can rapidly generate highly photo‐realistic 3D avatars of human subjects. However, while the avatars look like their human counterparts, their movements often do not mimic their own due to existing challenges in accurate motion capture and retargeting. A better understanding of factors that influence the perception of biological motion would be valuable for creating virtual avatars that capture the essence of their human subjects. To investigate these issues, we captured 22 subjects walking in an open space. We then performed a study where participants were asked to identify their own motion in varying visual representations and scenarios. Similarly, participants were asked to identify the motion of familiar individuals. Unlike prior studies that used captured footage with simple “point‐light” displays, we rendered the motion on photo‐realistic 3D virtual avatars of the subject. We found that self‐recognition was significantly higher for virtual avatars than with point‐light representations. Users were more confident of their responses when identifying their motion presented on their virtual avatar. Recognition rates varied considerably between motion types for recognition of others, but not for self‐recognition. Overall, our results are consistent with previous studies that used recorded footage and offer key insights into the perception of motion rendered on virtual avatars.","[{'authorId': '3294674', 'name': 'S. Narang'}, {'authorId': '10817944', 'name': 'A. Best'}, {'authorId': '2149730', 'name': 'Andrew W. Feng'}, {'authorId': '34728215', 'name': 'Sin-Hwa Kang'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}, {'authorId': '145109163', 'name': 'Ari Shapiro'}]",28.0,"{'bibtex': '@Article{Narang2017MotionRO,\n author = {S. Narang and A. Best and Andrew W. Feng and Sin-Hwa Kang and Dinesh Manocha and Ari Shapiro},\n journal = {Computer Animation and Virtual Worlds},\n title = {Motion recognition of self and others on realistic 3D avatars},\n volume = {28},\n year = {2017}\n}\n'}",,"{'volume': '28', 'name': 'Computer Animation and Virtual Worlds'}",24.0,Motion recognition of self and others on realistic 3D avatars,2017.0
1850,9c9e452376340e26faadfd4af300aab2cc53621a,"Recently, there has been a significant amount of work on the recognition of emotions from speech and biosignals. Most approaches to emotion recognition so far concentrate on a single modality and do not take advantage of the fact that an integrated multimodal analysis may help to resolve ambiguities and compensate for errors. In this paper, we describe various methods for fusing physiological and voice data at the feature-level and the decision-level as well as a hybrid integration scheme. The results of the integrated recognition approach are then compared with the individual recognition results from each modality.","[{'authorId': '11384820', 'name': 'Jonghwa Kim'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '39957689', 'name': 'M. Rehm'}, {'authorId': '30169286', 'name': 'Thurid Vogt'}, {'authorId': '6164138', 'name': 'J. Wagner'}]",63.0,"{'bibtex': '@Inproceedings{Kim2005IntegratingIF,\n author = {Jonghwa Kim and E. André and M. Rehm and Thurid Vogt and J. Wagner},\n pages = {809-812},\n title = {Integrating information from speech and physiological signals to achieve emotional sensitivity},\n year = {2005}\n}\n'}",,{'pages': '809-812'},19.0,Integrating information from speech and physiological signals to achieve emotional sensitivity,2005.0
1851,9ca8d729ea640b9367cc7e91b853ee8ed8e75a32,,"[{'authorId': '2110799084', 'name': 'Lixing Huang'}, {'authorId': '2065275646', 'name': 'Louis-Philippe Morency'}, {'authorId': '2064713461', 'name': 'Jonathan Gratch'}]",156.0,"{'bibtex': '@Inproceedings{Huang2011VirtualR2,\n author = {Lixing Huang and Louis-Philippe Morency and Jonathan Gratch},\n pages = {68-79},\n title = {Virtual Rapport 2.0},\n year = {2011}\n}\n'}",,{'pages': '68-79'},36.0,Virtual Rapport 2.0,2011.0
1853,9cdb0c27520c3160d18362bcdeb1b1bd2ee09713,,"[{'authorId': '3213354', 'name': 'B. Gelder'}]",690.0,"{'bibtex': '@Article{Gelder2006TowardsTN,\n author = {B. Gelder},\n journal = {Nature Reviews Neuroscience},\n pages = {242-249},\n title = {Towards the neurobiology of emotional body language},\n volume = {7},\n year = {2006}\n}\n'}",,"{'volume': '7', 'pages': '242-249', 'name': 'Nature Reviews Neuroscience'}",133.0,Towards the neurobiology of emotional body language,2006.0
1854,9ce631d8474187cb4d468d2b8bdba5984f66b1f1,,"[{'authorId': '1775438', 'name': 'G. Venture'}, {'authorId': '2354572', 'name': 'H. Kadone'}, {'authorId': '1400133821', 'name': 'Tianxiang Zhang'}, {'authorId': '2582927', 'name': 'J. Grèzes'}, {'authorId': '1706764', 'name': 'A. Berthoz'}, {'authorId': '1912361', 'name': 'Halim Hicheur'}]",82.0,"{'bibtex': '@Article{Venture2014RecognizingEC,\n author = {G. Venture and H. Kadone and Tianxiang Zhang and J. Grèzes and A. Berthoz and Halim Hicheur},\n journal = {International Journal of Social Robotics},\n pages = {621-632},\n title = {Recognizing Emotions Conveyed by Human Gait},\n volume = {6},\n year = {2014}\n}\n'}",,"{'volume': '6', 'pages': '621-632', 'name': 'International Journal of Social Robotics'}",29.0,Recognizing Emotions Conveyed by Human Gait,2014.0
1856,9d164c20015a447bc8b7875c82822bb7342f3f92,,"[{'authorId': '2289828', 'name': 'C. Harlow'}, {'authorId': '2072712874', 'name': 'Shiquan Peng'}]",58.0,"{'bibtex': '@Article{Harlow2001AutomaticVC,\n author = {C. Harlow and Shiquan Peng},\n journal = {Transportation Research Part C-emerging Technologies},\n pages = {231-247},\n title = {Automatic vehicle classification system with range sensors},\n volume = {9},\n year = {2001}\n}\n'}",,"{'volume': '9', 'pages': '231-247', 'name': 'Transportation Research Part C-emerging Technologies'}",24.0,Automatic vehicle classification system with range sensors,2001.0
1857,9d80e083087f1d6f985f515d7fd974eba0f5bae7,,"[{'authorId': '21112145', 'name': 'Joshua D. Greene'}, {'authorId': '2480714', 'name': 'J. Haidt'}]",1642.0,"{'bibtex': '@Article{Greene2002HowW,\n author = {Joshua D. Greene and J. Haidt},\n journal = {Trends in Cognitive Sciences},\n pages = {517-523},\n title = {How (and where) does moral judgment work?},\n volume = {6},\n year = {2002}\n}\n'}",,"{'volume': '6', 'pages': '517-523', 'name': 'Trends in Cognitive Sciences'}",63.0,How (and where) does moral judgment work?,2002.0
1858,9d952862420dcd3e1d17d5e551eaa124ee1021eb,,"[{'authorId': '144074133', 'name': 'M. Hoogendoorn'}, {'authorId': '1726343', 'name': 'Jan Treur'}, {'authorId': '1881843', 'name': 'C. N. V. D. Wal'}, {'authorId': '1809908', 'name': 'A. V. Wissen'}]",16.0,"{'bibtex': '@Article{Hoogendoorn2011AgentBasedMO,\n author = {M. Hoogendoorn and Jan Treur and C. N. V. D. Wal and A. V. Wissen},\n journal = {Trans. Comput. Collect. Intell.},\n pages = {152-179},\n title = {Agent-Based Modelling of the Emergence of Collective States Based on Contagion of Individual States in Groups},\n volume = {3},\n year = {2011}\n}\n'}",,"{'volume': '3', 'pages': '152-179', 'name': 'Trans. Comput. Collect. Intell.'}",46.0,Agent-Based Modelling of the Emergence of Collective States Based on Contagion of Individual States in Groups,2011.0
1859,9dcbc0995422e017cec91f9c4302485fe88b0f3b,"In social media, people often press a ""Like"" button to indicate their shared interest in a particular content or to acknowledge the user who posted the content. Such activities form relationships and networks among people, raising interesting questions about their unique characteristics and implications. However, little research has investigated such Likes as a main study focus. To address this lack of understanding, based on a theoretical framework, we present an analysis of the structural, influential, and contextual aspects of Like activities from the test datasets of 20 million users and their 2 billion Like activities in Instagram. Our study results first highlight that Like activities and networks increase exponentially, and are formed and developed by one's friends and many random users. Second, we observe that five other essential Instagram elements influence the number of Likes to different extents, but following others will not necessarily increase the number of Likes that one receives. Third, we explore the relationship between LDA-based topics and Likes, characterize two user groups-specialists and generalists-and show that specialists tend to receive more Likes and promote themselves more than generalists. We finally discuss theoretical and practical implications and future research directions.","[{'authorId': '1835709', 'name': 'J. Jang'}, {'authorId': '35655049', 'name': 'Kyungsik Han'}, {'authorId': '145948198', 'name': 'Dongwon Lee'}]",44.0,"{'bibtex': '@Article{Jang2015NoRI,\n author = {J. Jang and Kyungsik Han and Dongwon Lee},\n journal = {Proceedings of the 26th ACM Conference on Hypertext & Social Media},\n title = {No Reciprocity in ""Liking"" Photos: Analyzing Like Activities in Instagram},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 26th ACM Conference on Hypertext & Social Media'},43.0,"No Reciprocity in ""Liking"" Photos: Analyzing Like Activities in Instagram",2015.0
1860,9dd45c22cd7fd84d616de698dbdd96fa343ec2db,,"[{'authorId': '1708945', 'name': 'W. Gerstner'}, {'authorId': '34847772', 'name': 'W. M. Kistler'}]",391.0,"{'bibtex': '@Article{Gerstner2002MathematicalFO,\n author = {W. Gerstner and W. M. Kistler},\n journal = {Biological Cybernetics},\n pages = {404-415},\n title = {Mathematical formulations of Hebbian learning},\n volume = {87},\n year = {2002}\n}\n'}",,"{'volume': '87', 'pages': '404-415', 'name': 'Biological Cybernetics'}",52.0,Mathematical formulations of Hebbian learning,2002.0
1861,9dd9101cdb97a1b613f6168c1c77444c39b1fa81,"Individuals with Autism Spectrum Disorder (ASD) are known to have significantly limited social interaction abilities, which are often manifested in different non-verbal cues of communication such as facial expression, atypical eye gaze response. While prior works leveraged the role of pupil response for screening ASD, limited works have been carried out to find the influence of emotion stimuli on pupil response for ASD screening. We, in this paper, design, develop, and evaluate a light-weight LSTM (Long-short Term Memory) model that captures pupil responses (pupil diameter, fixation duration, and fixation location) based on the social interaction with a virtual agent and detects ASD sessions based on short interactions. Our findings demonstrate that all the pupil responses vary significantly in the ASD sessions in response to the different emotion (angry, happy, neutral) stimuli applied. These findings reinforce the ASD screening with an average accuracy of 77%, while the accuracy improves further (>80%) with respect to angry and happy emotion stimuli.","[{'authorId': '40598977', 'name': 'Surjya Ghosh'}, {'authorId': '1720741', 'name': 'T. Guha'}]",1.0,"{'bibtex': '@Article{Ghosh2021TowardsAS,\n author = {Surjya Ghosh and T. Guha},\n booktitle = {Annual International Conference of the IEEE Engineering in Medicine and Biology Society},\n journal = {2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)},\n pages = {820-823},\n title = {Towards Autism Screening through Emotion-guided Eye Gaze Response},\n year = {2021}\n}\n'}","[{'paperId': 'a18a443ccc1164c87b1cb1832f006a965017b110', 'title': 'Technologies to support the diagnosis and/or treatment of neurodevelopmental disorders: A systematic review'}]","{'name': '2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)', 'pages': '820-823'}",24.0,Towards Autism Screening through Emotion-guided Eye Gaze Response,2021.0
1862,9dd9e154c79176b7df5f1e52e5d534089b302f1d,,"[{'authorId': '3641490', 'name': 'J. Eastwood'}, {'authorId': '2562337', 'name': 'D. Smilek'}, {'authorId': '5367295', 'name': 'P. Merikle'}]",601.0,"{'bibtex': '@Article{Eastwood2001DifferentialAG,\n author = {J. Eastwood and D. Smilek and P. Merikle},\n journal = {Perception & Psychophysics},\n pages = {1004-1013},\n title = {Differential attentional guidance by unattended faces expressing positive and negative emotion},\n volume = {63},\n year = {2001}\n}\n'}",,"{'volume': '63', 'pages': '1004-1013', 'name': 'Perception & Psychophysics'}",45.0,Differential attentional guidance by unattended faces expressing positive and negative emotion,2001.0
1863,9ddd1b65e133de101c0100e90e0588c075d6fba8,"Regulating emotions volitionally requires the inhibition and modification of an elicited emotional action readiness and includes phases of reflection, planning and self-regulation. The proposed internalization model of reflective emotion regulation argues that caregivers’ co-regulation of emotionally challenging events plays a constitutive role for the development of 4- to 6-year-olds’ reflective emotion regulation. The model specifies the gradual shift from co- to self-regulation by focusing on two important ways how caregivers structure emotionally challenging interactions: Through emotion talk, caregivers promote the development of preschoolers’ emotional awareness. Once established, they support children in establishing a repertoire of effective emotion regulation strategies and they guide preschoolers’ emerging skills to generate, evaluate, and select from alternative appraisals or behavioral responses.","[{'authorId': '115964516', 'name': 'Judith Rebecca Silkenbeumer'}, {'authorId': '2052760391', 'name': 'Eva-Maria Schiller'}, {'authorId': '6051757', 'name': 'Manfred Holodynski'}, {'authorId': '4185170', 'name': 'Joscha Kärtner'}]",23.0,"{'bibtex': '@Inproceedings{Silkenbeumer2016TheRO,\n author = {Judith Rebecca Silkenbeumer and Eva-Maria Schiller and Manfred Holodynski and Joscha Kärtner},\n pages = {17-32},\n title = {The role of co-regulation for the development of social-emotional competence},\n volume = {2},\n year = {2016}\n}\n'}",,"{'volume': '2', 'pages': '17-32', 'name': ''}",87.0,The role of co-regulation for the development of social-emotional competence,2016.0
1864,9dfcec6f6fe0645d13c1c2618eb500cd30287ae5,"The rapid growth of Internet users and diverse Web 5.0 technologies has made the research scholars and educators to think of intelligent tools such as; expert-like virtual agents as an effective e-learning tool. The success of an e-learning tool relies primarily on the level of acceptance of the tool by the e-learners and their state of emotion while using the tool. Therefore, addressing e-learner's emotional competences is important to understand the implicit emotion of the young minds. In this context, virtual agents are expected to play the role of instructors in a virtual learning environment. This paper aims to study the affective design of the Virtual Agent in terms of attractiveness and realism that adopts Kansei Engineering, an emerging technology used to explore and validate the emotion structure defined as a complex state of feelings that can result in the changes of human behaviour and thoughts of e-learners. Adding on, the paper also highlights and validates the six universal emotions such as anger, happiness, surprise, disgust, sadness and fear. The findings suggest that the most attractive and realistic virtual agents have higher Kansei value. Some implications of the findings in relation to expert-like virtual agent design and Kansei are discussed in this paper.","[{'authorId': '9157318', 'name': 'C. R. Ramachandiran'}, {'authorId': '2191058', 'name': 'N. Jomhari'}]",1.0,"{'bibtex': '@Conference{Ramachandiran2014ElearnersKE,\n author = {C. R. Ramachandiran and N. Jomhari},\n booktitle = {International Conference on User Science and Engineering},\n journal = {2014 3rd International Conference on User Science and Engineering (i-USEr)},\n pages = {36-41},\n title = {E-learners Kansei experience towards expert-like virtual agent: A case study},\n year = {2014}\n}\n'}","[{'paperId': '953c3448757683f6b441c2c3ff12c409abe1e5ab', 'title': 'Virtual Reality Based Behavioural Learning for Autistic Children.'}]","{'name': '2014 3rd International Conference on User Science and Engineering (i-USEr)', 'pages': '36-41'}",28.0,E-learners Kansei experience towards expert-like virtual agent: A case study,2014.0
1865,9e0197052e4d35d0a8d81bb2cd37721753270f1c,,"[{'authorId': '2118717268', 'name': 'Tengfei Xu'}, {'authorId': '2064971197', 'name': 'Dongdong Shi'}, {'authorId': '2108324508', 'name': 'Juan Chen'}, {'authorId': '2149199538', 'name': 'Tao Li'}, {'authorId': '2087501138', 'name': 'P. Lin'}, {'authorId': '119597887', 'name': 'Jian Ma'}]",16.0,"{'bibtex': '@Article{Xu2020DynamicsOE,\n author = {Tengfei Xu and Dongdong Shi and Juan Chen and Tao Li and P. Lin and Jian Ma},\n journal = {Physics Letters A},\n pages = {126080},\n title = {Dynamics of emotional contagion in dense pedestrian crowds},\n volume = {384},\n year = {2020}\n}\n'}",,"{'volume': '384', 'pages': '126080', 'name': 'Physics Letters A'}",30.0,Dynamics of emotional contagion in dense pedestrian crowds,2020.0
1866,9e2a69d52cbd91e6d2bb242d13412fa3920f9baa,"Facial actions are key elements of non-verbal behavior. Perceivers’ reactions to others’ facial expressions often represent a match or mirroring (e.g., they smile to a smile). However, the information conveyed by an expression depends on context. Thus, when shown by an opponent, a smile conveys bad news and evokes frowning. The availability of anthropomorphic agents capable of facial actions raises the question of how people respond to such agents in social context. We explored this issue in a study where participants played a strategic game with or against a facially expressive android. Electromyography (EMG) recorded participants’ reactions over zygomaticus muscle (smiling) and corrugator muscle (frowning). We found that participants’ facial responses to android’s expressions reflect their informational value, rather than a direct match. Overall, participants smiled more, and frowned less, when winning than losing. Critically, participants’ responses to the game outcome were similar regardless of whether it was conveyed via the android’s smile or frown. Furthermore, the outcome had greater impact on people’s facial reactions when it was conveyed through android’s face than a computer screen. These findings demonstrate that facial actions of artificial agents impact human facial responding. They also suggest a sophistication in human-robot communication that highlights the signaling value of facial expressions.","[{'authorId': '3092566', 'name': 'Galit Hofree'}, {'authorId': '12114845', 'name': 'P. Ruvolo'}, {'authorId': '2073397460', 'name': 'Audrey Reinert'}, {'authorId': '2218905', 'name': 'M. Bartlett'}, {'authorId': '3122131', 'name': 'P. Winkielman'}]",11.0,"{'bibtex': '@Article{Hofree2018BehindTR,\n author = {Galit Hofree and P. Ruvolo and Audrey Reinert and M. Bartlett and P. Winkielman},\n journal = {Frontiers in Neurorobotics},\n title = {Behind the Robot’s Smiles and Frowns: In Social Context, People Do Not Mirror Android’s Expressions But React to Their Informational Value},\n volume = {12},\n year = {2018}\n}\n'}",,"{'volume': '12', 'name': 'Frontiers in Neurorobotics'}",58.0,"Behind the Robot’s Smiles and Frowns: In Social Context, People Do Not Mirror Android’s Expressions But React to Their Informational Value",2018.0
1867,9e7c6355f3a50cf5aa3c2a1e4d5db56d289b14a4,,"[{'authorId': '1780319', 'name': 'K. Norman'}, {'authorId': '2872825', 'name': 'Sean M. Polyn'}, {'authorId': '2096648', 'name': 'Greg Detre'}, {'authorId': '2327323', 'name': 'J. Haxby'}]",2225.0,"{'bibtex': '@Article{Norman2006BeyondMM,\n author = {K. Norman and Sean M. Polyn and Greg Detre and J. Haxby},\n journal = {Trends in Cognitive Sciences},\n pages = {424-430},\n title = {Beyond mind-reading: multi-voxel pattern analysis of fMRI data},\n volume = {10},\n year = {2006}\n}\n'}",,"{'volume': '10', 'pages': '424-430', 'name': 'Trends in Cognitive Sciences'}",57.0,Beyond mind-reading: multi-voxel pattern analysis of fMRI data,2006.0
1868,9e865ddf1ddccc49b39c795c09aa02a0392dab2d,,"[{'authorId': '9059336', 'name': 'A. S. DiCriscio'}, {'authorId': '2562268', 'name': 'V. Troiani'}]",27.0,"{'bibtex': '@Article{DiCriscio2017PupilAC,\n author = {A. S. DiCriscio and V. Troiani},\n journal = {Scientific Reports},\n title = {Pupil adaptation corresponds to quantitative measures of autism traits in children},\n volume = {7},\n year = {2017}\n}\n'}",,"{'volume': '7', 'name': 'Scientific Reports'}",56.0,Pupil adaptation corresponds to quantitative measures of autism traits in children,2017.0
1869,9ea80fd6ce6ad3e12e43338802d26443ce126f9f,"The emulation theory of representation is developed and explored as a framework that can revealingly synthesize a wide variety of representational functions of the brain. The framework is based on constructs from control theory (forward models) and signal processing (Kalman filters). The idea is that in addition to simply engaging with the body and environment, the brain constructs neural circuits that act as models of the body and environment. During overt sensorimotor engagement, these models are driven by efference copies in parallel with the body and environment, in order to provide expectations of the sensory feedback, and to enhance and process sensory information. These models can also be run off-line in order to produce imagery, estimate outcomes of different actions, and evaluate and develop motor plans. The framework is initially developed within the context of motor control, where it has been shown that inner models running in parallel with the body can reduce the effects of feedback delay problems. The same mechanisms can account for motor imagery as the off-line driving of the emulator via efference copies. The framework is extended to account for visual imagery as the off-line driving of an emulator of the motor-visual loop. I also show how such systems can provide for amodal spatial imagery. Perception, including visual perception, results from such models being used to form expectations of, and to interpret, sensory input. I close by briefly outlining other cognitive functions that might also be synthesized within this framework, including reasoning, theory of mind phenomena, and language.","[{'authorId': '3107833', 'name': 'R. Grush'}]",1176.0,"{'bibtex': '@Article{Grush2004TheET,\n author = {R. Grush},\n journal = {Behavioral and Brain Sciences},\n pages = {377 - 396},\n title = {The emulation theory of representation: Motor control, imagery, and perception},\n volume = {27},\n year = {2004}\n}\n'}",,"{'volume': '27', 'pages': '377 - 396', 'name': 'Behavioral and Brain Sciences'}",337.0,"The emulation theory of representation: Motor control, imagery, and perception",2004.0
1870,9eb485c8eac410c6a9a704ee2f7d98ab95cb07ce,"It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content. One such type of information consists of cues to the speaker's personality traits, the most fundamental dimension of variation between humans. Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality affects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications. However, to date, there is little work on the automatic recognition of personality traits. This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality. While other work reports classification results, we experiment with classification, regression and ranking models. For each model, we analyse the effect of different feature sets on accuracy. Results show that for some traits, any type of statistical model performs significantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classifiers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using self-reports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models confirms previous findings linking language and personality, while revealing many new linguistic markers.","[{'authorId': '1769183', 'name': 'François Mairesse'}, {'authorId': '2234088162', 'name': 'M. Walker'}, {'authorId': '1760412', 'name': 'M. Mehl'}, {'authorId': '145914568', 'name': 'Roger K. Moore'}]",894.0,"{'bibtex': '@Article{Mairesse2007UsingLC,\n author = {François Mairesse and M. Walker and M. Mehl and Roger K. Moore},\n journal = {J. Artif. Intell. Res.},\n pages = {457-500},\n title = {Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text},\n volume = {30},\n year = {2007}\n}\n'}",,"{'volume': '30', 'pages': '457-500', 'name': 'J. Artif. Intell. Res.'}",94.0,Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text,2007.0
1871,9ed50b34b8264750d5377f83cbe271a5bb335bc1,"Recent calls to improve the quality of education in schools have drawn attention to the importance of teachers’ preparation for work in classroom settings. Although the practicum has long been the traditional means for pre-service teachers to learn and practice classroom teaching, it does not always offer student teachers the time, safe practice experiences, repetition, or extensive feedback needed for them to gain adequate knowledge, skills, and confidence. Well-designed simulations can augment the practicum and address these gaps. This study evaluated the design of simSchool (v.1), an online simulation for pre-service teachers, using student teachers’ ratings of selected factors, including realism, appropriateness of content and curriculum, appropriateness for target users, and user interaction. Based on these ratings, the study identified strengths and weaknesses, and suggested improvements for the software. Participant ratings varied considerably but indicated that certain aspects of the simulation, such as its educational value, classroom challenges, and simulated student characteristics, were moderately well received. However, user interface navigation and the range and realism of simulated teacher–student interactions should be improved.","[{'authorId': '112854120', 'name': 'Farnaz Badiee'}, {'authorId': '47330994', 'name': 'D. Kaufman'}]",42.0,"{'bibtex': '@Article{Badiee2015DesignEO,\n author = {Farnaz Badiee and D. Kaufman},\n journal = {SAGE Open},\n title = {Design Evaluation of a Simulation for Teacher Education},\n volume = {5},\n year = {2015}\n}\n'}",,"{'volume': '5', 'name': 'SAGE Open'}",57.0,Design Evaluation of a Simulation for Teacher Education,2015.0
1872,9edf74cfa5d65e198832242d7362727fda68f34d,"The annual meeting of the AASHTO Subcommittee on Design, Task Force on Geometric Design was held in Charleston, South Carolina during the period July 13 through July 16, 2003. The purpose of the meeting was to review several documents undergoing revision, review the proposed revisions to the superelevation discussion in the Green Book, consider future research, and other administrative matters of concern to the Task Force Membership.","[{'authorId': '94698724', 'name': 'Bryan E. Little'}, {'authorId': '94945302', 'name': 'T. A. Mize'}, {'authorId': '36691104', 'name': 'R. J. Bailey'}]",2361.0,"{'bibtex': '@Inproceedings{Little2000AmericanAO,\n author = {Bryan E. Little and T. A. Mize and R. J. Bailey},\n title = {American Association of State Highway and Transportation Officials. Highway Drainage Guidelines American Association of State Highway and Transportation Officials. LRFD Bridge Design Specifications},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",3.0,American Association of State Highway and Transportation Officials. Highway Drainage Guidelines American Association of State Highway and Transportation Officials. LRFD Bridge Design Specifications,2000.0
1873,9ef698ea3e28aea9d65642e662f864053553a68d,,"[{'authorId': '1739256', 'name': 'B. D. Carolis'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1802126', 'name': 'I. Poggi'}, {'authorId': '145332819', 'name': 'Mark Steedman'}]",283.0,"{'bibtex': '@Inproceedings{Carolis2004APMLAM,\n author = {B. D. Carolis and C. Pelachaud and I. Poggi and Mark Steedman},\n pages = {65-86},\n title = {APML, a Markup Language for Believable Behavior Generation},\n year = {2004}\n}\n'}",,{'pages': '65-86'},31.0,"APML, a Markup Language for Believable Behavior Generation",2004.0
1875,9f3179d54bde8c29ed56d996d1010216691be895,"People share their photos with captions to interact with followers, and followers tap ""likes"" and leave ""comments"" on the posts. These activities are significant to build relationships and networks between people on Instagram. Numerous researches on human-human interaction on social media have been done for a long time, but communications with the virtual agent based on computer-generated lead new research questions to the field of human-computer interaction. We crawled quantitative data from three virtual agents accounts on Instagram to analyze the phenomenon that actual users on social media want to interact and engage with virtual agents. The purpose of our work is to find out how the number of ""likes"" and ""comments"" are associated with photography and caption. Throughout the quantitative works, we found out that people have tendencies to leave more ""likes"" and ""comments"" on the post where virtual agents express their emotions. We conclude that emotion and relationship between virtual agents themselves stir up strong interest actual users on social media.","[{'authorId': '2421079', 'name': 'Jaeeun Shin'}, {'authorId': '2108243215', 'name': 'Sangwon Lee'}]",8.0,"{'bibtex': '@Article{Shin2020IntimacyBA,\n author = {Jaeeun Shin and Sangwon Lee},\n booktitle = {International Conference on Ubiquitous Information Management and Communication},\n journal = {2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM)},\n pages = {1-4},\n title = {Intimacy Between Actual Users and Virtual Agents: Interaction through ""likes"" and ""comments""},\n year = {2020}\n}\n'}","[{'paperId': '215e172746f0130168a75769682885bb6a571f71', 'title': 'Social Media Users’ Affective, Attitudinal, and Behavioral Responses to Virtual Human Emotions'}, {'paperId': '2d075954caac1250d459f0febc2623163fab4302', 'title': 'Can Computer Virtual Influencers Replace Human Influencers in the Future? An Empirical Investigation in the Age of Digital Transformation'}, {'paperId': 'f944d61a602318d09a10d02898233bb749361fd1', 'title': ""Fashion Virtual Influencers: Antecedents Influencing Females' Behavioral Intentions in Jordan""}, {'paperId': '85d7f26842bbda10dd482af7dc9e0c6897502a4d', 'title': 'Unveiling Behind-the-Scenes Human Interventions and Examining Source Orientation in Virtual Influencer Endorsements'}, {'paperId': '575e03d770727707d5bc939fd3c500a3fec70960', 'title': 'Parasocial interactions with real and virtual influencers: The role of perceived similarity and human-likeness'}, {'paperId': 'ddf35cb00b55ea2fb5d29f577bc2ec8acaae49be', 'title': '""I Felt a Little Crazy Following a \'Doll\'""'}, {'paperId': '0601c24976d6dfefb2b4dab4cb1626debf5acfad', 'title': 'Computers as Social Actors? Examining How Users Perceive and Interact with Virtual Influencers on Social Media'}, {'paperId': '5f384fedf125578af559c9c54a9f6e051affa148', 'title': 'Technology Adoption Models: Users’ Online Social Media Behavior Towards Visual Information'}]","{'name': '2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM)', 'pages': '1-4'}",8.0,"Intimacy Between Actual Users and Virtual Agents: Interaction through ""likes"" and ""comments""",2020.0
1876,9f367a7ea95814d827bf4859129868c8e4a5e1c1,,"[{'authorId': '10125477', 'name': 'Céline Jacob'}, {'authorId': '1892904', 'name': 'N. Guéguen'}, {'authorId': '4889270', 'name': 'Angélique Martin'}, {'authorId': '2084060441', 'name': 'Gaëlle Boulbry'}]",74.0,"{'bibtex': ""@Article{Jacob2011RetailSM,\n author = {Céline Jacob and N. Guéguen and Angélique Martin and Gaëlle Boulbry},\n journal = {Journal of Retailing and Consumer Services},\n pages = {381-388},\n title = {Retail salespeople's mimicry of customers: Effects on consumer behavior},\n volume = {18},\n year = {2011}\n}\n""}",,"{'volume': '18', 'pages': '381-388', 'name': 'Journal of Retailing and Consumer Services'}",52.0,Retail salespeople's mimicry of customers: Effects on consumer behavior,2011.0
1877,9f3ce188893ce94f58e5a0eda3722bad404e7caa,"The authors have been developing human-like head robots in order to develop new head mechanisms and functions for a humanoid robot that has the ability to communicate naturally with a human by expressing human-like emotion. We believe that in the future, it will be necessary for personal robots to generate active behavior to interact bilaterally between human and robot. Therefore, we developed an emotion expression humanoid robot WE-4R (Waseda eye no.4 refined) in 2003. The need model consisting of the ""appetite,"" the ""need for security"" and the ""need for exploration"" was introduced to the mental model for humanoid robots. We also defined the ""need matrix"" and introduced the ""equations of need"" to describe the robot needs. Robots with the need model can generate and express active behavior according to their need. This paper describes the need model that is implemented in the emotion expression humanoid robot WE-4R.","[{'authorId': '2065285579', 'name': 'H. Miwa'}, {'authorId': '46677342', 'name': 'K. Itoh'}, {'authorId': '2057391878', 'name': 'D. Ito'}, {'authorId': '1678499', 'name': 'H. Takanobu'}, {'authorId': '1737432', 'name': 'A. Takanishi'}]",35.0,"{'bibtex': '@Article{Miwa2003IntroductionOT,\n author = {H. Miwa and K. Itoh and D. Ito and H. Takanobu and A. Takanishi},\n journal = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},\n pages = {1400-1406 vol.2},\n title = {Introduction of the need model for humanoid robots to generate active behavior},\n volume = {2},\n year = {2003}\n}\n'}",,"{'volume': '2', 'pages': '1400-1406 vol.2', 'name': 'Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)'}",10.0,Introduction of the need model for humanoid robots to generate active behavior,2003.0
1878,9f5ac049fe4aa8274f76a9ee1dbf9d135158e0c4,"Research in organizational behavior focuses on expressed and felt emotions as indicators of employee health and satisfaction. In contrast, less conceptual and empirical work addresses the display of feelings as part of the job. This paper proposes a conceptual framework to guide theory development and research about the causes, qualities, and consequences of emotions that are expressed to fulfill role expectations.","[{'authorId': '2539868', 'name': 'A. Rafaeli'}, {'authorId': '3169834', 'name': 'R. I. Sutton'}]",1456.0,"{'bibtex': '@Article{Rafaeli1987ExpressionOE,\n author = {A. Rafaeli and R. I. Sutton},\n journal = {Academy of Management Review},\n pages = {23-37},\n title = {Expression of Emotion as Part of the Work Role},\n volume = {12},\n year = {1987}\n}\n'}",,"{'volume': '12', 'pages': '23-37', 'name': 'Academy of Management Review'}",40.0,Expression of Emotion as Part of the Work Role,1987.0
1879,9f86366feecbcfdf6c5be165fcf38c679164cc89,"This Viewpoint discusses the opportunities and ethical implications of using machine learning technologies, which can rapidly collect and learn from large amounts of personal data, to provide individalized patient care.","[{'authorId': '6120000', 'name': 'Alison M Darcy'}, {'authorId': '7172935', 'name': 'A. Louie'}, {'authorId': '34934943', 'name': 'L. Roberts'}]",304.0,"{'bibtex': '@Article{Darcy2016MachineLA,\n author = {Alison M Darcy and A. Louie and L. Roberts},\n journal = {JAMA},\n pages = {\n          551-2\n        },\n title = {Machine Learning and the Profession of Medicine.},\n volume = {315 6},\n year = {2016}\n}\n'}",,"{'volume': '315 6', 'pages': '\n          551-2\n        ', 'name': 'JAMA'}",6.0,Machine Learning and the Profession of Medicine.,2016.0
1880,9f8829bd05291512aea33c84697fada9fc0b16dc,,"[{'authorId': '2609243', 'name': 'Aryel Beck'}, {'authorId': '1713009', 'name': 'L. Cañamero'}, {'authorId': '3171783', 'name': 'Antoine Hiolle'}, {'authorId': '144252425', 'name': 'L. Damiano'}, {'authorId': '1809640', 'name': 'P. Cosi'}, {'authorId': '2526140', 'name': 'F. Tesser'}, {'authorId': '2547905', 'name': 'Giacomo Sommavilla'}]",66.0,"{'bibtex': '@Article{Beck2013InterpretationOE,\n author = {Aryel Beck and L. Cañamero and Antoine Hiolle and L. Damiano and P. Cosi and F. Tesser and Giacomo Sommavilla},\n journal = {International Journal of Social Robotics},\n pages = {325-334},\n title = {Interpretation of Emotional Body Language Displayed by a Humanoid Robot: A Case Study with Children},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '325-334', 'name': 'International Journal of Social Robotics'}",37.0,Interpretation of Emotional Body Language Displayed by a Humanoid Robot: A Case Study with Children,2013.0
1881,9f88ce3ee9a8b1aaebbe51306602ba0895b82979,,"[{'authorId': '2239042739', 'name': 'Peter J. Bieling'}, {'authorId': '2239042892', 'name': 'Martin M. Antony'}, {'authorId': '2239042892', 'name': 'Martin M. Antony'}, {'authorId': '6796306', 'name': 'R. Swinson'}]",615.0,"{'bibtex': '@Article{Bieling1998TheSA,\n author = {Peter J. Bieling and Martin M. Antony and Martin M. Antony and R. Swinson},\n journal = {Behaviour research and therapy},\n pages = {\n          777-88\n        },\n title = {The State-Trait Anxiety Inventory, Trait version: structure and content re-examined.},\n volume = {36 7-8},\n year = {1998}\n}\n'}",,"{'volume': '36 7-8', 'pages': '\n          777-88\n        ', 'name': 'Behaviour research and therapy'}",20.0,"The State-Trait Anxiety Inventory, Trait version: structure and content re-examined.",1998.0
1882,9f9bd3cfcf5850d738015eb60241456eeacb2919,,"[{'authorId': '153196760', 'name': 'P. Robert'}, {'authorId': '4797664', 'name': 'K. Lanctôt'}, {'authorId': '114185955', 'name': 'L. Agüera-Ortiz'}, {'authorId': '3160020', 'name': 'P. Aalten'}, {'authorId': '2166562522', 'name': 'François Brémond'}, {'authorId': '4238269', 'name': 'M. Defrancesco'}, {'authorId': '2080994290', 'name': 'C. Hanon'}, {'authorId': '145768419', 'name': 'R. David'}, {'authorId': '2114244374', 'name': 'B. Dubois'}, {'authorId': '21834198', 'name': 'K. Dujardin'}, {'authorId': '2099406843', 'name': 'M. Husain'}, {'authorId': '144632221', 'name': 'A. König'}, {'authorId': '145293730', 'name': 'R. Levy'}, {'authorId': '5634092', 'name': 'V. Mantua'}, {'authorId': '7912759', 'name': 'D. Meulien'}, {'authorId': '2115908421', 'name': 'Daniel W. Miller'}, {'authorId': '117054199', 'name': 'H. Moebius'}, {'authorId': '5802006', 'name': 'J. Rasmussen'}, {'authorId': '49561639', 'name': 'G. Robert'}, {'authorId': '7747482', 'name': 'M. Ruthirakuhan'}, {'authorId': '143837246', 'name': 'F. Stella'}, {'authorId': '2235415', 'name': 'J. Yesavage'}, {'authorId': '51186373', 'name': 'Radia Zeghari'}, {'authorId': '4192721', 'name': 'V. Manera'}]",188.0,"{'bibtex': '@Article{Robert2018IsIT,\n author = {P. Robert and K. Lanctôt and L. Agüera-Ortiz and P. Aalten and François Brémond and M. Defrancesco and C. Hanon and R. David and B. Dubois and K. Dujardin and M. Husain and A. König and R. Levy and V. Mantua and D. Meulien and Daniel W. Miller and H. Moebius and J. Rasmussen and G. Robert and M. Ruthirakuhan and F. Stella and J. Yesavage and Radia Zeghari and V. Manera},\n journal = {European Psychiatry},\n pages = {71 - 76},\n title = {Is it time to revise the diagnostic criteria for apathy in brain disorders? The 2018 international consensus group},\n volume = {54},\n year = {2018}\n}\n'}",,"{'volume': '54', 'pages': '71 - 76', 'name': 'European Psychiatry'}",40.0,Is it time to revise the diagnostic criteria for apathy in brain disorders? The 2018 international consensus group,2018.0
1883,9fbffcff3fb9b90e092fabd9bf9d6786f3d22f23,"Facial autonomic responses may contribute to emotional communication and reveal individual affective style. In this study, the authors examined how observed pupillary size modulates processing of facial expression, extending the finding that incidentally perceived pupils influence ratings of sadness but not those of happy, angry, or neutral facial expressions. Healthy subjects rated the valence and arousal of photographs depicting facial muscular expressions of sadness, surprise, fear, and disgust. Pupil sizes within the stimuli were experimentally manipulated. Subjects themselves were scored with an empathy questionnaire. Diminishing pupil size linearly enhanced intensity and valence judgments of sad expressions (but not fear, surprise, or disgust). At debriefing, subjects were unaware of differences in pupil size across stimuli. These observations complement an earlier study showing that pupil size directly influences processing of sadness but not other basic emotional facial expressions. Furthermore, across subjects, the degree to which pupil size influenced sadness processing correlated with individual differences in empathy score. Together, these data demonstrate a central role of sadness processing in empathetic emotion and highlight the salience of implicit autonomic signals in affective communication.","[{'authorId': '2247127725', 'name': 'Neil A Harrison'}, {'authorId': '2249717128', 'name': 'C. E. Wilson'}, {'authorId': '2247122717', 'name': 'Hugo D Critchley'}, {'authorId': '2247128150', 'name': 'A. Harrison'}]",85.0,"{'bibtex': '@Article{Harrison2007ProcessingOO,\n author = {Neil A Harrison and C. E. Wilson and Hugo D Critchley and A. Harrison},\n journal = {Emotion},\n pages = {\n          724-9\n        },\n title = {Processing of observed pupil size modulates perception of sadness and predicts empathy.},\n volume = {7 4},\n year = {2007}\n}\n'}",,"{'volume': '7 4', 'pages': '\n          724-9\n        ', 'name': 'Emotion'}",37.0,Processing of observed pupil size modulates perception of sadness and predicts empathy.,2007.0
1884,9fd351c71b8dabd09f131d9ee01807674a91c381,"This article presents an overview of a relatively recent cognitive architecture, and its internal control structures, that is, its motivational and metacognitive mechanisms. The chapter starts with a look at some general ideas underlying this cognitive architecture and the relevance of these ideas to cognitive modeling of agents. It then presents a sketch of some details of the architecture and their uses in cognitive modeling of specific tasks.","[{'authorId': '145966408', 'name': 'R. Sun'}]",25.0,"{'bibtex': '@Inproceedings{Sun2005TheMA,\n author = {R. Sun},\n title = {The Motivational and Metacognitive Control in CLARION},\n year = {2005}\n}\n'}",,,38.0,The Motivational and Metacognitive Control in CLARION,2005.0
1885,9fdc872bcf20c3515aff1130fe574f1a132a9e24,,"[{'authorId': '2671891', 'name': 'Zerrin Kasap'}, {'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}]",84.0,"{'bibtex': '@Article{Kasap2007IntelligentVH,\n author = {Zerrin Kasap and N. Magnenat-Thalmann},\n journal = {Intell. Decis. Technol.},\n pages = {3-15},\n title = {Intelligent virtual humans with autonomy and personality: State-of-the-art},\n volume = {1},\n year = {2007}\n}\n'}",,"{'volume': '1', 'pages': '3-15', 'name': 'Intell. Decis. Technol.'}",189.0,Intelligent virtual humans with autonomy and personality: State-of-the-art,2007.0
1888,9fdde358066f0e9ad7c9c6872e06a7cf6cd3b654,,"[{'authorId': '117992260', 'name': 'Lykken Dt'}, {'authorId': '2227993146', 'name': 'Richard Rose'}, {'authorId': '101868644', 'name': 'B. Luther'}, {'authorId': '143935798', 'name': 'M. Maley'}]",253.0,"{'bibtex': '@Article{Dt1966CorrectingPM,\n author = {Lykken Dt and Richard Rose and B. Luther and M. Maley},\n journal = {Psychological bulletin},\n pages = {\n          481-4\n        },\n title = {Correcting psychophysiological measures for individual differences in range.},\n volume = {66 6},\n year = {1966}\n}\n'}",,"{'volume': '66 6', 'pages': '\n          481-4\n        ', 'name': 'Psychological bulletin'}",2.0,Correcting psychophysiological measures for individual differences in range.,1966.0
1889,a009e11ffa7c6b2a21da3d66675858a04ea8c396,"Facial expressions are of eminent importance for social interaction as they convey information about other individuals’ emotions and social intentions. According to the predominant “basic emotion” approach, the perception of emotion in faces is based on the rapid, automatic categorization of prototypical, universal expressions. Consequently, the perception of facial expressions has typically been investigated using isolated, de-contextualized, static pictures of facial expressions that maximize the distinction between categories. However, in everyday life, an individual’s face is not perceived in isolation, but almost always appears within a situational context, which may arise from other people, the physical environment surrounding the face, as well as multichannel information from the sender. Furthermore, situational context may be provided by the perceiver, including already present social information gained from affective learning and implicit processing biases such as race bias. Thus, the perception of facial expressions is presumably always influenced by contextual variables. In this comprehensive review, we aim at (1) systematizing the contextual variables that may influence the perception of facial expressions and (2) summarizing experimental paradigms and findings that have been used to investigate these influences. The studies reviewed here demonstrate that perception and neural processing of facial expressions are substantially modified by contextual information, including verbal, visual, and auditory information presented together with the face as well as knowledge or processing biases already present in the observer. These findings further challenge the assumption of automatic, hardwired categorical emotion extraction mechanisms predicted by basic emotion theories. Taking into account a recent model on face processing, we discuss where and when these different contextual influences may take place, thus outlining potential avenues in future research.","[{'authorId': '34439843', 'name': 'M. Wieser'}, {'authorId': '2256291', 'name': 'T. Brosch'}]",319.0,"{'bibtex': '@Article{Wieser2012FacesIC,\n author = {M. Wieser and T. Brosch},\n journal = {Frontiers in Psychology},\n title = {Faces in Context: A Review and Systematization of Contextual Influences on Affective Face Processing},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Psychology'}",156.0,Faces in Context: A Review and Systematization of Contextual Influences on Affective Face Processing,2012.0
1890,a00cc2b73f27a72cb16ee2e2be0232b39ae444a4,,"[{'authorId': '48887356', 'name': 'Joanna Schug'}, {'authorId': '145413880', 'name': 'D. Matsumoto'}, {'authorId': '3739936', 'name': 'Yutaka Horita'}, {'authorId': '35508387', 'name': 'T. Yamagishi'}, {'authorId': '47062630', 'name': 'K. Bonnet'}]",121.0,"{'bibtex': '@Article{Schug2010EmotionalEA,\n author = {Joanna Schug and D. Matsumoto and Yutaka Horita and T. Yamagishi and K. Bonnet},\n journal = {Evolution and Human Behavior},\n pages = {87-94},\n title = {Emotional expressivity as a signal of cooperation},\n volume = {31},\n year = {2010}\n}\n'}",,"{'volume': '31', 'pages': '87-94', 'name': 'Evolution and Human Behavior'}",52.0,Emotional expressivity as a signal of cooperation,2010.0
1891,a0234d30dfd26a8c0c75fc631c291df6f82a1400,"Conversational agents stand to play an important role in supporting behavior change and well-being in many domains. With users able to interact with conversational agents through both text and voice, understanding how designing for these channels supports behavior change is important. To begin answering this question, we designed a conversational agent for the workplace that supports workers' activity journaling and self-learning through reflection. Our agent, named Robota, combines chat-based communication as a Slack Bot and voice interaction through a personal device using a custom Amazon Alexa Skill. Through a 3-week controlled deployment, we examine how voice-based and chat-based interaction affect workers' reflection and support self-learning. We demonstrate that, while many current technical limitations exist, adding dedicated mobile voice interaction separate from the already busy chat modality may further enable users to step back and reflect on their work. We conclude with discussion of the implications of our findings to design of workplace self-tracking systems specifically and to behavior-change systems in general.","[{'authorId': '3299344', 'name': 'Rafal Kocielnik'}, {'authorId': '2667384', 'name': 'Daniel Avrahami'}, {'authorId': '144841404', 'name': 'Jennifer Marlow'}, {'authorId': '2087234532', 'name': 'Di Lu'}, {'authorId': '1786841', 'name': 'Gary Hsieh'}]",76.0,"{'bibtex': '@Article{Kocielnik2018DesigningFW,\n author = {Rafal Kocielnik and Daniel Avrahami and Jennifer Marlow and Di Lu and Gary Hsieh},\n journal = {Proceedings of the 2018 Designing Interactive Systems Conference},\n title = {Designing for Workplace Reflection: A Chat and Voice-Based Conversational Agent},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 2018 Designing Interactive Systems Conference'},80.0,Designing for Workplace Reflection: A Chat and Voice-Based Conversational Agent,2018.0
1892,a03242b258194bff2e60200ec90608257f2ccc24,"Abstract : One of the key steps in creating quality interactive drama is the ability to create quality interactive characters (or believable agents). Two important aspects of such characters will be that they appear emotional and that they can engage in social interactions. My basic approach to these problems has been to use a broad agent architecture and minimal amounts of modeling of other agent in the environment. This approach is based on an understanding of the artistic nature of the problem. To enable agent-builders (artists) to create emotional agents, I provide a general framework for building emotional agents, default emotion-processing rules, and discussion about how to create quality, emotional characters. My framework gets a lot of its power from being part of a broad agent architecture. The concept is simple: the agent will be emotionally richer if there are more things to have emotions about and more ways to express them. This reliance on breadth has also meant that I have been able to create simple emotion models that rely on perception and motivation instead of deep modeling of other agents and complex cognitive processing. To enable agent builders to create social behaviors for believable agents, I have designed a methodology that provides heuristics for incorporating personality into social behaviors and suggests how to model other agents in the environment. I propose an approach to modeling other agents that calls for limiting the amount of modeling of other agents to that which is sufficient to create the desired behavior. Using this technique, I have been able to build robust social behaviors that use surprisingly little representation. I have used this methodology to build a number of social behaviors, like negotiation and making friends. I have built three simulations containing seven agents to drive and test this work.","[{'authorId': '145207410', 'name': 'J. Bates'}, {'authorId': '2228592956', 'name': 'Jaime Carbonell'}, {'authorId': '2054613750', 'name': 'Reid G. Simmons'}, {'authorId': '145788442', 'name': 'A. Sloman'}, {'authorId': '2053266146', 'name': 'W. Scott'}, {'authorId': '2229738965', 'name': 'Neal Reilly'}]",488.0,"{'bibtex': '@Inproceedings{Bates1996BelievableSA,\n author = {J. Bates and Jaime Carbonell and Reid G. Simmons and A. Sloman and W. Scott and Neal Reilly},\n title = {Believable Social and Emotional Agents.},\n year = {1996}\n}\n'}",,"{'volume': '', 'name': ''}",129.0,Believable Social and Emotional Agents.,1996.0
1896,a0650855634a156db81a01dcdceff931e9f1ac04,"In development for thirty years, Soar is a general cognitive architecture that integrates knowledge-intensive reasoning, reactive execution, hierarchical reasoning, planning, and learning from experience, with the goal of creating a general computational system that has the same cognitive abilities as humans. In contrast, most AI systems are designed to solve only one type of problem, such as playing chess, searching the Internet, or scheduling aircraft departures. Soar is both a software system for agent development and a theory of what computational structures are necessary to support human-level agents. Over the years, both software system and theory have evolved. This book offers the definitive presentation of Soar from theoretical and practical perspectives, providing comprehensive descriptions of fundamental aspects and new components. The current version of Soar features major extensions, adding reinforcement learning, semantic memory, episodic memory, mental imagery, and an appraisal-based model of emotion. This book describes details of Soar's component memories and processes and offers demonstrations of individual components, components working in combination, and real-world applications. Beyond these functional considerations, the book also proposes requirements for general cognitive architectures and explicitly evaluates how well Soar meets those requirements.","[{'authorId': '1715438', 'name': 'J. Laird'}]",896.0,"{'bibtex': '@Inproceedings{Laird2012TheSC,\n author = {J. Laird},\n title = {The Soar Cognitive Architecture},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",27.0,The Soar Cognitive Architecture,2012.0
1898,a068668b7b9aef0ea2f4041ab9d09d2ad5385fb6,,"[{'authorId': '49824574', 'name': 'G. Ball'}, {'authorId': '1778725', 'name': 'J. Breese'}]",253.0,"{'bibtex': '@Inproceedings{Ball2001EmotionAP,\n author = {G. Ball and J. Breese},\n pages = {189-219},\n title = {Emotion and personality in a conversational agent},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '189-219', 'name': ''}",0.0,Emotion and personality in a conversational agent,2001.0
1899,a06b1eb7972e0bbbffcea5f4a1a6409d3416598c,"ABSTRACT Deficits in social cognition following acquired brain injury (ABI) have been found to be both prevalent and disabling. Despite this, relatively little attention has been given to identifying the characteristics of such deficits in a systematic way. We describe the development of self and informant versions of a new questionnaire designed to measure the changes in social cognition that may occur following ABI, the Brain Injury Rehabilitation Trust (BIRT) Social Cognition Questionnaire (BSCQ). Seventy-two participants (Mean age  = 36 years, SD = 12), with different forms of ABI (76% traumatic brain injury, 8% cerebrovascular accident, 15% other) and who were on average 20 months post-injury (SD = 16), took part in the study. The measure demonstrates excellent psychometric properties, including high test-retest (.94) and split-half (.92) reliability, high internal consistency (Cronbach’s alpha = .92), and good concurrent validity. The questionnaire measures characteristics that are distinguishable from measures of cognitive ability. There was moderate overlap between self-report and informant versions of the questionnaire (r = .50), but the informant version had the strongest predictive value of outcome, measured with the Mayo-Portland Adaptability Inventory III, one year later. The potential uses of the measure in relation to theory and practice are discussed. The results suggest that the BSCQ is a useful screening tool for those with ABI.","[{'authorId': '37833856', 'name': 'Charlotte Cattran'}, {'authorId': '48525624', 'name': 'M. Oddy'}, {'authorId': '11884014', 'name': 'S. Ramos'}, {'authorId': '48906391', 'name': 'A. Goodson'}, {'authorId': '8559579', 'name': 'R. Wood'}]",4.0,"{'bibtex': '@Article{Cattran2018TheDO,\n author = {Charlotte Cattran and M. Oddy and S. Ramos and A. Goodson and R. Wood},\n journal = {Neuropsychological Rehabilitation},\n pages = {633 - 648},\n title = {The development of a measure of social cognition following acquired brain injury},\n volume = {28},\n year = {2018}\n}\n'}",,"{'volume': '28', 'pages': '633 - 648', 'name': 'Neuropsychological Rehabilitation'}",38.0,The development of a measure of social cognition following acquired brain injury,2018.0
1900,a0ab701f213b8b4da1cd5f63e762e5a663dc6c1c,"We present a Pedestrian Dominance Model (PDM) to identify the dominance characteristics of pedestrians for robot navigation. Through a perception study on a simulated dataset of pedestrians, PDM models the perceived dominance levels of pedestrians with varying motion behaviors corresponding to trajectory, speed, and personal space. At runtime, we use PDM to identify the dominance levels of pedestrians to facilitate socially-aware navigation for the robots. PDM can predict dominance levels from trajectories with $\sim 85$% accuracy. Prior studies in psychology literature indicate that when interacting with humans, people are more comfortable around people that exhibit complementary movement behaviors. Our algorithm leverages this by enabling the robots to exhibit complementing responses to pedestrian dominance. We also present an application of PDM for generating dominance-based collision-avoidance behaviors in the navigation of autonomous vehicles among pedestrians. We demonstrate the benefits of our algorithm for robots navigating among tens of pedestrians in simulated environments.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '40894651', 'name': 'Emily Kubin'}, {'authorId': '144236783', 'name': 'Austin Wang'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",22.0,"{'bibtex': '@Article{Randhavane2018PedestrianDM,\n author = {Tanmay Randhavane and Aniket Bera and Emily Kubin and Austin Wang and Kurt Gray and Dinesh Manocha},\n journal = {2019 International Conference on Robotics and Automation (ICRA)},\n pages = {5621-5628},\n title = {Pedestrian Dominance Modeling for Socially-Aware Robot Navigation},\n year = {2018}\n}\n'}",,"{'pages': '5621-5628', 'name': '2019 International Conference on Robotics and Automation (ICRA)'}",53.0,Pedestrian Dominance Modeling for Socially-Aware Robot Navigation,2018.0
1901,a0b7b6d5c582cf75cd5ff0b7caf456468c452d31,"This paper presents an empirically tested theoretical framework to explain user engagement and end‐user satisfaction with interactive agents. Such a framework is not only important from a scientific point of view; application designers may find a set of dos and don'ts that help them create more satisfying embodied agents in different task domains and social settings. From a multidisciplinary perspective, we have conducted a series of experiments to verify underlying mechanisms in the processes of interacting and engaging with embodied agents in various task domains. Our results show that the most commonly held views are not always tenable; sometimes other factors provide better explanations for liking an embodied agent or end‐user satisfaction. For example, it is not realism but rather affordances and ethics that are key for understanding user responses, and a beautiful design is not always the most preferable. From our results, guidelines for designers and future research are reflected upon. Copyright © 2009 John Wiley & Sons, Ltd.","[{'authorId': '1823909', 'name': 'H. C. V. Vugt'}, {'authorId': '71825175', 'name': 'J. Hoorn'}, {'authorId': '3132010', 'name': 'E. Konijn'}]",31.0,"{'bibtex': '@Article{Vugt2009InteractiveEW,\n author = {H. C. V. Vugt and J. Hoorn and E. Konijn},\n journal = {Computer Animation and Virtual Worlds},\n title = {Interactive engagement with embodied agents: an empirically validated framework},\n volume = {20},\n year = {2009}\n}\n'}",,"{'volume': '20', 'name': 'Computer Animation and Virtual Worlds'}",25.0,Interactive engagement with embodied agents: an empirically validated framework,2009.0
1903,a0c356da5a913c2290dac051b28a420ede632868,"We compile baselines, along with dataset split, for multimodal sentiment analysis. In this paper, we explore three different deep-learning-based architectures for multimodal sentiment classification, each improving upon the previous. Further, we evaluate these architectures with multiple datasets with fixed train/test partition. We also discuss some major issues, frequently ignored in multimodal sentiment analysis research, e.g., the role of speaker-exclusive models, the importance of different modalities, and generalizability. This framework illustrates the different facets of analysis to be considered while performing multimodal sentiment analysis and, hence, serves as a new benchmark for future research in this emerging field.","[{'authorId': '1746416', 'name': 'Soujanya Poria'}, {'authorId': '35122767', 'name': 'Navonil Majumder'}, {'authorId': '8223433', 'name': 'Devamanyu Hazarika'}, {'authorId': '49943757', 'name': 'E. Cambria'}, {'authorId': '145125161', 'name': 'A. Hussain'}, {'authorId': '1747784', 'name': 'Alexander Gelbukh'}]",147.0,"{'bibtex': '@Article{Poria2018MultimodalSA,\n author = {Soujanya Poria and Navonil Majumder and Devamanyu Hazarika and E. Cambria and A. Hussain and Alexander Gelbukh},\n journal = {IEEE Intelligent Systems},\n pages = {17-25},\n title = {Multimodal Sentiment Analysis: Addressing Key Issues and Setting Up the Baselines},\n volume = {33},\n year = {2018}\n}\n'}",,"{'volume': '33', 'pages': '17-25', 'name': 'IEEE Intelligent Systems'}",25.0,Multimodal Sentiment Analysis: Addressing Key Issues and Setting Up the Baselines,2018.0
1905,a0cc94417d5cb36051ac4c0b35c7cae6340f44fc,"Negative emotions such as anxiety, frustration, or apathy can have an impact on the brain capability in terms of memory and cognitive functions. This is particularly visible in Alzheimer’s disease where the participants can have a deterioration of their brain connections which are often the cause of the disorders detected in Alzheimer's participants. It seems important to reduce these symptoms to allow better access to memory and cognitive abilities. Immersion in Virtual Reality is a means of providing the participant with a sense of presence in an environment that isolates them from external factors that can induce negative emotions. The virtual travel is a method that can mobilize the attention of the subject and revive their interest and curiosity. We present here, an experiment in which a participant is immersed in a virtual train using a virtual headset and EEG device to measure the brain signals. To measure the impact of this train on the memory and cognitive functions, some cognitive tasks have been included before and after the travel. Experiments have been done on participants with mild cognitive disorder. Preliminary results show an increase of memory functions and in certain cases of cognitive functions, while negative emotions are reduced.","[{'authorId': '28987363', 'name': 'H. Abdessalem'}, {'authorId': '68990793', 'name': 'A. Byrns'}, {'authorId': '47258254', 'name': 'M. Cuesta'}, {'authorId': '4192721', 'name': 'V. Manera'}, {'authorId': '134599701', 'name': 'P. Robert'}, {'authorId': '34277926', 'name': 'M. Bruneau'}, {'authorId': '145580293', 'name': 'S. Belleville'}, {'authorId': '1788058', 'name': 'C. Frasson'}]",6.0,"{'bibtex': ""@Inproceedings{Abdessalem2020ApplicationOV,\n author = {H. Abdessalem and A. Byrns and M. Cuesta and V. Manera and P. Robert and M. Bruneau and S. Belleville and C. Frasson},\n pages = {52-60},\n title = {Application of Virtual Travel for Alzheimer's Disease},\n year = {2020}\n}\n""}",,{'pages': '52-60'},35.0,Application of Virtual Travel for Alzheimer's Disease,2020.0
1906,a113b3a576cd05ec56a039a3ee89e6d9b7ee208f,"Evaluation of spatial learning and memory in rodents is commonly carried out using different maze settings such as the Multiple T-Maze. State-of-the-art analysis is primarily based on statistics of quantitative measures stemming from animal trajectories in a maze, e.g. path length or correct decisions made. Currently trajectories themselves are analyzed and evaluated one at a time and comparison of multiple trajectories is a tedious task. The resulting ﬁndings may not fully answer complex questions that behavioral researchers encounter as well, e.g., why do animals behave in a certain way or can atypical behaviour be detected? This paper describes an innovative approach on how exploratory analysis for Multiple T-Maze studies can be enhanced through interactive visual analysis. We explain our solution for analyzing a whole ensemble of data at once and support the ﬁnding of orientational characteristics and migration patterns within the ensemble. We also abstract the analysis tasks for Multiple T-Maze studies and, based on these tasks, we extend a coordinated multiple views system to support the solving of fundamental problems which behavioral researchers face. Besides views of standard charts we deploy a multi-resolution heat map and the Gate-O-Gon, which is a novel visual element. It gives clues on the animals’ general movement orientation and distribution of revisited gates, as well as enhances the discovery of patterns in movement and identifying of irregular behavior. Finally we demonstrate the usefulness of the newly proposed approach using a real life data set consisting of 400 Multiple T-Maze runs.","[{'authorId': '83994270', 'name': 'Fabrizia Bechtold'}, {'authorId': '2050227', 'name': 'R. Splechtna'}, {'authorId': '1715967', 'name': 'K. Matkovič'}]",3.0,"{'bibtex': '@Inproceedings{Bechtold2018VisualEA,\n author = {Fabrizia Bechtold and R. Splechtna and K. Matkovič},\n pages = {203-213},\n title = {Visual Exploratory Analysis for Multiple T-Maze Studies},\n year = {2018}\n}\n'}",,{'pages': '203-213'},30.0,Visual Exploratory Analysis for Multiple T-Maze Studies,2018.0
1907,a1249e045a8db977ee827a8c4e2af7c3195b10d7,"Complex memory of personal events is thought to depend on coordinated reinstatement of cortical representations by the medial temporal lobes (MTL). MTL-cortical theta and gamma coupling is believed to mediate such coordination, but which cortical structures are critical for retrieval and how they influence oscillatory coupling is unclear. We used magnetoencephalography (MEG) combined with continuous theta burst stimulation (cTBS) to (i) clarify the roles of theta and gamma oscillations in network-wide communication during naturalistic memory retrieval, and (ii) understand the causal relationship between cortical network nodes and oscillatory communication. Retrieval was associated with MTL-posterior neocortical theta phase coupling and theta-gamma phase-amplitude coupling relative to a rest period. Precuneus cTBS altered MTL-neocortical communication by modulating theta and gamma oscillatory coupling. These findings provide a mechanistic account for MTL-cortical communication and demonstrate that the precuneus is a critical cortical node of oscillatory activity, coordinating cross-regional interactions that drive remembering.","[{'authorId': '5784462', 'name': 'Melissa Hebscher'}, {'authorId': '1957052', 'name': 'Jed A. Meltzer'}, {'authorId': '2220346', 'name': 'A. Gilboa'}]",49.0,"{'bibtex': '@Article{Hebscher2019ACR,\n author = {Melissa Hebscher and Jed A. Meltzer and A. Gilboa},\n journal = {eLife},\n title = {A causal role for the precuneus in network-wide theta and gamma oscillatory activity during complex memory retrieval},\n volume = {8},\n year = {2019}\n}\n'}",,"{'volume': '8', 'name': 'eLife'}",73.0,A causal role for the precuneus in network-wide theta and gamma oscillatory activity during complex memory retrieval,2019.0
1908,a1c5ac62bbdbe8cbb28fd14e895a3807de15a653,"In a group setting, it is possible for attributes of one group member to indirectly affect how other group members are perceived. In this paper, we explore whether one group member's agency (e.g. if they are real or virtual) can indirectly affect behavior with other group members. We also consider whether variations in the agency of a group member directly affects behavior with that group member. To do so, we examined gaze behavior during a team training exercise, in which sixty-nine nurses worked with a surgeon and an anesthesiologist to prepare a simulated patient for surgery. The agency of the surgeon and the anesthesiologist were varied between conditions. Nurses' gaze behavior was coded using videos of their interactions. Agency was observed to directly affect behavior, such that participants spent more time gazing at virtual teammates than human teammates. However, participants continued to obey polite gaze norms with virtual teammates. In contrast, agency was not observed to indirectly affect gaze behavior. The presence of a second human did not affect participants' gaze behavior with virtual teammates.","[{'authorId': '78886496', 'name': 'Andrew C. Robb'}, {'authorId': '2870739', 'name': 'A. Kleinsmith'}, {'authorId': '2127223', 'name': 'Andrew Cordar'}, {'authorId': '4535740', 'name': 'C. White'}, {'authorId': '1705856', 'name': 'S. Lampotang'}, {'authorId': '2639010', 'name': 'A. Wendling'}, {'authorId': '1708157', 'name': 'Benjamin C. Lok'}]",7.0,"{'bibtex': '@Article{Robb2016DoVI,\n author = {Andrew C. Robb and A. Kleinsmith and Andrew Cordar and C. White and S. Lampotang and A. Wendling and Benjamin C. Lok},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {1336-1345},\n title = {Do Variations in Agency Indirectly Affect Behavior with Others? An Analysis of Gaze Behavior},\n volume = {22},\n year = {2016}\n}\n'}",,"{'volume': '22', 'pages': '1336-1345', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",31.0,Do Variations in Agency Indirectly Affect Behavior with Others? An Analysis of Gaze Behavior,2016.0
1909,a2428eb9531abfbb463934163b68ac7de223bf91,"We present the design of an online social skills development interface for teenagers with autism spectrum disorder (ASD), who often lack access to social skills training. The interface is intended to enable private conversation practice anywhere, anytime using a browser. Users converse informally with an on-screen persona, receiving feedback on nonverbal cues in real-time, and summary feedback. The prototype was developed in consultation with an expert UX designer, two psychologists, and a pediatrician. Using the data from 47 individuals, feedback and dialogue generation were automated using the hidden Markov model and a schema-driven dialogue manager capable of handling multi-topic conversations. We conducted a study with nine high-functioning ASD teenagers, and through thematic analysis of post-experiment interviews, identified several key design considerations, notably: 1) Users should be fully briefed at the outset about the purpose and limitations of the system, to avoid unrealistic expectations. 2) An interface should incorporate positive acknowledgment of behavior change. 3) Realistic appearance of a virtual agent and responsiveness are important in engaging users. 4) Conversation personalization, for instance in prompting laconic users for more input and reciprocal questions, would help the teenagers engage for longer terms and increase the system's utility.","[{'authorId': '22858671', 'name': 'M. R. Ali'}, {'authorId': '51905143', 'name': 'Zahra Rasazi'}, {'authorId': '50059663', 'name': 'A. Mamun'}, {'authorId': '51900408', 'name': 'Raina Langevin'}, {'authorId': '3154478', 'name': 'Reza Rawassizadeh'}, {'authorId': '2404386', 'name': 'Lenhart K. Schubert'}, {'authorId': '144619896', 'name': 'Ehsan Hoque'}]",35.0,"{'bibtex': '@Article{Ali2018AVC,\n author = {M. R. Ali and Zahra Rasazi and A. Mamun and Raina Langevin and Reza Rawassizadeh and Lenhart K. Schubert and Ehsan Hoque},\n journal = {ArXiv},\n title = {A Virtual Conversational Agent for Teens with Autism: Experimental Results and Design Lessons},\n volume = {abs/1811.03046},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1811.03046', 'name': 'ArXiv'}",78.0,A Virtual Conversational Agent for Teens with Autism: Experimental Results and Design Lessons,2018.0
1910,a2721c95d1cd65bfc1492a4e5ca73be219d93bc4,,"[{'authorId': '144951065', 'name': 'M. Schröder'}, {'authorId': '2349051', 'name': 'P. Baggia'}, {'authorId': '1763455', 'name': 'F. Burkhardt'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '2061240446', 'name': 'Christian Peter'}, {'authorId': '2213115', 'name': 'E. Zovato'}]",62.0,"{'bibtex': '@Inproceedings{Schröder2011EmotionMLA,\n author = {M. Schröder and P. Baggia and F. Burkhardt and C. Pelachaud and Christian Peter and E. Zovato},\n pages = {316-325},\n title = {EmotionML - An Upcoming Standard for Representing Emotions and Related States},\n year = {2011}\n}\n'}",,{'pages': '316-325'},23.0,EmotionML - An Upcoming Standard for Representing Emotions and Related States,2011.0
1911,a29f96a968a58d5dcd8eb4cfed07d4707f83fa6e,"In this paper we present GAMYGDALA, an emotional appraisal engine that enables game developers to easily add emotions to their Non-Player Characters (NPC). Our approach proposes a solution that is positioned between event coding of affect, where individual events have predetermined annotated emotional consequences for NPCs, and a full blown cognitive appraisal model. Instead, for an NPC that needs emotions the game developer defines goals and annotates game events with a relation to these goals. Based on this input, GAMYGDALA produces an emotion for that NPC according to the well-known OCC model. In this paper we provide evidence for the following: GAMYGDALA provides black-box Game-AI independent emotion support, is efficient for large numbers of NPCs, and is psychologically grounded.","[{'authorId': '144155622', 'name': 'Alexandru Popescu'}, {'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '1733088', 'name': 'M. Someren'}]",68.0,"{'bibtex': '@Article{Popescu2014GAMYGDALAAE,\n author = {Alexandru Popescu and J. Broekens and M. Someren},\n journal = {IEEE Transactions on Affective Computing},\n pages = {32-44},\n title = {GAMYGDALA: An Emotion Engine for Games},\n volume = {5},\n year = {2014}\n}\n'}",,"{'volume': '5', 'pages': '32-44', 'name': 'IEEE Transactions on Affective Computing'}",45.0,GAMYGDALA: An Emotion Engine for Games,2014.0
1912,a2b0ac0e7329fe1b5e9692cc535cfd53b18b0982,"This study assessed the profiles of psychological health and changes in neurohormones of adolescents with mild depression after 12 weeks of dance movement therapy (DMT). Forty middle school seniors (mean age: 16 years old) volunteered to participate in this study and were randomly assigned into either a dance movement group (n = 20) or a control group (n = 20). All subscale scores of psychological distress and global scores decreased significantly after the 12 weeks in the DMT group. Plasma serotonin concentration increased and dopamine concentration decreased in the DMT group. These results suggest that DMT may stabilize the sympathetic nervous system. In conclusion, DMT may be effective in beneficially modulating concentrations of serotonin and dopamine, and in improving psychological distress in adolescents with mild depression.","[{'authorId': '6241785', 'name': 'Young-Ja Jeong'}, {'authorId': '2151797703', 'name': 'S. Hong'}, {'authorId': '152522673', 'name': 'Myeong Soo Lee'}, {'authorId': '49360403', 'name': 'Min-Cheol Park'}, {'authorId': '51033371', 'name': 'Yong-Kyu. Kim'}, {'authorId': '5768258', 'name': 'Chae-Moon Suh'}]",243.0,"{'bibtex': '@Article{Jeong2005DANCEMT,\n author = {Young-Ja Jeong and S. Hong and Myeong Soo Lee and Min-Cheol Park and Yong-Kyu. Kim and Chae-Moon Suh},\n journal = {International Journal of Neuroscience},\n pages = {1711 - 1720},\n title = {DANCE MOVEMENT THERAPY IMPROVES EMOTIONAL RESPONSES AND MODULATES NEUROHORMONES IN ADOLESCENTS WITH MILD DEPRESSION},\n volume = {115},\n year = {2005}\n}\n'}",,"{'volume': '115', 'pages': '1711 - 1720', 'name': 'International Journal of Neuroscience'}",17.0,DANCE MOVEMENT THERAPY IMPROVES EMOTIONAL RESPONSES AND MODULATES NEUROHORMONES IN ADOLESCENTS WITH MILD DEPRESSION,2005.0
1913,a2b5d6c1cc2ad1249db907865f0bae25751d2c4c,,"[{'authorId': '3180680', 'name': 'Z. Ruttkay'}, {'authorId': '34960866', 'name': 'C. Dormann'}, {'authorId': '3331413', 'name': 'H. Noot'}]",52.0,"{'bibtex': '@Inproceedings{Ruttkay2004EmbodiedCA,\n author = {Z. Ruttkay and C. Dormann and H. Noot},\n pages = {27-66},\n title = {Embodied Conversational Agents on a Common Ground},\n year = {2004}\n}\n'}",,{'pages': '27-66'},46.0,Embodied Conversational Agents on a Common Ground,2004.0
1914,a2ba548810431fbfc0db43b537dda71ccdf45dc2,"Automatically animating virtual humans with actions that reflect real human motions is still a challenge. We present a framework for animation that is based on utilizing empirical and validated data from movement observation and cognitive psychology. To illustrate these, we demonstrate a mapping from effort motion factors onto expressive arm movements, and from cognitive data to autonomous attention behaviors. We conclude with a discussion on the implications of this approach for the future of real time virtual human animation.","[{'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '36224552', 'name': 'D. Chi'}, {'authorId': '1405537339', 'name': 'Sonu Chopra-Khullar'}]",59.0,"{'bibtex': '@Article{Badler1999VirtualHA,\n author = {N. Badler and D. Chi and Sonu Chopra-Khullar},\n journal = {Proceedings Computer Animation 1999},\n pages = {128-137},\n title = {Virtual human animation based on movement observation and cognitive behavior models},\n year = {1999}\n}\n'}",,"{'pages': '128-137', 'name': 'Proceedings Computer Animation 1999'}",44.0,Virtual human animation based on movement observation and cognitive behavior models,1999.0
1915,a2c0b1e13bb8a96f7933d94306a6c81f62ac9d82,"The rapid dissemination of technology such as the Internet across geographical and ethnic lines is opening up opportunities for computer agents to negotiate with people of diverse cultural and organizational affiliations. To negotiate proficiently with people in different cultures, agents need to be able to adapt to the way behavioral traits of other participants change over time. This article describes a new agent for repeated bilateral negotiation that was designed to model and adapt its behavior to the individual traits exhibited by its negotiation partner. The agent’s decision-making model combined a social utility function that represented the behavioral traits of the other participant, as well as a rule-based mechanism that used the utility function to make decisions in the negotiation process. The agent was deployed in a strategic setting in which both participants needed to complete their individual tasks by reaching agreements and exchanging resources, the number of negotiation rounds was not fixed in advance and agreements were not binding. The agent negotiated with human subjects in the United States and Lebanon in situations that varied the dependency relationships between participants at the onset of negotiation. There was no prior data available about the way people would respond to different negotiation strategies in these two countries. Results showed that the agent was able to adopt a different negotiation strategy to each country. Its average performance across both countries was equal to that of people. However, the agent outperformed people in the United States, because it learned to make offers that were likely to be accepted by people, while being more beneficial to the agent than to people. In contrast, the agent was outperformed by people in Lebanon, because it adopted a high reliability measure which allowed people to take advantage of it. These results provide insight for human-computer agent designers in the types of multicultural settings that we considered, showing that adaptation is a viable approach towards the design of computer agents to negotiate with people when there is no prior data of their behavior.","[{'authorId': '1721094', 'name': 'Y. Gal'}, {'authorId': '1691597', 'name': 'Sarit Kraus'}, {'authorId': '21621673', 'name': 'M. Gelfand'}, {'authorId': '2237615', 'name': 'H. Khashan'}, {'authorId': '31640388', 'name': 'Elizabeth Salmon'}]",42.0,"{'bibtex': '@Article{Gal2011AnAA,\n author = {Y. Gal and Sarit Kraus and M. Gelfand and H. Khashan and Elizabeth Salmon},\n journal = {ACM Trans. Intell. Syst. Technol.},\n pages = {8:1-8:24},\n title = {An Adaptive Agent for Negotiating with People in Different Cultures},\n volume = {3},\n year = {2011}\n}\n'}",,"{'volume': '3', 'pages': '8:1-8:24', 'name': 'ACM Trans. Intell. Syst. Technol.'}",40.0,An Adaptive Agent for Negotiating with People in Different Cultures,2011.0
1916,a2c6a69f95a4f77ed29cefc9b43139ed7cd6f5be,,"[{'authorId': '51966268', 'name': 'W. Estes'}, {'authorId': '113469609', 'name': 'B. Skinner'}]",1337.0,"{'bibtex': '@Article{Estes1941SomeQP,\n author = {W. Estes and B. Skinner},\n journal = {Journal of Experimental Psychology},\n pages = {390-400},\n title = {Some quantitative properties of anxiety},\n volume = {29},\n year = {1941}\n}\n'}",,"{'volume': '29', 'pages': '390-400', 'name': 'Journal of Experimental Psychology'}",1.0,Some quantitative properties of anxiety,1941.0
1917,a2e31c895b1652b2d8b3b5eb4240494d96051fc4,"ABSTRACT Daily life entails having to cope with many stressful situations. Although stress-related reactions could sometimes provoke impairments in physiological processes due to the frequency of exposure or the stress burden of the event, physiological recovery after coping with stressors is highly implied in the aversive consequences of stress. To analyze the effects of listening to relaxing music (generated by the Melomics computer system) on the cardiovascular recovery and subjective feelings of anxiety after undergoing an acute-stress episode, a double-blind randomized controlled trial was conducted in healthy adults (N = 24; M = 23.05 years, SD = 2.97). Participants reported their levels of psychiatric symptomatology and anxiety and were then exposed to a stress induction protocol. Afterward, they underwent a period of recovery where they would be exposed to either a relaxing music track or silence, depending on a random assignation. Heart-derived functioning and self-reported anxiety were monitored throughout the study stages. All the participants showed stress-related reactions throughout the study stages, as it was shown for the study outcomes. Regarding the effect of listening to music, participants who listened to relaxing music during the recovery stage showed higher levels of sample entropy than controls, highlighting a large effect size on this difference (η2partial = .59). Relaxing music promotes more adaptive emotional regulation after coping with an acutely stressful event. This study aims to shed light on the actual effects of music interventions, and encourage the use of music-based interventions on health services.","[{'authorId': '1400017870', 'name': 'A. de la Torre-Luque'}, {'authorId': '1401783389', 'name': 'R. Caparros-Gonzalez'}, {'authorId': '118955281', 'name': 'T. Bastard'}, {'authorId': '144955542', 'name': 'Francisco Vico'}, {'authorId': '1403560826', 'name': 'G. Buela-Casal'}]",30.0,"{'bibtex': '@Article{Torre-Luque2017AcuteSR,\n author = {A. de la Torre-Luque and R. Caparros-Gonzalez and T. Bastard and Francisco Vico and G. Buela-Casal},\n journal = {Nordic Journal of Music Therapy},\n pages = {124 - 141},\n title = {Acute stress recovery through listening to Melomics relaxing music: A randomized controlled trial},\n volume = {26},\n year = {2017}\n}\n'}",,"{'volume': '26', 'pages': '124 - 141', 'name': 'Nordic Journal of Music Therapy'}",76.0,Acute stress recovery through listening to Melomics relaxing music: A randomized controlled trial,2017.0
1918,a2e4b7c46420ad36fe69ed2be39437b918a19e0a,,"[{'authorId': '144037531', 'name': 'Mark C. Coulson'}]",591.0,"{'bibtex': '@Article{Coulson2004AttributingET,\n author = {Mark C. Coulson},\n journal = {Journal of Nonverbal Behavior},\n pages = {117-139},\n title = {Attributing Emotion to Static Body Postures: Recognition Accuracy, Confusions, and Viewpoint Dependence},\n volume = {28},\n year = {2004}\n}\n'}",,"{'volume': '28', 'pages': '117-139', 'name': 'Journal of Nonverbal Behavior'}",50.0,"Attributing Emotion to Static Body Postures: Recognition Accuracy, Confusions, and Viewpoint Dependence",2004.0
1919,a2ec66b68bdc119443a3b07448f4fab3258f314a,"In reporting Implicit Association Test (IAT) results, researchers have most often used scoring conventions described in the first publication of the IAT (A.G. Greenwald, D.E. McGhee, & J.L.K. Schwartz, 1998). Demonstration IATs available on the Internet have produced large data sets that were used in the current article to evaluate alternative scoring procedures. Candidate new algorithms were examined in terms of their (a) correlations with parallel self-report measures, (b) resistance to an artifact associated with speed of responding, (c) internal consistency, (d) sensitivity to known influences on IAT measures, and (e) resistance to known procedural influences. The best-performing measure incorporates data from the IAT's practice trials, uses a metric that is calibrated by each respondent's latency variability, and includes a latency penalty for errors. This new algorithm strongly outperforms the earlier (conventional) procedure.","[{'authorId': '3864246', 'name': 'A. Greenwald'}, {'authorId': '2862527', 'name': 'Brian A. Nosek'}, {'authorId': '1968771', 'name': 'M. Banaji'}]",5292.0,"{'bibtex': '@Article{Greenwald2003UnderstandingAU,\n author = {A. Greenwald and Brian A. Nosek and M. Banaji},\n journal = {Journal of personality and social psychology},\n pages = {\n          197-216\n        },\n title = {Understanding and using the implicit association test: I. An improved scoring algorithm.},\n volume = {85 2},\n year = {2003}\n}\n'}",,"{'volume': '85 2', 'pages': '\n          197-216\n        ', 'name': 'Journal of personality and social psychology'}",36.0,Understanding and using the implicit association test: I. An improved scoring algorithm.,2003.0
1920,a2f4631f973344505555707006dce597c2f696d4,"The use of computational models of emotion in virtual agents enhances the realism of these agents in a variety of domains, including virtual reality training and entertainment computing. We consider these two domains as prototypical for Multi-emotional-Agent- Systems (MeASs), which are the focus of this paper. MeASs typically include groups of agents organised into clusters, for example a special-force unit. While each agent in such a group has its own emotional model, resulting in realistic individual emotional behaviour, the group as a whole can show unrealistic emotional behaviour. Currently there is no method to enforce emotional consistency of a cluster of agents while allowing agents to have individual emotions. Our approach introduces an emotionalstate component that is a separate step in the computational model of emotion used by individual agents. The introduction of this emotional-state component enables multiple architectures for group emotions. We evaluate these architectures and conclude that several enable consistent integration of individual emotions and group emotions. We believe that our research enables agent- and scenario designers to benefit from the individual realism a computational model of emotion brings to virtual agents, without losing group consistency. Furthermore, by choosing one architecture versus another, designers can trade-off quality of the group emotion for computational performance.","[{'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '1840656', 'name': 'Niels Netten'}, {'authorId': '145678408', 'name': 'D. DeGroot'}]",4.0,"{'bibtex': '@Inproceedings{Broekens2005ConsistentDE,\n author = {J. Broekens and Niels Netten and D. DeGroot},\n title = {Consistent Dynamic-Group Emotions for Virtual Agents},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",10.0,Consistent Dynamic-Group Emotions for Virtual Agents,2005.0
1921,a301e2594fb5d41fb481789449993af0ec4811b6,,"[{'authorId': '144220013', 'name': 'D. Friedman'}, {'authorId': '143903462', 'name': 'A. Steed'}, {'authorId': '144931212', 'name': 'M. Slater'}]",164.0,"{'bibtex': '@Inproceedings{Friedman2007SpatialSB,\n author = {D. Friedman and A. Steed and M. Slater},\n pages = {252-263},\n title = {Spatial Social Behavior in Second Life},\n year = {2007}\n}\n'}",,{'pages': '252-263'},19.0,Spatial Social Behavior in Second Life,2007.0
1922,a303aebc0be81f1865ae4e6ae2470c9b17924052,"Accurately interpreting and expressing affect is fundamental to empathetic relationships. A platform for sensing and interpreting several aspects of users’ nonverbal affective information and responding through an expressive agent has been developed. The platform includes integration of multi-modal affective sensors with a real time inference engine, a behavior engine, and a 3d scriptable expressive humanoid agent within a graphical virtual environment. Currently the sensors include a pressure-sensitive mouse, a BlueTooth wireless skin conductivity sensor, a TekScan pressure sensor on a chair, and a stereo head tracking system as well as an IBM Blue Eyes infrared-sensitive camera. These sensors feed into custom algorithms for analysis of individual channels of information, such as postural and facial expressions, which in turn are combined with additional channels of information to make an inference about the user’s affective state. The system further synchronizes this sensor data with the agent behaviors and with video of the user and his or her on-screen activity. This platform is seen as a generalpurpose tool applicable to research in several areas, including how to design an affective learning companion, and how to further basic understanding of empathy and emotion contagion in human-agent interaction. 1. ACM classification keywords Agent architectures, Agent programming languages and environments, Sensors, Emotion, Affective user interface, Agent and intelligent systems, E-Learning and education, children, Pedagogical agents.","[{'authorId': '14741670', 'name': 'W. Burleson'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}, {'authorId': '2307214', 'name': 'K. Perlin'}, {'authorId': '144695084', 'name': 'J. Lippincott'}]",71.0,"{'bibtex': '@Inproceedings{Burleson2004APF,\n author = {W. Burleson and Rosalind W. Picard and K. Perlin and J. Lippincott},\n title = {A platform for affective agent research},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",56.0,A platform for affective agent research,2004.0
1923,a306fffc1854bd958b2e86e7085e45ae693e6388,,"[{'authorId': '2106252', 'name': 'Susanne van Mulken'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '2110564814', 'name': 'Jochen Müller'}]",285.0,"{'bibtex': '@Inproceedings{Mulken1998ThePE,\n author = {Susanne van Mulken and E. André and Jochen Müller},\n pages = {53-66},\n title = {The Persona Effect: How Substantial Is It?},\n year = {1998}\n}\n'}",,{'pages': '53-66'},20.0,The Persona Effect: How Substantial Is It?,1998.0
1924,a33a06ddc762fb855b6954c08d5aca603080b011,"One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others’ feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.","[{'authorId': '2516777', 'name': 'Hannah Rashkin'}, {'authorId': '51324296', 'name': 'Eric Michael Smith'}, {'authorId': '6649233', 'name': 'Margaret Li'}, {'authorId': '90841478', 'name': 'Y-Lan Boureau'}]",603.0,"{'bibtex': '@Inproceedings{Rashkin2018TowardsEO,\n author = {Hannah Rashkin and Eric Michael Smith and Margaret Li and Y-Lan Boureau},\n pages = {5370-5381},\n title = {Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},\n year = {2018}\n}\n'}",,{'pages': '5370-5381'},69.0,Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset,2018.0
1925,a354854c71d60a4490c42ae47464fbb9807d02bf,"2 Taxonomy is always a contentious issue because the world does not come to us in neat little packages (S. Personality has been conceptualized from a variety of theoretical perspectives, and at various levels of Each of these levels has made unique contributions to our understanding of individual differences in behavior and experience. However, the number of personality traits, and scales designed to measure them, escalated without an end in sight (Goldberg, 1971). Researchers, as well as practitioners in the field of personality assessment, were faced with a bewildering array of personality scales from which to choose, with little guidance and no overall rationale at hand. What made matters worse was that scales with the same name often measure concepts that are not the same, and scales with different names often measure concepts that are quite similar. Although diversity and scientific pluralism are useful, the systematic accumulation of findings and the communication among researchers became difficult amidst the Babel of concepts and scales. Many personality researchers had hoped that they might devise the structure that would transform the Babel into a community speaking a common language. However, such an integration was not to be achieved by any one researcher or by any one theoretical perspective. As Allport once put it, "" each assessor has his own pet units and uses a pet battery of diagnostic devices "" (1958, p. 258). What personality psychology needed was a descriptive model, or taxonomy, of its subject matter. One of the central goals of scientific taxonomies is the definition of overarching domains within which large numbers of specific instances can be understood in a simplified way. Thus, in personality psychology, a taxonomy would permit researchers to study specified domains of personality characteristics, rather than examining separately the thousands of particular attributes that make human beings individual and unique. Moreover, a generally accepted taxonomy would greatly facilitate the accumulation and communication of empirical findings by offering a standard vocabulary, or nomenclature. After decades of research, the field is approaching consensus on a general taxonomy of personality traits, the "" Big Five "" personality dimensions. These dimensions do not represent a particular theoretical perspective but were derived from analyses of the natural-language terms people use to describe themselves 3 and others. Rather than replacing all previous systems, the Big Five taxonomy serves an integrative function because it can represent the various and diverse systems of personality …","[{'authorId': '2254103', 'name': 'O. John'}, {'authorId': '50040921', 'name': 'S. Srivastava'}]",8579.0,"{'bibtex': '@Inproceedings{John1999TheBF,\n author = {O. John and S. Srivastava},\n title = {The Big Five Trait taxonomy: History, measurement, and theoretical perspectives.},\n year = {1999}\n}\n'}",,"{'volume': '', 'name': ''}",179.0,"The Big Five Trait taxonomy: History, measurement, and theoretical perspectives.",1999.0
1926,a35c404d7c8bb77ea5c3de3e22f70f4a13fc4450,"This paper introduces a new application area for agents in the computer interface: the support of human-human interaction. We discuss an interface agent prototype that is designed to support human-human communication in virtual environments. The prototype interacts with users strategically during conversation, spending most of its time listening. The prototype mimics a party host, trying to find a safe common topic for guests whose conversation has lagged. We performed an experimental evaluation of the prototype's ability to assist in cross-cultural conversations. We designed the prototype to introduce safe or unsafe topics to conversation pairs, through a series of questions and suggestions. The agent made positive contributions to participants' experience of the conversation, influenced their perception of each other and of each others' national group, and even seemed to effect their style of behavior. We discuss the implications of our research for the design of social agents to support human-human interaction.","[{'authorId': '1740889', 'name': 'K. Isbister'}, {'authorId': '2268916', 'name': 'Hideyuki Nakanishi'}, {'authorId': '143807934', 'name': 'T. Ishida'}, {'authorId': '2029850', 'name': 'C. Nass'}]",187.0,"{'bibtex': '@Article{Isbister2000HelperAD,\n author = {K. Isbister and Hideyuki Nakanishi and T. Ishida and C. Nass},\n journal = {Proceedings of the SIGCHI conference on Human Factors in Computing Systems},\n title = {Helper agent: designing an assistant for human-human interaction in a virtual meeting space},\n year = {2000}\n}\n'}",,{'name': 'Proceedings of the SIGCHI conference on Human Factors in Computing Systems'},15.0,Helper agent: designing an assistant for human-human interaction in a virtual meeting space,2000.0
1927,a35f7649f9794217111462bc1cdcb4aef0883916,"Results of 3 experiments suggest that feeling empathy for a member of a stigmatized group can improve attitudes toward the group as a whole. In Experiments 1 and 2, inducing empathy for a young woman with AIDS (Experiment 1) or a homeless man (Experiment 2) led to more positive attitudes toward people with AIDS or toward the homeless, respectively. Experiment 3 tested possible limits of the empathy-attitude effect by inducing empathy toward a member of a highly stigmatized group, convicted murderers, and measuring attitudes toward this group immediately and then 1-2 weeks later. Results provided only weak evidence of improved attitudes toward murderers immediately but strong evidence of improved attitudes 1-2 weeks later.","[{'authorId': '2254731342', 'name': 'Daniel Batson'}, {'authorId': '2254753054', 'name': 'Marina P Polycarpou'}, {'authorId': '1398650093', 'name': 'E. Harmon-Jones'}, {'authorId': '2254725412', 'name': 'Heidi J Imhoff'}, {'authorId': '2254753556', 'name': 'Erin C Mitchener'}, {'authorId': '2254740364', 'name': 'Lori L Bednar'}, {'authorId': '116010008', 'name': 'Tricia R. Klein'}, {'authorId': '2254739956', 'name': 'Loft Highberger'}, {'authorId': '5329600', 'name': 'Lori Highberger'}, {'authorId': '2254757905', 'name': 'Monica Biernat'}, {'authorId': '2254740900', 'name': 'Chris Crandall'}, {'authorId': '2254761619', 'name': 'Ahogni N'}]",1262.0,"{'bibtex': '@Article{Batson1997EmpathyAA,\n author = {Daniel Batson and Marina P Polycarpou and E. Harmon-Jones and Heidi J Imhoff and Erin C Mitchener and Lori L Bednar and Tricia R. Klein and Loft Highberger and Lori Highberger and Monica Biernat and Chris Crandall and Ahogni N},\n journal = {Journal of personality and social psychology},\n pages = {\n          105-18\n        },\n title = {Empathy and attitudes: can feeling for a member of a stigmatized group improve feelings toward the group?},\n volume = {72 1},\n year = {1997}\n}\n'}",,"{'volume': '72 1', 'pages': '\n          105-18\n        ', 'name': 'Journal of personality and social psychology'}",51.0,Empathy and attitudes: can feeling for a member of a stigmatized group improve feelings toward the group?,1997.0
1928,a3688ebcce02bbafa87e3f644841d7d78172fc08,"This study addresses the question of whether any facial expressions of emotion are universal. Recent studies showing that members of literate cultures associated the same emotion concepts with the same facial behaviors could not demonstrate that at least some facial expressions of emotion are universal; the cultures compared had all been exposed to some of the same mass media presentations of facial expression, and these may have taught the people in each culture to recognize the unique facial expressions of other cultures. To show that members of a preliterate culture who had minimal exposure to literate cultures would associate the same emotion concepts with the same facial behaviors as do members of Western and Eastern literate cultures, data were gathered in New Guinea by telling subjects a story, showing them a set of three faces, and asking them to select the face which showed the emotion appropriate to the story. The results provide evidence in support of the hypothesis that the association between particular facial muscular patterns and discrete emotions is universal.","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '1388284460', 'name': 'W. Friesen'}]",4488.0,"{'bibtex': '@Article{Ekman1971ConstantsAC,\n author = {P. Ekman and W. Friesen},\n journal = {Journal of personality and social psychology},\n pages = {\n          124-9\n        },\n title = {Constants across cultures in the face and emotion.},\n volume = {17 2},\n year = {1971}\n}\n'}",,"{'volume': '17 2', 'pages': '\n          124-9\n        ', 'name': 'Journal of personality and social psychology'}",29.0,Constants across cultures in the face and emotion.,1971.0
1930,a36ad82ed7d1365286e2179ef248a1782f8846e4,,"[{'authorId': '3213879', 'name': 'R. Reisenzein'}]",171.0,"{'bibtex': '@Article{Reisenzein2009EmotionsAM,\n author = {R. Reisenzein},\n journal = {Cognitive Systems Research},\n pages = {6-20},\n title = {Emotions as metarepresentational states of mind: Naturalizing the belief–desire theory of emotion},\n volume = {10},\n year = {2009}\n}\n'}",,"{'volume': '10', 'pages': '6-20', 'name': 'Cognitive Systems Research'}",122.0,Emotions as metarepresentational states of mind: Naturalizing the belief–desire theory of emotion,2009.0
1931,a3b7a9dff171c8e9885143bdbb3c4b1d5bb181dd,The design and creation of an episodic memory system for a cognitive robot is detailed. This memory system is interfaced with a machine emotion system with the goal of producing intelligent behaviors in a cognitive humanoid robot. The design of the system is tested and analyzed through the explanation of a case in which emotion assists in the retrieval of an episode.,"[{'authorId': '144210058', 'name': 'W. Dodd'}, {'authorId': '3393944', 'name': 'Ridelto A. Gutierrez'}]",60.0,"{'bibtex': '@Article{Dodd2005TheRO,\n author = {W. Dodd and Ridelto A. Gutierrez},\n journal = {ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.},\n pages = {692-697},\n title = {The role of episodic memory and emotion in a cognitive robot},\n year = {2005}\n}\n'}",,"{'pages': '692-697', 'name': 'ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.'}",14.0,The role of episodic memory and emotion in a cognitive robot,2005.0
1932,a3c65ff9f61249c550d069a950ca283ab0ad4677,"We analyzed the performance of an agent based on an appraisal theory of human emotion with respect to how it modulates play in a social dilemma game. An experiment with 117 participants showed how the agent was rated on dimensions of Human-Uniqueness (HU), separating humans from animals, and Human-Nature (HN), separating humans from machines. We showed that our appraisal theoretic agent significantly improved on both HN and HU ratings, compared to the baselines. We also showed that perception of humanness positively affects cooperation and enjoyment.","[{'authorId': '3372144', 'name': 'M. Ghafurian'}, {'authorId': '81350809', 'name': 'Neil Budnarain'}, {'authorId': '145803385', 'name': 'J. Hoey'}]",14.0,"{'bibtex': '@Article{Ghafurian2019RoleOE,\n author = {M. Ghafurian and Neil Budnarain and J. Hoey},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1979-1981},\n title = {Role of Emotions in Perception of Humanness of Virtual Agents},\n year = {2019}\n}\n'}","[{'paperId': '2be77d1621c8b6db2b19daaea1cae7b15655f2f9', 'title': 'Linking Personality and Trust in Intelligent Virtual Assistants'}, {'paperId': 'd0a61dc9a335dd55eaaf0e21d7081af2317a5250', 'title': 'An Instrument for measuring users’ meta-intents'}, {'paperId': '0551c6a186196e2f184868408d3cd049c778887e', 'title': 'Socially Interactive Agents for Supporting Aging'}, {'paperId': '54faa01dc3dd82d4c6ff0c38c8c44f91c35dfe14', 'title': 'The Long-Term Efficacy of “Social Buffering” in Artificial Social Agents: Contextual Affective Perception Matters'}, {'paperId': '78d751f857571434f326b1fa61a34260814e4ffc', 'title': 'Normative Emotional Agents: A Viewpoint Paper'}, {'paperId': '460d304be9ff4a9999fb57e7949e87eac7a7584b', 'title': 'The Zoomorphic Miro Robot’s Affective Expression Design and Perceived Appearance'}, {'paperId': 'ec63d61364f74b47fa5f96ccb962197d7a058c40', 'title': 'From Assistants to Friends: Investigating Emotional Intelligence of IPAs in Hindi and English'}, {'paperId': 'f3f112c6c3273e6ae16568664135ab64497cfb38', 'title': 'Power of Gijinka: Designing Virtual Teachers for Ecosystem Conservation Education'}, {'paperId': '77353fca75206a4c2270807de543bd17bf689839', 'title': 'Users, Tasks, and Conversational Agents: A Personality Study'}, {'paperId': 'b138eff61a5fe4adf513873d389620e34e916677', 'title': 'Social Robots for the Care of Persons with Dementia'}, {'paperId': '6df6ad53daa2e870caf3304af97f6ae66b2fc7a5', 'title': 'Using Emotions to Complement Multi-Modal Human-Robot Interaction in Urban Search and Rescue Scenarios'}, {'paperId': '98d50246576b2924fd7677c71dca527aa16639aa', 'title': 'Emotional Alignment Between Older Adults and Online Personalities: Implications for Assistive Technologies'}, {'paperId': '5b6fa2723d7183a5fcd25f788855e9ed74c62e15', 'title': 'Mapping Perceptions of Humanness in Intelligent Personal Assistant Interaction'}, {'paperId': 'a21566c1074e099b634d384c83e8abc61be0e7f2', 'title': 'Design and Evaluation of Affective Expressions of a Zoomorphic Robot'}]",{'pages': '1979-1981'},14.0,Role of Emotions in Perception of Humanness of Virtual Agents,2019.0
1933,a3dffdb774d2c452c69a2e4f58c86520ebd7ec06,"In this article, we introduce a computational model for the appraisal processes underlying emotion regulation from the perspective of Affect Control Theory. According to this theory, the affective meaning of emotions, behaviours, objects, and other entities can be assessed and projected onto a three dimensional space of evaluation, potency, and activity. This concept was applied to events occurring in the environment of an affective agent in order to study the dynamics of emotional changes caused by these events. Several appraisal processes were used to effectively analyze the affective impact of the occurred events and to interpret them in terms of the three dimensions of the affect control theory. A fuzzy automata framework was investigated and found to be a good fit to represent the dynamics of changes in the affective states and to effectively picture the transitions between different emotional response levels. Based on the results obtained from conducted experiments, we can argue that the proposed model has the potential to be used in predicting the emotional changes in (human/virtual) agents as a result of the occurrence of emotion-triggering events. Furthermore, it would appear that the proposed model has the capability to be used as the kernel of an extended system developed for reverse engineering events, and to generate and apply those in-favor of emotion regulation process.","[{'authorId': '9458014', 'name': 'Ahmad Soleimani'}, {'authorId': '1763459', 'name': 'Ziad Kobti'}]",2.0,"{'bibtex': '@Article{Soleimani2016AFC,\n author = {Ahmad Soleimani and Ziad Kobti},\n journal = {2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},\n pages = {1797-1804},\n title = {A fuzzy computational model for emotion regulation based on Affect Control Theory},\n year = {2016}\n}\n'}",,"{'pages': '1797-1804', 'name': '2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)'}",110.0,A fuzzy computational model for emotion regulation based on Affect Control Theory,2016.0
1934,a3e43b8a86dfe91167f8cfa5d0ef84078291cddb,,"[{'authorId': '2957203', 'name': 'John Bragin'}]",9.0,"{'bibtex': '@Article{Bragin2013CognitiveAC,\n author = {John Bragin},\n journal = {J. Artif. Soc. Soc. Simul.},\n title = {Cognitive Agent-Based Computing-I: a Unified Framework for Modeling Complex Adaptive Systems Using Agent-Based & Complex Network-Based Methods (SpringerBriefs in Cognitive Computation) by Muaz A Niazi and Amir Hussain},\n volume = {16},\n year = {2013}\n}\n'}",,"{'volume': '16', 'name': 'J. Artif. Soc. Soc. Simul.'}",0.0,Cognitive Agent-Based Computing-I: a Unified Framework for Modeling Complex Adaptive Systems Using Agent-Based & Complex Network-Based Methods (SpringerBriefs in Cognitive Computation) by Muaz A Niazi and Amir Hussain,2013.0
1935,a3f4602ce1524b7a06540ce088c09a0b1de233f4,,"[{'authorId': '1802934', 'name': 'A. Ortony'}]",249.0,"{'bibtex': '@Inproceedings{Ortony1988OnMB,\n author = {A. Ortony},\n title = {On making believable emotional agents believable},\n year = {1988}\n}\n'}",,,0.0,On making believable emotional agents believable,1988.0
1936,a3f581dc4194e9c1bf744216610f813cc29f53a5,,"[{'authorId': '2489004', 'name': 'E. Roesch'}, {'authorId': '1951923', 'name': 'L. Tamarit'}, {'authorId': '2118256', 'name': 'L. Revéret'}, {'authorId': '1797080', 'name': 'D. Grandjean'}, {'authorId': '143868107', 'name': 'D. Sander'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",114.0,"{'bibtex': '@Article{Roesch2011FACSGenAT,\n author = {E. Roesch and L. Tamarit and L. Revéret and D. Grandjean and D. Sander and K. Scherer},\n journal = {Journal of Nonverbal Behavior},\n pages = {1-16},\n title = {FACSGen: A Tool to Synthesize Emotional Facial Expressions Through Systematic Manipulation of Facial Action Units},\n volume = {35},\n year = {2011}\n}\n'}",,"{'volume': '35', 'pages': '1-16', 'name': 'Journal of Nonverbal Behavior'}",59.0,FACSGen: A Tool to Synthesize Emotional Facial Expressions Through Systematic Manipulation of Facial Action Units,2011.0
1937,a3f817c62b38d908bcd7c22f19226f57b751b6e0,"We outline some of the benefits of shared visual information for collaborative repair tasks and report on a study comparing collaborative performance on a manual task by workers and helpers who are located side-by-side or connected via audio-video or audio-only links. Results show that the dyads complete the task more quickly and accurately when helpers are co-located than when they are connected via an audio link. However, they didn't achieve similar efficiency gains when they communicated through an audio/video link. These results demonstrate the value of a shared visual work space, but raise questions about the adequacy of current video communication technology for implementing it.","[{'authorId': '1692772', 'name': 'Susan R. Fussell'}, {'authorId': '1702853', 'name': 'R. Kraut'}, {'authorId': '143760899', 'name': 'J. Siegel'}]",361.0,"{'bibtex': '@Inproceedings{Fussell2000CoordinationOC,\n author = {Susan R. Fussell and R. Kraut and J. Siegel},\n pages = {21-30},\n title = {Coordination of communication: effects of shared visual context on collaborative work},\n year = {2000}\n}\n'}",,{'pages': '21-30'},23.0,Coordination of communication: effects of shared visual context on collaborative work,2000.0
1938,a408a9ba5c11c466ebb79794de9bfdfd4ff0e892,,"[{'authorId': '48279878', 'name': 'E. Hatfield'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}, {'authorId': '8611261', 'name': 'Richard L. Rapson'}]",1035.0,"{'bibtex': '@Article{Hatfield1993EmotionalC,\n author = {E. Hatfield and J. Cacioppo and Richard L. Rapson},\n journal = {Current Directions in Psychological Science},\n pages = {100 - 96},\n title = {Emotional Contagion},\n volume = {2},\n year = {1993}\n}\n'}",,"{'volume': '2', 'pages': '100 - 96', 'name': 'Current Directions in Psychological Science'}",0.0,Emotional Contagion,1993.0
1940,a430dabab264b8b70d3c26f9307c85ce8ebd094b,,"[{'authorId': '145860194', 'name': 'J. Allwood'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1850299', 'name': 'K. Grammer'}, {'authorId': '3074458', 'name': 'E. Ahlsén'}, {'authorId': '3065152', 'name': 'E. Oberzaucher'}, {'authorId': '1852082', 'name': 'Markus Koppensteiner'}]",37.0,"{'bibtex': '@Article{Allwood2007TheAO,\n author = {J. Allwood and S. Kopp and K. Grammer and E. Ahlsén and E. Oberzaucher and Markus Koppensteiner},\n journal = {Language Resources and Evaluation},\n pages = {255-272},\n title = {The analysis of embodied communicative feedback in multimodal corpora: a prerequisite for behavior simulation},\n volume = {41},\n year = {2007}\n}\n'}",,"{'volume': '41', 'pages': '255-272', 'name': 'Language Resources and Evaluation'}",20.0,The analysis of embodied communicative feedback in multimodal corpora: a prerequisite for behavior simulation,2007.0
1941,a462ab5e68412f81d7c1947bb8baefa10ff63266,Social story is a very popular intervention and used widely as a social learning tool for children with autism spectrum disorder (ASD). The purpose of this research was to investigate the usage of social stories in encouraging social interaction of children with ASD. The subjects for this study were 4 learners with ASD between the ages of 5 to 8 who were attending school in an inclusive setting. A single case experiment with A-B-A-B design was used for all 4 subjects. Three of the subjects showed great improvement in their ability to make friends. Improvement was also seen in their communication based on visual obeservation to the targeted behavior chart whereas 1 subject only showed very little improvement. The findings of this study suggest that the usage of social stories in improving the social interaction of children with ASD has a positive impact.,"[{'authorId': '2071649452', 'name': 'S. Balakrishnan'}, {'authorId': '113747877', 'name': 'Aliza Alias'}]",13.0,"{'bibtex': '@Inproceedings{Balakrishnan2017UsageOS,\n author = {S. Balakrishnan and Aliza Alias},\n pages = {91-97},\n title = {Usage of Social Stories in Encouraging Social Interaction of Children with Autism Spectrum Disorder},\n volume = {1},\n year = {2017}\n}\n'}",,"{'volume': '1', 'pages': '91-97', 'name': ''}",27.0,Usage of Social Stories in Encouraging Social Interaction of Children with Autism Spectrum Disorder,2017.0
1942,a48ab2ec145ffd2666372ecf27f7bc34160d85c4,"Research on emotion recognition has been dominated by studies of photographs of facial expressions. A full understanding of emotion perception and its neural substrate will require investigations that employ dynamic displays and means of expression other than the face. Our aims were: (i) to develop a set of dynamic and static whole-body expressions of basic emotions for systematic investigations of clinical populations, and for use in functional-imaging studies; (ii) to assess forced-choice emotion-classification performance with these stimuli relative to the results of previous studies; and (iii) to test the hypotheses that more exaggerated whole-body movements would produce (a) more accurate emotion classification and (b) higher ratings of emotional intensity. Ten actors portrayed 5 emotions (anger, disgust, fear, happiness, and sadness) at 3 levels of exaggeration, with their faces covered. Two identical sets of 150 emotion portrayals (full-light and point-light) were created from the same digital footage, along with corresponding static images of the ‘peak’ of each emotion portrayal. Recognition tasks confirmed previous findings that basic emotions are readily identifiable from body movements, even when static form information is minimised by use of point-light displays, and that full-light and even point-light displays can convey identifiable emotions, though rather less efficiently than dynamic displays. Recognition success differed for individual emotions, corroborating earlier results about the importance of distinguishing differences in movement characteristics for different emotional expressions. The patterns of misclassifications were in keeping with earlier findings on emotional clustering. Exaggeration of body movement (a) enhanced recognition accuracy, especially for the dynamic point-light displays, but notably not for sadness, and (b) produced higher emotional-intensity ratings, regardless of lighting condition, for movies but to a lesser extent for stills, indicating that intensity judgments of body gestures rely more on movement (or form-from-movement) than static form information.","[{'authorId': '48037255', 'name': 'A. Atkinson'}, {'authorId': '2598679', 'name': 'W. Dittrich'}, {'authorId': '47903077', 'name': 'A. Gemmell'}, {'authorId': '2423497', 'name': 'A. Young'}]",704.0,"{'bibtex': '@Article{Atkinson2004EmotionPF,\n author = {A. Atkinson and W. Dittrich and A. Gemmell and A. Young},\n journal = {Perception},\n pages = {717 - 746},\n title = {Emotion Perception from Dynamic and Static Body Expressions in Point-Light and Full-Light Displays},\n volume = {33},\n year = {2004}\n}\n'}",,"{'volume': '33', 'pages': '717 - 746', 'name': 'Perception'}",83.0,Emotion Perception from Dynamic and Static Body Expressions in Point-Light and Full-Light Displays,2004.0
1943,a499b8e33d43e6cda7d9fa705a8afc25cbf84b6c,"Computer generated characters are now commonplace 
in television and film. In some media productions like the Matrix™ they feature as frequently as the real cast. A visual media that is being explored by the research community is that of real-time improvisational theatre using virtual characters. This is a non-trivial problem with many research challenges; this paper starts to address one, which is the automatic generation of appropriate non-verbal 
communication between characters based on their personality and relationship to one another. We focus on our of model interpersonal attitude used for generating expressive postures and eye gaze in computer animated characters. Our model consists of two principle dimensions, affiliation and status. It takes into account the relationships between the attitudes of two characters and allows for a large degree of variation between characters, both in how they react to other characters’ behaviour and in the ways in which they express attitude.","[{'authorId': '1945636', 'name': 'D. Ballin'}, {'authorId': '2246989429', 'name': 'M. Gillies'}, {'authorId': '2246988927', 'name': 'Barry Crabtree'}, {'authorId': '1945636', 'name': 'D. Ballin'}, {'authorId': '2246989429', 'name': 'M. Gillies'}, {'authorId': '145279822', 'name': 'I. B. Crabtree'}]",39.0,"{'bibtex': '@Inproceedings{Ballin2004AFF,\n author = {D. Ballin and M. Gillies and Barry Crabtree and D. Ballin and M. Gillies and I. B. Crabtree},\n pages = {203-210},\n title = {A framework for interpersonal attitude and non-verbal communication in improvisational visual media production},\n year = {2004}\n}\n'}",,"{'volume': '', 'pages': '203-210', 'name': ''}",32.0,A framework for interpersonal attitude and non-verbal communication in improvisational visual media production,2004.0
1944,a4cec122a08216fe8a3bc19b22e78fbaea096256,,"[{'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}]",58373.0,"{'bibtex': '@Article{LeCun2015DeepL,\n author = {Yann LeCun and Yoshua Bengio and Geoffrey E. Hinton},\n journal = {Nature},\n pages = {436-444},\n title = {Deep Learning},\n volume = {521},\n year = {2015}\n}\n'}",,"{'volume': '521', 'pages': '436-444', 'name': 'Nature'}",820.0,Deep Learning,2015.0
1945,a4d3a13f325fbd6dc6306fab2e263776c2c94572,,"[{'authorId': '46829922', 'name': 'S. Osimo'}, {'authorId': '145661678', 'name': 'Rodrigo Pizarro'}, {'authorId': '2891686', 'name': 'B. Spanlang'}, {'authorId': '144931212', 'name': 'M. Slater'}]",156.0,"{'bibtex': '@Article{Osimo2015ConversationsBS,\n author = {S. Osimo and Rodrigo Pizarro and B. Spanlang and M. Slater},\n journal = {Scientific Reports},\n title = {Conversations between self and self as Sigmund Freud—A virtual body ownership paradigm for self counselling},\n volume = {5},\n year = {2015}\n}\n'}",,"{'volume': '5', 'name': 'Scientific Reports'}",58.0,Conversations between self and self as Sigmund Freud—A virtual body ownership paradigm for self counselling,2015.0
1946,a4f7d3a3da4b26ed6fb436326d44af12653ed86e,,"[{'authorId': '6643983', 'name': 'B. Marroquín'}]",291.0,"{'bibtex': '@Article{Marroquín2011InterpersonalER,\n author = {B. Marroquín},\n journal = {Clinical psychology review},\n pages = {\n          1276-90\n        },\n title = {Interpersonal emotion regulation as a mechanism of social support in depression.},\n volume = {31 8},\n year = {2011}\n}\n'}",,"{'volume': '31 8', 'pages': '\n          1276-90\n        ', 'name': 'Clinical psychology review'}",226.0,Interpersonal emotion regulation as a mechanism of social support in depression.,2011.0
1947,a4ffbc202c5c5ae2221dd9c2949b9b5935adb5cc,,"[{'authorId': '3555773', 'name': 'C. Seashore'}]",87.0,"{'bibtex': '@Misc{None,\n author = {C. Seashore},\n journal = {Psychological Review},\n pages = {403-406},\n title = {Experimental psychology: A manual of laboratory practice.},\n volume = {8}\n}\n'}",,"{'volume': '8', 'pages': '403-406', 'name': 'Psychological Review'}",0.0,Experimental psychology: A manual of laboratory practice.,
1948,a504bfdcaace7cccdc1dde453a5df8a359418e76,"Ecphory is a process by which retrieval information provided by a cue is correlated with the information stored in an episodic memory trace, thus providing the basis for the subjective experience of remembering and the corresponding memory performance. Particularly relevant to the study of ecphoric processes are experiments in which the material that is to be remembered is held nominally constant and in which both encoding conditions and retrieval conditions are systematically varied. The results of such experiments have imposed certain constraints on theories of retrieval and have led to the revision of several previously popular theoretical ideas. Some illustrative experimental data are described, and one version of a theoretical schema of retrieval is summarized. The schema holds that what a person remembers is a product of a synergistic interaction between the memory trace and the retrieval information, the nature and particular features of the recollective experience being determined by the properties of both the trace and the cue.","[{'authorId': '2083280287', 'name': 'Endel Tulving'}]",141.0,"{'bibtex': '@Article{Tulving1983EcphoricPI,\n author = {Endel Tulving},\n journal = {Philosophical Transactions of the Royal Society B},\n pages = {361-371},\n title = {Ecphoric processes in episodic memory},\n volume = {302},\n year = {1983}\n}\n'}",,"{'volume': '302', 'pages': '361-371', 'name': 'Philosophical Transactions of the Royal Society B'}",48.0,Ecphoric processes in episodic memory,1983.0
1949,a517af17b12e2a5c5db81d5abf95c7165dbf8412,"Existing research attempts to create realistic crowd simulations by incorporating personality and emotion into intelligent agents. However, personality and emotion were considered separately in existing studies, where the interactions of them are ignored. The main objective of this paper is to propose and implement a framework for crowd simulation with integration of the impacts and interactions of personality and emotion. An interactive solution based on the proposed framework is also developed for visualizing the crowd navigation behavior and collecting the related trajectory data. Three simulated scenarios: pass through, narrow passage, and emergence situation are used to validate the framework and compare the results with recent studies.","[{'authorId': '26802016', 'name': 'Jacob Sinclair'}, {'authorId': '35151077', 'name': 'Carrie Siu Man Lui'}]",4.0,"{'bibtex': '@Inproceedings{Sinclair2015IntegratingPA,\n author = {Jacob Sinclair and Carrie Siu Man Lui},\n title = {Integrating personality and emotion for human crowd simulation},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",14.0,Integrating personality and emotion for human crowd simulation,2015.0
1950,a5291a0e2b5d44f93cf2f3c6ef35bf8e18a28bf4,"Despite increase in mental health awareness, there is still a large portion of people not receiving treatment. There are various side effects associated with drugs and not everyone has access to therapy, whether due to cost or availability. Self-administrable therapies allow for greater ease of access and as such, a novel virtual reality based self-attachment therapy is currently being developed. One of its main goals is for the user to develop a bond and reparent a child avatar representing one’s childhood self. To do so efficiently, the emotions, felt by the user when discussing their childhood, should be transferred to this child avatar. This project aims to train an emotion recognition model to allow the detection of multiple emotions. In addition, it aims to implement the self-attachment theory scenario to manage the dialogue between a user and a virtual therapist. The two parts of the project are to be integrated within the proposed virtual reality application. The devised multimodal and multi-emotion recognition model trained on CMUMOSEI dataset, achieves state-of-the-art results on happiness detection and competitive results for other emotions. The scenario was implemented using FAtiMA toolkit and together with the emotion recognition model was successfully integrated within the virtual reality application.","[{'authorId': '1694989', 'name': 'A. Edalat'}, {'authorId': '40185455', 'name': 'Georgios Rizos'}]",1.0,"{'bibtex': '@Inproceedings{Edalat2020MultiemotionRA,\n author = {A. Edalat and Georgios Rizos},\n title = {Multi-emotion Recognition and Dialogue Manager for VR-based Self-attachment Therapy},\n year = {2020}\n}\n'}",,,79.0,Multi-emotion Recognition and Dialogue Manager for VR-based Self-attachment Therapy,2020.0
1951,a54f5795def74a346d3e221147035caea24c42be,,"[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '145813496', 'name': 'C. Martinho'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '2803283', 'name': 'P. McOwan'}]",98.0,"{'bibtex': '@Article{Castellano2009AffectRF,\n author = {Ginevra Castellano and Iolanda Leite and André Pereira and C. Martinho and Ana Paiva and P. McOwan},\n journal = {Journal on Multimodal User Interfaces},\n pages = {89-98},\n title = {Affect recognition for interactive companions: challenges and\xa0design in real world scenarios},\n volume = {3},\n year = {2009}\n}\n'}",,"{'volume': '3', 'pages': '89-98', 'name': 'Journal on Multimodal User Interfaces'}",48.0,Affect recognition for interactive companions: challenges and design in real world scenarios,2009.0
1952,a55c126b31e265314b2d502f5928588b4a52ef0d,"What is the structure of emotion? Emotion is too broad a class of events to be a single scientific category, and no one structure suffices. As an illustration, core affect is distinguished from prototypical emotional episode. Core affect refers to consciously accessible elemental processes of pleasure and activation, has many causes, and is always present. Its structure involves two bipolar dimensions. Prototypical emotional episode refers to a complex process that unfolds over time, involves causally connected subevents (antecedent; appraisal; physiological, affective, and cognitive changes; behavioral response; self-categorization), has one perceived cause, and is rare. Its structure involves categories (anger, fear, shame, jealousy, etc.) vertically organized as a fuzzy hierarchy and horizontally organized as part of a circumplex.","[{'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '1731779', 'name': 'L. F. Barrett'}]",2583.0,"{'bibtex': '@Article{Russell1999CoreAP,\n author = {J. Russell and L. F. Barrett},\n journal = {Journal of personality and social psychology},\n pages = {\n          805-19\n        },\n title = {Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant.},\n volume = {76 5},\n year = {1999}\n}\n'}",,"{'volume': '76 5', 'pages': '\n          805-19\n        ', 'name': 'Journal of personality and social psychology'}",131.0,"Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant.",1999.0
1953,a568ab075435ab3e59b7ea3eda70dcf78d82f742,,"[{'authorId': '2779835', 'name': 'Scott W. McQuiggan'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",154.0,"{'bibtex': '@Article{McQuiggan2007ModelingAE,\n author = {Scott W. McQuiggan and James C. Lester},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {348-360},\n title = {Modeling and evaluating empathy in embodied companion agents},\n volume = {65},\n year = {2007}\n}\n'}",,"{'volume': '65', 'pages': '348-360', 'name': 'Int. J. Hum. Comput. Stud.'}",41.0,Modeling and evaluating empathy in embodied companion agents,2007.0
1954,a56e71f4fec7ad97e00989902ec9c77ed1a708a7,,"[{'authorId': '3141704', 'name': 'A. Camurri'}, {'authorId': '1846900', 'name': 'Ingrid Lagerlöf'}, {'authorId': '145558735', 'name': 'G. Volpe'}]",419.0,"{'bibtex': '@Article{Camurri2003RecognizingEF,\n author = {A. Camurri and Ingrid Lagerlöf and G. Volpe},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {213-225},\n title = {Recognizing emotion from dance movement: comparison of spectator recognition and automated techniques},\n volume = {59},\n year = {2003}\n}\n'}",,"{'volume': '59', 'pages': '213-225', 'name': 'Int. J. Hum. Comput. Stud.'}",14.0,Recognizing emotion from dance movement: comparison of spectator recognition and automated techniques,2003.0
1956,a57dd17ff1374eb72e44c1dd3553f51dfccd0d58,"Computer-controlled, human-like virtual agents (VAs), are often embedded into immersive virtual environments (IVEs) in order to enliven a scene or to assist users. Certain constraints need to be fulfilled, e.g., a collision avoidance strategy allowing users to maintain their personal space. Violating this flexible protective zone causes discomfort in real-world situations and in IVEs. However, no studies on collision avoidance for small-scale IVEs have been conducted yet. Our goal is to close this gap by presenting the results of a controlled user study in a CAVE. 27 participants were immersed in a small-scale office with the task of reaching the office door. Their way was blocked either by a male or female VA, representing their co-worker. The VA showed different behavioral patterns regarding gaze and locomotion. Our results indicate that participants preferred collaborative collision avoidance: they expect the VA to step aside in order to get more space to pass while being willing to adapt their own walking paths.","[{'authorId': '3249697', 'name': 'A. Bönsch'}, {'authorId': '2638784', 'name': 'B. Weyers'}, {'authorId': '39812907', 'name': 'J. Wendt'}, {'authorId': '3261970', 'name': 'Sebastian Freitag'}, {'authorId': '144483066', 'name': 'T. Kuhlen'}]",27.0,"{'bibtex': '@Article{Bönsch2016CollisionAI,\n author = {A. Bönsch and B. Weyers and J. Wendt and Sebastian Freitag and T. Kuhlen},\n journal = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},\n pages = {145-148},\n title = {Collision avoidance in the presence of a virtual agent in small-scale virtual environments},\n year = {2016}\n}\n'}",,"{'pages': '145-148', 'name': '2016 IEEE Symposium on 3D User Interfaces (3DUI)'}",15.0,Collision avoidance in the presence of a virtual agent in small-scale virtual environments,2016.0
1957,a585300d7369839ba73999835f4a40a412f1ee9f,"In second-language communication, emotional feedbacks play a preponderant role in instilling positive emotions and thereby facilitating the production of the target language by second-language learners. In contrast, facial expressions help convey emotion, intent, and sometimes even desired actions more effectively. Additionally, according to the facial feedback hypothesis, a major component of several contemporary theories of emotion, facial expressions can regulate emotional behavior and experience. The aim of this study was to determine whether and to what extent emotional expressions reproduced by virtual agents could provide empathetic support to second-language learners during communication tasks. To do so, using the Facial Coding Action System, we implemented a prototype virtual agent that can display a collection of nonverbal feedbacks, including Ekman’ six basic universal emotions and gazing and nodding behaviors. Then, we designed a Wizard of Oz experiment in which second-language learners were assigned independent speaking tasks with a virtual agent. In this paper, we outline our proposed method and report on an initial experimental evaluation which validated the meaningfulness of our approach. Moreover, we present our next steps for improving the system and validating its usefulness through large-scale experiments.","[{'authorId': '3350062', 'name': 'Emmanuel Ayedoun'}, {'authorId': '1980261', 'name': 'Masataka Tokumaru'}]",0.0,"{'bibtex': '@Article{Ayedoun2022TowardsEE,\n author = {Emmanuel Ayedoun and Masataka Tokumaru},\n booktitle = {Multimodal Technologies and Interaction},\n journal = {Multimodal Technol. Interact.},\n pages = {77},\n title = {Towards Emotionally Expressive Virtual Human Agents to Foster L2 Production: Insights from a Preliminary Woz Experiment},\n volume = {6},\n year = {2022}\n}\n'}",[],"{'name': 'Multimodal Technol. Interact.', 'pages': '77', 'volume': '6'}",41.0,Towards Emotionally Expressive Virtual Human Agents to Foster L2 Production: Insights from a Preliminary Woz Experiment,2022.0
1958,a5b4f139473e58ab836e4ce9923bdbf6381a6fac,I. An Introduction to DBT Skills Training 1. Rationale for Dialectical Behavior Therapy Skills Training 2. Planning to Conduct DBT Skills Training 3. Structuring Skills Training Sessions 4. Skills Training Treatment Targets and Procedures 5. Application of Fundamental DBT Strategies in Behavioral Skills Training Part I Appendices II. Teaching Notes for DBT Skills Modules 6. General Skills: Orientation and Analyzing Behavior 7. Mindfulness Skills 8. Interpersonal Effectiveness Skills 9. Emotion Regulation Skills 10. Distress Tolerance Skills Index,"[{'authorId': '5574109', 'name': 'M. Linehan'}]",564.0,"{'bibtex': '@Inproceedings{Linehan2014DBTST,\n author = {M. Linehan},\n title = {DBT Skills Training: Manual},\n year = {2014}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,DBT Skills Training: Manual,2014.0
1959,a5d2af2a518e2c74761bdc3d976657ac48c9d2f8,"Contemporary games are making significant strides towards offering complex, immersive experiences for players. We can now explore sprawling 3D virtual environments populated by beautifully rendered characters and objects with autonomous behavior, engage in highly visceral action-oriented experiences offering a variety of missions with multiple solutions, and interact in ever-expanding online worlds teeming with physically customizable player avatars.","[{'authorId': '114402462', 'name': 'Michael Mateas'}, {'authorId': '143727207', 'name': 'A. Stern'}]",625.0,"{'bibtex': '@Inproceedings{Mateas2003FacadeAE,\n author = {Michael Mateas and A. Stern},\n title = {Façade: An Experiment in Building a Fully-Realized Interactive Drama},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",46.0,Façade: An Experiment in Building a Fully-Realized Interactive Drama,2003.0
1960,a5d522b7d62dbd8ea338c9d34c338ab5cdc436a6,,"[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",171.0,"{'bibtex': '@Article{Gratch2005EvaluatingAC,\n author = {J. Gratch and S. Marsella},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {23-43},\n title = {Evaluating a Computational Model of Emotion},\n volume = {11},\n year = {2005}\n}\n'}",,"{'volume': '11', 'pages': '23-43', 'name': 'Autonomous Agents and Multi-Agent Systems'}",61.0,Evaluating a Computational Model of Emotion,2005.0
1962,a5e7330f3fed7bcf4fe7afbd3b00805e2519c5b8,"This article presents a theory of reasoning about moral propositions that is based on four fundamental principles. First, no simple criterion picks out propositions about morality from within the larger set of deontic propositions concerning what is permissible and impermissible in social relations, the law, games, and manners. Second, the mechanisms underlying emotions and deontic evaluations are independent and operate in parallel, and so some scenarios elicit emotions prior to moral evaluations, some elicit moral evaluations prior to emotions, and some elicit them at the same time. Third, deontic evaluations depend on inferences, either unconscious intuitions or conscious reasoning. Fourth, human beliefs about what is, and isn’t, moral are neither complete nor consistent. The article marshals the evidence, which includes new studies, corroborating these principles, and discusses the relations between them and other current theories of moral reasoning.","[{'authorId': '48399217', 'name': 'M. Bucciarelli'}, {'authorId': '7394143', 'name': 'S. Khemlani'}, {'authorId': '1384194899', 'name': 'P. Johnson-Laird'}]",100.0,"{'bibtex': '@Article{Bucciarelli2008ThePO,\n author = {M. Bucciarelli and S. Khemlani and P. Johnson-Laird},\n journal = {Judgment and Decision Making},\n title = {The psychology of moral reasoning},\n year = {2008}\n}\n'}",,{'name': 'Judgment and Decision Making'},65.0,The psychology of moral reasoning,2008.0
1963,a6077fa80467e741bdbbfdcc29003aeefe174179,"In emergency situation, mass panic often causes more causalities than the disaster itself. The crowd emotional model could be used to simulate how crowd behavior in emergency scenarios and be helpful for developing crowd evacuation plans in emergency situations. However, existing crowd emotional models usually set model parameters in an empirical manner and are not validated by real cases. In this paper, a crowd emotional model is proposed to simulate the crowd movement in outdoor emergency situations. First of all, the crowd entropy and the movement difference are proposed to describe the emotional impact of the crowd scene on the agents. The perception of vision and hearing are considered, and the calculation formulas of the agent's emotional intensity and crowd emotional contagion are proposed. By calculating individual trajectories in the real video, the cumulative differences between the movements of the real crowd and the corresponding virtual crowd are analyzed. At last, a multi‐parameter optimization method is implemented by the differential evolution algorithm. To verify the parameters in models, three videos which are generated from three real cases, including explosion attack, shooting incident, and crowd disturbance are selected for experimental verification. The results showed that the proposed model could be a feasible method for optimizing parameters to simulate the emergency scenario.","[{'authorId': '2089956568', 'name': 'Lin Zhuo'}, {'authorId': '2109341502', 'name': 'Zhen Liu'}, {'authorId': '91436120', 'name': 'Tingting Liu'}, {'authorId': '2059011525', 'name': 'Chih-Chieh Hung'}, {'authorId': '9398597', 'name': 'Yanjie Chai'}]",2.0,"{'bibtex': '@Article{Zhuo2021ModelingCE,\n author = {Lin Zhuo and Zhen Liu and Tingting Liu and Chih-Chieh Hung and Yanjie Chai},\n booktitle = {Comput. Animat. Virtual Worlds},\n journal = {Computer Animation and Virtual Worlds},\n title = {Modeling crowd emotion from emergent event video},\n volume = {32},\n year = {2021}\n}\n'}","[{'paperId': 'a0841dbedf31e9624c1a0b56bc7df0d14fc2ba89', 'title': 'Modeling heterogeneous behaviors with different strategies in a terrorist attack'}, {'paperId': 'b802dfed89f7a3547ac2b8793ddba2925cdf11f7', 'title': 'Emotion-Based Crowd Model Evaluation Method Based on Features Distribution Distance'}]","{'name': 'Computer Animation and Virtual Worlds', 'volume': '32'}",36.0,Modeling crowd emotion from emergent event video,2021.0
1964,a62cac3db6a4199325ad6618981756acb844baa2,"We review a programme of research on the attribution of humanness to people, and the ways in which lesser humanness is attributed to some compared to others. We first present evidence that humanness has two distinct senses, one representing properties that are unique to our species, and the other—human nature—those properties that are essential or fundamental to the human category. An integrative model of dehumanisation is then laid out, in which distinct forms of dehumanisation correspond to the denial of the two senses of humanness, and the likening of people to particular kinds of nonhuman entities (animals and machines). Studies demonstrating that human nature attributes are ascribed more to the self than to others are reviewed, along with evidence of the phenomenon's cognitive and motivational basis. Research also indicates that both kinds of humanness are commonly denied to social groups, both explicitly and implicitly, and that they may cast a new light on the study of stereotype content. Our approach to the study of dehumanisation complements the tradition of research on infrahumanisation, and indicates new directions for exploring the importance of humanness as a dimension of social perception.","[{'authorId': '3041938', 'name': 'N. Haslam'}, {'authorId': '2409902', 'name': 'S. Loughnan'}, {'authorId': '1996561', 'name': 'Y. Kashima'}, {'authorId': '35001946', 'name': 'P. Bain'}]",248.0,"{'bibtex': '@Article{Haslam2008AttributingAD,\n author = {N. Haslam and S. Loughnan and Y. Kashima and P. Bain},\n journal = {European Review of Social Psychology},\n pages = {55 - 85},\n title = {Attributing and denying humanness to others},\n volume = {19},\n year = {2008}\n}\n'}",,"{'volume': '19', 'pages': '55 - 85', 'name': 'European Review of Social Psychology'}",56.0,Attributing and denying humanness to others,2008.0
1965,a633ea2875fde8d5d21ebb96611b4d9fc098d605,"Speech signal processing is an active area of research, the most dominant source of exchanging information among human beings, and the best way for human–computer interaction (HCI). Human behavior assessments and emotion recognition from a speech signal, such as speech emotion recognition (SER) is an emerging HCI area of exploration with various real time claims. The performance of an efficient SER system depends on feature learning, which include salient and discriminative information such as high‐level deep features. In this paper, we proposed a two‐stream deep convolutional neural network with an iterative neighborhood component analysis (INCA) to learn mutually spatial‐spectral features and select the most discriminative optimal features for the final prediction. Our model is composed of two channels, and each channel is associated with the convolutional neural network structure to extract cues from the oral signals. The first channel extracts feature from the spectral domain, and the second channel extracts features from the spatial domain, which are then fused and fed to the INCA to remove the severance and select the optimal features for the final model training. The joint refine features are passed from the fully connected network with a softmax classifier to yield the predictions of the different emotions. We trained our proposed system using three benchmarks, which included the EMO‐DB, SAVEE, and RAVDESS emotional speech corpora, and we tested the prediction performance to secure 95%, 82%, and 85% recognition rates. The performance of the system shows the effectiveness and significance of the proposed system.","[{'authorId': '1478898295', 'name': 'Mustaqeem'}, {'authorId': '2111751120', 'name': 'Soonil Kwon'}]",53.0,"{'bibtex': '@Article{Mustaqeem2021OptimalFS,\n author = {Mustaqeem and Soonil Kwon},\n journal = {International Journal of Intelligent Systems},\n pages = {5116 - 5135},\n title = {Optimal feature selection based speech emotion recognition using two‐stream deep convolutional neural network},\n volume = {36},\n year = {2021}\n}\n'}",,"{'volume': '36', 'pages': '5116 - 5135', 'name': 'International Journal of Intelligent Systems'}",75.0,Optimal feature selection based speech emotion recognition using two‐stream deep convolutional neural network,2021.0
1966,a6401e102c03a441992b3e45f7b63eec09d4b89d,"Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask- oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier","[{'authorId': '2957953', 'name': 'Hongshen Chen'}, {'authorId': '1390612725', 'name': 'Xiaorui Liu'}, {'authorId': '50559722', 'name': 'Dawei Yin'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}]",569.0,"{'bibtex': '@Article{Chen2017ASO,\n author = {Hongshen Chen and Xiaorui Liu and Dawei Yin and Jiliang Tang},\n journal = {ArXiv},\n title = {A Survey on Dialogue Systems: Recent Advances and New Frontiers},\n volume = {abs/1711.01731},\n year = {2017}\n}\n'}",,"{'volume': 'abs/1711.01731', 'name': 'ArXiv'}",133.0,A Survey on Dialogue Systems: Recent Advances and New Frontiers,2017.0
1967,a64eeb6f564b5490f9544bd3524c781809238291,"The past five years have witnessed a growth of interest in modeling behavior moderators within cognitive architectures. Incorporating these effects within human behavior models driving training simulations is essential for producing more realistic simulated agent performance and improved training. We describe a methodology for modeling a broad range of interacting behavior moderators in terms of architecture parameters, and a cognitive architecture that implements this methodology. We then present results of an evaluation experiment that demonstrates the architecture’s ability to produce observable behavior differences resulting from distinct individual profiles. The demonstration illustrates distinct behaviors of three types of commanders (‘normal’, ‘anxious’, and ‘aggressive’) within a simulated Stability and Support Operations scenario.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",13.0,"{'bibtex': '@Inproceedings{Hudlicka2015ModelingEO,\n author = {E. Hudlicka},\n title = {Modeling Effects of Behavior Moderators on Performance: Evaluation of the MAMID Methodology and Architecture},\n year = {2015}\n}\n'}",,"{'volume': '', 'name': ''}",4.0,Modeling Effects of Behavior Moderators on Performance: Evaluation of the MAMID Methodology and Architecture,2015.0
1968,a655bfdc6917c387aab364a7316c88f4a884409c,"Facial expressions of emotions influence the perception of robots in first encounters. People can judge trustworthiness, likability, and aggressiveness in a few milliseconds by simply observing other individuals’ faces. While first impressions have been extensively studied in adult-robot interaction, they have been addressed in child-robot interaction only rarely. This knowledge is crucial, as the first impression children build of robots might influence their willingness to interact with them over extended periods of time, for example in applications where robots play the role of companions or tutors. The present study focuses on investigating the effects of facial expressions of emotions on children’s perceptions of trust towards robots during first encounters. We constructed a set of facial expressions of happiness and anger varying in terms of intensity. We implemented these facial expressions onto a Furhat robot that was either male-like or female-like. 129 children were exposed to the robot’s expressions for a few seconds. We asked them to evaluate the robot in terms of trustworthiness, likability, and competence and investigated how emotion type, emotion intensity, and gender-likeness affected the perception of the robot. Results showed that a few seconds are enough for children to make a trait inference based on the robot’s emotion. We observed that emotion type, emotion intensity, and gender-likeness did not directly affect trust, but the perception of likability and competence of the robot served as facilitator to judge trustworthiness.","[{'authorId': '2052028871', 'name': 'Natalia Calvo-Barajas'}, {'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",24.0,"{'bibtex': '@Article{Calvo-Barajas2020TheEO,\n author = {Natalia Calvo-Barajas and G. Perugia and Ginevra Castellano},\n journal = {2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},\n pages = {165-171},\n title = {The Effects of Robot’s Facial Expressions on Children’s First Impressions of Trustworthiness},\n year = {2020}\n}\n'}",,"{'pages': '165-171', 'name': '2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)'}",39.0,The Effects of Robot’s Facial Expressions on Children’s First Impressions of Trustworthiness,2020.0
1969,a662d1af0f936c31072c2c1611687c624fd9414c,,"[{'authorId': '33877256', 'name': 'R. Elliott'}, {'authorId': '1740734', 'name': 'J. Glauert'}, {'authorId': '7250309', 'name': 'R. Kennaway'}, {'authorId': '143818308', 'name': 'I. Marshall'}, {'authorId': '47569307', 'name': 'É. Sáfár'}]",151.0,"{'bibtex': '@Article{Elliott2008LinguisticMA,\n author = {R. Elliott and J. Glauert and R. Kennaway and I. Marshall and É. Sáfár},\n journal = {Universal Access in the Information Society},\n pages = {375-391},\n title = {Linguistic modelling and language-processing technologies for Avatar-based sign language presentation},\n volume = {6},\n year = {2008}\n}\n'}",,"{'volume': '6', 'pages': '375-391', 'name': 'Universal Access in the Information Society'}",35.0,Linguistic modelling and language-processing technologies for Avatar-based sign language presentation,2008.0
1970,a66a8783e249b1a253823023f3488a32e840d7ec,"A long tradition of research suggests a relationship between emotional mimicry and pro-social behavior, but the nature of this relationship is unclear. Does mimicry cause rapport and cooperation, or merely reflect it? Virtual humans can provide unique insights into these social processes by allowing unprecedented levels of experimental control. In a 2 x 2 factorial design, we examined the impact of facial mimicry and counter-mimicry in the iterated prisoner's dilemma. Participants played with an agent that copied their smiles and frowns or one that showed the opposite pattern -- i.e., that frowned when they smiled. As people tend to smile more than frown, we independently manipulated the contingency of expressions to ensure any effects are due to mimicry alone, and not the overall positivity/negativity of the agent: i.e., participants saw either a reflection of their own expressions or saw the expressions shown to a previous participant. Results show that participants smiled significantly more when playing an agent that mimicked them. Results also show a complex association between smiling, feelings of rapport, and cooperation. We discuss the implications of these findings on virtual human systems and theories of cooperation.","[{'authorId': '2065815350', 'name': 'Rens Hoegen'}, {'authorId': '47871040', 'name': 'J. Schalk'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",12.0,"{'bibtex': ""@Article{Hoegen2018TheIO,\n author = {Rens Hoegen and J. Schalk and Gale M. Lucas and J. Gratch},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {The impact of agent facial mimicry on social behavior in a prisoner's dilemma},\n year = {2018}\n}\n""}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},31.0,The impact of agent facial mimicry on social behavior in a prisoner's dilemma,2018.0
1971,a685a6d034e32f86cd97948ac27eadf98d127c7b,"We present a real-time algorithm to automatically classify the dynamic behavior or personality of a pedestrian based on his or her movements in a crowd video. Our classification criterion is based on Personality Trait Theory. We present a statistical scheme that dynamically learns the behavior of every pedestrian in a scene and computes that pedestrian’s motion model. This model is combined with global crowd characteristics to compute the movement patterns and motion dynamics, which can also be used to predict the crowd movement and behavior. We highlight its performance in identifying the personalities of different pedestrians in lowand high-density crowd videos. We also evaluate the accuracy by comparing the results with a user study.","[{'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",48.0,"{'bibtex': '@Inproceedings{Bera2017AggressiveTO,\n author = {Aniket Bera and Tanmay Randhavane and Dinesh Manocha},\n pages = {112-118},\n title = {Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos},\n year = {2017}\n}\n'}",,{'pages': '112-118'},33.0,"Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos",2017.0
1972,a6b136a13f0b3122ef62d687038b174eca74d1a3,"The significant role of emotions in evolution and adaptation suggests that there must be more than 1 mechanism for generating them. Nevertheless, much of current emotion theory focuses on cognitive processes (appraisal, attribution, and construal) as the sole, or primary, means of eliciting emotions. As an alternative to this position, the present model describes 4 types of emotion-activating systems, 3 of which involve noncognitive information processing. From an evolutionary-developmental perspective, the systems maybe viewed as a loosely organized hierarchical arrangement, with neural systems, the simplest and most rapid, at the base and cognitive systems, the most complex and versatile, at the top. The emotion-activating systems operate under a number of constraints, including genetically influenced individual differences. The hierarchical organization of the systems for generating emotions provides an adaptive advantage.","[{'authorId': '38430881', 'name': 'C. Izard'}]",787.0,"{'bibtex': '@Article{Izard1993FourSF,\n author = {C. Izard},\n journal = {Psychological review},\n pages = {\n          68-90\n        },\n title = {Four systems for emotion activation: cognitive and noncognitive processes.},\n volume = {100 1},\n year = {1993}\n}\n'}",,"{'volume': '100 1', 'pages': '\n          68-90\n        ', 'name': 'Psychological review'}",208.0,Four systems for emotion activation: cognitive and noncognitive processes.,1993.0
1973,a6bb5f963fd47831df1ccd587f84fb82d86ce62c,,"[{'authorId': '2262139257', 'name': 'John Stainer'}]",4622.0,"{'bibtex': ""@Article{Stainer1882TheE,\n author = {John Stainer},\n journal = {Hall's Journal of Health},\n pages = {318 - 318},\n title = {The Emotions},\n volume = {29},\n year = {1882}\n}\n""}",,"{'volume': '29', 'pages': '318 - 318', 'name': ""Hall's Journal of Health""}",0.0,The Emotions,1882.0
1975,a6e6e3cd94d1cf481ff73eb3bcf885bec1ce442a,"Young, middle-aged, and older adults' emotion regulation strategies in interpersonal problems were examined. Participants imagined themselves in anger- or sadness-eliciting situations with a close friend. Factor analyses of a new questionnaire supported a 4-factor model of emotion regulation strategies, including passivity, expressing emotions, seeking emotional information or support, and solving the problem. Results suggest that age differences in emotion regulation (such as older adults' increased endorsement of passive emotion regulation relative to young adults) are partially due to older adults' decreased ability to integrate emotion and cognition, increased prioritization of emotion regulation goals, and decreased tendency to express anger.","[{'authorId': '5373056', 'name': 'A. H. Coats'}, {'authorId': '1401641948', 'name': 'F. Blanchard-Fields'}]",111.0,"{'bibtex': '@Article{Coats2008EmotionRI,\n author = {A. H. Coats and F. Blanchard-Fields},\n journal = {Psychology and aging},\n pages = {\n          39-51\n        },\n title = {Emotion regulation in interpersonal problems: the role of cognitive-emotional complexity, emotion regulation goals, and expressivity.},\n volume = {23 1},\n year = {2008}\n}\n'}",,"{'volume': '23 1', 'pages': '\n          39-51\n        ', 'name': 'Psychology and aging'}",45.0,"Emotion regulation in interpersonal problems: the role of cognitive-emotional complexity, emotion regulation goals, and expressivity.",2008.0
1976,a7027dc22ffae5ac91392d0bf482a8bad4f10c0d,,"[{'authorId': '16210036', 'name': 'D. Hutton'}]",44.0,"{'bibtex': '@Article{Hutton2002AdvancesIT,\n author = {D. Hutton},\n journal = {Kybernetes},\n title = {Advances in the Evolutionary Synthesis of Intelligent Agents},\n volume = {31},\n year = {2002}\n}\n'}",,"{'volume': '31', 'name': 'Kybernetes'}",0.0,Advances in the Evolutionary Synthesis of Intelligent Agents,2002.0
1977,a7195e0336e889cd33d164bed51374a10229b0cb,,"[{'authorId': '30091296', 'name': 'Rishabh Dabral'}, {'authorId': '30190473', 'name': 'Anurag Mundhada'}, {'authorId': '29944175', 'name': 'Uday Kusupati'}, {'authorId': '30038676', 'name': 'S. Afaque'}, {'authorId': '2109325320', 'name': 'Abhishek Sharma'}, {'authorId': '49147969', 'name': 'Arjun Jain'}]",178.0,"{'bibtex': '@Inproceedings{Dabral2017Learning3H,\n author = {Rishabh Dabral and Anurag Mundhada and Uday Kusupati and S. Afaque and Abhishek Sharma and Arjun Jain},\n pages = {679-696},\n title = {Learning 3D Human Pose from Structure and Motion},\n year = {2017}\n}\n'}",,{'pages': '679-696'},43.0,Learning 3D Human Pose from Structure and Motion,2017.0
1979,a7462963ff7d383ee76d48100d67b973b476b1f2,"The ability to identify the emotions of others is a key component of what is known as social cognition. Narratives exploit this mechanism to create an emotional bond with the characters and to maintain the engagement of the audience throughout the story. In this paper, we illustrate a case study in emotion understanding in stories that exploits a computational agent to explore emotion impairment in a group of traumatic brain injured people. The study focuses on moral emotions, aiming to investigate the differences in moral functioning that characterize traumatic brain injured patients. After comparing the understanding of the moral and emotional facets of the agent's behavior in traumatic brain injured patients and in neurologically intact controls, slight–yet meaningful–differences were observed between the two groups. We describe the test methodology and results, highlighting their implications for the design of rehabilitation applications based on virtual agents.","[{'authorId': '5351581', 'name': 'Eleonora Ceccaldi'}, {'authorId': '144411873', 'name': 'R. Damiano'}, {'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '3792596', 'name': 'V. Galetto'}, {'authorId': '2775305', 'name': 'M. Zettin'}]",2.0,"{'bibtex': '@Article{Ceccaldi2020AnEA,\n author = {Eleonora Ceccaldi and R. Damiano and C. Battaglino and V. Galetto and M. Zettin},\n booktitle = {Frontiers in Psychology},\n journal = {Frontiers in Psychology},\n title = {An Emotional Agent for Moral Impairment Rehabilitation in TBI Patients},\n volume = {11},\n year = {2020}\n}\n'}","[{'paperId': '14af5f9ba80fdb7d63a91ac2ed4fb85511741621', 'title': 'Design, development, and use of conversational agents in rehabilitation for adults with brain-related neurological conditions: a scoping review'}, {'paperId': 'c19b5b6393aba3447209e3c18d6c72044373e4ae', 'title': 'Contemplative Interactions: Exploring the Use of Defamiliarization in a Serious Game to Promote Reflective Thinking about Personal Health'}]","{'name': 'Frontiers in Psychology', 'volume': '11'}",49.0,An Emotional Agent for Moral Impairment Rehabilitation in TBI Patients,2020.0
1980,a74fecf31b725457ea0fdb0b2acfe8a16e71bf30,"SUMMARY The problem of creating mouth animation synchronized to recorded speech is discussed. Review of a model of speech sound generation indicates that the automatic derivation of mouth movement from a speech soundtrack is a tractable problem. Several automatic lip-sync techniques are compared, and one method is described in detail. In this method a common speech synthesis method, linear prediction, is adapted to provide simple and accurate phoneme recognition. The recognized phonemes are associated with mouth positions to provide keyframes for computer animation of speech. Experience with this technique indicates that automatic lipsync can produce useful results.","[{'authorId': '2116914412', 'name': 'John Lewis'}]",105.0,"{'bibtex': '@Article{Lewis1991AutomatedLB,\n author = {John Lewis},\n journal = {Comput. Animat. Virtual Worlds},\n pages = {118-122},\n title = {Automated lip-sync: Background and techniques},\n volume = {2},\n year = {1991}\n}\n'}",,"{'volume': '2', 'pages': '118-122', 'name': 'Comput. Animat. Virtual Worlds'}",28.0,Automated lip-sync: Background and techniques,1991.0
1981,a7820a1327396ce448b8e65f0cdbdfbd005c2c76,"We report on an experiment to implement an autonomous creature situated in a two-dimensional world, that shows various learning and problem-solving capabilities, within the Society of Mind framework. This goal is approached from a developmental perspective, where phases in the experiment correspond broadly to cognitive stages in the development of an infant. This paper describes the first stage, the creature being a newborn whose behavior is strongly driven by motivational states—impulses to action based on bodily needs—and basic emotions—peripheral and cognitive responsestriggered by the recognition of a significant event. Physiological parameters are used to model both concepts, which are seen by analogy with control systems. Motivations drive behavior selection and organization based on the notions of arousal and satiation, and the exploitation principle. Emotions exert further control by sending “hormones” that may affect the intensity of the selected behavior, enable it, or prevent it. They also influence the attentional and perception mechanisms.","[{'authorId': '1719487', 'name': 'Dolores Cañamero'}]",351.0,"{'bibtex': '@Inproceedings{Cañamero1997ModelingMA,\n author = {Dolores Cañamero},\n pages = {148-155},\n title = {Modeling motivations and emotions as a basis for intelligent behavior},\n year = {1997}\n}\n'}",,{'pages': '148-155'},18.0,Modeling motivations and emotions as a basis for intelligent behavior,1997.0
1982,a789638e61c6241cab757231f1ce234cb2b96a79,,"[{'authorId': '1775321', 'name': 'J. Gross'}, {'authorId': '2246294034', 'name': 'Ross A. Thompson'}]",3173.0,"{'bibtex': '@Inproceedings{Gross2007EmotionRC,\n author = {J. Gross and Ross A. Thompson},\n title = {Emotion regulation: Conceptual foundations},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",79.0,Emotion regulation: Conceptual foundations,2007.0
1983,a7976c2bacfbb194ddbe7fd10c2e50a545cf4081,"Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.","[{'authorId': '3035541', 'name': 'Klaus Greff'}, {'authorId': '2100612', 'name': 'R. Srivastava'}, {'authorId': '2865775', 'name': 'J. Koutník'}, {'authorId': '1714059', 'name': 'Bas R. Steunebrink'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}]",4386.0,"{'bibtex': '@Article{Greff2015LSTMAS,\n author = {Klaus Greff and R. Srivastava and J. Koutník and Bas R. Steunebrink and J. Schmidhuber},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2222-2232},\n title = {LSTM: A Search Space Odyssey},\n volume = {28},\n year = {2015}\n}\n'}",,"{'volume': '28', 'pages': '2222-2232', 'name': 'IEEE Transactions on Neural Networks and Learning Systems'}",54.0,LSTM: A Search Space Odyssey,2015.0
1984,a79d2fefeb292241d1612318c264e342b807684c,"In this paper we will discuss different types of control over synthetic characters in interactive stories. We will argue that, to attain a deeper and more engaging control, in certain conditions, users should be able to inspect, disclose, and modify the characters minds. To illustrate this idea, we will present a collaborative virtual environment called {\it Teatrix}, designed for children to build their own stories - fairy tales. In {\it Teatrix}, virtual actors play roles (such as: villain, hero, etc.) and may be controlled either by children or by the system. {\it Teatrix} allows children to go into the minds of the characters through a special tool named {\it “Hot Seating”}. {\it Teatrix} is already in use by children ages between 7 and 9 in the context of a Computer integrated Classroom (CiC) scenario installed in a school. The initial evaluations show that the use of the “Hot Seatin” tool is a fundamental element for children to feel in control of their characters and thus stay in character for their virtual performances.","[{'authorId': '3452275', 'name': 'I. Alexandre'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '143825592', 'name': 'R. Prada'}]",29.0,"{'bibtex': '@Inproceedings{Alexandre2001IsTW,\n author = {I. Alexandre and Ana Paiva and R. Prada},\n pages = {370-376},\n title = {Is the wolf angry or... just hungry?},\n year = {2001}\n}\n'}",,{'pages': '370-376'},15.0,Is the wolf angry or... just hungry?,2001.0
1985,a7bb9a6327ad78764d8c8231e03b5738031c6142,,"[{'authorId': '38584363', 'name': 'D. Carney'}, {'authorId': '47355138', 'name': 'Judith A. Hall'}, {'authorId': '6475231', 'name': 'Lavonia Smith Lebeau'}]",269.0,"{'bibtex': '@Article{Carney2005BeliefsAT,\n author = {D. Carney and Judith A. Hall and Lavonia Smith Lebeau},\n journal = {Journal of Nonverbal Behavior},\n pages = {105-123},\n title = {Beliefs about the nonverbal expression of social power},\n volume = {29},\n year = {2005}\n}\n'}",,"{'volume': '29', 'pages': '105-123', 'name': 'Journal of Nonverbal Behavior'}",43.0,Beliefs about the nonverbal expression of social power,2005.0
1986,a7c248280200ba9b43c49076c761f0605083a0cd,"This paper describes systems that examine and react to an individual's changing context. Such systems can promote and mediate people's interactions with devices, computers, and other people, and they can help navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate environment is most important for this form of computing since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four catagories of context-aware applications: proximate selection, automatic contextual reconfiguration, contextual information and commands, and contex-triggered actions. Instances of these application types have been prototyped on the PARCTAB, a wireless, palm-sized computer.","[{'authorId': '1711856', 'name': 'Bill N. Schilit'}, {'authorId': '152926444', 'name': 'N. Adams'}, {'authorId': '1802351', 'name': 'R. Want'}]",3895.0,"{'bibtex': '@Article{Schilit1994ContextAwareCA,\n author = {Bill N. Schilit and N. Adams and R. Want},\n journal = {1994 First Workshop on Mobile Computing Systems and Applications},\n pages = {85-90},\n title = {Context-Aware Computing Applications},\n year = {1994}\n}\n'}",,"{'pages': '85-90', 'name': '1994 First Workshop on Mobile Computing Systems and Applications'}",22.0,Context-Aware Computing Applications,1994.0
1987,a7cce524b3949a017e9b9a898b85034c566f3306,"Virtual humans have been the focus of computer graphics research for several years now. The amalgamation of computer graphics and artificial intelligence has lead to the possibility of creating believable virtual personalities. The focus has shifted from modeling and animation towards imparting personalities to virtual humans. The aim is to create virtual humans that can interact spontaneously using a natural language, emotions and gestures. This paper discusses a system that allows the design of personality for emotional virtual human. We adopt the Five Factor Model (FFM) of personality from psychology studies. To realize the model, we use Bayesian Belief Network. We introduce a layered approach for modeling personality, moods and emotions. In order to demonstrate a virtual human with emotional personality, we integrate the system into a chat application. Thus, the system enables the developer to design and implement personalities and enables the user to interact with them.","[{'authorId': '144243072', 'name': 'S. Kshirsagar'}]",232.0,"{'bibtex': '@Inproceedings{Kshirsagar2002AMP,\n author = {S. Kshirsagar},\n pages = {107-115},\n title = {A multilayer personality model},\n year = {2002}\n}\n'}",,{'pages': '107-115'},13.0,A multilayer personality model,2002.0
1989,a7d72161d52ce1fd9631ee3a220ba5c0ee767dd3,,"[{'authorId': '2075833471', 'name': 'R. M. Maatman'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",148.0,"{'bibtex': '@Inproceedings{Maatman2005NaturalBO,\n author = {R. M. Maatman and J. Gratch and S. Marsella},\n pages = {25-36},\n title = {Natural Behavior of a Listening Agent},\n year = {2005}\n}\n'}",,{'pages': '25-36'},28.0,Natural Behavior of a Listening Agent,2005.0
1990,a7d93c24abf6e5b1e2789eccc6d5422a49b587f6,,"[{'authorId': '73104970', 'name': 'Rashed Aldabas'}]",5.0,"{'bibtex': '@Article{Aldabas2019EffectivenessOS,\n author = {Rashed Aldabas},\n journal = {Technology and Disability},\n title = {Effectiveness of social stories for children with autism: A comprehensive review},\n year = {2019}\n}\n'}",,{'name': 'Technology and Disability'},44.0,Effectiveness of social stories for children with autism: A comprehensive review,2019.0
1991,a7d96a2cdf7710b95342868f2b1fdb00223e7f4e,"Virtual reality applications have begun to offer great potential for communication in recent years. Creating an immersive virtual social environment that simulates a real social environment requires providing users with communication cues such as visual, verbal, and nonverbal cues to increase their sense of inhabiting the virtual world. In this work, we will investigate the influence of avatar representation and behavior on communication in an immersive, multiuser, same-place virtual environment by comparing three conditions of avatar representation: video see-through, scanned realistic avatar, and no-avatar representations. Subjective and objective measurements will be used to describe participants' observations and track their movement behavior to ascertain the effect of avatar representations on communication, based on personal presence, social presence, and trustworthiness.","[{'authorId': '2892331', 'name': 'Sahar A. Aseeri'}, {'authorId': '1785908', 'name': 'V. Interrante'}]",12.0,"{'bibtex': '@Article{Aseeri2018TheIO,\n author = {Sahar A. Aseeri and V. Interrante},\n journal = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},\n pages = {823-824},\n title = {The Influence of Avatar Representation and Behavior on Communication in Social Immersive Virtual Environments},\n year = {2018}\n}\n'}",,"{'pages': '823-824', 'name': '2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)'}",6.0,The Influence of Avatar Representation and Behavior on Communication in Social Immersive Virtual Environments,2018.0
1992,a83d537259865df30f748e7dbc30b72ff0ea4731,"Within a second of seeing an emotional facial expression, people typically match that expression. These rapid facial reactions (RFRs), often termed mimicry, are implicated in emotional contagion, social perception, and embodied affect, yet ambiguity remains regarding the mechanism(s) involved. Two studies evaluated whether RFRs to faces are solely nonaffective motor responses or whether emotional processes are involved. Brow (corrugator, related to anger) and forehead (frontalis, related to fear) activity were recorded using facial electromyography (EMG) while undergraduates in two conditions (fear induction vs. neutral) viewed fear, anger, and neutral facial expressions. As predicted, fear induction increased fear expressions to angry faces within 1000 ms of exposure, demonstrating an emotional component of RFRs. This did not merely reflect increased fear from the induction, because responses to neutral faces were unaffected. Considering RFRs to be merely nonaffective automatic reactions is inaccurate. RFRs are not purely motor mimicry; emotion influences early facial responses to faces. The relevance of these data to emotional contagion, autism, and the mirror system-based perspectives on imitation is discussed.","[{'authorId': '48254408', 'name': 'E. Moody'}, {'authorId': '21464189', 'name': 'D. McIntosh'}, {'authorId': '2062750330', 'name': 'Laura J Mann'}, {'authorId': '40520042', 'name': 'Kimberly R Weisser'}]",225.0,"{'bibtex': '@Article{Moody2007MoreTM,\n author = {E. Moody and D. McIntosh and Laura J Mann and Kimberly R Weisser},\n journal = {Emotion},\n pages = {\n          447-57\n        },\n title = {More than mere mimicry? The influence of emotion on rapid facial reactions to faces.},\n volume = {7 2},\n year = {2007}\n}\n'}",,"{'volume': '7 2', 'pages': '\n          447-57\n        ', 'name': 'Emotion'}",67.0,More than mere mimicry? The influence of emotion on rapid facial reactions to faces.,2007.0
1993,a8546edc5732b971fefc302bacdf496b409e1880,"A robot's appearance and behavior provide cues to the robot's abilities and propensities. We hypothesize that an appropriate match between a robot's social cues and its task improve the people's acceptance of and cooperation with the robot. In an experiment, people systematically preferred robots for jobs when the robot's humanlikeness matched the sociability required in those jobs. In two other experiments, people complied more with a robot whose demeanor matched the seriousness of the task.","[{'authorId': '40533864', 'name': 'Jennifer Goetz'}, {'authorId': '2070796426', 'name': 'Sara Kiesler'}, {'authorId': '2063989220', 'name': 'Aaron Powers'}]",773.0,"{'bibtex': '@Article{Goetz2003MatchingRA,\n author = {Jennifer Goetz and Sara Kiesler and Aaron Powers},\n journal = {The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.},\n pages = {55-60},\n title = {Matching robot appearance and behavior to tasks to improve human-robot cooperation},\n year = {2003}\n}\n'}",,"{'pages': '55-60', 'name': 'The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.'}",28.0,Matching robot appearance and behavior to tasks to improve human-robot cooperation,2003.0
1995,a85faf78df04e14c767f8821610e932bee13456c,"One approach in pursuit of general intelligent agents has been to concentrate on the underlying cognitive architecture, of which Soar is a prime example. In the past, Soar has relied on a minimal number of architectural modules together with purely symbolic representations of knowledge. This paper presents the cognitive architecture approach to general intelligence and the traditional, symbolic Soar architecture. This is followed by major additions to Soar: non-symbolic representations, new learning mechanisms, and long-term memories.","[{'authorId': '1715438', 'name': 'J. Laird'}]",402.0,"{'bibtex': '@Inproceedings{Laird2008ExtendingTS,\n author = {J. Laird},\n pages = {224-235},\n title = {Extending the Soar Cognitive Architecture},\n year = {2008}\n}\n'}",,{'pages': '224-235'},106.0,Extending the Soar Cognitive Architecture,2008.0
1996,a8705ab7b1471ff2d76af84b7b3540e38e4c83dd,"FAtiMA Toolkit is a collection of open-source tools that is designed to facilitate the creation and use of cognitive agents with socioemotional skills. The toolkit was developed with a focus on accessibility so it could be used by both researchers and game developers. It provides a computational model of emotions that is based on the OCC appraisal theory as well as an explicit dialogue structure that is familiar to game developers while maintaining the flexibility of an approach based on autonomous agents. Among various use cases, the toolkit has been successfully applied by an external game studio in their development of two serious games.","[{'authorId': '28004507', 'name': 'Manuel Guimarães'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '145255182', 'name': 'P. A. Santos'}, {'authorId': '2151066261', 'name': 'João Dias'}]",7.0,"{'bibtex': '@Inproceedings{Guimarães2019AnAT,\n author = {Manuel Guimarães and S. Mascarenhas and R. Prada and P. A. Santos and João Dias},\n pages = {2357-2359},\n title = {An Accessible Toolkit for the Creation of Socio-EmotionalAgents},\n year = {2019}\n}\n'}",,{'pages': '2357-2359'},9.0,An Accessible Toolkit for the Creation of Socio-EmotionalAgents,2019.0
1997,a884f00fe18a6abf837b2ccb490165ded90fc29a,"We present AutoTutor and Affective AutoTutor as examples of innovative 21st century interactive intelligent systems that promote learning and engagement. AutoTutor is an intelligent tutoring system that helps students compose explanations of difficult concepts in Newtonian physics and enhances computer literacy and critical thinking by interacting with them in natural language with adaptive dialog moves similar to those of human tutors. AutoTutor constructs a cognitive model of students' knowledge levels by analyzing the text of their typed or spoken responses to its questions. The model is used to dynamically tailor the interaction toward individual students' zones of proximal development. Affective AutoTutor takes the individualized instruction and human-like interactivity to a new level by automatically detecting and responding to students' emotional states in addition to their cognitive states. Over 20 controlled experiments comparing AutoTutor with ecological and experimental controls such reading a textbook have consistently yielded learning improvements of approximately one letter grade after brief 30--60-minute interactions. Furthermore, Affective AutoTutor shows even more dramatic improvements in learning than the original AutoTutor system, particularly for struggling students with low domain knowledge. In addition to providing a detailed description of the implementation and evaluation of AutoTutor and Affective AutoTutor, we also discuss new and exciting technologies motivated by AutoTutor such as AutoTutor-Lite, Operation ARIES, GuruTutor, DeepTutor, MetaTutor, and AutoMentor. We conclude this article with our vision for future work on interactive and engaging intelligent tutoring systems.","[{'authorId': '1383996606', 'name': 'S. D’Mello'}, {'authorId': '1769251', 'name': 'A. Graesser'}]",306.0,"{'bibtex': '@Article{D’Mello2012AutoTutorAA,\n author = {S. D’Mello and A. Graesser},\n journal = {ACM Trans. Interact. Intell. Syst.},\n pages = {23:1-23:39},\n title = {AutoTutor and affective autotutor: Learning by talking with cognitively and emotionally intelligent computers that talk back},\n volume = {2},\n year = {2012}\n}\n'}",,"{'volume': '2', 'pages': '23:1-23:39', 'name': 'ACM Trans. Interact. Intell. Syst.'}",163.0,AutoTutor and affective autotutor: Learning by talking with cognitively and emotionally intelligent computers that talk back,2012.0
1998,a889d242f3136db159bb5384d4dbc8b9c1f7cf71,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}, {'authorId': '47060925', 'name': 'N. Epstein'}]",2353.0,"{'bibtex': '@Article{Mehrabian1972AMO,\n author = {A. Mehrabian and N. Epstein},\n journal = {Journal of personality},\n pages = {\n          525-43\n        },\n title = {A measure of emotional empathy.},\n volume = {40 4},\n year = {1972}\n}\n'}",,"{'volume': '40 4', 'pages': '\n          525-43\n        ', 'name': 'Journal of personality'}",14.0,A measure of emotional empathy.,1972.0
1999,a8e8ddde3c2b8a92126e88e56272bcc7af75d40b,"Relations were examined between configurations of Big Five Traits (Extraversion, Agreeableness, Conscientiousness, Neuroticism, Openness to Experience) and 16 fundamental motives (Social Contact, Curiosity, Honor, Power, Order, Idealism, Independence, Status, Vengeance, Romance, Family, Activity, Saving, Acceptance, Eating, Tranquility) in 138 university students (93 women, 45 men; M age = 20.3 yr., SD = 4.5). Big Five traits were measured with the NEO-PI–R and motives were measured with the Reiss Profile of Fundamental Goals and Motivation Sensitivities. The traits were significantly related with all the motives (adjusted R2 = .06 to .43) except Physical Activity. Four motives were related with only one trait and nine configurations of two or more traits were correlated with the remaining 11 motives. Total motive scores across all participants, an index of the strength of overall motivation, were positively correlated with Extraversion and Neuroticism and negatively with Agreeableness.","[{'authorId': '47400491', 'name': 'Kenneth Ray Olson'}, {'authorId': '2056850820', 'name': 'D. A. Weber'}]",90.0,"{'bibtex': '@Article{Olson2004RelationsBB,\n author = {Kenneth Ray Olson and D. A. Weber},\n journal = {Psychological Reports},\n pages = {795 - 802},\n title = {Relations between Big Five Traits and Fundamental Motives},\n volume = {95},\n year = {2004}\n}\n'}",,"{'volume': '95', 'pages': '795 - 802', 'name': 'Psychological Reports'}",13.0,Relations between Big Five Traits and Fundamental Motives,2004.0
2000,a8ea40704911fa3e596cf2c0a4ee1399b3768cde,"In this paper, we investigate obstacle avoidance behavior during real walking in a large immersive projection setup. We analyze the walking behavior of users when avoiding real and virtual static obstacles. In order to generalize our study, we consider both anthropomorphic and inanimate objects, each having his virtual and real counterpart. The results showed that users exhibit different locomotion behaviors in the presence of real and virtual obstacles, and in the presence of anthropomorphic and inanimate objects. Precisely, the results showed a decrease of walking speed as well as an increase of the clearance distance (i. e., the minimal distance between the walker and the obstacle) when facing virtual obstacles compared to real ones. Moreover, our results suggest that users act differently due to their perception of the obstacle: users keep more distance when the obstacle is anthropomorphic compared to an inanimate object and when the orientation of anthropomorphic obstacle is from the profile compared to a front position. We discuss implications on future large shared immersive projection spaces.","[{'authorId': '1854224', 'name': 'F. Argelaguet'}, {'authorId': '1851306', 'name': 'A. Olivier'}, {'authorId': '34638348', 'name': 'G. Bruder'}, {'authorId': '2235773', 'name': 'J. Pettré'}, {'authorId': '1693899', 'name': 'A. Lécuyer'}]",90.0,"{'bibtex': '@Article{Argelaguet2015VirtualPL,\n author = {F. Argelaguet and A. Olivier and G. Bruder and J. Pettré and A. Lécuyer},\n journal = {2015 IEEE Virtual Reality (VR)},\n pages = {75-80},\n title = {Virtual proxemics: Locomotion in the presence of obstacles in large immersive projection environments},\n year = {2015}\n}\n'}",,"{'pages': '75-80', 'name': '2015 IEEE Virtual Reality (VR)'}",21.0,Virtual proxemics: Locomotion in the presence of obstacles in large immersive projection environments,2015.0
2002,a8f4bf67575c72bee34d806b4a98caaa25982bcf,,"[{'authorId': '143815380', 'name': 'O. Kim'}, {'authorId': '38957782', 'name': 'Y. Pang'}, {'authorId': '153188133', 'name': 'Jung-Hee Kim'}]",109.0,"{'bibtex': '@Article{Kim2019TheEO,\n author = {O. Kim and Y. Pang and Jung-Hee Kim},\n journal = {BMC Psychiatry},\n title = {The effectiveness of virtual reality for people with mild cognitive impairment or dementia: a meta-analysis},\n volume = {19},\n year = {2019}\n}\n'}",,"{'volume': '19', 'name': 'BMC Psychiatry'}",64.0,The effectiveness of virtual reality for people with mild cognitive impairment or dementia: a meta-analysis,2019.0
2003,a9235c88b3d83a5c8b6519227bf29422ae020816,,"[{'authorId': '145862897', 'name': 'G. Dawson'}, {'authorId': '3053914', 'name': 'A. Meltzoff'}, {'authorId': '4329421', 'name': 'J. Osterling'}, {'authorId': '31368850', 'name': 'J. Rinaldi'}, {'authorId': '2072719592', 'name': 'Emily Brown'}]",1069.0,"{'bibtex': '@Article{Dawson1998ChildrenWA,\n author = {G. Dawson and A. Meltzoff and J. Osterling and J. Rinaldi and Emily Brown},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {479-485},\n title = {Children with Autism Fail to Orient to Naturally Occurring Social Stimuli},\n volume = {28},\n year = {1998}\n}\n'}",,"{'volume': '28', 'pages': '479-485', 'name': 'Journal of Autism and Developmental Disorders'}",15.0,Children with Autism Fail to Orient to Naturally Occurring Social Stimuli,1998.0
2004,a94cce2a48521ed4e1ec6bdb87b6bbf8d6fac505,We have developed an interactive virtual audience platform for public speaking training. Users' public speaking behavior is automatically analyzed using audiovisual sensors. The virtual characters display indirect feedback depending on user's behavior descriptors correlated with public speaking performance. We used the system to collect a dataset of public speaking performances in different training conditions.,"[{'authorId': '40325099', 'name': 'Mathieu Chollet'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '145109163', 'name': 'Ari Shapiro'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}]",32.0,"{'bibtex': '@Inproceedings{Chollet2014AnIV,\n author = {Mathieu Chollet and Giota Stratou and Ari Shapiro and Louis-Philippe Morency and Stefan Scherer},\n pages = {1657-1658},\n title = {An interactive virtual audience platform for public speaking training},\n year = {2014}\n}\n'}",,{'pages': '1657-1658'},5.0,An interactive virtual audience platform for public speaking training,2014.0
2005,a963d05b9d4acd347ad528e7d098eb53d8f555a2,,"[{'authorId': '145968635', 'name': 'B. Kitchenham'}, {'authorId': '145662410', 'name': 'P. Brereton'}, {'authorId': '1896276', 'name': 'D. Budgen'}, {'authorId': '145689630', 'name': 'M. Turner'}, {'authorId': '2067719224', 'name': 'J. Bailey'}, {'authorId': '1700115', 'name': 'S. Linkman'}]",2994.0,"{'bibtex': '@Article{Kitchenham2009SystematicLR,\n author = {B. Kitchenham and P. Brereton and D. Budgen and M. Turner and J. Bailey and S. Linkman},\n journal = {Inf. Softw. Technol.},\n pages = {7-15},\n title = {Systematic literature reviews in software engineering - A systematic literature review},\n volume = {51},\n year = {2009}\n}\n'}",,"{'volume': '51', 'pages': '7-15', 'name': 'Inf. Softw. Technol.'}",50.0,Systematic literature reviews in software engineering - A systematic literature review,2009.0
2006,a96746bf3d7f2ed4a2e2177ef801a55fe8c8c764,,"[{'authorId': '35757964', 'name': 'S. Vine'}, {'authorId': '2109059532', 'name': 'Xiaobo Liu'}, {'authorId': '39815793', 'name': 'F. Zheng'}, {'authorId': '144325425', 'name': 'J. Polak'}]",40.0,"{'bibtex': '@Article{Vine2016AutomatedCQ,\n author = {S. Vine and Xiaobo Liu and F. Zheng and J. Polak},\n journal = {Transportation Research Part C-emerging Technologies},\n pages = {35-54},\n title = {Automated cars: Queue discharge at signalized intersections with ‘Assured-Clear-Distance-Ahead’ driving strategies},\n volume = {62},\n year = {2016}\n}\n'}",,"{'volume': '62', 'pages': '35-54', 'name': 'Transportation Research Part C-emerging Technologies'}",46.0,Automated cars: Queue discharge at signalized intersections with ‘Assured-Clear-Distance-Ahead’ driving strategies,2016.0
2007,a96ceb51dfc20eb8da42891b68a573b5c88021de,,"[{'authorId': '38746416', 'name': 'Koji Kamei'}, {'authorId': '3326975', 'name': 'Tetsushi Ikeda'}, {'authorId': '34656564', 'name': 'M. Shiomi'}, {'authorId': '2064478755', 'name': 'Hiroyuki Kidokoro'}, {'authorId': '1785513', 'name': 'A. Utsumi'}, {'authorId': '3290442', 'name': 'K. Shinozawa'}, {'authorId': '47281633', 'name': 'T. Miyashita'}, {'authorId': '1781078', 'name': 'N. Hagita'}]",18.0,"{'bibtex': '@Article{Kamei2012CooperativeCN,\n author = {Koji Kamei and Tetsushi Ikeda and M. Shiomi and Hiroyuki Kidokoro and A. Utsumi and K. Shinozawa and T. Miyashita and N. Hagita},\n journal = {annals of telecommunications - annales des télécommunications},\n pages = {329-340},\n title = {Cooperative customer navigation between robots outside and inside a retail shop—an implementation on the ubiquitous market platform},\n volume = {67},\n year = {2012}\n}\n'}",,"{'volume': '67', 'pages': '329-340', 'name': 'annals of telecommunications - annales des télécommunications'}",21.0,Cooperative customer navigation between robots outside and inside a retail shop—an implementation on the ubiquitous market platform,2012.0
2008,a9f2f40605644a56822e69068a3967eebc7227f5,"In this paper, we show that gender plays an important role in the automatic assessment of psychological conditions such as depression and post-traumatic stress disorder (PTSD). We identify a directly interpretable and intuitive set of predictive indicators, selected from three general categories of nonverbal behaviors: affect, expression variability and motor variability. For the analysis, we introduce a semi-structured virtual human interview dataset which includes 53 video recorded interactions. Our experiments on automatic classification of psychological conditions show that a gender-dependent approach significantly improves the performance over a gender agnostic one.","[{'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",66.0,"{'bibtex': '@Article{Stratou2013AutomaticNB,\n author = {Giota Stratou and Stefan Scherer and J. Gratch and Louis-Philippe Morency},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {147-152},\n title = {Automatic Nonverbal Behavior Indicators of Depression and PTSD: Exploring Gender Differences},\n year = {2013}\n}\n'}",,"{'pages': '147-152', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",31.0,Automatic Nonverbal Behavior Indicators of Depression and PTSD: Exploring Gender Differences,2013.0
2009,aa124a389ab617eedcbd4d33a718e2c1213e25ee,"Personal space (PS), the flexible protective zone maintained around oneself, is a key element of everyday social interactions. It, e.g., affects people's interpersonal distance and is thus largely involved when navigating through social environments. However, the PS is regulated dynamically, its size depends on numerous social and personal characteristics and its violation evokes different levels of discomfort and physiological arousal. Thus, gaining more insight into this phenomenon is important. We contribute to the PS investigations by presenting the results of a controlled experiment in a CAVE, focusing on German males in the age of 18 to 30 years. The PS preferences of 27 participants have been sampled while they were approached by either a single embodied, computer-controlled virtual agent (VA) or by a group of three VAs. In order to investigate the influence of a VA's emotions, we altered their facial expression between angry and happy. Our results indicate that the emotion as well as the number of VAs approaching influence the PS: larger distances are chosen to angry VAs compared to happy ones; single VAs are allowed closer compared to the group. Thus, our study is a foundation for social and behavioral studies investigating PS preferences.","[{'authorId': '3249697', 'name': 'A. Bönsch'}, {'authorId': '145880611', 'name': 'Sina Radke'}, {'authorId': '47973447', 'name': 'H. Overath'}, {'authorId': '51243025', 'name': 'L. Asche'}, {'authorId': '39812907', 'name': 'J. Wendt'}, {'authorId': '2824434', 'name': 'Tom Vierjahn'}, {'authorId': '2782974', 'name': 'U. Habel'}, {'authorId': '144483066', 'name': 'T. Kuhlen'}]",50.0,"{'bibtex': ""@Article{Bönsch2018SocialVH,\n author = {A. Bönsch and Sina Radke and H. Overath and L. Asche and J. Wendt and Tom Vierjahn and U. Habel and T. Kuhlen},\n booktitle = {IEEE Conference on Virtual Reality and 3D User Interfaces},\n journal = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},\n pages = {199-206},\n title = {Social VR: How Personal Space is Affected by Virtual Agents' Emotions},\n year = {2018}\n}\n""}","[{'paperId': '38d7dfcb522040687c70a8b7422b15ede1c7eee5', 'title': '""\\""Hello I am here\\"": Proximal Nonverbal Cues Role in Initiating Social Interactions in VR""'}, {'paperId': '43513e97d078d8844ace5a358caba0a401a98deb', 'title': 'Effects of appearance and gender on pre-touch proxemics in virtual reality'}, {'paperId': '7d72f0b06dbf4723254ac2e81f4b2bd2b52db3af', 'title': 'Human‐virtual crowd interaction: Towards understanding the effects of crowd avoidance proximity in an immersive virtual environment'}, {'paperId': '551c7ab29a1da4460714b07fdfdc914c6d64384e', 'title': 'When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence'}, {'paperId': 'aaf46c13aae18a959e0a5ac43198656da96ac9ee', 'title': 'Evaluating gaze behaviors as pre-touch reactions for virtual agents'}, {'paperId': '459d473de784bc048d7b9873cecdb3e7c564e935', 'title': 'A Large-Scale Study of Proxemics and Gaze in Groups'}, {'paperId': '45e9f1e50940432cdd7013549115be78e041350e', 'title': 'Avoiding virtual humans in a constrained environment: Exploration of novel behavioural measures'}, {'paperId': '8db31f3216cb5f2fd7d4ca2fde455107a90d4441', 'title': 'Nonverbal Communication in Immersive Virtual Reality through the Lens of Presence: A Critical Review'}, {'paperId': '241cfeb5e50c81d0246c5c97653b070e1d6a8cee', 'title': '""I\'ve talked without intending to"": Self-disclosure and Reciprocity via Embodied Avatar'}, {'paperId': 'ca81646a0ac41e5964842f0d11d416aae6d2393e', 'title': 'Assessing Inhibitory Control in the Real World Is Virtually Possible: A Virtual Reality Demonstration'}, {'paperId': '7b7346557890afa8ea6f7d608e72138528268e5e', 'title': 'Interaction in Social Space'}, {'paperId': '3cb128bf1a7453a1f2803c75b4f23c9eb51259d3', 'title': 'Are we ready for metaverse?: a measurement study of social virtual reality platforms'}, {'paperId': '338d1102c3e8eac7d9b2729cf91ab60a913fc8fc', 'title': 'Investigation of Personal Space perception in Augmented Reality'}, {'paperId': 'f10017ef8dc58411f524480bfa84f58944d67a99', 'title': 'Does having a virtual body make a difference during cinematic vr experiences?'}, {'paperId': '2e668538ed9b9c4577b9b5d53db3d7a3a3efe7eb', 'title': 'Proxemics for Human-Agent Interaction in Augmented Reality'}, {'paperId': '7662c044a9bdee269d830297ab261c28a15d4aa7', 'title': 'Digital Proxemics: Designing Social and Collaborative Interaction in Virtual Environments'}, {'paperId': '1f8c08f09d2108a6409ca89328b9241fb691d365', 'title': 'Perspective Taking and Avatar-Self Merging'}, {'paperId': 'c08659ebde31d136236fb11b1fd8080546ff7a11', 'title': 'Reality Check of Metaverse: A First Look at Commercial Social Virtual Reality Platforms'}, {'paperId': '18e47d190d41322280722c4389973381e09cf5b8', 'title': 'Proximity in VR: The Importance of Character Attractiveness and Participant Gender'}, {'paperId': 'a5cc57b26c628b139670c6a6722b05b744888896', 'title': 'Experiencing Simulated Confrontations in Virtual Reality'}, {'paperId': '04fdfbd9bc5078d6786de3fb58a14d09e1c2ddd2', 'title': 'Toward Understanding the Effects of Virtual Character Appearance on Avoidance Movement Behavior'}, {'paperId': '62411a32a307a6d9b81e5f34d41fe24c7a42132c', 'title': 'Influence of Interactivity and Social Environments on User Experience and Social Acceptability in Virtual Reality'}, {'paperId': 'cf322e0987f9cd3b0e0c0106be1b9f540012fad5', 'title': 'Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop'}, {'paperId': 'c85b586a9b1c1f166fb3b9c0517b549dadeaab19', 'title': 'Crowd Navigation in VR: Exploring Haptic Rendering of Collisions'}, {'paperId': 'd9392172c65838b71b47275383f5f5fb2785597f', 'title': 'The Effect of Gender and Attractiveness of Motion on Proximity in Virtual Reality'}, {'paperId': '939c4463b1bbc444eb17d98a58ecc6f854f0630d', 'title': 'The Action Consistency of Casting in Virtual Environment*'}, {'paperId': '355b92f0d9a1434d3ab7173cb6b6d44c87cc7229', 'title': 'A Design Space for Social Presence in VR'}, {'paperId': '7dbca475bfc609b6fd250219788bad5b37d37139', 'title': ""The Impact of a Virtual Agent's Non-Verbal Emotional Expression on a User's Personal Space Preferences""}, {'paperId': '6916a8ca69d93eb3a54f6be93be919e613ff4c29', 'title': ""Inferring a User's Intent on Joining or Passing by Social Groups""}, {'paperId': '7e254376af0825906ff7ddfebf54c395db622827', 'title': 'Do Virtual Humans Dream of Digital Sheep?'}, {'paperId': '5cd6d363759fb5dfe2c8cbd05c44fe8036f38440', 'title': 'Social VR: A New Medium for Remote Communication and Collaboration'}, {'paperId': 'da8fec2616880729531cbfbad74f4a76e7795c7d', 'title': 'Social presence and place illusion are affected by photorealism in embodied VR'}, {'paperId': 'cd6288770cdc7ad1ebe4372ce1ddb970c6e5e585', 'title': 'Is Photorealism Important for Perception of Expressive Virtual Humans in Virtual Reality?'}, {'paperId': '3ed8217ccadb43bb68f0f9419094b1383ad626f6', 'title': 'Virtual Agents from 360° Video for Interactive Virtual Reality'}, {'paperId': '8a934f52e72cec96bcaf742bad9c43c07b3fcb6b', 'title': 'Social interaction in augmented reality'}, {'paperId': 'f7509f4edb2345b0279b207d9b815ce1024078ca', 'title': 'Automatic Generation of Interactive 3D Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video'}, {'paperId': 'ed14a276e0aa7bc458fb612f94c59f6ec183ada8', 'title': '[DC] Joint Locomotion with Virtual Agents in Immersive Environments'}, {'paperId': 'c241b4b48c8cd5e38cf338de18c3e0caaaeb6613', 'title': 'You or Me? Personality Traits Predict Sacrificial Decisions in an Accident Situation'}, {'paperId': '32546070c354206fed78792484045f7b7240825f', 'title': 'Not Alone Here?! Scalability and User Experience of Embodied Ambient Crowds in Distributed Social Virtual Reality'}, {'paperId': '13f34d1310560de77e1839661b82832ac2eaa304', 'title': 'Virtual Humans as Co-Workers: A Novel Methodology to Study Peer Effects'}, {'paperId': 'fb76bc677a075da6b64970c80d285d2b79e35f6b', 'title': ""Users' locomotor behavior in collaborative virtual reality""}, {'paperId': 'bea478b42bcb8b690971700da49c1fa86f3a8ac6', 'title': 'Locomotion with Virtual Agents in the Realm of Social Virtual Reality'}, {'paperId': '7f3369b9927d8616ed28743f22096774eb894f57', 'title': 'Exploring EEG-Annotated Affective Animations in Virtual Reality: Suggestions for Improvement'}, {'paperId': '1cbb316ec3349289e8a5a8b23037500272e89bb9', 'title': 'Listening to, and Remembering Conversations between Two Talkers: Cognitive Research using Embodied Conversational Agents in Audiovisual Virtual Environments'}, {'paperId': 'b8ec7f7b99be62f1fb2aa9d44b035b63bd35820c', 'title': 'A Study on Applying Proxemics to Camera Position in VR Animation <Help Us!>'}, {'paperId': 'f9d1d0f3178bef8a346f0f83b90014beea8ba3a8', 'title': 'Interpersonal Space in Social Virtual Reality'}, {'paperId': 'e66d1715cde5058f44e611a6d2e84c50abbf4360', 'title': 'Effect of Social Settings on Proxemics During Social Interactions in Real and Virtual Conditions'}, {'paperId': 'b22ba4c9859fff4cda76d8fb2294467141c2b394', 'title': '[DC] Joint Locomotion with Virtual Agents in Immersive Environments'}, {'paperId': 'bb4c4c2482e543dcb67708f3ad775f1e3fa2ab8f', 'title': 'Towards Understanding the Influence of a Virtual Agent ’ s Emotional Expression on Personal Space'}, {'paperId': '528836373159b455f16b6dc57fd98439dd5f9cee', 'title': 'Investigating the Influence of an Approaching Virtual Agent ’ s Emotional Expression on a User ’ s Personal Space Preferences'}]","{'name': '2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)', 'pages': '199-206'}",48.0,Social VR: How Personal Space is Affected by Virtual Agents' Emotions,2018.0
2010,aa197fc8774c4717b6172b64284985e526e5a2c5,,"[{'authorId': '1768404', 'name': 'T. Nomura'}, {'authorId': '2056645824', 'name': 'Akira Nakao'}]",26.0,"{'bibtex': '@Article{Nomura2010ComparisonOI,\n author = {T. Nomura and Akira Nakao},\n journal = {International Journal of Social Robotics},\n pages = {147-157},\n title = {Comparison on Identification of Affective Body Motions by\xa0Robots Between Elder People and University Students: A\xa0Case\xa0Study in Japan},\n volume = {2},\n year = {2010}\n}\n'}",,"{'volume': '2', 'pages': '147-157', 'name': 'International Journal of Social Robotics'}",20.0,Comparison on Identification of Affective Body Motions by Robots Between Elder People and University Students: A Case Study in Japan,2010.0
2011,aa1a1db36963841b54fae0213fb90c2144236a71,"In this paper we propose a new persuasion dialogue game for agent communication. We show how this dialogue game is modeled by a framework based on social commitments and arguments. Called Commitment and Argument Network (CAN), this framework allows us to model communication dynamics in terms of actions that agents apply to commitments and in terms of argumentation relations. This dialogue game is specified by indicating its entry conditions, its dynamics and its exit conditions. In order to solve the problem of the acceptance of arguments, the protocol integrates the concept of agents' trustworthiness in its specification.","[{'authorId': '1812107', 'name': 'J. Bentahar'}, {'authorId': '1727720', 'name': 'B. Moulin'}, {'authorId': '1399443272', 'name': 'B. Chaib-draa'}, {'authorId': '34680332', 'name': 'Steven R. W. Foy'}]",7.0,"{'bibtex': '@Inproceedings{Bentahar2004APD,\n author = {J. Bentahar and B. Moulin and B. Chaib-draa and Steven R. W. Foy},\n title = {A Persuasion Dialogue Game based on Commitments and Arguments},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",44.0,A Persuasion Dialogue Game based on Commitments and Arguments,2004.0
2012,aa20cd74bd14c688d63e8cf064ca8b6f53870512,"In an experiment we manipulated a robot's voice in two ways: First, we varied robot gender; second, we equipped the robot with a human-like or a robot-like synthesized voice. Moreover, we took into account user gender and tested effects of these factors on human-robot acceptance, psychological closeness and psychological anthropomorphism. When participants formed an impression of a same-gender robot, the robot was perceived more positively. Participants also felt more psychological closeness to the same-gender robot. Similarly, the same-gender robot was anthropomorphized more strongly, but only when it utilized a human-like voice. Results indicate that a projection mechanism could underlie these effects.","[{'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '2724558', 'name': 'Dieta Kuchenbrandt'}, {'authorId': '3120498', 'name': 'Simon Bobinger'}, {'authorId': '2072122817', 'name': 'Laura E. de Ruiter'}, {'authorId': '2065189', 'name': 'F. Hegel'}]",240.0,"{'bibtex': '@Article{Eyssel2012IfYS,\n author = {F. Eyssel and Dieta Kuchenbrandt and Simon Bobinger and Laura E. de Ruiter and F. Hegel},\n journal = {2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {125-126},\n title = {‘If you sound like me, you must be more human’: On the interplay of robot and user features on human-robot acceptance and anthropomorphism},\n year = {2012}\n}\n'}",,"{'pages': '125-126', 'name': '2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",11.0,"‘If you sound like me, you must be more human’: On the interplay of robot and user features on human-robot acceptance and anthropomorphism",2012.0
2013,aa3c4d6799638836d2a20673fb910d64180a6da4,"Farmers constitute 54.6% of the Indian population, but earn only 13.9% of the national GDP. This gross mismatch can be alleviated by improving farmers' access to information and expert advice (e.g., knowing which seeds to sow and how to treat pests can significantly impact yield). In this paper, we report our experience of designing a conversational agent, called FarmChat, to meet the information needs of farmers in rural India. We conducted an evaluative study with 34 farmers near Ranchi in India, focusing on assessing the usability of the system, acceptability of the information provided, and understanding the user population's unique preferences, needs, and challenges in using the technology. We performed a comparative study with two different modalities: audio-only and audio+text. Our results provide a detailed understanding on how literacy level, digital literacy, and other factors impact users' preferences for the interaction modality. We found that a conversational agent has the potential to effectively meet the information needs of farmers at scale. More broadly, our results could inform future work on designing conversational agents for user populations with limited literacy and technology experience.","[{'authorId': '2089551047', 'name': 'Mohit Jain'}, {'authorId': '38724234', 'name': 'Pratyush Kumar'}, {'authorId': '66101406', 'name': 'Ishita Bhansali'}, {'authorId': '144921048', 'name': 'Q. Liao'}, {'authorId': '1752847', 'name': 'K. Truong'}, {'authorId': '1701358', 'name': 'Shwetak N. Patel'}]",70.0,"{'bibtex': '@Article{Jain2018FarmChatAC,\n author = {Mohit Jain and Pratyush Kumar and Ishita Bhansali and Q. Liao and K. Truong and Shwetak N. Patel},\n journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},\n pages = {170:1-170:22},\n title = {FarmChat: A Conversational Agent to Answer Farmer Queries},\n volume = {2},\n year = {2018}\n}\n'}",,"{'volume': '2', 'pages': '170:1-170:22', 'name': 'Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.'}",54.0,FarmChat: A Conversational Agent to Answer Farmer Queries,2018.0
2014,aa6be519b394b44ab24c6ad964f8a2c6a9b23571,"This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations","[{'authorId': '1394237404', 'name': 'R. Olfati-Saber'}, {'authorId': '2226866845', 'name': 'J. Alex Fax'}, {'authorId': '144696890', 'name': 'R. Murray'}]",9277.0,"{'bibtex': '@Article{Olfati-Saber2007ConsensusAC,\n author = {R. Olfati-Saber and J. Alex Fax and R. Murray},\n journal = {Proceedings of the IEEE},\n pages = {215-233},\n title = {Consensus and Cooperation in Networked Multi-Agent Systems},\n volume = {95},\n year = {2007}\n}\n'}",,"{'volume': '95', 'pages': '215-233', 'name': 'Proceedings of the IEEE'}",118.0,Consensus and Cooperation in Networked Multi-Agent Systems,2007.0
2015,aa6c3c0ee2883f90ec97f10e4491e3f02271d861,,"[{'authorId': '2106126217', 'name': 'N. Priya'}, {'authorId': '2063522518', 'name': 'Abhisek Tiwari'}, {'authorId': '145470045', 'name': 'S. Saha'}]",2.0,"{'bibtex': '@Inproceedings{Priya2021ContextAJ,\n author = {N. Priya and Abhisek Tiwari and S. Saha},\n pages = {582-595},\n title = {Context Aware Joint Modeling of Domain Classification, Intent Detection and Slot Filling with Zero-Shot Intent Detection Approach},\n year = {2021}\n}\n'}",,{'pages': '582-595'},1.0,"Context Aware Joint Modeling of Domain Classification, Intent Detection and Slot Filling with Zero-Shot Intent Detection Approach",2021.0
2016,aa88bf03209db2df8f0e30d30095e1c2449e930d,,"[{'authorId': '48160993', 'name': 'J. Kätsyri'}, {'authorId': '2556968', 'name': 'M. Sams'}]",96.0,"{'bibtex': '@Article{Kätsyri2008TheEO,\n author = {J. Kätsyri and M. Sams},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {233-242},\n title = {The effect of dynamics on identifying basic emotions from synthetic and natural faces},\n volume = {66},\n year = {2008}\n}\n'}",,"{'volume': '66', 'pages': '233-242', 'name': 'Int. J. Hum. Comput. Stud.'}",59.0,The effect of dynamics on identifying basic emotions from synthetic and natural faces,2008.0
2018,aa9e7ceb55cae5e8667a0ca19d226e00dd172afe,"Avatarscanbeemployedasamotivationaltool,forexample,allowingnon-verbalcommunicationthat can be close to human communication. We describe two lab studies where we presented participants with avatars that communicated verbally via text and visually via expressions. In the first study, participants rated five different categories of captions and corresponding avatars. Results showed that the most persuasive, consistent and trustworthy verbal feedback was given in a humanized form. The second study was an exhaustive forced choice experiment where participants chose the happiest avatar from a pair displayed. Results showed participants found visual avatars more expressive and easier to understand than their verbal counterparts, and that users respond differently when presented with negative or positive emotions. This paper contributes to a better understanding of how to design feedback for expressive avatars.","[{'authorId': '2067726616', 'name': 'Michelle Scott'}, {'authorId': '145850235', 'name': 'Lucas Pereira'}, {'authorId': '145902847', 'name': 'Ian Oakley'}]",19.0,"{'bibtex': '@Article{Scott2015ShowMO,\n author = {Michelle Scott and Lucas Pereira and Ian Oakley},\n journal = {Interact. Comput.},\n pages = {458-469},\n title = {Show Me or Tell Me: Designing Avatars for Feedback},\n volume = {27},\n year = {2015}\n}\n'}",,"{'volume': '27', 'pages': '458-469', 'name': 'Interact. Comput.'}",42.0,Show Me or Tell Me: Designing Avatars for Feedback,2015.0
2019,aaa0bdff667ee673e44805b312c3425184a0ef4e,,"[{'authorId': '2625947', 'name': 'P. Faratin'}, {'authorId': '145600625', 'name': 'C. Sierra'}, {'authorId': '144626042', 'name': 'N. Jennings'}]",609.0,"{'bibtex': '@Article{Faratin2002UsingSC,\n author = {P. Faratin and C. Sierra and N. Jennings},\n journal = {Artif. Intell.},\n pages = {205-237},\n title = {Using similarity criteria to make issue trade-offs in automated negotiations},\n volume = {142},\n year = {2002}\n}\n'}",,"{'volume': '142', 'pages': '205-237', 'name': 'Artif. Intell.'}",61.0,Using similarity criteria to make issue trade-offs in automated negotiations,2002.0
2020,aaa695ca2b79985d66dbcabbbe792af423da58e4,"The results of three experiments suggest that pre-existing mood increases the intensity of affectively congruent emotions while dampening the intensity of incongruent emotions independent of attributional knowledge. This result was obtained using a new method for inducing mood states unobtrusively and with minimal or no cognitive concomitants. The results of Experiment 1 revealed that for participants who were exposed to positive feedback a pre-existing positive mood led to stronger feelings of pride in comparison to negative mood. The results of Experiments 2 and 3 suggest that pre-existing mood directly influences the experience of subsequently elicited emotions independent of what one knows about the causes of this feeling. When participants were required to differentiate between the funniness of a cartoon and their subjective humour response, mood influenced only the latter judgement (Experiment 2). In Experiment 3, reminding participants of the mood induction resulted in a contrast effect in judging the funniness of a cartoon. However, the pre-existing mood continued to exert an assimilation effect on the overt mirth response. In conclusion, these results suggest that the feeling and knowledge component are partly independent bases of emotional responses.","[{'authorId': '47921749', 'name': 'Roland Neumann'}, {'authorId': '2848483', 'name': 'Beate Seibt'}, {'authorId': '4574442', 'name': 'F. Strack'}]",89.0,"{'bibtex': '@Article{Neumann2001TheIO,\n author = {Roland Neumann and Beate Seibt and F. Strack},\n journal = {Cognition and Emotion},\n pages = {725 - 747},\n title = {The influence of mood on the intensity of emotional responses: Disentangling feeling and knowing},\n volume = {15},\n year = {2001}\n}\n'}",,"{'volume': '15', 'pages': '725 - 747', 'name': 'Cognition and Emotion'}",43.0,The influence of mood on the intensity of emotional responses: Disentangling feeling and knowing,2001.0
2021,aadb364df890b00b16164ea161003e0039057c0d,"As part of a study of dementia, 162 normal volunteers in the age range of 65-84 years were given a battery of nine neuropsychological tests assessing temporal orientation, short-term memory, language functions, and visuoperceptive capacity. When compared to subjects less than 65 years of age, the groups showed little evidence of generalized decline in cognitive function before the age of 80 years. The 80-84 years subgroup showed a higher overall failure rate on the tests than the younger subgroups. Nevertheless, 70% of all subjects in the 80-84 years subgroup made no more than one failure on the nine tests. There were substantial differences among the tests in respect to their sensitivity to the effects of aging. The largest decline in performance was shown on tests of short-term visual memory, serial digit learning, and facial recognition. The other verbal, memory, and visuoperceptive tests were performed well up to the age of 80 years. The findings are interpreted as providing limited support for the hypothesis that normal aging does not necessarily involve a general decline in level of cognitive functioning. The clinical application of the tests that were sensitive or insensitive to the effects of aging is considered.","[{'authorId': '2203961', 'name': 'A. Benton'}, {'authorId': '5231695', 'name': 'P. Eslinger'}, {'authorId': '2656777', 'name': 'A. Damasio'}]",217.0,"{'bibtex': '@Article{Benton1981NormativeOO,\n author = {A. Benton and P. Eslinger and A. Damasio},\n journal = {Journal of clinical neuropsychology},\n pages = {\n          33-42\n        },\n title = {Normative observations on neuropsychological test performances in old age.},\n volume = {3 1},\n year = {1981}\n}\n'}",,"{'volume': '3 1', 'pages': '\n          33-42\n        ', 'name': 'Journal of clinical neuropsychology'}",20.0,Normative observations on neuropsychological test performances in old age.,1981.0
2022,aae78eafd6b1a653344c5e8e51a440f86e2658e6,,"[{'authorId': '2231651440', 'name': 'Justin Kim'}, {'authorId': '5421549', 'name': 'R. Loucks'}, {'authorId': '4879547', 'name': 'Amy L. Palmer'}, {'authorId': '6502685', 'name': 'Annemarie C. Brown'}, {'authorId': '1396511145', 'name': 'Kimberly M. Solomon'}, {'authorId': '2231277788', 'name': 'shley N. Marchante'}, {'authorId': '1869485', 'name': 'P. Whalen'}]",766.0,"{'bibtex': '@Article{Kim2011TheSA,\n author = {Justin Kim and R. Loucks and Amy L. Palmer and Annemarie C. Brown and Kimberly M. Solomon and shley N. Marchante and P. Whalen},\n journal = {Behavioural Brain Research},\n pages = {403-410},\n title = {The structural and functional connectivity of the amygdala: From normal emotion to pathological anxiety},\n volume = {223},\n year = {2011}\n}\n'}",,"{'volume': '223', 'pages': '403-410', 'name': 'Behavioural Brain Research'}",130.0,The structural and functional connectivity of the amygdala: From normal emotion to pathological anxiety,2011.0
2023,aaff3d3da7d8b18cc5d9f85d5b1af63ec174fe84,"Given recent technological developments in robotics, artificial intelligence, and virtual reality, it is perhaps unsurprising that the arrival of emotionally expressive and reactive artificial agents is imminent. However, if such agents are to become integrated into our social milieu, it is imperative to establish an understanding of whether and how humans perceive emotion in artificial agents. In this review, we incorporate recent findings from social robotics, virtual reality, psychology, and neuroscience to examine how people recognize and respond to emotions displayed by artificial agents. First, we review how people perceive emotions expressed by an artificial agent, such as facial and bodily expressions. Second, we evaluate the similarities and differences in the consequences of perceived emotions in artificial compared to human agents. Besides accurately recognizing the emotional state of an artificial agent, it is critical to understand how humans respond to those emotions. Does interacting with an angry robot induce the same responses in people as interacting with an angry person? Similarly, does watching a robot rejoice when it wins a game elicit similar feelings of elation in the human observer? Here, we provide an overview of the current state of emotion expression and perception during interactions with artificial agents, as well as a clear articulation of the challenges and guiding principles to be addressed as we move ever closer to truly emotional artificial agents.","[{'authorId': '1974324', 'name': 'R. Hortensius'}, {'authorId': '118504855', 'name': 'Felix Hekele'}, {'authorId': '1850742', 'name': 'Emily S. Cross'}]",87.0,"{'bibtex': '@Article{Hortensius2018ThePO,\n author = {R. Hortensius and Felix Hekele and Emily S. Cross},\n booktitle = {IEEE Transactions on Cognitive and Developmental Systems},\n journal = {IEEE Transactions on Cognitive and Developmental Systems},\n pages = {852-864},\n title = {The Perception of Emotion in Artificial Agents},\n volume = {10},\n year = {2018}\n}\n'}","[{'paperId': 'cf44c14aa55aa86f96b169c0584dbf1b59c1d1d2', 'title': 'Comparing an android head with its digital twin regarding the dynamic expression of emotions'}, {'paperId': '0b423e6aabe0fde36d185b338bd24e79a9218066', 'title': 'Empathic embarrassment towards non-human agents in virtual environments'}, {'paperId': '9f8bdf447dc426ce2df13cf1e76f0d7faf6da2f3', 'title': 'Opening Up to Social Robots: How Emotions Drive Self-Disclosure Behavior'}, {'paperId': '149dc74d9a5d8802f73784d57ccdfd34e6375280', 'title': 'Fintech Agents: Technologies and Theories'}, {'paperId': '392432a1aa368ef71e2bbabf837aee1645a82809', 'title': 'Implicit measures of anthropomorphism: affective priming and recognition of apparent animal emotions'}, {'paperId': '3ff6ef61fb9fae57106d25aaec12705e6bbce6a0', 'title': 'Face Editing Based on Facial Recognition Features'}, {'paperId': '5da1ff3df313ceeca19e233bada564920b13457b', 'title': 'Perceptions of intelligence & sentience shape children’s interactions with robot reading companions'}, {'paperId': 'f20c18814c98f0e47d1ddd5a715c1f34c0d35062', 'title': 'The Media Inequality, Uncanny Mountain, and the Singularity is Far from Near: Iwaa and Sophia Robot versus a Real Human Being'}, {'paperId': 'dc3d0280ce6910280d8f95c75020d4a50280eb15', 'title': 'Facial Expression Recognition Through Cross-Modality Attention Fusion'}, {'paperId': 'c856d76c777f641b9467a7f97bfe6dc1a9bd0098', 'title': 'Exploring the Social Influence of Virtual Humans Unintentionally Conveying Conflicting Emotions'}, {'paperId': '121d4e498cbf20c80bed1d1bd679beaec53223aa', 'title': 'Investigating the effect of cardio-visual synchrony on prosocial behavior towards a social robot'}, {'paperId': 'bcf46bbc465a0680c30c234c870f238be091d23b', 'title': 'Development and validation of a robot social presence measurement dimension scale'}, {'paperId': 'fef931f4dcc3e1808d0975c97882a84cb11fc20f', 'title': 'User experience of human-robot long-term interactions'}, {'paperId': '245e484081a097803199061ef27e9af00b5c0940', 'title': 'An Overview of Emotion in Artificial Intelligence'}, {'paperId': '6d1ba9c5224da957782b147319d76134aed24ecd', 'title': 'A Model Combining BDI Logic and Temporal Logics for Decision-Making in Emergency'}, {'paperId': '8a73edeb38754fbcc0ee1e603cad2489f77a6eb6', 'title': 'Social Robots as Communication Partners to Support Emotional Health and Well-Being'}, {'paperId': '25ae14f247390f87f8ede82865ea4f5c604ea816', 'title': 'A Cross-Cultural Comparison on Implicit and Explicit Attitudes Towards Artificial Agents'}, {'paperId': '3b76f11dad8d743ebca3416457f608cee2cfcee7', 'title': 'No Evidence for an Effect of the Smell of Hexanal on Trust in Human–Robot Interaction'}, {'paperId': '07052d964fb858cf4b982100a4c7d8b00add3efc', 'title': 'Examining the impact of emotion and agency on negotiator behavior'}, {'paperId': '96eb16d70350a764d80542814920ef0ddb87b859', 'title': 'Artificial empathy in marketing interactions: Bridging the human-AI gap in affective and social customer experience'}, {'paperId': '382bf7d95deea21190f08477c502f630c4a7d2bc', 'title': 'Subjective Evaluation of Basic Emotions from Audio–Visual Data'}, {'paperId': 'cf30e3c8f42a28a06c568bc641ac8496db0b806f', 'title': 'Facing the FACS—Using AI to Evaluate and Control Facial Action Units in Humanoid Robot Face Development'}, {'paperId': '2d040ee10a5553d1aa06611c57a5d7da6f2f8c90', 'title': 'Correlation Analysis of Japanese Literature and Psychotherapy Effects Based on an Equation Diagnosis Algorithm'}, {'paperId': '6f6638f964363965df3c1bd3dddc41102510ec3d', 'title': 'Comparative Analytical Survey on Cognitive Agents with Emotional Intelligence'}, {'paperId': '0a734c6d108d7643d24c09bc425b041df4e4720f', 'title': 'The Effect of Music and Light-Color as a Machine Empathic Response on Stress in Occupational Health'}, {'paperId': '646be806598e52cf7df70a1d83d68bfe2d6b4119', 'title': 'Data-driven emotional body language generation for social robotics'}, {'paperId': 'ad2f54569ce99205628583b458fe6426df421473', 'title': 'Mental State Attribution to Robots: A Systematic Review of Conceptions, Methods, and Findings'}, {'paperId': '0ae683c549d259264e93ebb2ae3bc6d0b5a125dc', 'title': 'Knowledge-augmented face perception: Prospects for the Bayesian brain-framework to align AI and human vision'}, {'paperId': '7e69f823341388f6cea2568ad380e5509474801b', 'title': 'Anthropomorphic Robotic Eyes: Structural Design and Non-Verbal Communication Effectiveness'}, {'paperId': 'fb54d7646b76c275451cdb74c17fa83643870fea', 'title': 'Social robots as depictions of social agents'}, {'paperId': '6516926e3c7cfd7b6ca548d848a43240f75ae55f', 'title': 'Emotion diffusion effect: Negative sentiment COVID-19 tweets of public organizations attract more responses from followers'}, {'paperId': 'e8e4c25dc9957394ba291884fcca32193ec651cd', 'title': 'The Role of Empathic Traits in Emotion Recognition and Emotion Contagion of Cozmo Robots'}, {'paperId': '000316b6f0e2f5d08f08ec1acd2667e2e0ecb7e1', 'title': 'Robo-Identity: Exploring Artificial Identity and Emotion via Speech Interactions'}, {'paperId': 'efc91373e5bfb0fb2322d0864e6c49c2556e35a2', 'title': 'Social Robots for Supporting Post-traumatic Stress Disorder Diagnosis and Treatment'}, {'paperId': '4e43dc65830413f80547c7aeca90d29ed53044a3', 'title': 'Perceived facial happiness during conversation correlates with insular and hypothalamus activity for humans, not robots'}, {'paperId': 'f87d0ced73e52800125d7ce69984c6b1aa14a189', 'title': 'Domain‐specific and domain‐general neural network engagement during human–robot interactions'}, {'paperId': '527973637a62010c19a3f010496d94c0359b333a', 'title': 'Perceptions of Anthropomorphism in a Chatbot Dialogue: The Role of Animacy and Intelligence'}, {'paperId': '6023e7b18a4fa3e11a953c7a6707a8dbb991c14d', 'title': 'A Machine Learning Tool to Determine State of Mind and Emotion'}, {'paperId': 'd3390eede0382c79a19b8d6bc61dc749decc595f', 'title': 'Robot Gaze Behavior Affects Honesty in Human-Robot Interaction'}, {'paperId': '1b006108488a28ab7aba71ba2c96282f06f837a8', 'title': 'Intelligent interactive technologies for mental health and well-being'}, {'paperId': 'd6daeb9dd9d8a43e109ea284d8b6065bbd2f98e2', 'title': 'Can communication with social robots influence how children develop empathy? Best-evidence synthesis'}, {'paperId': '112e1101efe9c32c4a3699acd758d703d7af91e3', 'title': 'Investigation and Analysis of Anxiety Dissemination in Various Situations and Research on Methods of Eliminating Anxiety in Population'}, {'paperId': '69a26d873c1e279bb950f1d984bf9375ffc8a79c', 'title': 'Computational Modeling of Emotion-Motivated Decisions for Continuous Control of Mobile Robots'}, {'paperId': '744eb94d8b507ece77ebf665c42335e5e2b3d4b6', 'title': 'A systematic mapping study on agent mining'}, {'paperId': 'a785707d75412ccf26d547403c5666363c835a06', 'title': 'Relationship Identification Between Conversational Agents Using Emotion Analysis'}, {'paperId': '750ff8254b23901808ad1885bba141aaa673c51d', 'title': 'Relationship Identification Between Conversational Agents Using Emotion Analysis'}, {'paperId': '2e4a8d991db497849b1b95d5ad24dac453dca472', 'title': 'Mind the Eyes: Artificial Agents’ Eye Movements Modulate Attentional Engagement and Anthropomorphic Attribution'}, {'paperId': 'fd36155b77669555208b48cb873aa40aa051746d', 'title': 'Empathy and Schadenfreude in Human–Robot Teams'}, {'paperId': '1f0c6cdcf7532f427133bd9108d0ec4195096ef5', 'title': 'Facial Expression Imitation Method for Humanoid Robot Based on Smooth-Constraint Reversed Mechanical Model (SRMM)'}, {'paperId': '57f3047f20afeab8cc9649c610e68066865975bd', 'title': 'Learning the Connectivity: Situational Graph Convolution Network for Facial Expression Recognition'}, {'paperId': '6df6ad53daa2e870caf3304af97f6ae66b2fc7a5', 'title': 'Using Emotions to Complement Multi-Modal Human-Robot Interaction in Urban Search and Rescue Scenarios'}, {'paperId': '67f31df6cfec64a42c2161b0641b23d7a8405e17', 'title': 'Emotional Valence Recognition on Virtual, Robotic, and Human Faces: a Comparative Study'}, {'paperId': '0935fe0484bc369028fc67cadb207a3e0238accd', 'title': 'Emotional Valence Recognition on Virtual, Robotic, and Human Faces: a Comparative Study'}, {'paperId': 'b0cfc5a2e4634344b0c6eb3beb1f9e6c66d14960', 'title': 'Facial Expression Recognition Using Spatial-Temporal Semantic Graph Network'}, {'paperId': '0b80bf3e42250c0f4f2919e9347d75495cfa5921', 'title': 'On Designing Expressive Robot Behavior: The Effect of Affective Cues on Interaction'}, {'paperId': '51337e4d4f6294e496ec3036cfe1bc3428662ea9', 'title': '“Hit the Robot on the Head With This Mallet” – Making a Case for Including More Open Questions in HRI Research'}, {'paperId': 'ae95137ce087bfff1121e78eb43899f73a38d41f', 'title': 'A Systematic Review of Ten Years of Research on Human Interaction with Social Robots'}, {'paperId': 'b073b188df0dc5870c19f18c4db4ebacc2b18501', 'title': 'Would you help a sad robot? Influence of robots’ emotional expressions on human-multi-robot collaboration'}, {'paperId': '5d8db1288eb4d1bcbb3cb54246e64caf1cd6a024', 'title': 'Learning Bodily Expression of Emotion for Social Robots Through Human Interaction'}, {'paperId': '690632f500d36a89e0096bd6c925ccff645e6919', 'title': 'Human–Robot Cooperation in Economic Games: People Show Strong Reciprocity but Conditional Prosociality Toward Robots'}, {'paperId': '54c7509f7b40e976c178c10cfbbb7c2eaa5ababd', 'title': 'Facial Expression Recognition via Deep Action Units Graph Network Based on Psychological Mechanism'}, {'paperId': '8b7ba1a9f1e74a3a2ee9d3f176fae345bc1c00a9', 'title': 'Tell me more! Assessing interactions with social robots from speech'}, {'paperId': '7f147868c64520c0cd87b21b40a8f82483d46b15', 'title': 'Social Cognition in the Age of Human–Robot Interaction'}, {'paperId': '034ff9763a0e545496349b788537264e8d9ebb7e', 'title': 'EMI: An Expressive Mobile Interactive Robot'}, {'paperId': '58a2078c225773ba92fb256e40905c30944167d2', 'title': 'Social Robots on a Global Stage: Establishing a Role for Culture During Human–Robot Interaction'}, {'paperId': '55cf7f686bdb39076b59e34bee3b8dc57f6352a9', 'title': '""Are You Sad, Cozmo?"" How Humans Make Sense of a Home Robot’s Emotion Displays'}, {'paperId': '0daae809a2018e7b2124570a8c031e310891ee98', 'title': 'Taxonomy of Trust-Relevant Failures and Mitigation Strategies'}, {'paperId': '5cb4144d255e5a9182de93f6cdaebf91ed0223b4', 'title': 'Dynamic human and avatar facial expressions elicit differential brain responses'}, {'paperId': 'a81e0fdd1911b25560c7255ecdeaff52eb459907', 'title': 'Classification of brain activities during language and music perception'}, {'paperId': 'f497db7caac44635f3b5ad0b3b8151426894bd3e', 'title': 'Classification of brain activities during language and music perception'}, {'paperId': '71fcaed447f0fb2ee764654a6009c883220c53a5', 'title': 'From social brains to social robots: applying neurocognitive insights to human–robot interaction'}, {'paperId': '65cf83a4b13177c0f5a063a9e0fc4d86da08b4df', 'title': 'Multimodal Integration of Emotional Signals from Voice, Body, and Context: Effects of (In)Congruence on Emotion Recognition and Attitudes Towards Robots'}, {'paperId': '07764c10d32358bed22881b21d76a500e57ce423', 'title': 'Meeting with social robots like the cat-cucumber meeting? An integrated model of human-robot first contact. Psychological perspective.'}, {'paperId': 'f6e834a7a53f097c58802516ef82ad1841ffe18f', 'title': 'A neurocognitive investigation of the impact of socialising with a robot on empathy for pain'}, {'paperId': 'd97ffd467569aadbe872dd384d3498758dce3241', 'title': 'Track Your Emotional Perception of 3-D Virtual Talking Head in Human-computer Interaction'}, {'paperId': '70998e14f795d82e367d890aae5476a91472f268', 'title': 'The Attribution of Emotional State - How Embodiment Features and Social Traits Affect the Perception of an Artificial Agent'}, {'paperId': '621197d43ef39408289ab46254ea26447c624645', 'title': 'From automata to animate beings: the scope and limits of attributing socialness to artificial agents'}, {'paperId': '27a95910325f2f8e5d0825da6f1f645922ad7c35', 'title': 'Accurate colluding agents detection by reputation measures'}, {'paperId': '150166353849fa2b1b3ef90eb19048846a7b7150', 'title': 'Modeling dynamic web polarization and proximity depolarization processes by compactness measures'}, {'paperId': '915c593701e67298d0e5388fa6f0d77977f62fe8', 'title': 'Research on Operation Intention Based on Flexible Tactile Sensing Handle'}, {'paperId': '8b461f3460f700bc91c4c299588b3119f6b1dde4', 'title': 'Non-verbal Emotional Expressions for Social Presence of Chatbot Interface'}, {'paperId': '9688b1b9508a5a643f564b999dc2ebc11fede14d', 'title': ""Recognition of a Robot's Affective Expressions Under Conditions with Limited Visibility""}, {'paperId': '59d583c1ee7bd6382c2313943626ae1b8d1ff23e', 'title': 'Do You Feel Me?: Learning Language from Humans with Robot Emotional Displays'}, {'paperId': '99a483733bed156bf4cf8a74a7121776e8dc8efd', 'title': 'Wykowska A . 2019 From social brains to social robots : applying neurocognitive insights to human – robot interaction'}, {'paperId': '099796c3e0428093ea3db494b5356169aafc89f3', 'title': 'Towards Truly Affective AAL Systems'}, {'paperId': '4410e37d9d684c99db40dab415ba616d59bf6040', 'title': 'Artificial agents as social companions: design guidelines for emotional interactions'}, {'paperId': '017767b50df3a0f32d056859ced76996470c7ce1', 'title': 'The Desiring Algorithm. The Sex Appeal of the Inorganic'}]","{'name': 'IEEE Transactions on Cognitive and Developmental Systems', 'pages': '852-864', 'volume': '10'}",143.0,The Perception of Emotion in Artificial Agents,2018.0
2024,ab0b6c4790fd96e513c97eadcf7058cfe7234766,"Emotion scientists often distinguish those emotions that are encountered universally, even among animals ( “primary emotions”), from those experienced by human beings ( “secondary emotions”). No attempt, however, has ever been made to capture the lay conception about this distinction and to find the criteria on which the distinction is based. The first study presented in this paper was conducted in three countries involving four languages, so as to allow for cross‐cultural comparisons. Results showed a remarkable convergence. People from all samples not only differentiated between “uniquely human” and “non‐uniquely human” emotions on a continuum, but they did so on the same basis as the one used by emotion scientists to distinguish between “primary” and “secondary” emotions. Study 2 focused on the implicit use of such a distinction. When confronted with a human (animal) context, participants reacted faster to secondary (vs primary) emotions. The implications of the human uniqueness of some emotions within the social and interpersonal contexts are discussed.","[{'authorId': '5639312', 'name': 'S. Demoulin'}, {'authorId': '114491351', 'name': 'J. Leyens'}, {'authorId': '3366020', 'name': 'M. Paladino'}, {'authorId': '1403615399', 'name': 'Ramón Rodríguez-Torres'}, {'authorId': '1401987657', 'name': 'Armando Rodríguez-Pérez'}, {'authorId': '1830225', 'name': 'J. Dovidio'}]",305.0,"{'bibtex': '@Article{Demoulin2004DimensionsO,\n author = {S. Demoulin and J. Leyens and M. Paladino and Ramón Rodríguez-Torres and Armando Rodríguez-Pérez and J. Dovidio},\n journal = {Cognition and Emotion},\n pages = {71 - 96},\n title = {Dimensions of “uniquely” and “non‐uniquely” human emotions},\n volume = {18},\n year = {2004}\n}\n'}",,"{'volume': '18', 'pages': '71 - 96', 'name': 'Cognition and Emotion'}",45.0,Dimensions of “uniquely” and “non‐uniquely” human emotions,2004.0
2025,ab2148d4ccaa18f055af7560485f38be13e6cc34,"In this article we consider the ways in which visual information is used as a conversational resource in the accomplishment of collaborative physical tasks. We focus on the role of visual information in maintaining task awareness and in achieving mutual understanding in conversation. We first describe the theoretical framework we use to analyze the role of visual information in physical collaboration. Then, we present two experiments that vary the amount and quality of the visual information available to participants during a collaborative bicycle repair task. We examine the effects of this visual information on performance and on conversational strategies. We conclude with a general discussion of how situational awareness and conversational grounding are achieved in collaborative tasks and with some design considerations for systems to support remote collaborative repair.","[{'authorId': '1702853', 'name': 'R. Kraut'}, {'authorId': '1692772', 'name': 'Susan R. Fussell'}, {'authorId': '143760899', 'name': 'J. Siegel'}]",422.0,"{'bibtex': '@Article{Kraut2003VisualIA,\n author = {R. Kraut and Susan R. Fussell and J. Siegel},\n journal = {Human–Computer Interaction},\n pages = {13 - 49},\n title = {Visual Information as a Conversational Resource in Collaborative Physical Tasks},\n volume = {18},\n year = {2003}\n}\n'}",,"{'volume': '18', 'pages': '13 - 49', 'name': 'Human–Computer Interaction'}",41.0,Visual Information as a Conversational Resource in Collaborative Physical Tasks,2003.0
2026,ab2730eece03ac4d66e8676ed61353e08c30f291,"This paper discusses the Integrated System for Facial Expression Recognition (ISFER), which performs facial expression analysis from a still dual facial view image. The system consists of three major parts: a facial data generator, a facial data evaluator and a facial data analyser. While the facial data generator applies fairly conventional techniques for facial feature extraction, the rest of the system represents a novel way of performing a reliable identification of 30 different face actions and a multiple classification of expressions into the six basic emotion categories. An expert system has been utilised to convert low level face geometry into high level face actions, and then this into highest level weighted emotion labels. The system evaluation results demonstrated rather high concurrent validity with human coding of facial expressions using FACS and formal instructions in emotion signals.","[{'authorId': '145387780', 'name': 'M. Pantic'}, {'authorId': '1713654', 'name': 'L. Rothkrantz'}]",64.0,"{'bibtex': '@Article{Pantic1999AnES,\n author = {M. Pantic and L. Rothkrantz},\n journal = {Proceedings 11th International Conference on Tools with Artificial Intelligence},\n pages = {113-120},\n title = {An expert system for multiple emotional classification of facial expressions},\n year = {1999}\n}\n'}",,"{'pages': '113-120', 'name': 'Proceedings 11th International Conference on Tools with Artificial Intelligence'}",20.0,An expert system for multiple emotional classification of facial expressions,1999.0
2027,ab3cd200735f3d11dcc980cf24e70320ce9d9a85,,"[{'authorId': '144011340', 'name': 'G. Norman'}]",3340.0,"{'bibtex': '@Article{Norman2010LikertSL,\n author = {G. Norman},\n journal = {Advances in Health Sciences Education},\n pages = {625-632},\n title = {Likert scales, levels of measurement and the “laws” of statistics},\n volume = {15},\n year = {2010}\n}\n'}",,"{'volume': '15', 'pages': '625-632', 'name': 'Advances in Health Sciences Education'}",20.0,"Likert scales, levels of measurement and the “laws” of statistics",2010.0
2028,ab53169405702e79e6823902f02aa412fb0a2fa0,,"[{'authorId': '144038070', 'name': 'Yao-bin Lu'}, {'authorId': '2254268814', 'name': 'Ling Zhao'}, {'authorId': None, 'name': 'Bin Wang'}]",638.0,"{'bibtex': ""@Article{Lu2010FromVC,\n author = {Yao-bin Lu and Ling Zhao and Bin Wang},\n journal = {Electron. Commer. Res. Appl.},\n pages = {346-360},\n title = {From virtual community members to C2C e-commerce buyers: Trust in virtual communities and its effect on consumers' purchase intention},\n volume = {9},\n year = {2010}\n}\n""}",,"{'volume': '9', 'pages': '346-360', 'name': 'Electron. Commer. Res. Appl.'}",73.0,From virtual community members to C2C e-commerce buyers: Trust in virtual communities and its effect on consumers' purchase intention,2010.0
2029,ab936a19d3c1af9587db0e568627b83373c9b57d,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '144668050', 'name': 'C. Gerritsen'}, {'authorId': '144287490', 'name': 'J. D. Man'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",13.0,"{'bibtex': '@Inproceedings{Bosse2012MeasuringSE,\n author = {T. Bosse and C. Gerritsen and J. D. Man and Jan Treur},\n pages = {322-330},\n title = {Measuring Stress-Reducing Effects of Virtual Training Based on Subjective Response},\n year = {2012}\n}\n'}",,{'pages': '322-330'},19.0,Measuring Stress-Reducing Effects of Virtual Training Based on Subjective Response,2012.0
2030,abb2ed00e3255b80fcb3a84f6829d11c6843c736,"Computer-mediated communication systems known as collaborative virtual environments (CVEs) allow geographically separated individuals to interact verbally and nonverbally in a shared virtual space in real time. We discuss a CVE-based research paradigm that transforms (i.e., filters and modifies) nonverbal behaviors during social interaction. Because the technology underlying CVEs allows a strategic decoupling of rendered behavior from the actual behavior of the interactants, conceptual and perceptual constraints inherent in face-to-face interaction need not apply. Decoupling algorithms can enhance or degrade facets of nonverbal behavior within CVEs, such that interactants can reap the benefits of nonverbal enhancement or suffer nonverbal degradation. Concepts underlying transformed social interaction (TSI), the ethics and implications of such a research paradigm, and data from a pilot study examining TSI are discussed.","[{'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '40458739', 'name': 'A. Beall'}, {'authorId': '2386187', 'name': 'J. Loomis'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '144097660', 'name': 'M. Turk'}]",250.0,"{'bibtex': '@Article{Bailenson2004TransformedSI,\n author = {J. Bailenson and A. Beall and J. Loomis and J. Blascovich and M. Turk},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {428-441},\n title = {Transformed Social Interaction: Decoupling Representation from Behavior and Form in Collaborative Virtual Environments},\n volume = {13},\n year = {2004}\n}\n'}",,"{'volume': '13', 'pages': '428-441', 'name': 'Presence: Teleoperators & Virtual Environments'}",73.0,Transformed Social Interaction: Decoupling Representation from Behavior and Form in Collaborative Virtual Environments,2004.0
2031,abc62b40e07e758e6ca013db2956a3f2d021dec1,,"[{'authorId': '144483472', 'name': 'A. Nijholt'}]",43.0,"{'bibtex': '@Article{Nijholt2004WhereCD,\n author = {A. Nijholt},\n journal = {Comput. Graph.},\n pages = {467-476},\n title = {Where computers disappear, virtual humans appear},\n volume = {28},\n year = {2004}\n}\n'}",,"{'volume': '28', 'pages': '467-476', 'name': 'Comput. Graph.'}",25.0,"Where computers disappear, virtual humans appear",2004.0
2032,abd1c342495432171beb7ca8fd9551ef13cbd0ff,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","[{'authorId': '2064160', 'name': 'A. Krizhevsky'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}]",103931.0,"{'bibtex': '@Article{Krizhevsky2012ImageNetCW,\n author = {A. Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},\n journal = {Communications of the ACM},\n pages = {84 - 90},\n title = {ImageNet classification with deep convolutional neural networks},\n volume = {60},\n year = {2012}\n}\n'}",,"{'volume': '60', 'pages': '84 - 90', 'name': 'Communications of the ACM'}",44.0,ImageNet classification with deep convolutional neural networks,2012.0
2033,abd3db64cb99c139aabb25a8bfa2770ac86c0a92,"Researchers demand much from their embodied conversational agents (ECAs), requiring them to be both life-like, as well as responsive to events in an interactive setting. We find that a flexible combination of animation approaches may be needed to satisfy these needs. In this paper we present SmartBody, an open source modular framework for animating ECAs in real time, based on the notion of hierarchically connected animation controllers. Controllers in SmartBody can employ arbitrary animation algorithms such as keyframe interpolation, motion capture or procedural animation. Controllers can also schedule or combine other controllers. We discuss our architecture in detail, including how we incorporate traditional approaches, and develop the notion of a controller as a reactive module within a generic framework, for realizing modular animation control. To illustrate the versatility of the architecture, we also discuss a range of applications that have used SmartBody successfully.","[{'authorId': '2096971', 'name': 'M. Thiébaux'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2979549', 'name': 'Andrew N. Marshall'}, {'authorId': '1682684', 'name': 'Marcelo Kallmann'}]",301.0,"{'bibtex': '@Inproceedings{Thiébaux2008SmartBodyBR,\n author = {M. Thiébaux and S. Marsella and Andrew N. Marshall and Marcelo Kallmann},\n pages = {151-158},\n title = {SmartBody: behavior realization for embodied conversational agents},\n year = {2008}\n}\n'}",,{'pages': '151-158'},29.0,SmartBody: behavior realization for embodied conversational agents,2008.0
2035,abf5ee102356e85b04520bf67709a24bbf3034fa,,"[{'authorId': '2109341502', 'name': 'Zhen Liu'}, {'authorId': '2115377494', 'name': 'Yuan Hong'}, {'authorId': '1794500', 'name': 'Qiong Liu'}, {'authorId': '9398597', 'name': 'Yanjie Chai'}]",4.0,"{'bibtex': '@Article{Liu2011AnEM,\n author = {Zhen Liu and Yuan Hong and Qiong Liu and Yanjie Chai},\n booktitle = {Transactions on Edutainment},\n journal = {Trans. Edutainment},\n pages = {154-163},\n title = {An Emotion Model for Virtual Agents with Evolvable Motivation},\n volume = {6},\n year = {2011}\n}\n'}","[{'paperId': '0e13ab8d462d219c38a157e11d68d7e8c7d71307', 'title': 'On designing migrating agents: from autonomous virtual agents to intelligent robotic systems'}, {'paperId': '8c973ea5f1c0de1f5981285e626a865b5161746d', 'title': 'Behavior believability in virtual worlds: agents acting when they need to'}, {'paperId': 'b572ade6b88d33565db5543120503493bbf9f941', 'title': 'Intelligent Behaviors of Virtual Characters in Serious Games for Child Safety Education'}, {'paperId': 'ed3ca931d3706508a0760706d120a1ef37729c8a', 'title': 'Using Holograms to Increase Interaction in Museums'}]","{'name': 'Trans. Edutainment', 'pages': '154-163', 'volume': '6'}",18.0,An Emotion Model for Virtual Agents with Evolvable Motivation,2011.0
2036,ac32109b5b91b4490295d31ef051ffb98a0003d8,,"[{'authorId': '6367374', 'name': 'L. Wagels'}, {'authorId': '145880611', 'name': 'Sina Radke'}, {'authorId': '2455354', 'name': 'K. Goerlich'}, {'authorId': '2782974', 'name': 'U. Habel'}, {'authorId': '3270733', 'name': 'M. Votinov'}]",28.0,"{'bibtex': ""@Article{Wagels2017ExogenousTD,\n author = {L. Wagels and Sina Radke and K. Goerlich and U. Habel and M. Votinov},\n journal = {Hormones and Behavior},\n pages = {75-83},\n title = {Exogenous testosterone decreases men's personal distance in a social threat context},\n volume = {90},\n year = {2017}\n}\n""}",,"{'volume': '90', 'pages': '75-83', 'name': 'Hormones and Behavior'}",78.0,Exogenous testosterone decreases men's personal distance in a social threat context,2017.0
2038,ac338e286788dc790f13607bff607e0919beb378,"In order to test whether constant eye contact, formal posture, and varied vocal inflection increase source credibility and listener comprehensiOn, 144 College students in an introductory speeck communication course were placed in groups of equal size that .listened to the same informative speech. The speaker presented to each group a different combination cf the independent variables, and, after the subjects were tested for their comprehension of the speech, they'rated the speaker's credibility.. Analysis of the effects of the three independent Veriableh led to the following conclusions: Eye .contact seeps to enhance both listener comprehension and speaker credibility, though inconsistencies""between'eye contact and vocal inflection may lower the speaker's believability; the speaker's posture has little effect on either-cadibility or comprehension; varied or limited vocal lefiection has no-significant effect""dpon the speaker's credibility, except for the likability factor of credibility; and differences invocal inflection do not _affect listener comprehension. Tables of 'the analyzed data illustrate the text. (RL)* .ABSTRACT","[{'authorId': '69373932', 'name': 'S. Beebe'}]",21.0,"{'bibtex': '@Inproceedings{Beebe1976EffectsOE,\n author = {S. Beebe},\n title = {Effects of Eye Contact, Posture and Vocal Inflection upon Credibility and Comprehension.},\n year = {1976}\n}\n'}",,"{'volume': '', 'name': ''}",4.0,"Effects of Eye Contact, Posture and Vocal Inflection upon Credibility and Comprehension.",1976.0
2039,ac50a25a0ea450c7c05f643427e3da5ab43fa733,"The potential of emotional interaction between human and computer has recently interested researchers in human–computer interaction. The instructional impact of this interaction in learning environments has not been established, however. This study examined the impact of emotion and gender of a pedagogical agent as a learning companion (PAL) on social judgements, interest, self-efficacy, and learning. Two experiments investigated separately the effects of a PAL’s emotional expression and empathetic response. Experiment 1 focused on emotional expression (positive vs. negative vs. neutral) and gender (male vs. female) with a sample of 142 male and female college students in a computer literacy course. Experiment 2 investigated the impact of empathetic response (responsive vs. non-responsive) and gender with 56 pre-service teachers. Overall, the results yielded main and interaction effects of PAL emotion and gender on the dependent variables. In particular, the PAL’s empathetic response had a positive impact on learner interest and self-efficacy; PAL gender had a positive impact on recall. The findings imply that the emotion and the gender of the digital learning companion could be utilized to optimize college students’ motivation and learning.","[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '25550816', 'name': 'A. L. Baylor'}, {'authorId': '48977180', 'name': 'E. Shen'}]",193.0,"{'bibtex': '@Article{Kim2007PedagogicalAA,\n author = {Yanghee Kim and A. L. Baylor and E. Shen},\n journal = {J. Comput. Assist. Learn.},\n pages = {220-234},\n title = {Pedagogical agents as learning companions: the impact of agent emotion and gender},\n volume = {23},\n year = {2007}\n}\n'}",,"{'volume': '23', 'pages': '220-234', 'name': 'J. Comput. Assist. Learn.'}",85.0,Pedagogical agents as learning companions: the impact of agent emotion and gender,2007.0
2040,ac6ec6a266df0d8245959319d55caa1c133adf7e,"last decade, intensive research on emotional intelligence has advanced 
 significantly from its theoretical basis, analytical studies and processing 
 technology to exploratory applications in a wide range of real-life domains. 
 This paper brings new insights in the field of emotional, intelligent 
 software agents. The first part is devoted to an overview of the 
 state-of-the-art in emotional intelligence research with emphasis on 
 emotional agents. A wide range of applications in different areas like 
 modeling emotional agents, aspects of learning in emotional environments, 
 interactive emotional systems and so on are presented. After that we suggest 
 a systematic order of research steps with the idea of proposing an adequate 
 framework for several possible real-life applications of emotional agents. We 
 recognize that it is necessary to apply specific methods for dynamic data 
 analysis in order to identify and discover new knowledge from available 
 emotional information and data sets. The last part of the paper discusses 
 research activities for designing an agent-based architecture, in which 
 agents are capable of reasoning about and displaying some kind of emotions 
 based on emotions detected in human speech, as well as online documents. 
 [Projekat Ministarstva nauke Republike Srbije, br. OI174023: Intelligent 
 techniques and their integration into wide-spectrum decision support]","[{'authorId': '144395551', 'name': 'M. Ivanović'}, {'authorId': '1712090', 'name': 'Z. Budimac'}, {'authorId': '143651750', 'name': 'Miloš Radovanović'}, {'authorId': '3113695', 'name': 'V. Kurbalija'}, {'authorId': '143742626', 'name': 'Weihui Dai'}, {'authorId': '1740341', 'name': 'C. Bǎdicǎ'}, {'authorId': '1789290', 'name': 'M. Colhon'}, {'authorId': '33627984', 'name': 'S. Ninkovic'}, {'authorId': '34664252', 'name': 'Dejan Mitrovic'}]",21.0,"{'bibtex': '@Article{Ivanović2015EmotionalA,\n author = {M. Ivanović and Z. Budimac and Miloš Radovanović and V. Kurbalija and Weihui Dai and C. Bǎdicǎ and M. Colhon and S. Ninkovic and Dejan Mitrovic},\n journal = {Comput. Sci. Inf. Syst.},\n pages = {1121-1148},\n title = {Emotional agents - state of the art and applications},\n volume = {12},\n year = {2015}\n}\n'}",,"{'volume': '12', 'pages': '1121-1148', 'name': 'Comput. Sci. Inf. Syst.'}",89.0,Emotional agents - state of the art and applications,2015.0
2041,ac6f2191227b14d9cd29cd074fb6bf5c46d9ab0e,"The present study concerned the influence ofthe presence of others on facial expressions of emotion. The proposition that facial expressive displays are better predicted by the social context than by emotional state (A. J. Fridlund, 1991) was tested in an experiment varying both the sociality of the context and the intensity of the emotion elicitor as well as the relationship between expressor and audience. The results indicate that the intensity of expressive displays cannot be satisfactorily predicted by either of these factors alone but is influenced by a complex interplay of all 3 factors.","[{'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '47161796', 'name': 'R. Banse'}, {'authorId': '1742554', 'name': 'Arvid Kappas'}]",303.0,"{'bibtex': '@Article{Hess1995TheIO,\n author = {U. Hess and R. Banse and Arvid Kappas},\n journal = {Journal of Personality and Social Psychology},\n pages = {280-288},\n title = {The intensity of facial expression is determined by underlying affective state and social situation.},\n volume = {69},\n year = {1995}\n}\n'}",,"{'volume': '69', 'pages': '280-288', 'name': 'Journal of Personality and Social Psychology'}",40.0,The intensity of facial expression is determined by underlying affective state and social situation.,1995.0
2042,acae71006c0a8c3193f99b89ddf23e4d319a3d3c,"INTRODUCTION: Self-attachment is a new self-administrable psychotherapeutic intervention based on creating an a ﬀ ectional bond between the user and their childhood-self using their childhood photos to develop the capacity for a ﬀ ect self-regulation. Technological advances, such as virtual reality (VR), can enhance the procedure of this intervention and make it scalable. METHODS: We have developed a user-friendly, interactive VR platform for self-attachment featuring a virtual assistant and a customised child avatar that resembles the user in their childhood. The virtual agent interacts with the user and using an emotion recognition algorithm can provide suggestions for the user to undertake an appropriate self-attachment sub-protocol. Furthermore, the platform allows user interaction with the child avatar, such as embracing the avatar. RESULTS: We show by a small preliminary trial that such a VR experience can be realistic, leading to a positive emotion change in the user.","[{'authorId': '2133722561', 'name': 'Neophytos Polydorou'}, {'authorId': '1694989', 'name': 'A. Edalat'}]",4.0,"{'bibtex': '@Article{Polydorou2021AnIV,\n author = {Neophytos Polydorou and A. Edalat},\n booktitle = {EAI Endorsed Transactions on Pervasive Health and Technology},\n journal = {EAI Endorsed Trans. Pervasive Health Technol.},\n pages = {e5},\n title = {An interactive VR platform with emotion recognition for self-attachment intervention},\n volume = {7},\n year = {2021}\n}\n'}","[{'paperId': '8c770082c31dd847024e59f66ca1478568e35706', 'title': 'From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique'}, {'paperId': 'f77af8aed5be4764560e90437c23357d3bfa0eaf', 'title': 'A Pilot Study to Evaluate the Efficacy of Self-Attachment to Treat Chronic Anxiety and/or Depression in Iranian Women'}, {'paperId': '5f110c3a0c22ec1245b6abcb84a142faa9925c8b', 'title': 'An Empathetic AI Coach for Self-Attachment Therapy'}, {'paperId': '4a354ec1a187261b7dced3220a4d5b4011335075', 'title': 'Valence/Arousal Estimation of Occluded Faces from VR Headsets'}]","{'name': 'EAI Endorsed Trans. Pervasive Health Technol.', 'pages': 'e5', 'volume': '7'}",58.0,An interactive VR platform with emotion recognition for self-attachment intervention,2021.0
2043,acc90c2f727736958f01c16137b5c80680d6cb7e,"Thank you very much for reading attention theory and practice. Maybe you have knowledge that, people have search hundreds times for their chosen readings like this attention theory and practice, but end up in harmful downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they are facing with some harmful virus inside their laptop. attention theory and practice is available in our digital library an online access to it is set as public so you can download it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the attention theory and practice is universally compatible with any devices to read.","[{'authorId': '21149652', 'name': 'U. Boehm'}]",65.0,"{'bibtex': '@Inproceedings{Boehm2016AttentionTA,\n author = {U. Boehm},\n title = {Attention Theory And Practice},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Attention Theory And Practice,2016.0
2044,ad07b99615407b4bf5c0967571d6bd8b9f969ff7,"Empathy and Moral Development represents the life’s work of Professor Hoffman, integrating over 30 years of research with information and ideas gleaned from the psychological and social development theories of the last century. Starting with biblical concepts of sin and guilt and drawing on the germinal theories of historical figures such as Rousseau, Freud and Piaget, the author also discusses Kohlberg’s theory and modifications by later followers. 
 
The first chapter begins with a brief overview of the previous and current theories and the historical sources for the book, giving clear definitions and outlining the theory to follow. Each of the seven sections of the book expands on important key concepts introduced earlier. There are frequent references to and review of the previously discussed material. The first section of the book explains the “innocent bystander” model that has been used for decades to explore human moral development. Concepts from animal arousal and behavior models are linked with human infant research on early affective and cognitive development. The research quoted is illustrative and extensive. The section ends with a discussion of the differences between empathy, guilt, sympathy and injustice. Parts Two and Three introduce the concept of guilt and how parental discipline interacts with a child’s cognitive skills to guide moral development. This first half of the book could have been expanded further through the use of more examples and further exploration of how genetics and innate brain processes contribute to the development of empathy. Hoffman also seemed to emphasize the psychosocial aspects over the behavioral cognitive and genetic aspects, but it is a good review of the field to date. 
 
The second half of the book is very interesting. The examples given are more current, complete and involved. The author shines as he discusses his integration of existing theory and research into a comprehensive model. He sketches a brief picture of different types of guilt, and a theoretical hierarchy. He expands on his assertion that both parental discipline and peer interaction are necessary for the development of guilt and morality. His collected statistics on parental discipline and its effects on children’s moral development are impressive. He explains why empathy can operate in some situations and can be overwhelmed in others, even when the individuals involved are highly empathic (i.e. therapist burnout). Numerous examples are given of how guilt and empathy are motivators for human action, especially prosocial and “altruistic” actions. In light of the events of September 11, 2001, Hoffman’s concepts explain much of the individual and social group actions which followed. 
 
Section Four of the book asks the question “Is Empathy Enough” to explain moral action. Types of bias which may affect empathy and moral action are examined. Empathy’s self-destructive and self-limiting qualities are explored and integrated in Darwinian fashion. Hoffman states that a morality based on empathy alone would not be fair in large mixed or larger human groups and would lead to bias and conflict. To live together peaceably, Hoffman insists that empathy must be embedded in moral principles, the subject of the fifth part of the book. Hoffman shows how empathy (affect) becomes linked or bonded to moral principles (of cognitive and social origin) as the person develops. The synthesis is used powerfully to explain the perpetuation of social attributes of caring and justice in western society. Useful definitions of key concepts in justice research are included. 
 
Parts six and seven of the book are the slimmest and cover culture, wherein issues of the universal applicability of the key concepts are examined, and intervention, which hopefully can someday be expanded into its own volume. A few directions in designing empathic training for use in young offenders and other children at risk are given, but not in the kind of depth currently desperately needed in the field. 
 
The book is well organized in its scant 300 pages and set up as a graduate course. It is an easy book from which to learn. It would make an ideal text and makes for a brilliant discussion overall, as it presents the author’s theory that will, no doubt, form the basis for future research and intervention in this area.","[{'authorId': '115702696', 'name': 'Lori Ann Vogt'}]",1859.0,"{'bibtex': '@Inproceedings{Vogt2003EmpathyAM,\n author = {Lori Ann Vogt},\n pages = {46-47},\n title = {Empathy and Moral Development: Implications for caring and Justice.},\n volume = {12},\n year = {2003}\n}\n'}",,"{'volume': '12', 'pages': '46-47', 'name': ''}",0.0,Empathy and Moral Development: Implications for caring and Justice.,2003.0
2045,ad4e281293237bdb8efbbd639ac823536815f42b,,"[{'authorId': '143727348', 'name': 'R. Desikan'}, {'authorId': '2945611', 'name': 'F. Ségonne'}, {'authorId': '145815766', 'name': 'B. Fischl'}, {'authorId': '34838229', 'name': 'B. Quinn'}, {'authorId': '1769416', 'name': 'B. Dickerson'}, {'authorId': '2971502', 'name': 'D. Blacker'}, {'authorId': '1761695', 'name': 'R. Buckner'}, {'authorId': '143619169', 'name': 'A. Dale'}, {'authorId': '144079633', 'name': 'R. P. Maguire'}, {'authorId': '2349067', 'name': 'B. Hyman'}, {'authorId': '35047388', 'name': 'M. Albert'}, {'authorId': '2029704', 'name': 'R. Killiany'}]",9759.0,"{'bibtex': '@Article{Desikan2006AnAL,\n author = {R. Desikan and F. Ségonne and B. Fischl and B. Quinn and B. Dickerson and D. Blacker and R. Buckner and A. Dale and R. P. Maguire and B. Hyman and M. Albert and R. Killiany},\n journal = {NeuroImage},\n pages = {968-980},\n title = {An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest},\n volume = {31},\n year = {2006}\n}\n'}",,"{'volume': '31', 'pages': '968-980', 'name': 'NeuroImage'}",64.0,An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest,2006.0
2046,ad6de2d9a36f851da4bef2c9f576a24ac502920a,,"[{'authorId': '2472620', 'name': 'Chin-Chang Ho'}, {'authorId': '1690354', 'name': 'K. Macdorman'}]",355.0,"{'bibtex': '@Article{Ho2010RevisitingTU,\n author = {Chin-Chang Ho and K. Macdorman},\n journal = {Comput. Hum. Behav.},\n pages = {1508-1518},\n title = {Revisiting the uncanny valley theory: Developing and validating an alternative to the Godspeed indices},\n volume = {26},\n year = {2010}\n}\n'}",,"{'volume': '26', 'pages': '1508-1518', 'name': 'Comput. Hum. Behav.'}",55.0,Revisiting the uncanny valley theory: Developing and validating an alternative to the Godspeed indices,2010.0
2048,ad72b65ab67ebe502e27b16932ffafd9a69d61ca,,"[{'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '3313648', 'name': 'Brigitte Krenn'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2979549', 'name': 'Andrew N. Marshall'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1696134', 'name': 'Hannes Pirker'}, {'authorId': '1727838', 'name': 'K. Thórisson'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}]",464.0,"{'bibtex': '@Inproceedings{Kopp2006TowardsAC,\n author = {S. Kopp and Brigitte Krenn and S. Marsella and Andrew N. Marshall and C. Pelachaud and Hannes Pirker and K. Thórisson and H. Vilhjálmsson},\n pages = {205-217},\n title = {Towards a Common Framework for Multimodal Generation: The Behavior Markup Language},\n year = {2006}\n}\n'}",,{'pages': '205-217'},20.0,Towards a Common Framework for Multimodal Generation: The Behavior Markup Language,2006.0
2051,ada9f2200bee952427dc4bb35a77dcc83716b932,,"[{'authorId': '24323972', 'name': 'S. Samiee'}, {'authorId': '1966095', 'name': 'S. Azadi'}, {'authorId': '143700987', 'name': 'R. Kazemi'}, {'authorId': '3434981', 'name': 'A. Eichberger'}, {'authorId': '70018068', 'name': 'Branko Rogic'}, {'authorId': '69842764', 'name': 'Michael Semmer'}]",16.0,"{'bibtex': '@Inproceedings{Samiee2016PerformanceEO,\n author = {S. Samiee and S. Azadi and R. Kazemi and A. Eichberger and Branko Rogic and Michael Semmer},\n pages = {103-116},\n title = {Performance Evaluation of a Novel Vehicle Collision Avoidance Lane Change Algorithm},\n year = {2016}\n}\n'}",,"{'volume': '', 'pages': '103-116', 'name': ''}",19.0,Performance Evaluation of a Novel Vehicle Collision Avoidance Lane Change Algorithm,2016.0
2052,adbb20378727b892a1705425c8df6efec423c5ab,"There has long been interest in describing emotional experience in terms of underlying dimensions, but traditionally only two dimensions, pleasantness and arousal, have been reliably found. The reasons for these findings are reviewed, and integrating this review with two recent theories of emotions (Roseman, 1984; Scherer, 1982), we propose eight cognitive appraisal dimensions to differentiate emotional experience. In an investigation of this model, subjects recalled past experiences associated with each of 15 emotions, and rated them along the proposed dimensions. Six orthogonal dimensions, pleasantness, anticipated effort, certainty, attentional activity, self-other responsibility/control, and situational control, were recovered, and the emotions varied systematically along each of these dimensions, indicating a strong relation between the appraisal of one's circumstances and one's emotional state. The patterns of appraisal for the different emotions, and the role of each of the dimensions in differentiating emotional experience are discussed.","[{'authorId': '2110202993', 'name': 'Craig A. Smith'}, {'authorId': '4367292', 'name': 'P. Ellsworth'}]",3621.0,"{'bibtex': '@Article{Smith1985PatternsOC,\n author = {Craig A. Smith and P. Ellsworth},\n journal = {Journal of personality and social psychology},\n pages = {\n          813-38\n        },\n title = {Patterns of cognitive appraisal in emotion.},\n volume = {48 4},\n year = {1985}\n}\n'}",,"{'volume': '48 4', 'pages': '\n          813-38\n        ', 'name': 'Journal of personality and social psychology'}",53.0,Patterns of cognitive appraisal in emotion.,1985.0
2054,add99bb7a404accb3de776f6554901e83db8cbf3,"In virtual reality exposure therapy (VRET) for anxiety disorders, sense of presence in the virtual environment is considered the principal mechanism that enables anxiety to be felt. Existing studies on the relation between sense of presence and level of anxiety, however, have yielded mixed results on the correlation between the two. In this meta-analysis, we reviewed publications on VRET for anxiety that included self-reported presence and anxiety. The comprehensive search of the literature identified 33 publications with a total of 1196 participants. The correlation between self-reported sense of presence and anxiety was extracted and meta-analyzed. Potential moderators such as technology characteristics, sample characteristics including age, gender and clinical status, disorder characteristics and study design characteristics such as measurements were also examined. The random effects analysis showed a medium effect size for the correlation between sense of presence and anxiety (r = .28; 95% CI: 0.18–0.38). Moderation analyses revealed that the effect size of the correlation differed across different anxiety disorders, with a large effect size for fear of animals (r = .50; 95% CI: 0.30–0.66) and a no to small effect size for social anxiety disorder (r = .001; 95% CI: −0.19–0.19). Further, the correlation between anxiety and presence was stronger in studies with participants who met criteria for an anxiety disorder than in studies with a non-clinical population. Trackers with six degrees of freedom and displays with a larger field of view resulted in higher effect sizes, compared to trackers with three degrees of freedom and displays with a smaller field of view. In addition, no difference in effect size was found for the type of presence measurement and the type of anxiety measurement. This meta-analysis confirms the positive relation between sense of presence and anxiety and demonstrates that this relation can be affected by various moderating factors.","[{'authorId': '2055564023', 'name': 'Y. Ling'}, {'authorId': '2353999', 'name': 'H. Nefs'}, {'authorId': '145500304', 'name': 'N. Morina'}, {'authorId': '145412643', 'name': 'I. Heynderickx'}, {'authorId': '145495942', 'name': 'Willem-Paul Brinkman'}]",149.0,"{'bibtex': '@Article{Ling2014AMO,\n author = {Y. Ling and H. Nefs and N. Morina and I. Heynderickx and Willem-Paul Brinkman},\n journal = {PLoS ONE},\n title = {A Meta-Analysis on the Relationship between Self-Reported Presence and Anxiety in Virtual Reality Exposure Therapy for Anxiety Disorders},\n volume = {9},\n year = {2014}\n}\n'}",,"{'volume': '9', 'name': 'PLoS ONE'}",93.0,A Meta-Analysis on the Relationship between Self-Reported Presence and Anxiety in Virtual Reality Exposure Therapy for Anxiety Disorders,2014.0
2055,adf583eab0acddd2d536a141c04c10a944a42e08,"Color impacts student behavior within the physical learning environment. Due to the move toward including students with disabilities in the general education classroom, functional color applications are critical. This article reviews and analyzes existing literature and empirical evidence related to use of color in the classroom for students of all abilities. The three major areas reviewed were (1) the inclusive classroom for students with disabilities, (2) color theory, and (3) the physiological and psychological aspects of color. The results show that color is important in designing functional learning spaces. The results of this analysis may benefit educators, parents, and design professionals in designing beneficial learning environments for all students.","[{'authorId': '48442388', 'name': 'K. Gaines'}, {'authorId': '73231253', 'name': 'Z. Curry'}]",65.0,"{'bibtex': '@Inproceedings{Gaines2011TheIC,\n author = {K. Gaines and Z. Curry},\n title = {The Inclusive Classroom: The Effects of Color on Learning and Behavior},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",38.0,The Inclusive Classroom: The Effects of Color on Learning and Behavior,2011.0
2056,ae02cd14f9bd54cfbb3c1c481cf9408f78134a2c,,"[{'authorId': '2321683', 'name': 'N. Fragopanagos'}, {'authorId': '2110316079', 'name': 'John G. Taylor'}]",2192.0,"{'bibtex': '@Article{Fragopanagos2005EmotionRI,\n author = {N. Fragopanagos and John G. Taylor},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          389-405\n        },\n title = {Emotion recognition in human-computer interaction},\n volume = {18 4},\n year = {2005}\n}\n'}",,"{'volume': '18 4', 'pages': '\n          389-405\n        ', 'name': 'Neural networks : the official journal of the International Neural Network Society'}",34.0,Emotion recognition in human-computer interaction,2005.0
2058,ae100eb7e6699081804b0e6fc9ba566825fac611,"While reinforcement learning (RL) has been applied to turn-based board games for many years, more complex games involving decision-making in real-time are beginning to receive more attention. A challenge in such environments is that the time that elapses between deciding to take an action and receiving a reward based on its outcome can be longer than the interval between successive decisions. We explore this in the context of a non-player character (NPC) in a modern first-person shooter game. Such games take place in 3D environments where players, both human and computer-controlled, compete by engaging in combat and completing task objectives. We investigate the use of RL to enable NPCs to gather experience from game-play and improve their shooting skill over time from a reward signal based on the damage caused to opponents. We propose a new method for RL updates and reward calculations, in which the updates are carried out periodically, after each shooting encounter has ended, and a new weighted-reward mechanism is used which increases the reward applied to actions that lead to damaging the opponent in successive hits in what we term “hit clusters”.","[{'authorId': '2884627', 'name': 'F. Glavin'}, {'authorId': '32418087', 'name': 'M. G. Madden'}]",13.0,"{'bibtex': '@Article{Glavin2015LearningTS,\n author = {F. Glavin and M. G. Madden},\n journal = {2015 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {344-351},\n title = {Learning to shoot in first person shooter games by stabilizing actions and clustering rewards for reinforcement learning},\n year = {2015}\n}\n'}",,"{'pages': '344-351', 'name': '2015 IEEE Conference on Computational Intelligence and Games (CIG)'}",14.0,Learning to shoot in first person shooter games by stabilizing actions and clustering rewards for reinforcement learning,2015.0
2059,ae4a326aa346276794885340bc21ad4de375558d,,"[{'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '145813496', 'name': 'C. Martinho'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",66.0,"{'bibtex': '@Inproceedings{Pereira2010UsingET,\n author = {André Pereira and Iolanda Leite and S. Mascarenhas and C. Martinho and Ana Paiva},\n pages = {130-138},\n title = {Using Empathy to Improve Human-Robot Relationships},\n year = {2010}\n}\n'}",,{'pages': '130-138'},19.0,Using Empathy to Improve Human-Robot Relationships,2010.0
2060,ae5bc4f3469cd6b9edfd712cb516c857b0c2cf1a,"Preface The research reported in this thesis is the result of the creative and communicative environment I enjoyed during the last eight years in the Artificial Intelligence Group at the University of Bielefeld. Over the years the members of this group discussed even the most far-reaching ideas with me and without their professional and personal support this work would not have been possible. First and foremost, I am most grateful to my advisor Ipke Wachsmuth for trusting in my hidden abilities from the very beginning. With his idea to let me investigate the fascinating problem of emotion simulation for our virtual human MAX, he gave me the chance to experience the "" world of science "" to which I am now addicted. I would like to thank all members of the AI Group, including for reminding me from time to time that the emotion simulation cannot be applied to every problem of Computer Science. Special thanks go to Stefan Kopp for his long-standing technical and ideational support as my colleague and friend. I am also indebted to Helmut Prendinger for supervising me during three-month in 2005 as a Pre-doctoral fellow of the Japan Society for the Promotion of Science in Tokyo, Japan, and supporting me ever since. Further special thanks go to Klaus Scherer, who agreed to be the second reviewer of this thesis and found the time to share my fascination for this challenging research. I am also grateful to Roger Giles and Nick Thomas for proofreading parts of the manuscript and to for insightful discussions and advices. Finally, I owe many thanks to my family, especially to my mother Dorothea, my brother Jörg, and my son Jonas. But most importantly, without the love and practical as well as "" mental "" support of my wife Yoko, finishing this thesis would have been close to impossible— thank you so much for your love and encouragement.","[{'authorId': '1403827243', 'name': 'C. Becker-Asano'}]",108.0,"{'bibtex': '@Inproceedings{Becker-Asano2008WASABIAS,\n author = {C. Becker-Asano},\n pages = {1-188},\n title = {WASABI: Affect simulation for agents with believable interactivity},\n year = {2008}\n}\n'}",,{'pages': '1-188'},206.0,WASABI: Affect simulation for agents with believable interactivity,2008.0
2061,ae67674c6431140e0779786e61526bb5e3b0dec1,"Our goal is to create an ‘intelligent’ 3D agent able to send complex, ‘natural’ messages to users and, in the future, to converse with them. We look at the relationship between the agent's communicative intentions and the way that these intentions are expressed into verbal and nonverbal messages. In this paper, we concentrate on the study and generation of coordinated linguistic and gaze communicative acts. In this view we analyse gaze signals according to their functional meaning rather than to their physical actions. We propose a formalism where a communicative act is represented by two elements: a meaning (that corresponds to a set of goals and beliefs that the agent has the purpose to transmit to the interlocutor) and a signal, that is the nonverbal expression of that meaning. We also outline a methodology to generate messages that coordinate verbal with nonverbal signals.","[{'authorId': '1802126', 'name': 'I. Poggi'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1807752', 'name': 'F. D. Rosis'}]",138.0,"{'bibtex': '@Article{Poggi2000EyeCI,\n author = {I. Poggi and C. Pelachaud and F. D. Rosis},\n journal = {AI Commun.},\n pages = {169-182},\n title = {Eye Communication in a Conversational 3D Synthetic Agent},\n volume = {13},\n year = {2000}\n}\n'}",,"{'volume': '13', 'pages': '169-182', 'name': 'AI Commun.'}",33.0,Eye Communication in a Conversational 3D Synthetic Agent,2000.0
2062,ae9257f3be9f815db8d72819332372ac59c1316b,"Most studies investigating the recognition of facial expressions have focused on static displays of intense expressions. Consequently, researchers may have underestimated the importance of motion in deciphering the subtle expressions that permeate real-life situations. In two experiments, we examined the effect of motion on perception of subtle facial expressions and tested the hypotheses that motion improves affect judgment by (a) providing denser sampling of expressions, (b) providing dynamic information, (c) facilitating configural processing, and (d) enhancing the perception of change. Participants viewed faces depicting subtle facial expressions in four modes (single-static, multi-static, dynamic, and first-last). Experiment 1 demonstrated a robust effect of motion and suggested that this effect was due to the dynamic property of the expression. Experiment 2 showed that the beneficial effect of motion may be due more specifically to its role in perception of change. Together, these experiments demonstrated the importance of motion in identifying subtle facial expressions.","[{'authorId': '2059653', 'name': 'Z. Ambadar'}, {'authorId': '2809346', 'name': 'J. Schooler'}, {'authorId': '1737918', 'name': 'J. Cohn'}]",442.0,"{'bibtex': '@Article{Ambadar2005DecipheringTE,\n author = {Z. Ambadar and J. Schooler and J. Cohn},\n journal = {Psychological Science},\n pages = {403 - 410},\n title = {Deciphering the Enigmatic Face},\n volume = {16},\n year = {2005}\n}\n'}",,"{'volume': '16', 'pages': '403 - 410', 'name': 'Psychological Science'}",39.0,Deciphering the Enigmatic Face,2005.0
2063,ae9928754cd3e21af4f2a894fde3e9e76e7149bd,,"[{'authorId': '46511680', 'name': 'J. Ryan'}, {'authorId': '143931391', 'name': 'S. Lopez'}]",3546.0,"{'bibtex': '@Inproceedings{Ryan2001WechslerAI,\n author = {J. Ryan and S. Lopez},\n pages = {19-42},\n title = {Wechsler Adult Intelligence Scale-III},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '19-42', 'name': ''}",4.0,Wechsler Adult Intelligence Scale-III,2001.0
2064,aead9832b48372ecb284af5bf471768c60f69e18,,"[{'authorId': '2254205', 'name': 'Jean A. Pratt'}, {'authorId': '49817642', 'name': 'K. Hauser'}, {'authorId': '2164911', 'name': 'Zsolt Ugray'}, {'authorId': '2143207', 'name': 'Olga V. Patterson'}]",67.0,"{'bibtex': '@Article{Pratt2007LookingAH,\n author = {Jean A. Pratt and K. Hauser and Zsolt Ugray and Olga V. Patterson},\n journal = {Interact. Comput.},\n pages = {512-523},\n title = {Looking at human-computer interface design: Effects of ethnicity in computer agents},\n volume = {19},\n year = {2007}\n}\n'}",,"{'volume': '19', 'pages': '512-523', 'name': 'Interact. Comput.'}",49.0,Looking at human-computer interface design: Effects of ethnicity in computer agents,2007.0
2065,aec3f10357b843494b440151c6ec9ff21b0f6b35,,"[{'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '3347943', 'name': 'G. F. Siddiqui'}, {'authorId': '71825175', 'name': 'J. Hoorn'}]",11.0,"{'bibtex': '@Article{Pontier2010SpeedDW,\n author = {M. Pontier and G. F. Siddiqui and J. Hoorn},\n booktitle = {International Conference on Intelligent Virtual Agents},\n pages = {91-103},\n title = {Speed Dating with an Affective Virtual Agent - Developing a Testbed for Emotion Models},\n year = {2010}\n}\n'}","[{'paperId': '4c548c52b06f1dcf31a33ed5d85ab45fcef01323', 'title': 'Silicon Coppélia and the Formalization of the Affective Process'}, {'paperId': '0f47559692b7961429d8990f008274eb51e36cb9', 'title': 'Quality Assessment Methods for Textual Conversational Interfaces: A Multivocal Literature Review'}, {'paperId': '544ee111daca1f017e2d04b78c8e0c357c9c09d1', 'title': 'Dating a Synthetic Character is Like Dating a Man'}, {'paperId': '204425ef0d4d2b5cb50f105531d398196793741a', 'title': 'A Personalized Support Agent for Depressed Patients: Forecasting Patient Behavior Using a Mood and Coping Model'}, {'paperId': '06a33cdc33ba33d865a6399b7a714186191f1806', 'title': 'Validation of a Model for Coping and Mood for Virtual Agents'}, {'paperId': 'dbad4aff97a6f724306d92c57b3c30492e6d927f', 'title': 'Coppélius’ concoction: Similarity and complementarity among three affect-related agent models'}, {'paperId': '907c2471de8c9aeb74a4508624fd6127778168f4', 'title': 'Social Tools for Networked Learning: Current and Future Research Directions'}, {'paperId': 'cb3f2b3980972c3a5471ea838c995a9bb69bf300', 'title': 'Virtual Agents for Human Communication : Emotion Regulation and Involvement-Distance Trade-Offs in Embodied Conversational Agents and Robots'}, {'paperId': 'a62cefab6724323105de7bb5e13ad8aadd2f57c9', 'title': 'Machine Medical Ethics: When a Human Is Delusive but the Machine Has Its Wits About Him'}, {'paperId': '12026b3c1bd8a99e517141a3a9f16869bebf4741', 'title': 'How Women Think Robots Perceive Them - as if Robots were Men'}, {'paperId': 'f6957dfccf90254e970fa6da834649457fd1cd79', 'title': 'An Affective Virtual Agent for Natural Human-Agent Interaction'}]",{'pages': '91-103'},20.0,Speed Dating with an Affective Virtual Agent - Developing a Testbed for Emotion Models,2010.0
2066,aecbfd83f49a7cff9b1192b8156bc4b827f4b519,"We describe an empirical study comparing the accuracy of competing computational models of emotion in predicting human emotional responses in naturalistic emotion-eliciting situations. The results find clear differences in models' ability to forecast human emotional responses, and provide guidance on how to develop more accurate models of human emotion.","[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '2074566879', 'name': 'B. Stankovic'}]",57.0,"{'bibtex': '@Article{Gratch2009AssessingTV,\n author = {J. Gratch and S. Marsella and Ning Wang and B. Stankovic},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-8},\n title = {Assessing the validity of appraisal-based models of emotion},\n year = {2009}\n}\n'}",,"{'pages': '1-8', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",28.0,Assessing the validity of appraisal-based models of emotion,2009.0
2068,aeeea6eec2f063c006c13be865cec0c350244e5b,"We have acquired a set of audio-visual recordings of induced emotions. A collage of comedy clips and clips of disgusting content were shown to a number of participants, who displayed mostly expressions of disgust, happiness, and surprise in response. While displays of induced emotions may differ from those shown in everyday life in aspects such as the frequency with which they occur, they are regarded as highly naturalistic and spontaneous. We recorded 25 participants for approximately 5 minutes each. This collection of recordings has been added to the MMI Facial Expression Database, an online accessible, easily searchable resource that is freely available to the scientific community.","[{'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",429.0,"{'bibtex': '@Inproceedings{Valstar2010InducedD,\n author = {M. Valstar and M. Pantic},\n title = {Induced Disgust , Happiness and Surprise : an Addition to the MMI Facial Expression Database},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",16.0,"Induced Disgust , Happiness and Surprise : an Addition to the MMI Facial Expression Database",2010.0
2069,aeeebf7147657e5b7e80df2c167299c64c843932,"Human reasoning and behaviour is undoubtedly influenced by emotions. However, the role of emotion in reasoning has, until recently, been viewed as secondary, with preference given to game theory principles in order to explain how the reasoning of an individual affects sociable interaction and the phenomenon of cooperation. Despite this, development of emotional agent architectures has gained increased interest, resulting in multi-agent systems whose individuals use emotion to aid reasoning and behaviour selection. This paper details a novel emotional agent capable of simple, natural emotional responses to information received from the environment it is situated in. Such an agent is contrasted with the concept of a so-called rational agent, whose reasoning is determined by rational processes that are based upon game theoretic notions. We present a novel test-bed entitled Tileworld Dilemma which is inspired by the Tileworld test-bed and Robert Axelrod’s take on the Prisoner’s Dilemma. Tileworld Dilemma allows us to research two questions: firstly, is the rational behaviour demonstrated by the most successful strategy in Axelrod’s tournament, the “tit-for-tat” strategy, capable of being replicated using simple emotional responses produced by our emotional agent? Secondly, how can emotions enable and promote co-operation between agents in a society? To investigate these questions we pit an emotional agent with a range of emotional characters against the most notable strategies described in Axelrod’s tournament and analyse the behaviours and scores demonstrated/obtained by both the individuals and the total system. As a result, we discover that tolerance and responsiveness are integral emotional features with regards to the scoring of agents endowed with functional interpretations of emotions.","[{'authorId': '1404912402', 'name': 'Martyn Lloyd-Kelly'}, {'authorId': '144689159', 'name': 'Katie Atkinson'}, {'authorId': '1397978499', 'name': 'Trevor J. M. Bench-Capon'}]",16.0,"{'bibtex': '@Inproceedings{Lloyd-Kelly2012EmotionAA,\n author = {Martyn Lloyd-Kelly and Katie Atkinson and Trevor J. M. Bench-Capon},\n pages = {164-169},\n title = {Emotion as an Enabler of Co-operation},\n year = {2012}\n}\n'}",,{'pages': '164-169'},42.0,Emotion as an Enabler of Co-operation,2012.0
2070,aef2c4cf870a8f6d5d62d6bbc247845f6dc4af09,,"[{'authorId': '50350658', 'name': 'T. Allison'}, {'authorId': '30053632', 'name': 'Aina Puce'}, {'authorId': '145984907', 'name': 'G. McCarthy'}]",2353.0,"{'bibtex': '@Article{Allison2000SocialPF,\n author = {T. Allison and Aina Puce and G. McCarthy},\n journal = {Trends in Cognitive Sciences},\n pages = {267-278},\n title = {Social perception from visual cues: role of the STS region},\n volume = {4},\n year = {2000}\n}\n'}",,"{'volume': '4', 'pages': '267-278', 'name': 'Trends in Cognitive Sciences'}",117.0,Social perception from visual cues: role of the STS region,2000.0
2071,af0ee0ee74f953efc5f7d481a91a71c0d86dca5d,"The seventh Audio-Visual Emotion Challenge and workshop AVEC 2017 was held in conjunction with ACM Multimedia'17. This year, the AVEC series addresses two distinct sub-challenges: emotion recognition and depression detection. The Affect Sub-Challenge is based on a novel dataset of human-human interactions recorded 'in-the-wild', whereas the Depression Sub-Challenge is based on the same dataset as the one used in AVEC 2016, with human-agent interactions. In this summary, we mainly describe participation and its conditions.","[{'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",10.0,"{'bibtex': '@Article{Ringeval2017SummaryFA,\n author = {F. Ringeval and Björn Schuller and M. Valstar and J. Gratch and R. Cowie and M. Pantic},\n journal = {Proceedings of the 25th ACM international conference on Multimedia},\n title = {Summary for AVEC 2017: Real-life Depression and Affect Challenge and Workshop},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 25th ACM international conference on Multimedia'},11.0,Summary for AVEC 2017: Real-life Depression and Affect Challenge and Workshop,2017.0
2072,af5a71cd9dcde6e19c7430c9ea8c0f3c42f9844f,"Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication.","[{'authorId': '2110645261', 'name': 'H. Smith'}, {'authorId': '143687087', 'name': 'Michael Neff'}]",144.0,"{'bibtex': '@Article{Smith2018CommunicationBI,\n author = {H. Smith and Michael Neff},\n journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},\n title = {Communication Behavior in Embodied Virtual Reality},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems'},52.0,Communication Behavior in Embodied Virtual Reality,2018.0
2073,af71c2aa8ee0b5888dbe339e8030aea4b1efb87c,"Like most other literary-critical terms, ""character,"" ""figure"" or ""person"" are polysemic and ambiguous. For the purpose of this essay, ""character"" or ""person"" in narrative will be understood as designating a human or human-like individual, existing in some possible world, and capable of fulfilling the argument position in the propositional form DO(X) - that is, a Narrative Agent (=NA), to whom inner states, mental properties (traits, features) or complexes of such properties (personality models) can be ascribed on the basis of textual data. This particular explication of the term has the advantage of capturing the reader's intuitive understanding of the term, while at the same time enabling a disciplined and explicit study of this phenomenon within narratology. The ascription of individual mental traits to an NA may be called ""characterization,"" and the ascription of complexes of traits ""character-building"" or ""portraiture."" The two activities are logically and substantively different. The first is the primary one and is based on inference drawn from individual acts of the NA, details of his looks and setting, etc. (see below). Character-building, on the other hand, comes later and involves several additional operations. These include the accumulation of a number of traits from several successive acts of the NA, setting, or formal patterns; a generalization concerning their extent in terms of narrative time; the classification or categorization of these traits; their interrelation in terms of a network or hierarchy of traits; a confrontation of traits belonging to successive acts in order to infer second order traits such as ""inconsistent""; and finally, an attempt to interrelate the traits or trait-clusters into a unified stable constellation (configuration, pattern, Gestalt, personality model) of some duration in terms of narrative time. Character-building consists of a succession of individual operations of characterization, together with second order activities of continual patterning and repatterning of the traits obtained in the first order operations, until a fairly coherent","[{'authorId': '113677166', 'name': 'Uri Margolin'}]",38.0,"{'bibtex': '@Article{Margolin1986THEDA,\n author = {Uri Margolin},\n journal = {Poetics Today},\n pages = {205},\n title = {THE DOER AND THE DEED Action as a Basis for Characterization in Narrative},\n volume = {7},\n year = {1986}\n}\n'}",,"{'volume': '7', 'pages': '205', 'name': 'Poetics Today'}",3.0,THE DOER AND THE DEED Action as a Basis for Characterization in Narrative,1986.0
2074,af7b3323c7f3b80c3da6c0be78a85475aad1354a,"The uncanny valley hypothesis, proposed already in the 1970s, suggests that almost but not fully humanlike artificial characters will trigger a profound sense of unease. This hypothesis has become widely acknowledged both in the popular media and scientific research. Surprisingly, empirical evidence for the hypothesis has remained inconsistent. In the present article, we reinterpret the original uncanny valley hypothesis and review empirical evidence for different theoretically motivated uncanny valley hypotheses. The uncanny valley could be understood as the naïve claim that any kind of human-likeness manipulation will lead to experienced negative affinity at close-to-realistic levels. More recent hypotheses have suggested that the uncanny valley would be caused by artificial–human categorization difficulty or by a perceptual mismatch between artificial and human features. Original formulation also suggested that movement would modulate the uncanny valley. The reviewed empirical literature failed to provide consistent support for the naïve uncanny valley hypothesis or the modulatory effects of movement. Results on the categorization difficulty hypothesis were still too scarce to allow drawing firm conclusions. In contrast, good support was found for the perceptual mismatch hypothesis. Taken together, the present review findings suggest that the uncanny valley exists only under specific conditions. More research is still needed to pinpoint the exact conditions under which the uncanny valley phenomenon manifests itself.","[{'authorId': '48160993', 'name': 'J. Kätsyri'}, {'authorId': '46618154', 'name': 'Klaus Förger'}, {'authorId': '2950835', 'name': 'Meeri Mäkäräinen'}, {'authorId': '1894905', 'name': 'T. Takala'}]",281.0,"{'bibtex': '@Article{Kätsyri2015ARO,\n author = {J. Kätsyri and Klaus Förger and Meeri Mäkäräinen and T. Takala},\n journal = {Frontiers in Psychology},\n title = {A review of empirical evidence on different uncanny valley hypotheses: support for perceptual mismatch as one road to the valley of eeriness},\n volume = {6},\n year = {2015}\n}\n'}",,"{'volume': '6', 'name': 'Frontiers in Psychology'}",83.0,A review of empirical evidence on different uncanny valley hypotheses: support for perceptual mismatch as one road to the valley of eeriness,2015.0
2075,af8d3a41a0d809081b5d1eee2a8cf14601b14be0,,"[{'authorId': None, 'name': 'Spencer L Degu Kalkidan Hassen Solomon M Cristiana Nooshin James Abate Abate Abay Abbafati Abbasi Abbastabar '}, {'authorId': '50791451', 'name': 'S. James'}, {'authorId': '29530223', 'name': 'D. Abate'}, {'authorId': '3543310', 'name': 'K. H. Abate'}, {'authorId': '2253473877', 'name': 'Solomon M Abay'}, {'authorId': '8272851', 'name': 'C. Abbafati'}, {'authorId': '2236358461', 'name': 'Nooshin Abbasi'}, {'authorId': '3921695', 'name': 'H. Abbastabar'}, {'authorId': '1380173279', 'name': 'F. Abd-Allah'}, {'authorId': '16651307', 'name': 'Jemal Abdela'}, {'authorId': '8430656', 'name': 'A. Abdelalim'}, {'authorId': '2245810908', 'name': 'Ibrahim Abdollahpour'}, {'authorId': '3877692', 'name': 'R. Abdulkader'}, {'authorId': '8638890', 'name': 'Zegeye Abebe'}, {'authorId': '4473319', 'name': 'S. Abera'}, {'authorId': '51924425', 'name': 'Olifan Zewdie Abil'}, {'authorId': '22261520', 'name': 'H. Abraha'}, {'authorId': '1389737341', 'name': 'L. Abu-Raddad'}, {'authorId': '2245813477', 'name': 'Niveen M E Abu-Rmeileh'}, {'authorId': '5025795', 'name': 'Manfred Accrombessi'}, {'authorId': '2245812738', 'name': 'Dilaram Acharya'}, {'authorId': '2245811540', 'name': 'Pawan Acharya'}, {'authorId': '2257589958', 'name': 'Ilana N Ackerman'}, {'authorId': '152125837', 'name': 'Abdullahi Adamu'}, {'authorId': '12480889', 'name': 'O. Adebayo'}, {'authorId': '48041197', 'name': 'V. Adekanmbi'}, {'authorId': '8003495', 'name': 'O. Adetokunboh'}, {'authorId': '51877136', 'name': 'Mina G. Adib'}, {'authorId': '2191459031', 'name': 'Jose C. Adsuar'}, {'authorId': '5184272', 'name': 'K. Afanvi'}, {'authorId': '2238866218', 'name': 'Mohsen Afarideh'}, {'authorId': '6667488', 'name': 'A. Afshin'}, {'authorId': '2245811353', 'name': 'Gina Agarwal'}, {'authorId': '51929978', 'name': 'Kareha M Agesa'}, {'authorId': '2110049974', 'name': 'Rakesh Aggarwal'}, {'authorId': '14573009', 'name': 'S. Aghayan'}, {'authorId': '38031807', 'name': 'S. Agrawal'}, {'authorId': '2257551681', 'name': 'Alireza Ahmadi'}, {'authorId': '2058961704', 'name': 'Mehdi Ahmadi'}, {'authorId': '2037304', 'name': 'H. Ahmadieh'}, {'authorId': '48797032', 'name': 'M. Ahmed'}, {'authorId': '24312538', 'name': 'Amani Nidhal Aichour'}, {'authorId': '24284955', 'name': 'Ibtihel Aichour'}, {'authorId': '24290306', 'name': 'Miloud Taki Eddine Aichour'}, {'authorId': '2256515772', 'name': 'Tomi F Akinyemiju'}, {'authorId': '5841392', 'name': 'N. Akseer'}, {'authorId': '1382885412', 'name': 'Z. Al‐Aly'}, {'authorId': '7968190', 'name': 'A. Al-Eyadhy'}, {'authorId': '2178655519', 'name': 'Hesham M Al-Mekhlafi'}, {'authorId': '1404754550', 'name': 'Rajaa Al-Raddadi'}, {'authorId': '4466057', 'name': 'Fares Alahdab'}, {'authorId': '49584865', 'name': 'K. Alam'}, {'authorId': '4120272', 'name': 'Tahiya Alam'}, {'authorId': '2240625949', 'name': 'Alaa Alashi'}, {'authorId': '84296508', 'name': 'S. Alavian'}, {'authorId': '5808467', 'name': 'K. Alene'}, {'authorId': '8565153', 'name': 'M. Alijanzadeh'}, {'authorId': '1393628081', 'name': 'R. Alizadeh-Navaei'}, {'authorId': '3191034', 'name': 'S. Aljunid'}, {'authorId': '3515741', 'name': 'A. Alkerwi'}, {'authorId': '2245814213', 'name': 'François Alla'}, {'authorId': '2249387515', 'name': 'Peter Allebeck'}, {'authorId': '2257717361', 'name': 'Mohamed M L Alouani'}, {'authorId': '7506393', 'name': 'K. Altirkawi'}, {'authorId': '1382885100', 'name': 'N. Alvis-Guzmán'}, {'authorId': '8338000', 'name': 'A. Amare'}, {'authorId': '4045733', 'name': 'L. Aminde'}, {'authorId': '153304529', 'name': 'W. Ammar'}, {'authorId': '6786482', 'name': 'Y. Amoako'}, {'authorId': '5840271', 'name': 'N. Anber'}, {'authorId': '13730120', 'name': 'C. Andrei'}, {'authorId': '6354568', 'name': 'S. Androudi'}, {'authorId': '51881581', 'name': 'Megbaru Debalkie Animut'}, {'authorId': '5581219', 'name': 'Mina Anjomshoa'}, {'authorId': '23615626', 'name': 'Mustafa Geleto Ansha'}, {'authorId': '8408197', 'name': 'C. Antonio'}, {'authorId': '7345936', 'name': 'P. Anwari'}, {'authorId': '6239913', 'name': 'J. Arabloo'}, {'authorId': '2238104946', 'name': 'Antonio Arauz'}, {'authorId': '5685777', 'name': 'O. Aremu'}, {'authorId': '34816279', 'name': 'F. Ariani'}, {'authorId': '2257717270', 'name': 'Bahroom Armoon'}, {'authorId': '2237749014', 'name': 'Johan Ärnlöv'}, {'authorId': '2257638134', 'name': 'Amit Arora'}, {'authorId': '7411216', 'name': 'A. Artaman'}, {'authorId': '5386972', 'name': 'K. Aryal'}, {'authorId': '3961067', 'name': 'H. Asayesh'}, {'authorId': '2250999236', 'name': 'R. J. Asghar'}, {'authorId': '40370064', 'name': 'Z. Ataro'}, {'authorId': '2252861170', 'name': 'Sachin R Atre'}, {'authorId': '2151199248', 'name': 'M. Ausloos'}, {'authorId': '1390172488', 'name': 'L. Ávila-Burgos'}, {'authorId': '8554259', 'name': 'Euripide Avokpaho'}, {'authorId': '2534282', 'name': 'A. Awasthi'}, {'authorId': '8504503', 'name': 'B. A. Ayala Quintanilla'}, {'authorId': '34590275', 'name': 'R. Ayer'}, {'authorId': '3232220', 'name': 'P. Azzopardi'}, {'authorId': '10758721', 'name': 'A. Babazadeh'}, {'authorId': '2250939737', 'name': 'Hamid Badali'}, {'authorId': '145537648', 'name': 'A. Badawi'}, {'authorId': '51890105', 'name': 'A. G. Bali'}, {'authorId': '51296932', 'name': 'Katherine E Ballesteros'}, {'authorId': '5742561', 'name': 'S. Ballew'}, {'authorId': '2104898325', 'name': 'Maciej Banach'}, {'authorId': '15268455', 'name': 'J. Banoub'}, {'authorId': '3682904', 'name': 'A. Banstola'}, {'authorId': '31465432', 'name': 'A. Barać'}, {'authorId': '2240421235', 'name': 'Miguel A Barboza'}, {'authorId': '1387830552', 'name': 'S. Barker-Collo'}, {'authorId': '2247935442', 'name': 'T. Bärnighausen'}, {'authorId': '2440643', 'name': 'L. Barrero'}, {'authorId': '2131047254', 'name': 'B. Baune'}, {'authorId': '2152012', 'name': 'S. Bazargan-Hejazi'}, {'authorId': '4642370', 'name': 'Neeraj Bedi'}, {'authorId': '2238483712', 'name': 'Ettore Beghi'}, {'authorId': '4013616', 'name': 'Masoud Behzadifar'}, {'authorId': '3872319', 'name': 'M. Behzadifar'}, {'authorId': '2248360446', 'name': 'Yannick Béjot'}, {'authorId': '4815558', 'name': 'A. Belachew'}, {'authorId': '46182431', 'name': 'Yihalem Abebe Belay'}, {'authorId': '2247257692', 'name': 'Michelle L. Bell'}, {'authorId': '145905521', 'name': 'A. Bello'}, {'authorId': '2251962022', 'name': 'Isabela M Bensenor'}, {'authorId': '2242560536', 'name': 'Eduardo Bernabé'}, {'authorId': '2245812123', 'name': 'Robert S Bernstein'}, {'authorId': '4093701', 'name': 'M. Beuran'}, {'authorId': '51247778', 'name': 'Tina Beyranvand'}, {'authorId': '3644262', 'name': 'N. Bhala'}, {'authorId': '1379789528', 'name': 'S. Bhattarai'}, {'authorId': '2240445317', 'name': 'Soumyadeep Bhaumik'}, {'authorId': '13613845', 'name': 'Z. Bhutta'}, {'authorId': '5650512', 'name': 'B. Biadgo'}, {'authorId': '34174674', 'name': 'A. Bijani'}, {'authorId': '49692793', 'name': 'B. Bikbov'}, {'authorId': '47451359', 'name': 'V. Bilano'}, {'authorId': '9531358', 'name': 'Nigus Bililign'}, {'authorId': '8225952', 'name': 'M. B. Bin Sayeed'}, {'authorId': '6070916', 'name': 'D. Bisanzio'}, {'authorId': '50811830', 'name': 'B. Blacker'}, {'authorId': '2239447867', 'name': 'Fiona M. Blyth'}, {'authorId': '1399278399', 'name': 'I. Bou-Orm'}, {'authorId': '4759692', 'name': 'S. Boufous'}, {'authorId': '2066283190', 'name': 'R. Bourne'}, {'authorId': '2247289621', 'name': 'Oliver J. Brady'}, {'authorId': '2257109091', 'name': 'Michael Brainin'}, {'authorId': '2209343580', 'name': 'L. C. Brant'}, {'authorId': '4389878', 'name': 'A. Brazinova'}, {'authorId': '144376742', 'name': 'N. Breitborde'}, {'authorId': '2237664444', 'name': 'Hermann Brenner'}, {'authorId': '51915910', 'name': 'P. Briant'}, {'authorId': '2240636922', 'name': 'Andrew M Briggs'}, {'authorId': '46254218', 'name': 'A. Briko'}, {'authorId': '2245814677', 'name': 'Gabrielle B Britton'}, {'authorId': '48962604', 'name': 'T. Brugha'}, {'authorId': '2238868657', 'name': 'Rachelle Buchbinder'}, {'authorId': '2223390667', 'name': 'Reinhard Busse'}, {'authorId': '8444968', 'name': 'Z. Butt'}, {'authorId': '1398967834', 'name': 'Lucero Cahuana-Hurtado'}, {'authorId': '2249945163', 'name': 'Jorge Cano'}, {'authorId': '134626992', 'name': 'Rosario Cárdenas'}, {'authorId': '2245815118', 'name': 'J. Carrero'}, {'authorId': '8208882', 'name': 'A. Carter'}, {'authorId': '67266295', 'name': 'F. Carvalho'}, {'authorId': '1396794108', 'name': 'C. Castañeda-Orjuela'}, {'authorId': '115093311', 'name': 'Jacqueline Castillo Rivas'}, {'authorId': '40648067', 'name': 'Franz Castro'}, {'authorId': '2240430884', 'name': 'Ferrán Catalá-López'}, {'authorId': '47375203', 'name': 'Kelly M. Cercy'}, {'authorId': '2243759229', 'name': 'Ester Cerin'}, {'authorId': '51894656', 'name': 'Y. Chaiah'}, {'authorId': '2257541086', 'name': 'Alex R Chang'}, {'authorId': '2257948028', 'name': 'Hsing-Yi Chang'}, {'authorId': '48808996', 'name': 'Jung-Chen Chang'}, {'authorId': '6001547', 'name': 'F. Charlson'}, {'authorId': '2245811652', 'name': 'Aparajita Chattopadhyay'}, {'authorId': '3776828', 'name': 'V. Chattu'}, {'authorId': '2257597710', 'name': 'Pankaj Chaturvedi'}, {'authorId': '49170011', 'name': 'P. Chiang'}, {'authorId': '2256225900', 'name': 'Ken Lee Chin'}, {'authorId': '10227612', 'name': 'Abdulaal Chitheer'}, {'authorId': '88811832', 'name': 'J. Choi'}, {'authorId': '1759513', 'name': 'Rajiv Chowdhury'}, {'authorId': '153670186', 'name': 'H. Christensen'}, {'authorId': '145096723', 'name': 'D. Christopher'}, {'authorId': '2240064182', 'name': 'F. Cicuttini'}, {'authorId': '7947028', 'name': 'Liliana G. Ciobanu'}, {'authorId': '2053365784', 'name': 'Massimo Cirillo'}, {'authorId': '2256287257', 'name': 'R. M. Claro'}, {'authorId': '1397415315', 'name': 'D. Collado-Mateo'}, {'authorId': '2257490993', 'name': 'Cyrus Cooper'}, {'authorId': '2257584721', 'name': 'Josef Coresh'}, {'authorId': '2237996248', 'name': 'P. Cortesi'}, {'authorId': '3565510', 'name': 'Monica Cortinovis'}, {'authorId': '2249567071', 'name': 'Megan Costa'}, {'authorId': '46255582', 'name': 'Ewerton Cousin'}, {'authorId': '10029674', 'name': 'M. Criqui'}, {'authorId': '2257634564', 'name': 'Elizabeth A. Cromwell'}, {'authorId': '144919788', 'name': 'M. Cross'}, {'authorId': '3863653', 'name': 'J. Crump'}, {'authorId': '6224601', 'name': 'A. F. Dadi'}, {'authorId': '3624717', 'name': 'L. Dandona'}, {'authorId': '5666172', 'name': 'R. Dandona'}, {'authorId': '2257629849', 'name': 'Paul I Dargan'}, {'authorId': '4751403', 'name': 'A. Daryani'}, {'authorId': '46247361', 'name': 'R. Das Gupta'}, {'authorId': '2127509902', 'name': 'J. D. das Neves'}, {'authorId': '79649562', 'name': 'T. Dasa'}, {'authorId': '2248110059', 'name': 'Gail Davey'}, {'authorId': '40386300', 'name': 'A. Davis'}, {'authorId': '7267119', 'name': 'D. Davițoiu'}, {'authorId': '7829724', 'name': 'B. de Courten'}, {'authorId': '2245811969', 'name': 'Fernando Pio De La Hoz'}, {'authorId': '2245816469', 'name': 'Diego De Leo'}, {'authorId': '6728229', 'name': 'Jan‐Walter De Neve'}, {'authorId': '51881692', 'name': 'M. G. Degefa'}, {'authorId': '2251723584', 'name': 'L. Degenhardt'}, {'authorId': '51886975', 'name': 'Selina Deiparine'}, {'authorId': '2250837534', 'name': 'R. Dellavalle'}, {'authorId': '41158172', 'name': 'Gebre Teklemariam Demoz'}, {'authorId': '5928220', 'name': 'Kebede Deribe'}, {'authorId': '4161915', 'name': 'N. Dervenis'}, {'authorId': '2245811985', 'name': 'Don C. Des Jarlais'}, {'authorId': '5243540', 'name': 'Getenet Dessie'}, {'authorId': '4608017', 'name': 'Subhojit Dey'}, {'authorId': '3782043', 'name': 'S. Dharmaratne'}, {'authorId': '51887916', 'name': 'Mesfin Tadese Dinberu'}, {'authorId': '8860807', 'name': 'M. Dirac'}, {'authorId': '116681037', 'name': 'Shirin Djalalinia'}, {'authorId': '150186187', 'name': 'L. Doan'}, {'authorId': '1380379452', 'name': 'K. Dokova'}, {'authorId': '5062866', 'name': 'D. Doku'}, {'authorId': '2240443585', 'name': 'E. R. Dorsey'}, {'authorId': '2071438', 'name': 'Kerrie E. Doyle'}, {'authorId': '2245818043', 'name': 'T. Driscoll'}, {'authorId': '48115074', 'name': 'M. Dubey'}, {'authorId': '4375638', 'name': 'E. Dubljanin'}, {'authorId': '8542974', 'name': 'Eyasu Ejeta Duken'}, {'authorId': '2239649240', 'name': 'Bruce B. Duncan'}, {'authorId': '4518656', 'name': 'A. Durães'}, {'authorId': '3988495', 'name': 'H. Ebrahimi'}, {'authorId': '4997692', 'name': 'S. Ebrahimpour'}, {'authorId': '40900240', 'name': 'M. Echko'}, {'authorId': '3775564', 'name': 'D. Edvardsson'}, {'authorId': '1379789271', 'name': 'A. Effiong'}, {'authorId': '2257516013', 'name': 'Joshua R Ehrlich'}, {'authorId': '4660623', 'name': 'C. El Bcheraoui'}, {'authorId': '117084215', 'name': 'M. El Sayed Zaki'}, {'authorId': '1398967847', 'name': 'Z. El-Khatib'}, {'authorId': '4376805', 'name': 'H. Elkout'}, {'authorId': '2201045405', 'name': 'Iqbal Elyazar'}, {'authorId': '2080111091', 'name': 'A. Enayati'}, {'authorId': '7705540', 'name': 'A. Endries'}, {'authorId': '2245811672', 'name': 'Benjamin Er'}, {'authorId': '5971622', 'name': 'H. Erskine'}, {'authorId': '6237061', 'name': 'B. Eshrati'}, {'authorId': '4362142', 'name': 'S. Eskandarieh'}, {'authorId': '2237186735', 'name': 'Alireza Esteghamati'}, {'authorId': '5694096', 'name': 'S. Esteghamati'}, {'authorId': '145832132', 'name': 'H. Fakhim'}, {'authorId': '134833467', 'name': 'Vahid Fallah Omrani'}, {'authorId': '2245815034', 'name': 'Mahbobeh Faramarzi'}, {'authorId': '1404346347', 'name': 'M. Fareed'}, {'authorId': '2694161', 'name': 'F. Farhadi'}, {'authorId': '2245814106', 'name': 'Talha A Farid'}, {'authorId': '1717742', 'name': 'C. Farinha'}, {'authorId': '2257486520', 'name': 'Andrea Farioli'}, {'authorId': '79524205', 'name': 'Andre Faro'}, {'authorId': '4616927', 'name': 'M. Farvid'}, {'authorId': '5785969', 'name': 'F. Farzadfar'}, {'authorId': '2242798301', 'name': 'Valery L Feigin'}, {'authorId': '5494376', 'name': 'Netsanet Fentahun'}, {'authorId': '51879228', 'name': 'S. Fereshtehnejad'}, {'authorId': '2245815760', 'name': 'Eduarda Fernandes'}, {'authorId': '2257641143', 'name': 'Joao C Fernandes'}, {'authorId': '38334221', 'name': 'A. Ferrari'}, {'authorId': '79597694', 'name': 'Garumma Tolu Feyissa'}, {'authorId': '98591128', 'name': 'I. Filip'}, {'authorId': '2238725777', 'name': 'Florian Fischer'}, {'authorId': '4982486', 'name': 'C. Fitzmaurice'}, {'authorId': '7793834', 'name': 'N. Foigt'}, {'authorId': '116189314', 'name': 'Kyle Foreman'}, {'authorId': '1471137188', 'name': 'Jack T Fox'}, {'authorId': '47976495', 'name': 'Tahvi D Frank'}, {'authorId': '2238811388', 'name': 'Takeshi Fukumoto'}, {'authorId': '3756261', 'name': 'N. Fullman'}, {'authorId': '49324203', 'name': 'Thomas Fürst'}, {'authorId': '2245811391', 'name': 'João M Furtado'}, {'authorId': '2245812695', 'name': 'Neal D Futran'}, {'authorId': '48504286', 'name': 'S. Gall'}, {'authorId': '143740943', 'name': 'M. Ganji'}, {'authorId': '8461029', 'name': 'F. Gankpé'}, {'authorId': '1398465968', 'name': 'A. García-Basteiro'}, {'authorId': '2241479139', 'name': 'William M Gardner'}, {'authorId': '40997857', 'name': 'A. Gebre'}, {'authorId': '10053591', 'name': 'A. Gebremedhin'}, {'authorId': '51906589', 'name': 'Teklu Gebrehiwo Gebremichael'}, {'authorId': '35702695', 'name': 'Tilayie Feto Gelano'}, {'authorId': '6431370', 'name': 'J. Geleijnse'}, {'authorId': '120232099', 'name': 'R. Gènova-Maleras'}, {'authorId': '51928643', 'name': 'Y. C. D. Geramo'}, {'authorId': '1864915', 'name': 'P. Gething'}, {'authorId': '51921050', 'name': 'Kebede Embaye Gezae'}, {'authorId': '3536478', 'name': 'K. Ghadiri'}, {'authorId': '2245812912', 'name': 'Khalil Ghasemi Falavarjani'}, {'authorId': '1402636364', 'name': 'M. Ghasemi-Kasman'}, {'authorId': '2082765', 'name': 'M. Ghimire'}, {'authorId': '2245820192', 'name': 'Rakesh Ghosh'}, {'authorId': '2257723751', 'name': 'Aloke G. Ghoshal'}, {'authorId': '2246581958', 'name': 'Simona Giampaoli'}, {'authorId': '2242128849', 'name': 'P. Gill'}, {'authorId': '2238778077', 'name': 'Tiffany K. Gill'}, {'authorId': '50406834', 'name': 'I. Ginawi'}, {'authorId': '3640372', 'name': 'G. Giussani'}, {'authorId': '9025455', 'name': 'E. Gnedovskaya'}, {'authorId': '114738224', 'name': 'Ellen M. Goldberg'}, {'authorId': '27437418', 'name': 'Srinivas Goli'}, {'authorId': '77245465', 'name': 'H. Gómez-Dantés'}, {'authorId': '4539326', 'name': 'P. Gona'}, {'authorId': '5754239', 'name': 'S. Gopalani'}, {'authorId': '51921069', 'name': 'Taren M Gorman'}, {'authorId': '2241885078', 'name': 'A. Goulart'}, {'authorId': '33138629', 'name': 'B. N. Goulart'}, {'authorId': '11527043', 'name': 'A. Grada'}, {'authorId': '2243704950', 'name': 'Morgan E. Grams'}, {'authorId': '2257452162', 'name': 'Giuseppe Grosso'}, {'authorId': '5111667', 'name': 'H. Gugnani'}, {'authorId': '2247369687', 'name': 'Yuming Guo'}, {'authorId': '2258534306', 'name': 'Prakash C Gupta'}, {'authorId': '1617807983', 'name': 'Rahul Gupta'}, {'authorId': '2110344784', 'name': 'R. Gupta'}, {'authorId': '2245813904', 'name': 'Tanush Gupta'}, {'authorId': '2246441871', 'name': 'Bishal Gyawali'}, {'authorId': '2695200', 'name': 'J. Haagsma'}, {'authorId': '2240979710', 'name': 'Vladimir Hachinski'}, {'authorId': '1401624764', 'name': 'N. Hafezi-Nejad'}, {'authorId': '117482817', 'name': 'Hassan Haghparast Bidgoli'}, {'authorId': '2245815210', 'name': 'Tekleberhan B Hagos'}, {'authorId': '1865582', 'name': 'G. B. Hailu'}, {'authorId': '1401817213', 'name': 'Arvin Haj-Mirzaian'}, {'authorId': '1397932257', 'name': 'A. Haj-Mirzaian'}, {'authorId': '4734173', 'name': 'R. Hamadeh'}, {'authorId': '144110014', 'name': 'S. Hamidi'}, {'authorId': '5532090', 'name': 'A. Handal'}, {'authorId': '2237499321', 'name': 'Graeme J. Hankey'}, {'authorId': '2245965589', 'name': 'Yuantao Hao'}, {'authorId': '32288199', 'name': 'H. Harb'}, {'authorId': '2238319788', 'name': 'Sivadasanpillai Harikrishnan'}, {'authorId': '2067882521', 'name': 'J. Haro'}, {'authorId': '2078334945', 'name': 'M. Hasan'}, {'authorId': '6236514', 'name': 'H. Hassankhani'}, {'authorId': '14853114', 'name': 'H. Y. Hassen'}, {'authorId': '88664387', 'name': 'Rasmus J Havmoeller'}, {'authorId': '1404827955', 'name': 'Caitlin N Hawley'}, {'authorId': '2106961779', 'name': 'Roderick J Hay'}, {'authorId': '2088787', 'name': 'S. Hay'}, {'authorId': '102184805', 'name': 'A. Hedayatizadeh-Omran'}, {'authorId': '12994972', 'name': 'B. Heibati'}, {'authorId': '39608729', 'name': 'D. Hendrie'}, {'authorId': '10695533', 'name': 'A. Henok'}, {'authorId': '11045635', 'name': 'C. Herteliu'}, {'authorId': '10404591', 'name': 'S. Heydarpour'}, {'authorId': '22612052', 'name': 'D. T. Hibstu'}, {'authorId': '2257708894', 'name': 'Huong Thanh Hoang'}, {'authorId': '2242483409', 'name': 'Hans W. Hoek'}, {'authorId': '2111563784', 'name': 'Howard J Hoffman'}, {'authorId': '47082401', 'name': 'M. K. Hole'}, {'authorId': '4045220', 'name': 'Enayatollah Homaie Rad'}, {'authorId': '2135606874', 'name': 'Praveen Hoogar'}, {'authorId': '152796705', 'name': 'H. Hosgood'}, {'authorId': '2245905020', 'name': 'Seyed Mostafa Hosseini'}, {'authorId': '2138729058', 'name': 'M. Hosseinzadeh'}, {'authorId': '4297895', 'name': 'M. Hostiuc'}, {'authorId': '5769596', 'name': 'S. Hostiuc'}, {'authorId': '2243311118', 'name': 'Peter J. Hotez'}, {'authorId': '3166743', 'name': 'D. Hoy'}, {'authorId': '7236546', 'name': 'M. Hsairi'}, {'authorId': '3263282', 'name': 'A. Htet'}, {'authorId': '2246745623', 'name': 'Guoqing Hu'}, {'authorId': '2031795991', 'name': 'John J Emmanuel Huang'}, {'authorId': '1397305959', 'name': 'Chantal K Huynh'}, {'authorId': '6952820', 'name': 'K. Iburg'}, {'authorId': '46226106', 'name': 'C. Ikeda'}, {'authorId': '10286643', 'name': 'B. Ileanu'}, {'authorId': '3960883', 'name': 'O S Ilesanmi'}, {'authorId': '2066229537', 'name': 'Usman Iqbal'}, {'authorId': '46181760', 'name': 'S. Irvani'}, {'authorId': '40897204', 'name': 'C. Irvine'}, {'authorId': '2237975779', 'name': 'Sheikh Mohammed Shariful Islam'}, {'authorId': '5813941', 'name': 'F. Islami'}, {'authorId': '1810896', 'name': 'K. Jacobsen'}, {'authorId': '5055341', 'name': 'L. Jahangiry'}, {'authorId': '5427157', 'name': 'N. Jahanmehr'}, {'authorId': '2249741762', 'name': 'Sudhir Kumar Jain'}, {'authorId': '2148313071', 'name': 'Mihajlo Jakovljevic'}, {'authorId': '6188933', 'name': 'Mehdi Javanbakht'}, {'authorId': '34236843', 'name': 'A. Jayatilleke'}, {'authorId': '4103656', 'name': 'P. Jeemon'}, {'authorId': '2257487680', 'name': 'Ravi Prakash Jha'}, {'authorId': '2075161726', 'name': 'Vivekanand Jha'}, {'authorId': '2257460108', 'name': 'John S Ji'}, {'authorId': '2238118843', 'name': 'C. Johnson'}, {'authorId': '1851144', 'name': 'J. Jonas'}, {'authorId': '144494294', 'name': 'J. Jozwiak'}, {'authorId': '15721232', 'name': 'Suresh Jungari'}, {'authorId': '5652492', 'name': 'Mikk Jürisson'}, {'authorId': '2238866203', 'name': 'Zubair Kabir'}, {'authorId': '46565942', 'name': 'R. Kadel'}, {'authorId': '46260781', 'name': 'Amaha Kahsay'}, {'authorId': '48495225', 'name': 'Rizwan Kalani'}, {'authorId': '3381430', 'name': 'T. Kanchan'}, {'authorId': '2237964944', 'name': 'Manoochehr Karami'}, {'authorId': '5800517', 'name': 'B. Karami Matin'}, {'authorId': '2257494074', 'name': 'André Karch'}, {'authorId': '5808458', 'name': 'C. Karema'}, {'authorId': '2245817526', 'name': 'Narges Karimi'}, {'authorId': '11016464', 'name': 'Seyed M. Karimi'}, {'authorId': '82392523', 'name': 'A. Kasaeian'}, {'authorId': '46207022', 'name': 'D. H. Kassa'}, {'authorId': '2986070', 'name': 'G. Kassa'}, {'authorId': '46192488', 'name': 'T. Kassa'}, {'authorId': '6756736', 'name': 'N. Kassebaum'}, {'authorId': '3691127', 'name': 'S. Katikireddi'}, {'authorId': '2239975973', 'name': 'Norito Kawakami'}, {'authorId': '8266078', 'name': 'Ali Kazemi Karyani'}, {'authorId': '2257610109', 'name': 'Masoud Keighobadi'}, {'authorId': '144307689', 'name': 'P. Keiyoro'}, {'authorId': '5679487', 'name': 'L. Kemmer'}, {'authorId': '2245817907', 'name': 'Grant Rodgers Kemp'}, {'authorId': '2194601968', 'name': 'A. Kengne'}, {'authorId': '2245816038', 'name': 'Andre Keren'}, {'authorId': '2172234799', 'name': 'Yousef S. Khader'}, {'authorId': '51877672', 'name': 'Behzad Khafaei'}, {'authorId': '6169898', 'name': 'M. Khafaie'}, {'authorId': '2245817727', 'name': 'Alireza Khajavi'}, {'authorId': '46361321', 'name': 'I. Khalil'}, {'authorId': '33782404', 'name': 'E. Khan'}, {'authorId': '2152497008', 'name': 'M. Khan'}, {'authorId': '2246748934', 'name': 'Muhammad Ali Khan'}, {'authorId': '5181120', 'name': 'Y. Khang'}, {'authorId': '2245813336', 'name': 'Mohammad Khazaei'}, {'authorId': '3953479', 'name': 'Abdullah Khoja'}, {'authorId': '5145639', 'name': 'A. Khosravi'}, {'authorId': '1832574716', 'name': 'Mohammad Hossein Khosravi'}, {'authorId': '4582713', 'name': 'A. Kiadaliri'}, {'authorId': '46259246', 'name': 'Daniel N. Kiirithio'}, {'authorId': '2246788329', 'name': 'Cho-il Kim'}, {'authorId': '2111372935', 'name': 'Daniel H. Kim'}, {'authorId': '6567082', 'name': 'Pauline Kim'}, {'authorId': '2243895911', 'name': 'Young-Eun Kim'}, {'authorId': '90841871', 'name': 'Y. Kim'}, {'authorId': '3518889', 'name': 'R. Kimokoti'}, {'authorId': '4861657', 'name': 'Y. Kinfu'}, {'authorId': '46580287', 'name': 'A. Kisa'}, {'authorId': '1399769382', 'name': 'K. Kissimova-Skarbek'}, {'authorId': '2250941780', 'name': 'Mika Kivimäki'}, {'authorId': '3797208', 'name': 'A. Knudsen'}, {'authorId': '6436920', 'name': 'Jonathan M Kocarnik'}, {'authorId': '87991116', 'name': 'S. Kochhar'}, {'authorId': '2240443777', 'name': 'Yoshihiro Kokubo'}, {'authorId': '7997240', 'name': 'T. Kolola'}, {'authorId': '2250163614', 'name': 'Jacek A. Kopec'}, {'authorId': '152233784', 'name': 'S. Kosen'}, {'authorId': '2254310750', 'name': 'Georgios A Kotsakis'}, {'authorId': '5318184', 'name': 'P. Koul'}, {'authorId': '2188633440', 'name': 'Ai Koyanagi'}, {'authorId': '144389579', 'name': 'M. Kravchenko'}, {'authorId': '46434163', 'name': 'K. Krishan'}, {'authorId': '49573494', 'name': 'Kristopher J. Krohn'}, {'authorId': '6953990', 'name': 'B. Kuate Defo'}, {'authorId': '7862805', 'name': 'B. Kucuk Bicer'}, {'authorId': '2241655536', 'name': 'G. Kumar'}, {'authorId': '2246133102', 'name': 'Manasi Kumar'}, {'authorId': '3783694', 'name': 'H. Kyu'}, {'authorId': '2240058685', 'name': 'Deepesh P Lad'}, {'authorId': '51910568', 'name': 'S. Lad'}, {'authorId': '52238454', 'name': 'A. Lafranconi'}, {'authorId': '5577782', 'name': 'R. Lalloo'}, {'authorId': '5232086', 'name': 'T. Lallukka'}, {'authorId': '46998993', 'name': 'Faris Lami'}, {'authorId': '7330933', 'name': 'V. Lansingh'}, {'authorId': '144813820', 'name': 'A. Latifi'}, {'authorId': '144831322', 'name': 'Kathryn M. Lau'}, {'authorId': '2237967676', 'name': 'Jeffrey V. Lazarus'}, {'authorId': '8058936', 'name': 'J. Leasher'}, {'authorId': '52089781', 'name': 'J. R. Ledesma'}, {'authorId': '73155726', 'name': 'P. H. Lee'}, {'authorId': '11712726', 'name': 'J. Leigh'}, {'authorId': '2245814997', 'name': 'Janni Leung'}, {'authorId': '2238871443', 'name': 'Miriam Levi'}, {'authorId': '6677277', 'name': 'S. Lewycka'}, {'authorId': '2144504121', 'name': 'Shanshan Li'}, {'authorId': '2257957895', 'name': 'Yichong Li'}, {'authorId': '2246170367', 'name': 'Yu Liao'}, {'authorId': '8268491', 'name': 'Misgan Legesse Liben'}, {'authorId': '2245813743', 'name': 'Lee-Ling Lim'}, {'authorId': '2238204084', 'name': 'Stephen S. Lim'}, {'authorId': '2246228124', 'name': 'Shiwei Liu'}, {'authorId': '104126550', 'name': 'R. Lodha'}, {'authorId': '2239422192', 'name': 'Katharine J Looker'}, {'authorId': '2241213246', 'name': 'A. D. Lopez'}]",4988.0,"{'bibtex': '@Article{Abbastabar2018GlobalRA,\n author = {Spencer L Degu Kalkidan Hassen Solomon M Cristiana Nooshin James Abate Abate Abay Abbafati Abbasi Abbastabar  and S. James and D. Abate and K. H. Abate and Solomon M Abay and C. Abbafati and Nooshin Abbasi and H. Abbastabar and F. Abd-Allah and Jemal Abdela and A. Abdelalim and Ibrahim Abdollahpour and R. Abdulkader and Zegeye Abebe and S. Abera and Olifan Zewdie Abil and H. Abraha and L. Abu-Raddad and Niveen M E Abu-Rmeileh and Manfred Accrombessi and Dilaram Acharya and Pawan Acharya and Ilana N Ackerman and Abdullahi Adamu and O. Adebayo and V. Adekanmbi and O. Adetokunboh and Mina G. Adib and Jose C. Adsuar and K. Afanvi and Mohsen Afarideh and A. Afshin and Gina Agarwal and Kareha M Agesa and Rakesh Aggarwal and S. Aghayan and S. Agrawal and Alireza Ahmadi and Mehdi Ahmadi and H. Ahmadieh and M. Ahmed and Amani Nidhal Aichour and Ibtihel Aichour and Miloud Taki Eddine Aichour and Tomi F Akinyemiju and N. Akseer and Z. Al‐Aly and A. Al-Eyadhy and Hesham M Al-Mekhlafi and Rajaa Al-Raddadi and Fares Alahdab and K. Alam and Tahiya Alam and Alaa Alashi and S. Alavian and K. Alene and M. Alijanzadeh and R. Alizadeh-Navaei and S. Aljunid and A. Alkerwi and François Alla and Peter Allebeck and Mohamed M L Alouani and K. Altirkawi and N. Alvis-Guzmán and A. Amare and L. Aminde and W. Ammar and Y. Amoako and N. Anber and C. Andrei and S. Androudi and Megbaru Debalkie Animut and Mina Anjomshoa and Mustafa Geleto Ansha and C. Antonio and P. Anwari and J. Arabloo and Antonio Arauz and O. Aremu and F. Ariani and Bahroom Armoon and Johan Ärnlöv and Amit Arora and A. Artaman and K. Aryal and H. Asayesh and R. J. Asghar and Z. Ataro and Sachin R Atre and M. Ausloos and L. Ávila-Burgos and Euripide Avokpaho and A. Awasthi and B. A. Ayala Quintanilla and R. Ayer and P. Azzopardi and A. Babazadeh and Hamid Badali and A. Badawi and A. G. Bali and Katherine E Ballesteros and S. Ballew and Maciej Banach and J. Banoub and A. Banstola and A. Barać and Miguel A Barboza and S. Barker-Collo and T. Bärnighausen and L. Barrero and B. Baune and S. Bazargan-Hejazi and Neeraj Bedi and Ettore Beghi and Masoud Behzadifar and M. Behzadifar and Yannick Béjot and A. Belachew and Yihalem Abebe Belay and Michelle L. Bell and A. Bello and Isabela M Bensenor and Eduardo Bernabé and Robert S Bernstein and M. Beuran and Tina Beyranvand and N. Bhala and S. Bhattarai and Soumyadeep Bhaumik and Z. Bhutta and B. Biadgo and A. Bijani and B. Bikbov and V. Bilano and Nigus Bililign and M. B. Bin Sayeed and D. Bisanzio and B. Blacker and Fiona M. Blyth and I. Bou-Orm and S. Boufous and R. Bourne and Oliver J. Brady and Michael Brainin and L. C. Brant and A. Brazinova and N. Breitborde and Hermann Brenner and P. Briant and Andrew M Briggs and A. Briko and Gabrielle B Britton and T. Brugha and Rachelle Buchbinder and Reinhard Busse and Z. Butt and Lucero Cahuana-Hurtado and Jorge Cano and Rosario Cárdenas and J. Carrero and A. Carter and F. Carvalho and C. Castañeda-Orjuela and Jacqueline Castillo Rivas and Franz Castro and Ferrán Catalá-López and Kelly M. Cercy and Ester Cerin and Y. Chaiah and Alex R Chang and Hsing-Yi Chang and Jung-Chen Chang and F. Charlson and Aparajita Chattopadhyay and V. Chattu and Pankaj Chaturvedi and P. Chiang and Ken Lee Chin and Abdulaal Chitheer and J. Choi and Rajiv Chowdhury and H. Christensen and D. Christopher and F. Cicuttini and Liliana G. Ciobanu and Massimo Cirillo and R. M. Claro and D. Collado-Mateo and Cyrus Cooper and Josef Coresh and P. Cortesi and Monica Cortinovis and Megan Costa and Ewerton Cousin and M. Criqui and Elizabeth A. Cromwell and M. Cross and J. Crump and A. F. Dadi and L. Dandona and R. Dandona and Paul I Dargan and A. Daryani and R. Das Gupta and J. D. das Neves and T. Dasa and Gail Davey and A. Davis and D. Davițoiu and B. de Courten and Fernando Pio De La Hoz and Diego De Leo and Jan‐Walter De Neve and M. G. Degefa and L. Degenhardt and Selina Deiparine and R. Dellavalle and Gebre Teklemariam Demoz and Kebede Deribe and N. Dervenis and Don C. Des Jarlais and Getenet Dessie and Subhojit Dey and S. Dharmaratne and Mesfin Tadese Dinberu and M. Dirac and Shirin Djalalinia and L. Doan and K. Dokova and D. Doku and E. R. Dorsey and Kerrie E. Doyle and T. Driscoll and M. Dubey and E. Dubljanin and Eyasu Ejeta Duken and Bruce B. Duncan and A. Durães and H. Ebrahimi and S. Ebrahimpour and M. Echko and D. Edvardsson and A. Effiong and Joshua R Ehrlich and C. El Bcheraoui and M. El Sayed Zaki and Z. El-Khatib and H. Elkout and Iqbal Elyazar and A. Enayati and A. Endries and Benjamin Er and H. Erskine and B. Eshrati and S. Eskandarieh and Alireza Esteghamati and S. Esteghamati and H. Fakhim and Vahid Fallah Omrani and Mahbobeh Faramarzi and M. Fareed and F. Farhadi and Talha A Farid and C. Farinha and Andrea Farioli and Andre Faro and M. Farvid and F. Farzadfar and Valery L Feigin and Netsanet Fentahun and S. Fereshtehnejad and Eduarda Fernandes and Joao C Fernandes and A. Ferrari and Garumma Tolu Feyissa and I. Filip and Florian Fischer and C. Fitzmaurice and N. Foigt and Kyle Foreman and Jack T Fox and Tahvi D Frank and Takeshi Fukumoto and N. Fullman and Thomas Fürst and João M Furtado and Neal D Futran and S. Gall and M. Ganji and F. Gankpé and A. García-Basteiro and William M Gardner and A. Gebre and A. Gebremedhin and Teklu Gebrehiwo Gebremichael and Tilayie Feto Gelano and J. Geleijnse and R. Gènova-Maleras and Y. C. D. Geramo and P. Gething and Kebede Embaye Gezae and K. Ghadiri and Khalil Ghasemi Falavarjani and M. Ghasemi-Kasman and M. Ghimire and Rakesh Ghosh and Aloke G. Ghoshal and Simona Giampaoli and P. Gill and Tiffany K. Gill and I. Ginawi and G. Giussani and E. Gnedovskaya and Ellen M. Goldberg and Srinivas Goli and H. Gómez-Dantés and P. Gona and S. Gopalani and Taren M Gorman and A. Goulart and B. N. Goulart and A. Grada and Morgan E. Grams and Giuseppe Grosso and H. Gugnani and Yuming Guo and Prakash C Gupta and Rahul Gupta and R. Gupta and Tanush Gupta and Bishal Gyawali and J. Haagsma and Vladimir Hachinski and N. Hafezi-Nejad and Hassan Haghparast Bidgoli and Tekleberhan B Hagos and G. B. Hailu and Arvin Haj-Mirzaian and A. Haj-Mirzaian and R. Hamadeh and S. Hamidi and A. Handal and Graeme J. Hankey and Yuantao Hao and H. Harb and Sivadasanpillai Harikrishnan and J. Haro and M. Hasan and H. Hassankhani and H. Y. Hassen and Rasmus J Havmoeller and Caitlin N Hawley and Roderick J Hay and S. Hay and A. Hedayatizadeh-Omran and B. Heibati and D. Hendrie and A. Henok and C. Herteliu and S. Heydarpour and D. T. Hibstu and Huong Thanh Hoang and Hans W. Hoek and Howard J Hoffman and M. K. Hole and Enayatollah Homaie Rad and Praveen Hoogar and H. Hosgood and Seyed Mostafa Hosseini and M. Hosseinzadeh and M. Hostiuc and S. Hostiuc and Peter J. Hotez and D. Hoy and M. Hsairi and A. Htet and Guoqing Hu and John J Emmanuel Huang and Chantal K Huynh and K. Iburg and C. Ikeda and B. Ileanu and O S Ilesanmi and Usman Iqbal and S. Irvani and C. Irvine and Sheikh Mohammed Shariful Islam and F. Islami and K. Jacobsen and L. Jahangiry and N. Jahanmehr and Sudhir Kumar Jain and Mihajlo Jakovljevic and Mehdi Javanbakht and A. Jayatilleke and P. Jeemon and Ravi Prakash Jha and Vivekanand Jha and John S Ji and C. Johnson and J. Jonas and J. Jozwiak and Suresh Jungari and Mikk Jürisson and Zubair Kabir and R. Kadel and Amaha Kahsay and Rizwan Kalani and T. Kanchan and Manoochehr Karami and B. Karami Matin and André Karch and C. Karema and Narges Karimi and Seyed M. Karimi and A. Kasaeian and D. H. Kassa and G. Kassa and T. Kassa and N. Kassebaum and S. Katikireddi and Norito Kawakami and Ali Kazemi Karyani and Masoud Keighobadi and P. Keiyoro and L. Kemmer and Grant Rodgers Kemp and A. Kengne and Andre Keren and Yousef S. Khader and Behzad Khafaei and M. Khafaie and Alireza Khajavi and I. Khalil and E. Khan and M. Khan and Muhammad Ali Khan and Y. Khang and Mohammad Khazaei and Abdullah Khoja and A. Khosravi and Mohammad Hossein Khosravi and A. Kiadaliri and Daniel N. Kiirithio and Cho-il Kim and Daniel H. Kim and Pauline Kim and Young-Eun Kim and Y. Kim and R. Kimokoti and Y. Kinfu and A. Kisa and K. Kissimova-Skarbek and Mika Kivimäki and A. Knudsen and Jonathan M Kocarnik and S. Kochhar and Yoshihiro Kokubo and T. Kolola and Jacek A. Kopec and S. Kosen and Georgios A Kotsakis and P. Koul and Ai Koyanagi and M. Kravchenko and K. Krishan and Kristopher J. Krohn and B. Kuate Defo and B. Kucuk Bicer and G. Kumar and Manasi Kumar and H. Kyu and Deepesh P Lad and S. Lad and A. Lafranconi and R. Lalloo and T. Lallukka and Faris Lami and V. Lansingh and A. Latifi and Kathryn M. Lau and Jeffrey V. Lazarus and J. Leasher and J. R. Ledesma and P. H. Lee and J. Leigh and Janni Leung and Miriam Levi and S. Lewycka and Shanshan Li and Yichong Li and Yu Liao and Misgan Legesse Liben and Lee-Ling Lim and Stephen S. Lim and Shiwei Liu and R. Lodha and Katharine J Looker and A. D. Lopez and Stefan Lorkowski and P. Lotufo and Nicola Low and Rafael Lozano and T. Lucas and L. R. Lucchesi and R. Lunevicius and R. Lyons and Stefan Ma and Erlyn K Macarayan and Mark T Mackay and Fabiana Madotto and H. Magdy Abd El Razek and Muhammed Magdy Abd El Razek and Dhaval P. Maghavani and N. Mahotra and Hue Thi Mai and M. Majdan and Reza Majdzadeh and Azeem Majeed and R. Malekzadeh and Deborah Carvalho Malta and A. A. Mamun and Ana-Laura Manda and Helena Manguerra and Treh Manhertz and Mohammad Ali Mansournia and L. Mantovani and C. Mapoma and Joemer C. Maravilla and W. Marcenes and Ashley Marks and Francisco Rogerlândio Martins-Melo and Ira Martopullo and Winfried März and M. Marzan and T. Mashamba-Thompson and B. Massenburg and M. Mathur and Kunihiro Matsushita and P. Maulik and M. Mazidi and C. McAlinden and John J McGrath and M. McKee and M. Mehndiratta and Ravi Mehrotra and Kala M. Mehta and Varshil Mehta and Fabiola Mejía-Rodríguez and Tesfa Mekonen and A. Melese and M. Melku and Michele Meltzer and P. Memiah and Ziad A. Memish and W. Mendoza and D. T. Mengistu and G. Mengistu and George A. Mensah and S. T. Mereta and A. Meretoja and T. Meretoja and T. Meštrović and Naser Mohammad gholi Mezerji and Bartosz Miazgowski and T. Miazgowski and Anoushka I Millear and Ted R. Miller and Benjamin Miltz and G. Mini and M. Mirarefin and E. Mirrakhimov and A. Misganaw and Philip B Mitchell and H. Mitiku and B. Moazen and B. Mohajer and K. Mohammad and N. Mohammadifard and M. Mohammadnia-Afrouzi and Mohammed A Mohammed and S. Mohammed and F. Mohebi and Modhurima Moitra and A. Mokdad and M. Molokhia and L. Monasta and Y. Moodley and M. Moosazadeh and Ghobad Moradi and M. Moradi-Lakeh and Mehdi Moradinazar and P. Moraga and Lidia Morawska and Ilais Moreno Velásquez and J. Morgado-da-Costa and S. D. Morrison and M. Moschos and Seyyed Meysam Mousavi and K. B. Mruts and A. Muche and Kindie Fentahun Muchie and U. Mueller and O. Muhammed and Satinath Mukhopadhyay and Kate Muller and J. Mumford and Manoj V. Murhekar and Jonah Musa and K. I. Musa and Ghulam Mustafa and A. F. Nabhan and Chie Nagata and M. Naghavi and A. Naheed and A. Nahvijou and G. Naik and Nitish Naik and Farid Najafi and Luigi Naldi and Hae Sung Nam and V. Nangia and J. R. Nansseu and B. R. Nascimento and Gopalakrishnan Natarajan and N. Neamati and I. Negoi and R. Negoi and Subas Neupane and C. R. Newton and J. Ngunjiri and Anh Quynh Nguyen and Ha Thu Nguyen and Huong Lan Thi Nguyen and Huong Thanh Nguyen and L. H. Nguyen and Minh Nguyen and N. B. Nguyen and S. H. Nguyen and E. Nichols and D. N. A. Ningrum and M. Nixon and Nomonde Nolutshungu and Shuhei Nomura and O. Norheim and M. Noroozi and B. Norrving and J. J. Noubiap and H. Nouri and M. Nourollahpour Shiadeh and M. Nowroozi and E. Nsoesie and Peter S Nyasulu and Christopher M. Odell and R. Ofori-Asenso and F. Ogbo and In-Hwan Oh and O. Oladimeji and A. Olagunju and T. Olagunju and Pedro R Olivares and H. Olsen and B. Olusanya and K. Ong and S. Ong and Eyal Oren and Alberto Ortiz and E. Ota and Stanislav S. Otstavnov and Simon Øverland and M. Owolabi and M. P. A. and R. Pacella and Amir H. Pakpour and A. Pana and S. Panda‐Jonas and Andrea Parisi and Eun-Kee Park and Charles D H Parry and Shanti Patel and S. Pati and Snehal T. Patil and Ajay Patle and George C. Patton and V. R. Paturi and Katherine R Paulson and Neil Pearce and David M Pereira and N. Perico and Konrad Pesudovs and H. Pham and Michael Robert Phillips and D. Pigott and J. Pillay and M. Piradov and M. Pirsaheb and F. Pishgar and O. Plana-Ripoll and Dietrich Plass and S. Polinder and Svetlana Popova and Maarten J. Postma and A. Pourshams and H. Poustchi and D. Prabhakaran and Swayam Prakash and V. Prakash and Caroline A. Purcell and M. Purwar and Mostafa Qorbani and D. A. Quistberg and A. Radfar and Anwar Rafay and A. Rafiei and F. Rahim and Kazem Rahimi and A. Rahimi-Movaghar and V. Rahimi-Movaghar and Mahfuzar Rahman and Mohammad Hifz Ur Rahman and Muhammad Aziz Rahman and Sajjad Ur Rahman and R. Rai and Fatemeh Rajati and U. Ram and Prabhat Ranjan and A. Ranta and P. Rao and D. Rawaf and S. Rawaf and K. S. Reddy and R. Reiner and Nickolas Reinig and M. Reitsma and G. Remuzzi and Andre M. N. Renzaho and S. Resnikoff and S. Rezaei and M. S. Rezai and A. L. Ribeiro and Stephen R Robinson and L. Roever and L. Ronfani and G. Roshandel and Ali Rostami and Gregory A Roth and Ambuj Roy and Enrico Rubagotti and Perminder S. Sachdev and Nafis Sadat and B. Saddik and Ehsan Sadeghi and S. Saeedi Moghaddam and H. Safari and Y. Safari and R. Safari-Faramani and M. Safdarian and S. Safi and Saeid Safiri and Rajesh Sagar and A. Sahebkar and M. Sahraian and Haniye Sadat Sajadi and Nasir Salam and Joseph S. Salama and Payman Salamati and Komal Saleem and Zikria Saleem and Yahya Salimi and Joshua A. Salomon and S. Salvi and I. Salz and A. Samy and Juan R Sanabria and Y. Sang and D. Santomauro and I. Santos and João Vasco Santos and M. Šantrić Milićević and Bruno Piassi Sao Jose and M. Sardana and A. Sarker and N. Sarrafzadegan and B. Sartorius and S. Sarvi and B. Sathian and Maheswar Satpathy and Arundhati R Sawant and M. Sawhney and S. Saxena and Mete Saylan and Elke Schaeffner and M.I. Schmidt and I. Schneider and Ben Schöttker and D. Schwebel and Falk Schwendicke and James G Scott and M. Šekerija and S. Sepanlou and E. Serván-Mori and Seyedmojtaba Seyedmousavi and Hosein Shabaninejad and Azadeh Shafieesabet and M. Shahbazi and A. Shaheen and M. Shaikh and Mehran Shams-Beyranvand and M. Shamsi and M. Shamsizadeh and H. Sharafi and K. Sharafi and M. Sharif and M. Sharif-Alhoseini and Meenakshi Sharma and Rajesh Sharma and Jun She and Aziz Sheikh and Peilin Shi and Kenji Shibuya and M. Shigematsu and R. Shiri and R. Shirkoohi and K. Shishani and I. Shiue and F. Shokraneh and H. Shoman and M. Shrime and Si Si and S. Siabani and T. Siddiqi and I. Sigfusdottir and Rannveig Sigurvinsdottir and J. Silva and D. Silveira and N. Singam and Jasvinder A. Singh and Narinder Pal Singh and Virendra Singh and D. Sinha and Eirini Skiadaresi and E. Slepak and Karen Sliwa and David L Smith and Mari Smith and Adauto Martins Soares Filho and B. Sobaih and S. Sobhani and E. Sobngwi and S. Soneji and M. Soofi and Masoud Soosaraei and Reed J. D. Sorensen and Joan B Soriano and Ireneous N. Soyiri and L. Sposato and C. Sreeramareddy and V. Srinivasan and J. Stanaway and D. Stein and C. Steiner and Timothy J Steiner and Mark A Stokes and L. Stovner and Michelle L. Subart and A. Sudaryanto and M. Sufiyan and B. Sunguya and P. Sur and I. Sutradhar and Bryan L. Sykes and Dillon O Sylte and R. Tabarés-Seisdedos and S. Tadakamadla and B. T. Tadesse and Nikhil Tandon and S. Tassew and M. Tavakkoli and N. Taveira and Hugh R Taylor and Arash Tehrani-Banihashemi and Tigist Gashaw Tekalign and Shishay Wahdey Tekelemedhin and Merhawi Gebremedhin Tekle and H. Temesgen and M. Temsah and Omar Temsah and A. Terkawi and Mebrahtu Teweldemedhin and K. Thankappan and Nihal Thomas and B. Tilahun and Q. To and Marcello Tonelli and R. Topor-Madry and F. Topouzis and Anna E. Torre and M. Tortajada-Girbés and Mathilde Touvier and M. Tovani-Palone and J. Towbin and Bach Xuan Tran and Khanh B. Tran and C. Troeger and T. Truelsen and M. Tsilimbaris and Derrick Tsoi and Lorainne Tudor Car and E. Tuzcu and K. Ukwaja and Irfan Ullah and E. Undurraga and J. Unutzer and R. Updike and M. Usman and O. Uthman and M. Vaduganathan and A. Vaezi and P. Valdez and S. Varughese and T. Vasankari and N. Venketasubramanian and S. Villafaina and F. S. Violante and S. Vladimirov and V. Vlassov and S. Vollset and K. Vosoughi and I. Vujcic and Fasil Wagnew and Yasir Waheed and Stephen G Waller and Yafeng Wang and Yuan-Pang Wang and E. Weiderpass and Robert G Weintraub and Daniel J Weiss and Fitsum Weldegebreal and Kidu Gidey Weldegwergs and A. Werdecker and T. E. West and Harvey A. Whiteford and Justyna Widecka and T. Wijeratne and Lauren B. Wilner and Shadrach Wilson and Andrea S. Winkler and A. Wiyeh and C. S. Wiysonge and Charles D A Wolfe and Anthony D Woolf and Shouling Wu and Yun-Chun Wu and Grant M A Wyper and Denis Xavier and Gelin Xu and Simon R Yadgir and A. Yadollahpour and Seyed Hossein Yahyazadeh Jabbari and Tomohide Yamada and Lijing L. Yan and Yuichiro Yano and M. Yaseri and Y. J. Yasin and Alex Yeshaneh and Ebrahim M. Yimer and Paul Yip and E. Yisma and N. Yonemoto and Seok-Jun Yoon and M. Yotebieng and M. Younis and M. Yousefifard and Chuanhua Yu and V. Zadnik and Z. Zaidi and S. Zaman and M. Zamani and Z. Zare and A. Zeleke and Z. M. Zenebe and Kai Zhang and Zheng Zhao and Maigeng Zhou and S. Zodpey and Inbar Zucker and T. Vos and C. Murray},\n journal = {Lancet (London, England)},\n pages = {1789 - 1858},\n title = {Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990–2017: a systematic analysis for the Global Burden of Disease Study 2017},\n volume = {392},\n year = {2018}\n}\n'}",,"{'volume': '392', 'pages': '1789 - 1858', 'name': 'Lancet (London, England)'}",159.0,"Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990–2017: a systematic analysis for the Global Burden of Disease Study 2017",2018.0
2076,af9c9e1a6e9519d04403eed4050cd277c0e80160,"There is a growing need for applications that can dynamically interact with aging populations to gather information, monitor their health care, provide information, or even act as companions. Virtual human agents or virtual characters offer a technology that can enable human users to overcome the confusing interfaces found in current human-computer interactions. These artificially intelligent virtual characters have speech recognition, natural language and vision that will allow human users to interact with their computers in a more natural way. Additionally, sensors may be used to monitor the environment for specific behaviors that can be fused into a virtual human system. As a result, the virtual human may respond to a patient or elderly person in a manner that will have a powerful affect on their living situation. This paper will describe the virtual human technology developed and some current applications that apply the technology to virtual patients for mental health diagnosis and clinician training. Additionally the paper will discuss possible ways in which the virtual humans may be utilized for assisted health care and for the integration of multi-modal input to enhance the virtual human system.","[{'authorId': '3181776', 'name': 'Patrick G. Kenny'}, {'authorId': '145842705', 'name': 'T. Parsons'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '29861580', 'name': 'A. Rizzo'}]",60.0,"{'bibtex': '@Inproceedings{Kenny2008VirtualHF,\n author = {Patrick G. Kenny and T. Parsons and J. Gratch and A. Rizzo},\n pages = {6},\n title = {Virtual humans for assisted health care},\n year = {2008}\n}\n'}",,{'pages': '6'},13.0,Virtual humans for assisted health care,2008.0
2077,afb216f806006c5b173dc1d745d3371165695db8,"Our goal is to create socially responsible agents, either robots or virtual humans. In this paper, we present an integration of emotions, attachment, and learning in emotional decision‐making to achieve this goal. Based on emerging psychological theories, we aim at building human‐like emotional decision‐making, where emotions play a central role in selecting the next action to be performed by the agent. Here, we present our own approach for emotion appraisal where we use emotional attachment as an important impulse for determining the intensities of emotions. Emotions in their turn are used to calculate the emotional attachment toward the users and for learning to predict future consequences. We report on the results of a simulation evaluation where we assess the influence of emotions, attachment, and learning on decision‐making. It is our strong belief that by giving an agent the ability to have emotions and to feel empathy and emotional attachment toward others, we will ultimately give this agent the ability to learn and improve its social behavior skills through interactions with the users and through user feedback. Copyright © 2013 John Wiley & Sons, Ltd.","[{'authorId': '2497958', 'name': 'M. B. Moussa'}, {'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}]",19.0,"{'bibtex': '@Article{Moussa2013TowardSR,\n author = {M. B. Moussa and N. Magnenat-Thalmann},\n journal = {Computer Animation and Virtual Worlds},\n title = {Toward socially responsible agents: integrating attachment and learning in emotional decision‐making},\n volume = {24},\n year = {2013}\n}\n'}",,"{'volume': '24', 'name': 'Computer Animation and Virtual Worlds'}",20.0,Toward socially responsible agents: integrating attachment and learning in emotional decision‐making,2013.0
2078,afd2e426df00d9bd4bf261581a3d0d21f36f6d7a,,"[{'authorId': '1682486', 'name': 'Brian Ravenet'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",60.0,"{'bibtex': ""@Inproceedings{Ravenet2013FromAU,\n author = {Brian Ravenet and M. Ochs and C. Pelachaud},\n pages = {263-274},\n title = {From a User-created Corpus of Virtual Agent's Non-verbal Behavior to a Computational Model of Interpersonal Attitudes},\n year = {2013}\n}\n""}",,{'pages': '263-274'},27.0,From a User-created Corpus of Virtual Agent's Non-verbal Behavior to a Computational Model of Interpersonal Attitudes,2013.0
2080,affdf142139050f3ec86a4f283ae8391be0e7090,"The simulation of human behaviors in virtual environments has many applications. In many of these applications, situations arise in which the user has a face-to-face interaction with a virtual agent. In this work, we present an approach for multi-agent navigation that facilitates a face-to-face interaction between a real user and a virtual agent that is part of a virtual crowd. In order to predict whether the real user is approaching a virtual agent to have a face-to-face interaction or not, we describe a model of approach behavior for virtual agents. We present a novel interaction velocity prediction (IVP) algorithm that is combined with human body motion synthesis constraints and facial actions to improve the behavioral realism of virtual agents. We combine these techniques with full-body virtual crowd simulation and evaluate their benefits by conducting a user study using Oculus HMD in an immersive environment. Results of this user study indicate that the virtual agents using our interaction algorithms appear more responsive and are able to elicit more reaction from the users. Our techniques thus enable face-to-face interactions between a real user and a virtual agent and improve the sense of presence observed by the user.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",17.0,"{'bibtex': '@Article{Randhavane2017F2FCrowdsPA,\n author = {Tanmay Randhavane and Aniket Bera and Dinesh Manocha},\n journal = {PRESENCE: Teleoperators and Virtual Environments},\n pages = {228-246},\n title = {F2FCrowds: Planning Agent Movements to Enable Face-to-Face Interactions},\n volume = {26},\n year = {2017}\n}\n'}",,"{'volume': '26', 'pages': '228-246', 'name': 'PRESENCE: Teleoperators and Virtual Environments'}",62.0,F2FCrowds: Planning Agent Movements to Enable Face-to-Face Interactions,2017.0
2081,b00f5a4cdf2ae9f59da962c3962089d718020c77,,"[{'authorId': '7994792', 'name': 'J. Houwer'}, {'authorId': '2260912013', 'name': 'Dirk Hermans'}, {'authorId': '2260912013', 'name': 'Dirk Hermans'}, {'authorId': '2064945', 'name': 'A. Moors'}, {'authorId': '2264180576', 'name': 'Michael D. Robinson'}, {'authorId': '2264166849', 'name': 'Robin S. Edelstein'}, {'authorId': '2264185807', 'name': 'Jenny Yiend'}, {'authorId': '2264182737', 'name': 'Anne Richards'}, {'authorId': '2264158988', 'name': 'Tobias Brosch'}]",111.0,"{'bibtex': '@Article{Houwer2018CognitionAE,\n author = {J. Houwer and Dirk Hermans and Dirk Hermans and A. Moors and Michael D. Robinson and Robin S. Edelstein and Jenny Yiend and Anne Richards and Tobias Brosch},\n journal = {Cognitive Psychology},\n title = {Cognition and emotion},\n year = {2018}\n}\n'}",,{'name': 'Cognitive Psychology'},70.0,Cognition and emotion,2018.0
2082,b0554b6fa2fc30d29ed92b3f29f523c646506e11,"This thesis is about emotions, and more particularly about their logical formalization. The first part is dedicated to the state of the art, from the point of view of both psychology (history of theories of emotions) and computer science (presentation of emotional agents and their applications).The second part is dedicated to the logical formalisation of emotions. It introduces our logical framework, exposes and argues the formal definitions of twenty emotions, and proves some of their properties. Finally the last part is dedicated to practical applications and continuation prospects of this work. Such a work offers interesting contributions : it offers to the agent community a formal model of a great number of emotions; it shows the interest of BDI logics; and it opens research prospects about the dynamics of emotions and their influence on the behaviour of agents, a field not much explored for now.","[{'authorId': '2236335', 'name': 'C. Adam'}]",74.0,"{'bibtex': '@Inproceedings{Adam2007EmotionsFP,\n author = {C. Adam},\n title = {Emotions: from psychological theories to logical formalization and implementation in a BDI agent},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",164.0,Emotions: from psychological theories to logical formalization and implementation in a BDI agent,2007.0
2083,b05854f02fbe9eed795f09017542b94202508f71,"Although intuitive processes are critical for effective strategic decision making, there is little in the way of applied research on the topic. Apart from many popularized treatments of intuition in the literature today, there are only a handful of serious scholarly works on the subject. The majority of them are essentially theoretical in nature; field research in management settings is virtually nonexistent. This study examined this neglected but important process in strategic decision making. We surveyed senior managers of companies representing computer, banking, and utility industries in the United States and found that intuitive processes are used often in organizational decision making. Use of intuitive synthesis was found to be positively associated with organizational performance in an unstable environment, but negatively so in a stable environment.","[{'authorId': '145738909', 'name': 'M. Sinclair'}, {'authorId': '1406669431', 'name': 'E. Sadler‐Smith'}, {'authorId': '3129147', 'name': 'G. Hodgkinson'}]",675.0,"{'bibtex': '@Article{Sinclair2009TheRO,\n author = {M. Sinclair and E. Sadler‐Smith and G. Hodgkinson},\n journal = {Human Relations},\n pages = {393-417},\n title = {The Role of Intuition in Strategic Decision Making},\n volume = {53},\n year = {2009}\n}\n'}",,"{'volume': '53', 'pages': '393-417', 'name': 'Human Relations'}",61.0,The Role of Intuition in Strategic Decision Making,2009.0
2084,b072be95bd56c9c9fbf4b466da236d5f0fec9815,"The present research examined whether the recognizable nonverbal expressions associated with pride and shame may be biologically innate behavioral responses to success and failure. Specifically, we tested whether sighted, blind, and congenitally blind individuals across cultures spontaneously display pride and shame behaviors in response to the same success and failure situations—victory and defeat at the Olympic or Paralympic Games. Results showed that sighted, blind, and congenitally blind individuals from >30 nations displayed the behaviors associated with the prototypical pride expression in response to success. Sighted, blind, and congenitally blind individuals from most cultures also displayed behaviors associated with shame in response to failure. However, culture moderated the shame response among sighted athletes: it was less pronounced among individuals from highly individualistic, self-expression-valuing cultures, primarily in North America and West Eurasia. Given that congenitally blind individuals across cultures showed the shame response to failure, findings overall are consistent with the suggestion that the behavioral expressions associated with both shame and pride are likely to be innate, but the shame display may be intentionally inhibited by some sighted individuals in accordance with cultural norms.","[{'authorId': '37930132', 'name': 'J. Tracy'}, {'authorId': '145413880', 'name': 'D. Matsumoto'}]",431.0,"{'bibtex': '@Article{Tracy2008TheSE,\n author = {J. Tracy and D. Matsumoto},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {11655 - 11660},\n title = {The spontaneous expression of pride and shame: Evidence for biologically innate nonverbal displays},\n volume = {105},\n year = {2008}\n}\n'}",,"{'volume': '105', 'pages': '11655 - 11660', 'name': 'Proceedings of the National Academy of Sciences'}",58.0,The spontaneous expression of pride and shame: Evidence for biologically innate nonverbal displays,2008.0
2085,b07e6104d909b6d81a2f382d6d2a0400dd430f97,"Despite centuries of speculation about how to manage negative emotions, little is actually known about which emotion-regulation strategies people choose to use when confronted with negative situations of varying intensity. On the basis of a new process conception of emotion regulation, we hypothesized that in low-intensity negative situations, people would show a relative preference to choose to regulate emotions by engagement reappraisal, which allows emotional processing. However, we expected people in high-intensity negative situations to show a relative preference to choose to regulate emotions by disengagement distraction, which blocks emotional processing at an early stage before it gathers force. In three experiments, we created emotional contexts that varied in intensity, using either emotional pictures (Experiments 1 and 2) or unpredictable electric stimulation (Experiment 3). In response to these emotional contexts, participants chose between using either reappraisal or distraction as an emotion-regulation strategy. Results in all experiments supported our hypothesis. This pattern in the choice of emotion-regulation strategies has important implications for the understanding of healthy adaptation.","[{'authorId': '5236984', 'name': 'G. Sheppes'}, {'authorId': '5576156', 'name': 'S. Scheibe'}, {'authorId': '47763916', 'name': 'G. Suri'}, {'authorId': '1775321', 'name': 'J. Gross'}]",1097.0,"{'bibtex': '@Article{Sheppes2011EmotionRegulationC,\n author = {G. Sheppes and S. Scheibe and G. Suri and J. Gross},\n journal = {Psychological Science},\n pages = {1391 - 1396},\n title = {Emotion-Regulation Choice},\n volume = {22},\n year = {2011}\n}\n'}",,"{'volume': '22', 'pages': '1391 - 1396', 'name': 'Psychological Science'}",25.0,Emotion-Regulation Choice,2011.0
2086,b088a550a2841ee332ad8ca8a6398b3837916cb8,,"[{'authorId': '50480971', 'name': 'R. Lane'}, {'authorId': '145890227', 'name': 'L. Nadel'}, {'authorId': '143897032', 'name': 'John J. B. Allen'}, {'authorId': '2310149', 'name': 'A. Kaszniak'}]",61.0,"{'bibtex': '@Inproceedings{Lane2000TheSO,\n author = {R. Lane and L. Nadel and John J. B. Allen and A. Kaszniak},\n title = {The study of emotion from the perspective of cognitive neuroscience},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The study of emotion from the perspective of cognitive neuroscience,2000.0
2087,b08a998e86ec77ee2b88d0702469b169a8d51bf3,"This study examines the relationship between learners’ production of modified output and their working memory (WM) capacity. The task-based interactions of 42 college-level, native English-speaking learners of Spanish as a foreign language were examined. A relationship was found between learners’ WM test scores and their tendency to modify output. Specifically, greater processing capacity was related to greater production of modified output during interaction.","[{'authorId': '118340139', 'name': 'Alison Mackey'}, {'authorId': '48460302', 'name': 'R. Adams'}, {'authorId': '82135016', 'name': 'C. Stafford'}, {'authorId': '11063619', 'name': 'Paula M. Winke'}]",173.0,"{'bibtex': '@Article{Mackey2010ExploringTR,\n author = {Alison Mackey and R. Adams and C. Stafford and Paula M. Winke},\n journal = {Language Learning},\n pages = {501-533},\n title = {Exploring the Relationship between Modified Output and Working Memory Capacity.},\n volume = {60},\n year = {2010}\n}\n'}",,"{'volume': '60', 'pages': '501-533', 'name': 'Language Learning'}",75.0,Exploring the Relationship between Modified Output and Working Memory Capacity.,2010.0
2088,b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca,,"[{'authorId': '2247495712', 'name': 'Paul A. Viola'}, {'authorId': '145319478', 'name': 'Michael J. Jones'}]",15952.0,"{'bibtex': '@Article{Viola2001RobustRF,\n author = {Paul A. Viola and Michael J. Jones},\n journal = {International Journal of Computer Vision},\n pages = {137-154},\n title = {Robust Real-Time Face Detection},\n volume = {57},\n year = {2001}\n}\n'}",,"{'volume': '57', 'pages': '137-154', 'name': 'International Journal of Computer Vision'}",20.0,Robust Real-Time Face Detection,2001.0
2089,b0a4b7c77f82893c7acd5fe3eea6c8f39ba4b1c6,"We explored the relationship between interactants' social anxiety and the interactional fidelity of virtual humans. We specifically addressed whether the contingent non‐verbal feedback of virtual humans affects the association between interactants' social anxiety and their verbal self‐disclosure. This subject was investigated across three experimental conditions where participants interacted with real human videos and virtual humans in computer‐mediated interview interactions. The results demonstrated that socially anxious people revealed more information and greater intimate information about themselves when interacting with a virtual human when compared with real human video interaction, whereas less socially anxious people did not show this difference. We discuss the implication of this association between the interactional fidelity of virtual humans and social anxiety in a human interactant on the design of an embodied virtual agent for social skills' training and psychotherapy. Copyright © 2010 John Wiley & Sons, Ltd.","[{'authorId': '34728215', 'name': 'Sin-Hwa Kang'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",16.0,"{'bibtex': ""@Article{Kang2010VirtualHE,\n author = {Sin-Hwa Kang and J. Gratch},\n journal = {Computer Animation and Virtual Worlds},\n title = {Virtual humans elicit socially anxious interactants' verbal self‐disclosure},\n volume = {21},\n year = {2010}\n}\n""}",,"{'volume': '21', 'name': 'Computer Animation and Virtual Worlds'}",34.0,Virtual humans elicit socially anxious interactants' verbal self‐disclosure,2010.0
2090,b0ee222ae224a67039f85103a0166e12e4e416d6,"According to social baseline theory (Beckes & Coan, 2011), load sharing is a feature of close relationships whereby the burden of emotional distress is distributed across relationship partners. Load sharing varies by physical closeness and relationship quality. We investigated the effect of load sharing on emotional arousal via galvanic skin response, an indicator of sympathetic nervous system arousal, during a social stressor. Social stress was elicited in 66 adolescent girls (Mage = 15 years) using a spontaneous public-speaking task. Mother-daughter dyads reported their relationship quality, and physical closeness was manipulated by having mothers either touch or not touch their daughter's hand during the performance. We found evidence of load sharing among dyads who held hands, independent of relationship quality. However, without physical contact, load sharing was only evident among dyads with higher relationship quality. Thus, high relationship quality buffers against threat in a similar way to the physical comfort of a loved one.","[{'authorId': '3714961', 'name': 'Jessica P. Lougheed'}, {'authorId': '2140903', 'name': 'Peter Koval'}, {'authorId': '2329276', 'name': 'Tom Hollenstein'}]",34.0,"{'bibtex': '@Article{Lougheed2016SharingTB,\n author = {Jessica P. Lougheed and Peter Koval and Tom Hollenstein},\n journal = {Emotion},\n pages = {\n          83-93\n        },\n title = {Sharing the burden: The interpersonal regulation of emotional arousal in mother-daughter dyads.},\n volume = {16 1},\n year = {2016}\n}\n'}",,"{'volume': '16 1', 'pages': '\n          83-93\n        ', 'name': 'Emotion'}",52.0,Sharing the burden: The interpersonal regulation of emotional arousal in mother-daughter dyads.,2016.0
2091,b10560a6ee1c0f9d4de2387f68a59181a8749c61,"Affective computing is the study and development of systems and devices that can recognise, interpret, process, and simulate human affects. In this context, computational modelling of emotion is a major challenge in order to design believable virtual humans. This factor has an impact on both the individual behaviour and the collective one. Recently, researchers have shown an increased interest in the emotion contagion phenomenon in order to model emerging group behaviour. 
 
Stemming from works on multi-agent systems environments, we propose an architecture to manage both internal and external emotion dynamics. Emotions evolve in function of three influences: punctual events, temporal dynamics and external influences. In an embodied agent approach, the first is the responsibility of the agent's mind, the second of the agent's body, and the third of the environment. This functional architecture is then adapted to a multi-agent architecture, adding a control responsibility to the agent body. Finally, we show the results of several experiments to examine the properties of the architecture and its efficiency by comparing it to a full agent approach.","[{'authorId': '1708997', 'name': 'Julien Saunier'}, {'authorId': '31600786', 'name': 'H. Jones'}]",22.0,"{'bibtex': '@Article{Saunier2014MixedAD,\n author = {Julien Saunier and H. Jones},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {645-652},\n title = {Mixed agent/social dynamics for emotion computation},\n year = {2014}\n}\n'}","[{'paperId': '7d4c8e95560617ee59f82b1d879618955fe5279d', 'title': 'Emotion contagion in agent-based simulations of crowds: a systematic review'}, {'paperId': 'e38e16d215a9759a18cda25f495cf73a306f84a5', 'title': 'Modèle de comportement latéral des véhicules légers fondé sur des forces'}, {'paperId': '2b9b7003f80b377152762a0400d41a2222e8d293', 'title': 'A Review of Affective Computing Research Based on Function-Component-Representation Framework'}, {'paperId': '750ff8254b23901808ad1885bba141aaa673c51d', 'title': 'Relationship Identification Between Conversational Agents Using Emotion Analysis'}, {'paperId': 'bfd65fad8833a3758ef3669a02aac846d851082c', 'title': 'Computational Emotion Models: A Thematic Review'}, {'paperId': '1659c4df52d75a7799966e1872d5d60ebb5ea10b', 'title': 'Towards a Cyber-physical Systems Resilience Approach based on Artificial Emotions and Multi-agent Systems'}, {'paperId': 'bc16e78174767d7cec618bca7e09aeae3f92421d', 'title': 'A Workload Manager: The Pre-assessment in Sincere Software Agent Environment'}, {'paperId': '6ff11d03f1afdbfc89202bea9e33c6b464de6a18', 'title': 'Introducing dynamism in emotional agent societies'}, {'paperId': '887c6845a407273c81f9c72fa6e897f1f2330804', 'title': 'The AI Rebellion: Changing the Narrative'}, {'paperId': '34d449ffe4a24c95a74aebc94249ea9a0742e843', 'title': 'Computational models and optimal control strategies for emotion contagion in the human population in emergencies'}, {'paperId': '3f48adb5cf6b55a8bb3e23c20bdba9e6dc7c74ad', 'title': 'The rationality of sincere software agent in task completion'}, {'paperId': '59bc20f7d3898e6fc96f60d8184edc88cc6c4e12', 'title': ""Approches environnement-centrées pour la simulation de systèmes multi-agents : Pour un déplacement de la complexité des agents vers l'environnement. (Environment-centric approaches for Multi-Agent Based Simulation : Moving complexity from agents to environment)""}, {'paperId': '2baa180277650c47e466d5fb643b3062ab76d564', 'title': ""Un modèle d'environnement pour la simulation multi-agents des déplacements en milieu urbain""}, {'paperId': '783f49505ef8ebee7c6ecc437ca066664198c9ae', 'title': 'Is That How Everyone Really Feels? Emotional Contagion with Masking for Virtual Crowds'}, {'paperId': 'ae502782147d802e9a0aba2f6de8eca48f16a7a0', 'title': 'Organizational and Holonic Modelling of a Simulated and Synthetic Spatial Environment'}, {'paperId': 'ebcbfb87ebbd02a4c5ba7e8fbcbb5d307eb8f182', 'title': 'Agent Bodies: An Interface Between Agent and Environment'}, {'paperId': '2d511005beef67c05fc11c43794f5c3f64277c53', 'title': 'Study of a Model of Nervousness Propagation Within Virtual Crowds'}, {'paperId': '32188bbb8f6062bb5f77e34b1fc894ee32da681f', 'title': 'Artificial Emotions for Distributed Cyber-physical Systems Resilience'}, {'paperId': '9a343b99051b1f0669e0f276bf044d25e5c3af0b', 'title': 'Social Attitudes of AI Rebellion: A Framework'}, {'paperId': '07061225914adbf5289522c0b4a76f39cc25db6b', 'title': 'Gameplay Aware Emotional Model for Virtual Characters in Video Games'}, {'paperId': 'c85eb113ec26dc45af6bbbc15a00768c9440add8', 'title': ""Un modèle d'environnement pour la simulation multi-agents des déplacements en milieu urbain. (An environment model for the multi-agent simulation of mobility in urban areas)""}, {'paperId': '078765aee88112d417c355b2887b23538da468cd', 'title': 'Expression of Emotion in Virtual Crowds:Investigating Emotion Contagion and Perception of Emotional Behaviour in Crowd Simulation'}]",{'pages': '645-652'},27.0,Mixed agent/social dynamics for emotion computation,2014.0
2092,b14ff2a2f78cd3f410002976755580c169412bb9,"Both stable personality characteristics (traits) and transient emotions (states) influence cognition and behavior. In this paper, we describe a methodology for modeling these effects in terms of a set of parameters that control processing within a symbolic cognitive architecture. The underlying thesis of the approach is that the combined effects of these individual differences can be modeled by varying the architecture parameters that control both processing and the structure of knowledge within the architecture modules. We describe the architecture, provide operational definitions of representative trait and state influences in terms of the controlling parameters, and demonstrate how observed trait/state phenomena are modeled in the context of the current demonstration scenario: a peacekeeping training simulation.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",60.0,"{'bibtex': '@Article{Hudlicka2002ThisTW,\n author = {E. Hudlicka},\n journal = {Applied Artificial Intelligence},\n pages = {611 - 641},\n title = {This time with feeling: Integrated model of trait and state effects on cognition and behavior},\n volume = {16},\n year = {2002}\n}\n'}",,"{'volume': '16', 'pages': '611 - 641', 'name': 'Applied Artificial Intelligence'}",39.0,This time with feeling: Integrated model of trait and state effects on cognition and behavior,2002.0
2093,b19e8ceff5f2ed70ddb0a5cadc495777235dfd62,This study focuses on the antecedents and consequences of displayed emotion in organizations. I propose that customers “catch” the affect of employees through emotional contagion processes. Results...,"[{'authorId': '145490003', 'name': 'S. Pugh'}]",1353.0,"{'bibtex': '@Article{Pugh2001ServiceWA,\n author = {S. Pugh},\n journal = {Academy of Management Journal},\n pages = {1018-1027},\n title = {Service with a smile: Emotional contagion in the service encounter.},\n volume = {44},\n year = {2001}\n}\n'}",,"{'volume': '44', 'pages': '1018-1027', 'name': 'Academy of Management Journal'}",24.0,Service with a smile: Emotional contagion in the service encounter.,2001.0
2094,b1a11e7461ded60c12385629a558866c048063fd,"In natural viewing conditions, different stimulus categories such as people, objects, and natural scenes carry relevant affective information that is usually processed simultaneously. But these different signals may not always have the same affective meaning. Using body‐scene compound stimuli, we investigated how the brain processes fearful signals conveyed by either a body in the foreground or scenes in the background and the interaction between foreground body and background scene. The results showed that left and right extrastriate body areas (EBA) responded more to fearful than to neutral bodies. More interestingly, a threatening background scene compared to a neutral one showed increased activity in bilateral EBA and right‐posterior parahippocampal place area (PPA) and decreased activity in right retrosplenial cortex (RSC) and left‐anterior PPA. The emotional scene effect in EBA was only present when the foreground body was neutral and not when the body posture expressed fear (significant emotion‐by‐category interaction effect), consistent with behavioral ratings. The results provide evidence for emotional influence of the background scene on the processing of body expressions. Hum Brain Mapp 35:492–502, 2014. © 2012 Wiley Periodicals, Inc.","[{'authorId': '1962262771', 'name': 'Jan Van den Stock'}, {'authorId': '2610493', 'name': 'M. Vandenbulcke'}, {'authorId': '7318717', 'name': 'C. Sinke'}, {'authorId': '4628064', 'name': 'B. de Gelder'}]",51.0,"{'bibtex': '@Article{Stock2014AffectiveSI,\n author = {Jan Van den Stock and M. Vandenbulcke and C. Sinke and B. de Gelder},\n journal = {Human Brain Mapping},\n title = {Affective scenes influence fear perception of individual body expressions},\n volume = {35},\n year = {2014}\n}\n'}",,"{'volume': '35', 'name': 'Human Brain Mapping'}",53.0,Affective scenes influence fear perception of individual body expressions,2014.0
2095,b1afeacdae33129e84406ee2ed635fd81ae6efa7,"We propose the first approach to synthesize the synchronous 3D conversational body and hand gestures, as well as 3D face and head animations, of a virtual character from speech input. Our algorithm uses a CNN architecture that leverages the inherent correlation between facial expression and hand gestures. Synthesis of conversational body gestures is a multi-modal problem since many similar gestures can plausibly accompany the same input speech. To synthesize plausible body gestures in this setting, we train a Generative Adversarial Network (GAN) based model that measures the plausibility of the generated sequences of 3D body motion when paired with the input audio features. We also contribute a new corpus that contains more than 33 hours of annotated data from in-the-wild videos of talking people. To this end, we apply state-of-the-art monocular approaches for 3D body and hand pose estimation as well as 3D face performance capture to the video corpus. In this way, we can train on orders of magnitude more data than previous algorithms that resort to complex in-studio motion capture solutions, and thereby train more expressive synthesis algorithms. Our experiments and user study show the state-of-the-art quality of our speech-synthesized full 3D character animations.","[{'authorId': '3202217', 'name': 'I. Habibie'}, {'authorId': '9765909', 'name': 'Weipeng Xu'}, {'authorId': '39503308', 'name': 'Dushyant Mehta'}, {'authorId': '46458089', 'name': 'Lingjie Liu'}, {'authorId': '145156858', 'name': 'H. Seidel'}, {'authorId': '1403428213', 'name': 'Gerard Pons-Moll'}, {'authorId': '1854465', 'name': 'Mohamed A. Elgharib'}, {'authorId': '1680185', 'name': 'C. Theobalt'}]",52.0,"{'bibtex': '@Article{Habibie2021LearningS3,\n author = {I. Habibie and Weipeng Xu and Dushyant Mehta and Lingjie Liu and H. Seidel and Gerard Pons-Moll and Mohamed A. Elgharib and C. Theobalt},\n journal = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},\n title = {Learning Speech-driven 3D Conversational Gestures from Video},\n year = {2021}\n}\n'}",,{'name': 'Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents'},63.0,Learning Speech-driven 3D Conversational Gestures from Video,2021.0
2096,b1c3c69f3ef1b69cb22969555620918168650936,,"[{'authorId': '3471157', 'name': 'Suman Ojha'}, {'authorId': '2116743220', 'name': 'M. Williams'}]",6.0,"{'bibtex': '@Inproceedings{Ojha2017EmotionalA,\n author = {Suman Ojha and M. Williams},\n title = {Emotional Appraisal : A Computational Perspective},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotional Appraisal : A Computational Perspective,2017.0
2097,b1c9e25cf2d0400e373bf1c13264ba0e88e5e934,,"[{'authorId': '2259303', 'name': 'Hyung-il Ahn'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",80.0,"{'bibtex': '@Inproceedings{Ahn2005AffectiveCognitiveLA,\n author = {Hyung-il Ahn and Rosalind W. Picard},\n pages = {866-873},\n title = {Affective-Cognitive Learning and Decision Making: A Motivational Reward Framework for Affective Agents},\n year = {2005}\n}\n'}",,{'pages': '866-873'},12.0,Affective-Cognitive Learning and Decision Making: A Motivational Reward Framework for Affective Agents,2005.0
2098,b1cf3570cca62a28dbaf36d078f2b58ca6acab0b,"Abstrack, This study aims to intervene in one of the students who is in Al-Azhar Gresik High School. The subject has obsessive compulsive behavior in the form of using a handbody on the sole of the foot repeatedly. Researchers provide interventions in the form of cognitive behavior therapy as interventions to reduce obsessive compulsive behavior, and to improve mental health in schools. This therapy aims to oppose wrong thoughts and emotions by presenting evidence that contradicts beliefs about the problem at hand. The method used in this research is qualitative with a case study approach. Subjects were female who were 16 years old and had obsessive compulsive behavior for 14 months. The cognitive behavior therapy procedure is first, the subject is asked for relaxation in the form of progressivrizkie muscle relaxation to learn to stretch and relax various muscle groups and learn to notice the difference between tension and relax. Second, cognitive restructuring, reducing the anxiety level of the subject caused by negative thoughts and replacing them with more positive thoughts, and. Third, exposure with response prevention, to overcome obsessive compulsive behavior. Where the subject is faced with a situation of having the belief that must perform the usual ritual behavior but the subject is prevented from doing the ritual. If the subject can prevent not doing the ritual and it turns out that something terrible did not happen, this can help in changing the subject's belief in compulsive behavior. The intervention consisted of eight sessions with two sessions a week and 60 minutes each for each session. The results of this study indicate that cognitive behavior therapy is effectively used to reduce the subject's obsessive compulsive behavior, which is shown by decreasing levels of anxiety, negative thinking and compulsive behavior.","[{'authorId': '48434738', 'name': 'S. Chand'}, {'authorId': '35361247', 'name': 'Daniel P. Kuckel'}, {'authorId': '66787992', 'name': 'M. Huecker'}]",255.0,"{'bibtex': '@Article{Chand2020CognitiveBT,\n author = {S. Chand and Daniel P. Kuckel and M. Huecker},\n journal = {Definitions},\n title = {Cognitive Behavior Therapy},\n year = {2020}\n}\n'}",,{'name': 'Definitions'},14.0,Cognitive Behavior Therapy,2020.0
2099,b1e75fb00c3da98c4a808c7fad0bbe1f9a861003,"In this paper we continue to investigate how the deep neural network (DNN) based acoustic models for automatic speech recognition can be trained without hand-crafted feature extraction. Previously, we have shown that a simple fully connected feedforward DNN performs surprisingly well when trained directly on the raw time signal. The analysis of the weights revealed that the DNN has learned a kind of short-time time-frequency decomposition of the speech signal. In conventional feature extraction pipelines this is done manually by means of a filter bank that is shared between the neighboring analysis windows. Following this idea, we show that the performance gap between DNNs trained on spliced hand-crafted features and DNNs trained on raw time signal can be strongly reduced by introducing 1D-convolutional layers. Thus, the DNN is forced to learn a short-time filter bank shared over a longer time span. This also allows us to interpret the weights of the second convolutional layer in the same way as 2D patches learned on critical band energies by typical convolutional neural networks. The evaluation is performed on an English LVCSR task. Trained on the raw time signal, the convolutional layers allow to reduce the WER on the test set from 25.5% to 23.4%, compared to an MFCC based result of 22.1% using fully connected layers. Index Terms: acoustic modeling, raw time signal, convolutional neural networks","[{'authorId': '3207549', 'name': 'Pavel Golik'}, {'authorId': '1790221', 'name': 'Zoltán Tüske'}, {'authorId': '144490010', 'name': 'R. Schlüter'}, {'authorId': '145322333', 'name': 'H. Ney'}]",118.0,"{'bibtex': '@Inproceedings{Golik2015ConvolutionalNN,\n author = {Pavel Golik and Zoltán Tüske and R. Schlüter and H. Ney},\n pages = {26-30},\n title = {Convolutional neural networks for acoustic modeling of raw time signal in LVCSR},\n year = {2015}\n}\n'}",,{'pages': '26-30'},21.0,Convolutional neural networks for acoustic modeling of raw time signal in LVCSR,2015.0
2100,b219f0ddd236b446aa37244d947054a46dcbe555,"This article describes the virtual humans developed as part of the Mission Rehearsal Exercise project, a virtual reality-based training system. This project is an ambitious exercise in integration, both in the sense of integrating technology with entertainment industry content, but also in that we have joined a number of component technologies that have not been integrated before. This integration has not only raised new research issues, but it has also suggested some new approaches to difficult problems. We describe the key capabilities of the virtual humans, including task representation and reasoning, natural language dialogue, and emotion reasoning, and show how these capabilities are integrated to provide more human-level intelligence than would otherwise be possible.","[{'authorId': '1684040', 'name': 'W. Swartout'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1812270', 'name': 'R. Hill'}, {'authorId': '144547315', 'name': 'E. Hovy'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '144518646', 'name': 'D. Traum'}]",264.0,"{'bibtex': '@Article{Swartout2006TowardVH,\n author = {W. Swartout and J. Gratch and R. Hill and E. Hovy and S. Marsella and J. Rickel and D. Traum},\n journal = {AI Mag.},\n pages = {96-108},\n title = {Toward Virtual Humans},\n volume = {27},\n year = {2006}\n}\n'}",,"{'volume': '27', 'pages': '96-108', 'name': 'AI Mag.'}",45.0,Toward Virtual Humans,2006.0
2101,b255474d62f082fa97f50ea1174bf339522f6c99,"In interpersonal encounters, individuals often exhibit changes in their own facial expressions in response to emotional expressions of another person. Such changes are often called facial mimicry. While this tendency first appeared to be an automatic tendency of the perceiver to show the same emotional expression as the sender, evidence is now accumulating that situation, person, and relationship jointly determine whether and for which emotions such congruent facial behavior is shown. We review the evidence regarding the moderating influence of such factors on facial mimicry with a focus on understanding the meaning of facial responses to emotional expressions in a particular constellation. From this, we derive recommendations for a research agenda with a stronger focus on the most common forms of encounters, actual interactions with known others, and on assessing potential mediators of facial mimicry. We conclude that facial mimicry is modulated by many factors: attention deployment and sensitivity, detection of valence, emotional feelings, and social motivations. We posit that these are the more proximal causes of changes in facial mimicry due to changes in its social setting.","[{'authorId': '2848483', 'name': 'Beate Seibt'}, {'authorId': '1684604', 'name': 'A. Mühlberger'}, {'authorId': '4579242', 'name': 'Katja U. Likowski'}, {'authorId': '152592651', 'name': 'P. Weyers'}]",110.0,"{'bibtex': '@Article{Seibt2015FacialMI,\n author = {Beate Seibt and A. Mühlberger and Katja U. Likowski and P. Weyers},\n journal = {Frontiers in Psychology},\n title = {Facial mimicry in its social setting},\n volume = {6},\n year = {2015}\n}\n'}",,"{'volume': '6', 'name': 'Frontiers in Psychology'}",171.0,Facial mimicry in its social setting,2015.0
2102,b272c7fa96e2737803fb2640d3893f3a1b9d7d8e,"Ir we arc to understand fully any instance of a person's non-verbal behavior that is, any movement or position of the face and/or the bodywe must discover how that behavior became part of the penon's repertoire, the circumstances of its usc, and the rules which explain how the behavior contains or conveys information. We will call these three fundamental considerations ORIGIN, USAGE. and CODING. The interrelationships among and the differences within these three aspects of nonverbal behavior are extremely complex. The task of unraveling nonverbal behavior in these terms is enormously difficult; and it becomes impossible if we fail to consider the possibility of multiple categories of nonverbal behavior. The need to develop such a categorical scheme bas emerged from the results of our empirical studies over the past eight years, and has been crystallized by our two current research projects, the study of crosscultural differences in nonverbal behavior, and the study of nonverbal leakage of information during deceptive situations. We will briefly trace how some of the findings raised questions which led us to attempt to","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}]",2939.0,"{'bibtex': '@Article{Ekman1969TheRO,\n author = {P. Ekman and Wallace V. Friesen},\n journal = {Semiotica},\n pages = {49 - 98},\n title = {The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding},\n volume = {1},\n year = {1969}\n}\n'}",,"{'volume': '1', 'pages': '49 - 98', 'name': 'Semiotica'}",2.0,"The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding",1969.0
2103,b28d3bed00213d5407a4841919b9ba76cb672f69,"Using recent research, I argue that beliefs lie at the heart of personality and adaptive functioning and that they give us unique insight into how personality and functioning can be changed. I focus on two classes of beliefs—beliefs about the malleability of self-attributes and expectations of social acceptance versus rejection—and show how modest interventions have brought about important real-world changes. I conclude by suggesting that beliefs are central to the way in which people package their experiences and carry them forward, and that beliefs should play a more central role in the study of personality.","[{'authorId': '1392717415', 'name': 'C. Dweck'}]",383.0,"{'bibtex': '@Article{Dweck2008CanPB,\n author = {C. Dweck},\n journal = {Current Directions in Psychological Science},\n pages = {391 - 394},\n title = {Can Personality Be Changed? The Role of Beliefs in Personality and Change},\n volume = {17},\n year = {2008}\n}\n'}",,"{'volume': '17', 'pages': '391 - 394', 'name': 'Current Directions in Psychological Science'}",23.0,Can Personality Be Changed? The Role of Beliefs in Personality and Change,2008.0
2104,b2b1ecf267be0dcb454ae33fbd8545e8af88e23e,"We designed an observational study where participants (n = 17) were exposed to pictures and look-alike avatars pictures of themselves, a familiar friend or an unfamiliar person. By measuring participants’ brain activity with electroencephalography (EEG), we found face-recognition event related potentials (ERPs) in the visual cortex, around 200–250 ms, to be prominent for the different familiarity levels. A less positive component was found for self-recognized pictures (P200) than pictures of others, showing similar effects in both real faces and look-alike avatars. A rapid adaptation in the same component was found when comparing the neural processing of avatar faces vs. real faces, as if avatars in general were assimilated as real face representations over time. ERP results also showed that in the case of the self-avatar, the P200 component correlated with more complex conscious encodings of self-representation, i.e., the difference in voltage in the P200 between the self-avatar and the self-picture was reduced in participants that felt the avatar looked like them. This study is put into context within the literature of self-recognition and face recognition in the visual cortex. Additionally, the implications of these results on look-alike avatars are discussed both for future virtual reality (VR) and neuroscience studies.","[{'authorId': '1403064530', 'name': 'Mar González-Franco'}, {'authorId': '4413460', 'name': 'Anna I. Bellido'}, {'authorId': '34763211', 'name': 'K. J. Blom'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '1381777734', 'name': 'A. Rodríguez-Fornells'}]",29.0,"{'bibtex': '@Article{González-Franco2016TheNT,\n author = {Mar González-Franco and Anna I. Bellido and K. J. Blom and M. Slater and A. Rodríguez-Fornells},\n journal = {Frontiers in Human Neuroscience},\n title = {The Neurological Traces of Look-Alike Avatars},\n volume = {10},\n year = {2016}\n}\n'}",,"{'volume': '10', 'name': 'Frontiers in Human Neuroscience'}",74.0,The Neurological Traces of Look-Alike Avatars,2016.0
2105,b2c755445c3d5ccd2b1c315090da422a08e99145,"This paper presents Modelling Emotion Expression through Agent Oriented Methodology. Considering emotions of the intended users in the software engineering can uncover new requirements to improve and more accepted the system. While emotion is paying much attention nowadays, there is lacking systematic way to model the emotion based system. Without the systematic approach, it is hard to debug, design and develop an emotion based system. Since the emotional requirement of people has not being fully investigated, the research outcome propose the emotion modelling as part of the complete set of agent-oriented modelling for virtual character in eLearning system, The contribution of this paper is to introduce agent oriented modelling to systematic model an emotion based solution for an eLearning system and instructional video design. With the emotion model, it can serve as a guide to design, redesign, and discuss the emotion elements among the software development team. This is important for better debugging and project management especially for emotion led system.","[{'authorId': '1659202103', 'name': 'S. Zulkifli'}, {'authorId': '9340171', 'name': 'Cheah Wai Shiang'}, {'authorId': '9366035', 'name': 'Nurfauza Jali'}, {'authorId': '48920304', 'name': 'M. A. Khairuddin'}]",2.0,"{'bibtex': '@Article{Zulkifli2019ModellingEE,\n author = {S. Zulkifli and Cheah Wai Shiang and Nurfauza Jali and M. A. Khairuddin},\n booktitle = {Indonesian Journal of Electrical Engineering and Computer Science},\n journal = {Indonesian Journal of Electrical Engineering and Computer Science},\n title = {Modelling emotion expression through agent oriented methodology},\n year = {2019}\n}\n'}","[{'paperId': 'e936ddd003610a49e5c01c79bd51cfddce0f403a', 'title': 'Emotions in Requirements Engineering: A Systematic Mapping Study'}, {'paperId': '4e51a237071372395127996636e1518e403b0ee3', 'title': 'Modeling Emotion Oriented Approach through Agent-Oriented Approach'}]",{'name': 'Indonesian Journal of Electrical Engineering and Computer Science'},24.0,Modelling emotion expression through agent oriented methodology,2019.0
2106,b2c90eddd3b4627361f407d7948d5c52e3043028,"Developing an adequate and human-like virtual agent has been one of the primary applications of artificial intelligence. In the last few years, task-oriented dialogue systems have gained huge popularity because of their upsurging relevance and positive outcomes. In real-world, users may not always have a predefined and rigid task goal beforehand; they upgrade/downgrade/change their goal component dynamically depending upon their utility value and agent's serving capability. However, existing virtual agents fail to incorporate this dynamic behavior, leading to either unsuccessful task completion or an ungratified user experience. The paper presents an end to end multimodal dialogue system for dynamic and co-operative goal setting, which incorporates i) a multi-modal semantic state representation in policy learning to deal with multi-modal inputs, ii) a goal manager module in a traditional dialogue manager for handling dynamic and goal unavailability scenarios effectively, iii) an accumulative reward (task/persona/sentiment) for task success, personalized persuasion and user-adaptive behavior, respectively. The obtained experimental results and the comparisons with baselines firmly establish the need and efficacy of the proposed system.","[{'authorId': '2063522518', 'name': 'Abhisek Tiwari'}, {'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '2062808558', 'name': 'Shubhashis Sengupta'}, {'authorId': '40585053', 'name': 'Anutosh Maitra'}, {'authorId': '3040439', 'name': 'Roshni Ramnani'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",5.0,"{'bibtex': '@Article{Tiwari2021MultiModalDP,\n author = {Abhisek Tiwari and Tulika Saha and S. Saha and Shubhashis Sengupta and Anutosh Maitra and Roshni Ramnani and P. Bhattacharyya},\n journal = {2021 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Multi-Modal Dialogue Policy Learning for Dynamic and Co-operative Goal Setting},\n year = {2021}\n}\n'}",,"{'pages': '1-8', 'name': '2021 International Joint Conference on Neural Networks (IJCNN)'}",0.0,Multi-Modal Dialogue Policy Learning for Dynamic and Co-operative Goal Setting,2021.0
2107,b2dddbe65b156b77be3783cbf769c461cdc51251,"Crowd behaviour deviates from normal when an emergency evacuation is needed. Thus, simulation of evacuation situations has been identified as an important tool for assessing design choices of urban areas, such as buildings, stadiums, etc., and Agent Based Modelling has been employed to tackle such problems. In this paper, we propose that formal modelling can rigorously define but also naturally lead to realistic simulations of such cases. Our main contribution is presenting how formal state based methods, namely X-machines, can be employed to model agents in emergency evacuation plans. We also discuss the role of emotions, model artificial emotions that change the behaviour of agents under emergency situations, and provide a formalism that models the role of emotions and personality traits in order to create a more realistic scenario. Finally, we demonstrate how the developed formal models can be refined to code, a combination of Net logo and Prolog in this case, that is able to simulate crowd behaviour with and without artificial emotions.","[{'authorId': '3145061', 'name': 'I. Stamatopoulou'}, {'authorId': '2520399', 'name': 'I. Sakellariou'}, {'authorId': '145736293', 'name': 'P. Kefalas'}]",15.0,"{'bibtex': '@Article{Stamatopoulou2012FormalAM,\n author = {I. Stamatopoulou and I. Sakellariou and P. Kefalas},\n journal = {2012 IEEE 24th International Conference on Tools with Artificial Intelligence},\n pages = {1133-1138},\n title = {Formal Agent-Based Modelling and Simulation of Crowd Behaviour in Emergency Evacuation Plans},\n volume = {1},\n year = {2012}\n}\n'}",,"{'volume': '1', 'pages': '1133-1138', 'name': '2012 IEEE 24th International Conference on Tools with Artificial Intelligence'}",32.0,Formal Agent-Based Modelling and Simulation of Crowd Behaviour in Emergency Evacuation Plans,2012.0
2108,b2ed95f7e68865630df15add0852d2e246cc09c3,,"[{'authorId': '32761043', 'name': 'R. Petty'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}]",9167.0,"{'bibtex': '@Inproceedings{Petty1986TheEL,\n author = {R. Petty and J. Cacioppo},\n pages = {123-205},\n title = {The Elaboration Likelihood Model of Persuasion},\n year = {1986}\n}\n'}",,{'pages': '123-205'},266.0,The Elaboration Likelihood Model of Persuasion,1986.0
2109,b34bb33e6551387246721f332808fa4b857f4bd9,,"[{'authorId': '39047207', 'name': 'A. Sparks'}, {'authorId': '3378847', 'name': 'Tyler J. Burleigh'}, {'authorId': '3922648', 'name': 'Pat Barclay'}]",22.0,"{'bibtex': ""@Article{Sparks2016WeCS,\n author = {A. Sparks and Tyler J. Burleigh and Pat Barclay},\n journal = {Evolution and Human Behavior},\n pages = {210-216},\n title = {We can see inside: Accurate prediction of Prisoner's Dilemma decisions in announced games following a face-to-face interaction},\n volume = {37},\n year = {2016}\n}\n""}",,"{'volume': '37', 'pages': '210-216', 'name': 'Evolution and Human Behavior'}",68.0,We can see inside: Accurate prediction of Prisoner's Dilemma decisions in announced games following a face-to-face interaction,2016.0
2110,b351e23285facd1e4795836c349bc0d442488492,"With the ever increasing complexity of the user queries in a multi-domain based task-oriented dialogue system, it is imperative to facilitate robust Spoken Language Understanding (SLU) modules that perform multiple tasks in an unified way. In this paper, we present a novel multi-task approach for the joint modelling of three tasks together, namely, Domain Classification, Intent Detection and Slot-Filling. We hypothesize with the intuition that the cross dependencies of all these three tasks mutually help each other towards their representations and classifications which further simplify the SLU module in a multi-domain scenario. Towards this end, we propose a BERT language model based multi-task framework utilizing capsule networks and conditional random fields for addressing the classification and sequence labeling problems, respectively, for different tasks. Experimental results indicate that the proposed multi-task model outperformed several strong baselines and its single task counterparts on three benchmark datasets of different domains and attained state-of-the-art results on different tasks.","[{'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '2106126217', 'name': 'N. Priya'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",2.0,"{'bibtex': '@Article{Saha2021ATB,\n author = {Tulika Saha and N. Priya and S. Saha and P. Bhattacharyya},\n journal = {2021 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {A Transformer based Multi-task Model for Domain Classification, Intent Detection and Slot-Filling},\n year = {2021}\n}\n'}",,"{'pages': '1-8', 'name': '2021 International Joint Conference on Neural Networks (IJCNN)'}",37.0,"A Transformer based Multi-task Model for Domain Classification, Intent Detection and Slot-Filling",2021.0
2111,b35ef05cf12c8980684eb7e1d63452d3a5ffbb13,"Under what conditions will a bystander intervene to try to stop a violent attack by one person on another? It is generally believed that the greater the size of the crowd of bystanders, the less the chance that any of them will intervene. A complementary model is that social identity is critical as an explanatory variable. For example, when the bystander shares common social identity with the victim the probability of intervention is enhanced, other things being equal. However, it is generally not possible to study such hypotheses experimentally for practical and ethical reasons. Here we show that an experiment that depicts a violent incident at life-size in immersive virtual reality lends support to the social identity explanation. 40 male supporters of Arsenal Football Club in England were recruited for a two-factor between-groups experiment: the victim was either an Arsenal supporter or not (in-group/out-group), and looked towards the participant for help or not during the confrontation. The response variables were the numbers of verbal and physical interventions by the participant during the violent argument. The number of physical interventions had a significantly greater mean in the in-group condition compared to the out-group. The more that participants perceived that the Victim was looking to them for help the greater the number of interventions in the in-group but not in the out-group. These results are supported by standard statistical analysis of variance, with more detailed findings obtained by a symbolic regression procedure based on genetic programming. Verbal interventions made during their experience, and analysis of post-experiment interview data suggest that in-group members were more prone to confrontational intervention compared to the out-group who were more prone to make statements to try to diffuse the situation.","[{'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '2338435', 'name': 'Aitor Rovira'}, {'authorId': '144766839', 'name': 'Richard Southern'}, {'authorId': '2339529', 'name': 'David Swapp'}, {'authorId': '2151809664', 'name': 'J. Zhang'}, {'authorId': '30943266', 'name': 'C. Campbell'}, {'authorId': '144092773', 'name': 'M. Levine'}]",156.0,"{'bibtex': '@Article{Slater2013BystanderRT,\n author = {M. Slater and Aitor Rovira and Richard Southern and David Swapp and J. Zhang and C. Campbell and M. Levine},\n journal = {PLoS ONE},\n title = {Bystander Responses to a Violent Incident in an Immersive Virtual Environment},\n volume = {8},\n year = {2013}\n}\n'}",,"{'volume': '8', 'name': 'PLoS ONE'}",41.0,Bystander Responses to a Violent Incident in an Immersive Virtual Environment,2013.0
2112,b3b7b374eee3ea3c6fe35c1e7bdf9bf24c9456b0,,"[{'authorId': '84527386', 'name': 'R. Plutchik'}]",1735.0,"{'bibtex': '@Inproceedings{Plutchik1980AGP,\n author = {R. Plutchik},\n pages = {3-33},\n title = {A GENERAL PSYCHOEVOLUTIONARY THEORY OF EMOTION},\n year = {1980}\n}\n'}",,"{'volume': '', 'pages': '3-33', 'name': ''}",26.0,A GENERAL PSYCHOEVOLUTIONARY THEORY OF EMOTION,1980.0
2114,b3f6067a627b2c8952536c52414fddf892d735c6,,"[{'authorId': '1728894', 'name': 'C. Bartneck'}, {'authorId': '1768765', 'name': 'D. Kulić'}, {'authorId': '1735428', 'name': 'E. Croft'}, {'authorId': '3348963', 'name': 'Susana Zoghbi'}]",2041.0,"{'bibtex': '@Article{Bartneck2009MeasurementIF,\n author = {C. Bartneck and D. Kulić and E. Croft and Susana Zoghbi},\n journal = {International Journal of Social Robotics},\n pages = {71-81},\n title = {Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots},\n volume = {1},\n year = {2009}\n}\n'}",,"{'volume': '1', 'pages': '71-81', 'name': 'International Journal of Social Robotics'}",82.0,"Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots",2009.0
2115,b40ceb5ee02441b46a7b811a013d5d0df4f3639b,"We propose a real-time system that continuously recognizes emotions from body movements. The combined low-level 3D postural features and high-level kinematic and geometrical features are fed to a Random Forests classifier through summarization (statistical values) or aggregation (bag of features). In order to improve the generalization capability and the robustness of the system, a novel semisupervised adaptive algorithm is built on top of the conventional Random Forests classifier. The MoCap UCLIC affective gesture database (labeled with four emotions) was used to train the Random Forests classifier, which led to an overall recognition rate of 78% using a 10-fold cross-validation. Subsequently, the trained classifier was used in a stream-based semisupervised Adaptive Random Forests method for continuous unlabeled Kinect data classification. The very low update cost of our adaptive classifier makes it highly suitable for data stream applications. Tests performed on the publicly available emotion datasets (body gestures and facial expressions) indicate that our new classifier outperforms existing algorithms for data streams in terms of accuracy and computational costs.","[{'authorId': '2108446907', 'name': 'Weiyi Wang'}, {'authorId': '47822555', 'name': 'V. Enescu'}, {'authorId': '48077408', 'name': 'H. Sahli'}]",33.0,"{'bibtex': '@Article{Wang2016AdaptiveRE,\n author = {Weiyi Wang and V. Enescu and H. Sahli},\n journal = {ACM Trans. Interact. Intell. Syst.},\n pages = {18:1-18:21},\n title = {Adaptive Real-Time Emotion Recognition from Body Movements},\n volume = {5},\n year = {2016}\n}\n'}",,"{'volume': '5', 'pages': '18:1-18:21', 'name': 'ACM Trans. Interact. Intell. Syst.'}",65.0,Adaptive Real-Time Emotion Recognition from Body Movements,2016.0
2116,b43f9b861cb3959fb3a4891ef58c24ed1f64e461,"The delivery of mental health interventions via ubiquitous devices has shown much promise. A conversational chatbot is a promising oracle for delivering appropriate just-in-time interventions. However, designing emotionally-aware agents, specially in this context, is under-explored. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week long human-subject experiment with N=39 participants. EMMA provides emotionally appropriate micro-activities in an empathetic manner. We show that the system can be extended to detect a user's mood purely from smartphone sensor data. Our results show that our personalized machine learning model was perceived as likable via self-reports of emotion from users. Finally, we provide a set of guidelines for the design of emotion-aware bots for mHealth.","[{'authorId': '2214185', 'name': 'Asma Ghandeharioun'}, {'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '1817251', 'name': 'M. Czerwinski'}, {'authorId': '36516124', 'name': 'Kael Rowan'}]",55.0,"{'bibtex': '@Article{Ghandeharioun2018EMMAAE,\n author = {Asma Ghandeharioun and Daniel J. McDuff and M. Czerwinski and Kael Rowan},\n journal = {2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {1-7},\n title = {EMMA: An Emotion-Aware Wellbeing Chatbot},\n year = {2018}\n}\n'}",,"{'pages': '1-7', 'name': '2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)'}",52.0,EMMA: An Emotion-Aware Wellbeing Chatbot,2018.0
2117,b46194ac5696379cfa920ff08cda8d7c4cb6579c,"In this paper, we address the issue of augmenting text data in supervised Natural Language Processing problems, exemplified by deep online hate speech classification. A great challenge in this domain is that although the presence of hate speech can be deleterious to the quality of service provided by social platforms, it still comprises only a tiny fraction of the content that can be found online, which can lead to performance deterioration due to majority class overfitting. To this end, we perform a thorough study on the application of deep learning to the hate speech detection problem: a) we propose three text-based data augmentation techniques aimed at reducing the degree of class imbalance and to maximise the amount of information we can extract from our limited resources and b) we apply them on a selection of top-performing deep architectures and hate speech databases in order to showcase their generalisation properties. The data augmentation techniques are based on a) synonym replacement based on word embedding vector closeness, b) warping of the word tokens along the padded sequence or c) class-conditional, recurrent neural language generation. Our proposed framework yields a significant increase in multi-class hate speech detection, outperforming the baseline in the largest online hate speech database by an absolute 5.7% increase in Macro-F1 score and 30% in hate speech class recall.","[{'authorId': '40185455', 'name': 'Georgios Rizos'}, {'authorId': '2121244573', 'name': 'Konstantin Hemker'}, {'authorId': '145411696', 'name': 'Björn Schuller'}]",78.0,"{'bibtex': '@Article{Rizos2019AugmentTP,\n author = {Georgios Rizos and Konstantin Hemker and Björn Schuller},\n journal = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},\n title = {Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification},\n year = {2019}\n}\n'}",,{'name': 'Proceedings of the 28th ACM International Conference on Information and Knowledge Management'},50.0,Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification,2019.0
2118,b48f1a8a99a64346232e4ebb9e4105aeb62a81a2,"Automatic synthesis of realistic gestures promises to transform the fields of animation, avatars and communicative agents. In off‐line applications, novel tools can alter the role of an animator to that of a director, who provides only high‐level input for the desired animation; a learned network then translates these instructions into an appropriate sequence of body poses. In interactive scenarios, systems for generating natural animations on the fly are key to achieving believable and relatable characters. In this paper we address some of the core issues towards these ends. By adapting a deep learning‐based motion synthesis method called MoGlow, we propose a new generative model for generating state‐of‐the‐art realistic speech‐driven gesticulation. Owing to the probabilistic nature of the approach, our model can produce a battery of different, yet plausible, gestures given the same input speech signal. Just like humans, this gives a rich natural variation of motion. We additionally demonstrate the ability to exert directorial control over the output style, such as gesture level, speed, symmetry and spacial extent. Such control can be leveraged to convey a desired character personality or mood. We achieve all this without any manual annotation of the data. User studies evaluating upper‐body gesticulation confirm that the generated motions are natural and well match the input speech. Our method scores above all prior systems and baselines on these measures, and comes close to the ratings of the original recorded motions. We furthermore find that we can accurately control gesticulation styles without unnecessarily compromising perceived naturalness. Finally, we also demonstrate an application of the same method to full‐body gesticulation, including the synthesis of stepping motion and stance.","[{'authorId': '1717035', 'name': 'Simon Alexanderson'}, {'authorId': '2763884', 'name': 'G. Henter'}, {'authorId': '145372964', 'name': 'Taras Kucherenko'}, {'authorId': '1826819', 'name': 'J. Beskow'}]",134.0,"{'bibtex': '@Article{Alexanderson2020StyleControllableSG,\n author = {Simon Alexanderson and G. Henter and Taras Kucherenko and J. Beskow},\n journal = {Computer Graphics Forum},\n title = {Style‐Controllable Speech‐Driven Gesture Synthesis Using Normalising Flows},\n volume = {39},\n year = {2020}\n}\n'}",,"{'volume': '39', 'name': 'Computer Graphics Forum'}",99.0,Style‐Controllable Speech‐Driven Gesture Synthesis Using Normalising Flows,2020.0
2119,b49e195843aa588e9deb9138e5d7d4a5c377e795,"Affective Computing is a growing multidisciplinary field encompassing computer science, engineering, psychology, education, neuroscience, and many other disciplines. It explores how affective factors influence interactions between humans and technology, how affect sensing and affect generation techniques can inform our understanding of human affect, and on the design, implementation, and evaluation of systems that intricately involve affect at their core. The Oxford Handbook of Affective Computing will help both new and experienced researchers identify trends, concepts, methodologies, and applications in this burgeouning field. The volume features 41 chapters divided into five main sections: history and theory, detection, generation, methodologies, and applications. Section One begins with a look at the makings of AC and a historical review of the science of emotion. Chapters discuss the theoretical underpinnings of AC from an interdisciplinary perspective involving the affective, cognitive, social, media, and brain sciences. Section Two focuses on affect detection or affect recognition, which is one of the most commonly investigated areas in AC. Section Three examines aspects of affect generation including the synthesis of emotion and its expression via facial features, speech, postures and gestures. Cultural issues in affect generation are also discussed. Section Four features chapters on methodological issues in AC research, including data collection techniques, multimodal affect databases, emotion representation formats, crowdsourcing techniques, machine learning approaches, affect elicitation techniques, useful AC tools, and ethical issues in AC. Finally, Section Five highlights existing and future applications of AC in domains such as formal and informal learning, games, robotics, virtual reality, autism research, healthcare, cyberpsychology, music, deception, reflective writing, and cyberpsychology.With chapters authored by world leaders in each area, The Oxford Handbook of Affective Computing is suitable for use as a textbook in undergraduate or graduate courses in AC, and will serve as a valuable resource for students, researchers, and practitioners across the globe.","[{'authorId': '144792845', 'name': 'R. Calvo'}, {'authorId': '1383996606', 'name': 'S. D’Mello'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1742554', 'name': 'Arvid Kappas'}]",336.0,"{'bibtex': '@Inproceedings{Calvo2014TheOH,\n author = {R. Calvo and S. D’Mello and J. Gratch and Arvid Kappas},\n title = {The Oxford Handbook of Affective Computing},\n year = {2014}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,The Oxford Handbook of Affective Computing,2014.0
2121,b4a645ca958f1067594bb5c82aad322f2bbbe1a8,,"[{'authorId': '1807752', 'name': 'F. D. Rosis'}, {'authorId': '1755446', 'name': 'C. Castelfranchi'}, {'authorId': '49005891', 'name': 'P. Goldie'}, {'authorId': '1694255', 'name': 'V. Carofiglio'}]",14.0,"{'bibtex': '@Inproceedings{Rosis2011CognitiveEA,\n author = {F. D. Rosis and C. Castelfranchi and P. Goldie and V. Carofiglio},\n pages = {459-481},\n title = {Cognitive Evaluations and Intuitive Appraisals: Can Emotion Models Handle Them Both?},\n year = {2011}\n}\n'}",,"{'volume': '', 'pages': '459-481', 'name': ''}",48.0,Cognitive Evaluations and Intuitive Appraisals: Can Emotion Models Handle Them Both?,2011.0
2122,b4a7398788d8cc76756692a8c218b62b58ba73fd,"Emotions exert a profound influence on cognitive processes, both the fundamental processes mediating cognition, such as attention and memory, and higher-level processes including decision-making and learning. A number of emotion effects on cognition have been identified, but their mechanisms are not yet understood. In this paper I describe a methodology for modeling the effects of emotion on cognition, within a symbolic cognitive-affective architecture. The primary objective of the approach is to facilitate the construction of alternative mechanisms of observed emotion effects. The paper describes how the effects of anxiety are modeled and how alternative mechanisms of these effects can be explored.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",36.0,"{'bibtex': '@Inproceedings{Hudlicka2008ModelingTM,\n author = {E. Hudlicka},\n pages = {82-86},\n title = {Modeling the Mechanisms of Emotion Effects on Cognition},\n year = {2008}\n}\n'}",,{'pages': '82-86'},19.0,Modeling the Mechanisms of Emotion Effects on Cognition,2008.0
2123,b4d1afd621839ee49d64ea5eea91b10d6231db12,"The sixth Audio-Visual Emotion Challenge and workshop AVEC 2016 was held in conjunction ACM Multimedia'16. This year the AVEC series addresses two distinct sub-challenges, multi-modal emotion recognition and audio-visual depression detection. Both sub-challenges are in a way a return to AVEC's past editions: the emotion sub-challenge is based on the same dataset as the one used in AVEC 2015, and depression analysis was previously addressed in AVEC 2013/2014. In this summary, we mainly describe participation and its conditions.","[{'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",100.0,"{'bibtex': '@Article{Valstar2016SummaryFA,\n author = {M. Valstar and J. Gratch and Björn Schuller and F. Ringeval and R. Cowie and M. Pantic},\n journal = {Proceedings of the 24th ACM international conference on Multimedia},\n title = {Summary for AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the 24th ACM international conference on Multimedia'},12.0,"Summary for AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge",2016.0
2124,b4dc56f66aa67bc92cbb7c2dd98e93055f1b4fb0,"Core Affect is a state accessible to consciousness as a single simple feeling (feeling good or bad, energized or enervated) that can vary from moment to moment and that is the heart of, but not the whole of, mood and emotion. In four correlational studies (Ns = 535, 190, 234, 395), a 12-Point Affect Circumplex (12-PAC) model of Core Affect was developed that is finer grained than previously available and that integrates major dimensional models of mood and emotion. Self-report scales in three response formats were cross-validated for Core Affect felt during current and remembered moments. A technique that places any external variable into the 12-PAC showed that 29 of 38 personality scales and 30 of 30 mood scales are significantly related to Core Affect, but not in a way that revealed its basic dimensions.","[{'authorId': '6618455', 'name': 'Michelle Yik'}, {'authorId': '46367714', 'name': 'J. Russell'}, {'authorId': '48654914', 'name': 'J. H. Steiger'}]",437.0,"{'bibtex': '@Article{Yik2011A1C,\n author = {Michelle Yik and J. Russell and J. H. Steiger},\n journal = {Emotion},\n pages = {\n          705-31\n        },\n title = {A 12-Point Circumplex Structure of Core Affect.},\n volume = {11 4},\n year = {2011}\n}\n'}",,"{'volume': '11 4', 'pages': '\n          705-31\n        ', 'name': 'Emotion'}",119.0,A 12-Point Circumplex Structure of Core Affect.,2011.0
2125,b4dd665d32c09cc0995c26346074e7f2c7fb0c4c,"In this paper, we analyze face-to-face negotiation interactions with the goal of predicting the respondent's immediate reaction (i.e., accept or reject) to a negotiation offer. Supported by the theory of social rapport, we focus on mutual behaviors which are defined as nonverbal characteristics that occur due to interactional influence. These patterns include behavioral symmetry (e.g., synchronized smiles) as well as asymmetry (e.g., opposite postures) between the two negotiators. In addition, we put emphasis on finding audio-visual mutual behaviors that can be extracted automatically, with the vision of a real-time decision support tool. We introduce a dyadic negotiation dataset consisting of 42 face-to-face interactions and show experiments confirming the importance of multimodal and mutual behaviors.","[{'authorId': '2108106092', 'name': 'Sunghyun Park'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",23.0,"{'bibtex': '@Article{Park2013MutualBD,\n author = {Sunghyun Park and Stefan Scherer and J. Gratch and P. Carnevale and Louis-Philippe Morency},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {423-428},\n title = {Mutual Behaviors during Dyadic Negotiation: Automatic Prediction of Respondent Reactions},\n year = {2013}\n}\n'}",,"{'pages': '423-428', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",29.0,Mutual Behaviors during Dyadic Negotiation: Automatic Prediction of Respondent Reactions,2013.0
2126,b4edc00960b3fb83dcf48c73ee09e8486a1cdf64,"In this paper, we propose a method for using particle swarm optimization (PSO) to compute optimal guidance paths for various crowd densities in an agent‐based crowd simulation. The inputs of our system are guidance paths that provide hints for the movement directions of agents. Input guidance paths may not be located correctly (e.g., leading to congestion or high traveling cost); therefore, our method adjusts the guidance paths by using PSO. We consider several factors for evaluating the quality of a guidance path, including the average traveling time and interaction distance between agents. We apply our method in several examples. Experimental results show that our method can compute adaptive guidance paths for various crowd densities. Our system can simulate organized crowds that move in directions specified by the guidance paths. Copyright © 2015 John Wiley & Sons, Ltd.","[{'authorId': '1992587', 'name': 'Sai-Keung Wong'}, {'authorId': '2464268', 'name': 'P. Tang'}, {'authorId': '2149811678', 'name': 'Fu-Shun Li'}, {'authorId': '2108184118', 'name': 'Zong-Min Wang'}, {'authorId': '12493038', 'name': 'Shi Yu'}]",17.0,"{'bibtex': '@Article{Wong2015GuidancePS,\n author = {Sai-Keung Wong and P. Tang and Fu-Shun Li and Zong-Min Wang and Shi Yu},\n journal = {Computer Animation and Virtual Worlds},\n pages = {387 - 395},\n title = {Guidance path scheduling using particle swarm optimization in crowd simulation},\n volume = {26},\n year = {2015}\n}\n'}",,"{'volume': '26', 'pages': '387 - 395', 'name': 'Computer Animation and Virtual Worlds'}",20.0,Guidance path scheduling using particle swarm optimization in crowd simulation,2015.0
2127,b4f7cb021d130ca7f8949d53b746d60b216ce14c,"Participants compared the mental capacities of various human and nonhuman characters via online surveys. Factor analysis revealed two dimensions of mind perception, Experience (for example, capacity for hunger) and Agency (for example, capacity for self-control). The dimensions predicted different moral judgments but were both related to valuing of mind.","[{'authorId': '2676083', 'name': 'H. Gray'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1810430', 'name': 'D. Wegner'}]",1314.0,"{'bibtex': '@Article{Gray2007DimensionsOM,\n author = {H. Gray and Kurt Gray and D. Wegner},\n journal = {Science},\n pages = {619 - 619},\n title = {Dimensions of Mind Perception},\n volume = {315},\n year = {2007}\n}\n'}",,"{'volume': '315', 'pages': '619 - 619', 'name': 'Science'}",12.0,Dimensions of Mind Perception,2007.0
2128,b5201d1d70c8b0285d2b741e90ed140696f03251,"Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournament show how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease.","[{'authorId': '2263215799', 'name': 'Robert M. May'}]",21817.0,"{'bibtex': '@Article{May1981TheEO,\n author = {Robert M. May},\n journal = {Nature},\n pages = {291-292},\n title = {The evolution of cooperation},\n volume = {292},\n year = {1981}\n}\n'}",,"{'volume': '292', 'pages': '291-292', 'name': 'Nature'}",231.0,The evolution of cooperation,1981.0
2129,b524d8050121296244b502181e72717f50d44335,"Background. Mirror visual feedback (MVF), a phenomenon where movement of one limb is perceived as movement of the other limb, has the capacity to alleviate phantom limb pain or promote motor recovery of the upper limbs after stroke. The tool has received great interest from health professionals; however, a clear understanding of the mechanisms underlying the neural recovery owing to MVF is lacking. Objective. We performed a systematic review to assess the effect of MVF on brain activation during a motor task. Methods. We searched PubMed, CINAHL, and EMBASE databases for neuroimaging studies investigating the effect of MVF on the brain. Key details for each study regarding participants, imaging methods, and results were extracted. Results. The database search yielded 347 article, of which we identified 33 suitable for inclusion. Compared with a control condition, MVF increases neural activity in areas involved with allocation of attention and cognitive control (dorsolateral prefrontal cortex, posterior cingulate cortex, S1 and S2, precuneus). Apart from activation in the superior temporal gyrus and premotor cortex, there is little evidence that MVF activates the mirror neuron system. MVF increases the excitability of the ipsilateral primary motor cortex (M1) that projects to the “untrained” hand/arm. There is also evidence for ipsilateral projections from the contralateral M1 to the untrained/affected hand as a consequence of training with MVF. Conclusion. MVF can exert a strong influence on the motor network, mainly through increased cognitive penetration in action control, though the variance in methodology and the lack of studies that shed light on the functional connectivity between areas still limit insight into the actual underlying mechanisms.","[{'authorId': '2741246', 'name': 'F. Deconinck'}, {'authorId': '3999271', 'name': 'A. Smorenburg'}, {'authorId': '2338075', 'name': 'A. Benham'}, {'authorId': '3688722', 'name': 'A. Ledebt'}, {'authorId': '5116312', 'name': 'M. Feltham'}, {'authorId': '3765596', 'name': 'G. Savelsbergh'}]",158.0,"{'bibtex': '@Article{Deconinck2015ReflectionsOM,\n author = {F. Deconinck and A. Smorenburg and A. Benham and A. Ledebt and M. Feltham and G. Savelsbergh},\n journal = {Neurorehabilitation and Neural Repair},\n pages = {349 - 361},\n title = {Reflections on Mirror Therapy},\n volume = {29},\n year = {2015}\n}\n'}",,"{'volume': '29', 'pages': '349 - 361', 'name': 'Neurorehabilitation and Neural Repair'}",88.0,Reflections on Mirror Therapy,2015.0
2130,b52db024be11491393925344dee197f91b9b2221,,"[{'authorId': '49830492', 'name': 'Denis Bouchard'}]",29.0,"{'bibtex': '@Article{Bouchard2013SignL,\n author = {Denis Bouchard},\n journal = {Sign Language Studies},\n pages = {101 - 160},\n title = {Sign Languages & Language Universals: The Status of Order & Position in Grammar},\n volume = {91},\n year = {2013}\n}\n'}",,"{'volume': '91', 'pages': '101 - 160', 'name': 'Sign Language Studies'}",21.0,Sign Languages & Language Universals: The Status of Order & Position in Grammar,2013.0
2131,b54bcfca3fddc26b8889739a247a25e445818149,"From the Publisher: 
This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.","[{'authorId': '1746807', 'name': 'Dan Jurafsky'}, {'authorId': '10796472', 'name': 'James H. Martin'}]",4066.0,"{'bibtex': '@Inproceedings{Jurafsky2000SpeechAL,\n author = {Dan Jurafsky and James H. Martin},\n pages = {I-XXVI, 1-934},\n title = {Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition},\n year = {2000}\n}\n'}",,"{'pages': 'I-XXVI, 1-934'}",261.0,"Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition",2000.0
2132,b54f2fd56ec084aa8e1a3b05dd24d93ecd5c945a,Always Happy Hour—Mary U. Miller . . . . . . . . . . . . . . . . . . . . . . . .5 American War—Omar El Akkad . . . . . . . . . . . . . . . . . . . . . . . . . . .9 An American Sickness: How Healthcare Became Big Business and How You Can Take It Back—Elisabeth Rosenthal . . . . . . . . . . . . . . . . . . . . . . . 13 An Extraordinary Union—Alyssa Cole . . . . . . . . . . . . . . . . . . . . . . . 18 Anatomy of Terror: From the Death of Bin Laden to the Rise of the Islamic State—Ali Soufan . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Anything Is Possible—Elizabeth Strout . . . . . . . . . . . . . . . . . . . . . . 27 Apollo 8: The Thrilling Story of the First Mission to the Moon—Jeff rey Kluger . 32 Astrophysics for People in a Hurry—Neil deGrasse Tyson . . . . . . . . . . . . . 37,"[{'authorId': '1731779', 'name': 'L. F. Barrett'}]",881.0,"{'bibtex': '@Inproceedings{Barrett2017HowEA,\n author = {L. F. Barrett},\n title = {How Emotions Are Made: The Secret Life of the Brain},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,How Emotions Are Made: The Secret Life of the Brain,2017.0
2133,b5880e5f19bfe89b51cbeeeaf60d39e6b9f36a41,"The layout of a building, real or virtual, affects the flow patterns of its intended users. It is well established, for example, that the placement of pillars at proper locations can often facilitate pedestrian flow during the evacuation of a building. Such considerations are therefore important for architects, game level developers, and others whose domains involve agents navigating through buildings. In this paper, we take the first steps towards developing a simulation framework that can be used to study the optimal placement of architectural elements, such as pillars or doors, for the purposes of facilitating dense pedestrian flow during the evacuation of a building. In particular, we show that the steering algorithms used to model the local navigation abilities of the agents significantly affect the results, which motivates the need for a statistically valid approach and further study. Copyright © 2015 John Wiley & Sons, Ltd.","[{'authorId': '2994035', 'name': 'G. Berseth'}, {'authorId': '145274939', 'name': 'Muhammad Usman'}, {'authorId': '38590145', 'name': 'M. B. Haworth'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '1737527', 'name': 'P. Faloutsos'}]",41.0,"{'bibtex': '@Article{Berseth2015EnvironmentOF,\n author = {G. Berseth and Muhammad Usman and M. B. Haworth and Mubbasir Kapadia and P. Faloutsos},\n journal = {Computer Animation and Virtual Worlds},\n pages = {377 - 386},\n title = {Environment optimization for crowd evacuation},\n volume = {26},\n year = {2015}\n}\n'}",,"{'volume': '26', 'pages': '377 - 386', 'name': 'Computer Animation and Virtual Worlds'}",37.0,Environment optimization for crowd evacuation,2015.0
2134,b58b5a7bd4ad3dd0d8a821b83acfb9b69196276e,"In this article, we investigate the perception of gender from the motion of virtual humans under different emotional conditions and explore the effect of emotional bias on gender perception (e.g., anger being attributed to males more than females). As motion types can present different levels of physiological cues, we also explore how two types of motion (walking and conversations) are affected by emotional bias. Walking typically displays more physiological cues about gender (e.g., hip sway) and therefore is expected to be less affected by emotional bias. To investigate these effects, we used a corpus of captured facial and body motions from four male and four female actors, performing basic emotions through conversation and walk. We expected that the appearance of the model would also influence gender perception; therefore, we displayed both male and female motions on two virtual models of different sex. Two experiments were then conducted to assess gender judgments from these motions. In both experiments, participants were asked to rate how male or female they considered the motions to be under different emotional states, then classified the emotions to determine how accurately they were portrayed by actors. Overall, both experiments showed that gender ratings were affected by the displayed emotion. However, we found that conversations were influenced by gender stereotypes to a greater extent than walking motions. This was particularly true for anger, which was perceived as male on both male and female motions, and sadness, which was perceived as less male when portrayed by male actors. We also found a slight effect of the model when observing gender on different types of virtual models. These results have implications for the design and animation of virtual humans.","[{'authorId': '1710384', 'name': 'Katja Zibrek'}, {'authorId': '1869571', 'name': 'Ludovic Hoyet'}, {'authorId': '38592832', 'name': 'K. Ruhland'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",24.0,"{'bibtex': '@Article{Zibrek2015ExploringTE,\n author = {Katja Zibrek and Ludovic Hoyet and K. Ruhland and R. Mcdonnell},\n journal = {ACM Transactions on Applied Perception (TAP)},\n pages = {1 - 20},\n title = {Exploring the Effect of Motion Type and Emotions on the Perception of Gender in Virtual Humans},\n volume = {12},\n year = {2015}\n}\n'}",,"{'volume': '12', 'pages': '1 - 20', 'name': 'ACM Transactions on Applied Perception (TAP)'}",38.0,Exploring the Effect of Motion Type and Emotions on the Perception of Gender in Virtual Humans,2015.0
2135,b5ac51cef9564204b4dc615e3d5884d47bc23312,"- Various studies have been undertaken to adapt Augmented Reality (AR) technology for use in education. We see AR as being suitable for creating an enjoyable learning experience (edutainment) for students. In this study, we developed an AR application based on the same content as conventional printed teaching material focusing on the field of foreign language study. The learning efficacy of the two media was assessed by comparing verification test results and monitoring brain activity during the learning process. The results show that there is no significant difference in test results between the two media. However, we found that the subjects’ brains were more active while studying the printed teaching materials than the AR teaching materials. We believe this shows that the proposed method of study is overall a more natural one and, when compared with traditional methods of study, has the potential to be less stressful for students.","[{'authorId': '2988760', 'name': 'T. Miyosawa'}, {'authorId': '98088518', 'name': 'Mayuko Akahane'}, {'authorId': '49706232', 'name': 'Kentaro Hara'}, {'authorId': '4765645', 'name': 'Kikunori Shinohara'}]",11.0,"{'bibtex': '@Inproceedings{Miyosawa2012ApplyingAR,\n author = {T. Miyosawa and Mayuko Akahane and Kentaro Hara and Kikunori Shinohara},\n title = {Applying Augmented Reality to E-Learning for Foreign Language Study and its Evaluation},\n year = {2012}\n}\n'}",,,15.0,Applying Augmented Reality to E-Learning for Foreign Language Study and its Evaluation,2012.0
2136,b5b6d87f663fcc56cd7319ea49d6baaa653ecaae,"Zusammenfassung Die menschliche Mimik ist einzigartig in ihrer Fähigkeit unseren Emotionen Ausdruck zu verleihen und diese anderen Menschen zu übermitteln. Die mimische Expression grundlegender Emotionen ist über verschiedene Kulturen hinweg sehr ähnlich und du weist auch Gemeinsamkeiten zu anderen Säugetieren auf. Dies deutet auf einen gemeinsamen genetischen Ursprung des Zusammenhangs von Mimik und Emotion. Neuere Untersuchungen zeigen aber auch kulturelle Einflüsse und Unterschiede. Die Erkennung von Emotionen aus der Mimik und auch der Prozess des mimischen Ausdrucks der eigenen Emotionen erfolgt in einem äußerst komplexen zerebralen Netzwerk. Aufgrund der Komplexität des zerebralen Verarbeitungssystems gibt es eine Vielzahl von neurologischen und psychiatrischen Erkrankungen, welche die Kopplung von Mimik und Emotionen erheblich stören können. Auch durch das Tragen von Masken wird unsere Fähigkeit zur Übermittlung und zum Erkennen von Emotionen über die Mimik eingeschränkt. Durch die Mimik lassen sich aber nicht nur „echte“ Emotionen ausdrücken, sondern auch gespielte. Damit eröffnet die Mimik die Möglichkeit sozial erwünschten Ausdruck vorzuspielen und auch Emotionen bewusst vorzutäuschen. Diese Täuschungen sind jedoch zumeist nicht perfekt und können von kurzfristigen Gesichtsbewegungen begleitet sein, die auf die tatsächlich vorhandenen Emotionen hinweisen (Mikroexpressionen). Diese Mikroexpressionen sind von nur sehr kurzer Dauer und vom Menschen häufig kaum wahrnehmbar, jedoch das ideale Anwendungsgebiet für computergestützte Analysen. Diese automatische Identifikation von Mikroexpressionen hat in den letzten Jahren nicht nur wissenschaftliche Aufmerksamkeit erfahren, sondern ihr Einsatz wird auch in sicherheitsrelevanten Bereichen getestet. Der vorliegende Artikel fasst den aktuellen Wissensstand von Mimik und Emotionen zusammen. ABSTRACT Human facial expressions are unique in their ability to express our emotions and communicate them to others. The mimic expression of basic emotions is very similar across different cultures and has also many features in common with other mammals. This suggests a common genetic origin of the association between facial expressions and emotion. However, recent studies also show cultural influences and differences. The recognition of emotions from facial expressions, as well as the process of expressing one’s emotions facially, occurs within an extremely complex cerebral network. Due to the complexity of the cerebral processing system, there are a variety of neurological and psychiatric disorders that can significantly disrupt the coupling of facial expressions and emotions. Wearing masks also limits our ability to convey and recognize emotions through facial expressions. Through facial expressions, however, not only “real” emotions can be expressed, but also acted ones. Thus, facial expressions open up the possibility of faking socially desired expressions and also of consciously faking emotions. However, these pretenses are mostly imperfect and can be accompanied by short-term facial movements that indicate the emotions that are actually present (microexpressions). These microexpressions are of very short duration and often barely perceptible by humans, but they are the ideal application area for computer-aided analysis. This automatic identification of microexpressions has not only received scientific attention in recent years, but its use is also being tested in security-related areas. This article summarizes the current state of knowledge of facial expressions and emotions.","[{'authorId': '47016130', 'name': 'C. Klingner'}, {'authorId': '2149403111', 'name': 'O. Guntinas-Lichius'}]",1351.0,"{'bibtex': '@Article{Klingner2023FacialEA,\n author = {C. Klingner and O. Guntinas-Lichius},\n journal = {Laryngo- Rhino- Otologie},\n pages = {S115 - S125},\n title = {Facial expression and emotion},\n volume = {102},\n year = {2023}\n}\n'}",,"{'volume': '102', 'pages': 'S115 - S125', 'name': 'Laryngo- Rhino- Otologie'}",131.0,Facial expression and emotion,2023.0
2137,b5cc9d656be7520d7cc2dbdce539d9dcb9e11b9f,,"[{'authorId': '6637398', 'name': 'G. Schoretsanitis'}, {'authorId': '46532191', 'name': 'A. Kutynia'}, {'authorId': '3872564', 'name': 'K. Stegmayer'}, {'authorId': '2017213', 'name': 'W. Strik'}, {'authorId': '1706534', 'name': 'S. Walther'}]",35.0,"{'bibtex': '@Article{Schoretsanitis2016KeepAB,\n author = {G. Schoretsanitis and A. Kutynia and K. Stegmayer and W. Strik and S. Walther},\n journal = {European Psychiatry},\n pages = {1 - 7},\n title = {Keep at bay! – Abnormal personal space regulation as marker of paranoia in schizophrenia},\n volume = {31},\n year = {2016}\n}\n'}",,"{'volume': '31', 'pages': '1 - 7', 'name': 'European Psychiatry'}",41.0,Keep at bay! – Abnormal personal space regulation as marker of paranoia in schizophrenia,2016.0
2138,b60840f5061d705e74e8b2c9bc3bbd61e85b49ec,,"[{'authorId': '46222184', 'name': 'J. Brosig'}]",223.0,"{'bibtex': ""@Article{Brosig2002IdentifyingCB,\n author = {J. Brosig},\n journal = {Journal of Economic Behavior and Organization},\n pages = {275-290},\n title = {Identifying cooperative behavior: some experimental results in a prisoner's dilemma game},\n volume = {47},\n year = {2002}\n}\n""}",,"{'volume': '47', 'pages': '275-290', 'name': 'Journal of Economic Behavior and Organization'}",60.0,Identifying cooperative behavior: some experimental results in a prisoner's dilemma game,2002.0
2139,b638b5fd3cf2d1c4361ca5f8fc8f5c91a64a4e7b,"We present the Computer Expression Recognition Toolbox (CERT), a software tool for fully automatic real-time facial expression recognition, and officially release it for free academic use. CERT can automatically code the intensity of 19 different facial actions from the Facial Action Unit Coding System (FACS) and 6 different protoypical facial expressions. It also estimates the locations of 10 facial features as well as the 3-D orientation (yaw, pitch, roll) of the head. On a database of posed facial expressions, Extended Cohn-Kanade (CK+ [1]), CERT achieves an average recognition performance (probability of correctness on a two-alternative forced choice (2AFC) task between one positive and one negative example) of 90.1% when analyzing facial actions. On a spontaneous facial expression dataset, CERT achieves an accuracy of nearly 80%. In a standard dual core laptop, CERT can process 320 × 240 video images in real time at approximately 10 frames per second.","[{'authorId': '2724380', 'name': 'G. Littlewort'}, {'authorId': '143973061', 'name': 'J. Whitehill'}, {'authorId': '4072965', 'name': 'Tingfan Wu'}, {'authorId': '2039025', 'name': 'Ian R. Fasel'}, {'authorId': '145595070', 'name': 'M. Frank'}, {'authorId': '1741200', 'name': 'J. Movellan'}, {'authorId': '2218905', 'name': 'M. Bartlett'}]",575.0,"{'bibtex': '@Article{Littlewort2011TheCE,\n author = {G. Littlewort and J. Whitehill and Tingfan Wu and Ian R. Fasel and M. Frank and J. Movellan and M. Bartlett},\n journal = {Face and Gesture 2011},\n pages = {298-305},\n title = {The computer expression recognition toolbox (CERT)},\n year = {2011}\n}\n'}",,"{'pages': '298-305', 'name': 'Face and Gesture 2011'}",31.0,The computer expression recognition toolbox (CERT),2011.0
2140,b6536a8f688d6d712316e98fd38097a8a442709f,,"[{'authorId': '7444483', 'name': 'A. Fischer'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",90.0,"{'bibtex': '@Inproceedings{Fischer2016SocialFO,\n author = {A. Fischer and A. Manstead},\n pages = {424-439},\n title = {Social functions of emotion and emotion regulation},\n year = {2016}\n}\n'}",,"{'volume': '', 'pages': '424-439', 'name': ''}",0.0,Social functions of emotion and emotion regulation,2016.0
2141,b67c8a0ac3737c69eef925bb6bf5206e625760e6,,"[{'authorId': '47355138', 'name': 'Judith A. Hall'}, {'authorId': '4011070', 'name': 'J. Harrigan'}, {'authorId': '67149081', 'name': 'R. Rosenthal'}]",276.0,"{'bibtex': '@Article{Hall1995NonverbalBI,\n author = {Judith A. Hall and J. Harrigan and R. Rosenthal},\n journal = {Applied & Preventive Psychology},\n pages = {21-37},\n title = {Nonverbal behavior in clinician—patient interaction},\n volume = {4},\n year = {1995}\n}\n'}",,"{'volume': '4', 'pages': '21-37', 'name': 'Applied & Preventive Psychology'}",177.0,Nonverbal behavior in clinician—patient interaction,1995.0
2142,b694cb24ec28fdc5fe07c857f8ec377169ece0ab,,"[{'authorId': '1390019127', 'name': 'S. Baron-Cohen'}, {'authorId': '3159706', 'name': 'S. Wheelwright'}]",3492.0,"{'bibtex': '@Article{Baron-Cohen2004TheEQ,\n author = {S. Baron-Cohen and S. Wheelwright},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {163-175},\n title = {The Empathy Quotient: An Investigation of Adults with Asperger Syndrome or High Functioning Autism, and Normal Sex Differences},\n volume = {34},\n year = {2004}\n}\n'}",,"{'volume': '34', 'pages': '163-175', 'name': 'Journal of Autism and Developmental Disorders'}",61.0,"The Empathy Quotient: An Investigation of Adults with Asperger Syndrome or High Functioning Autism, and Normal Sex Differences",2004.0
2143,b69646bf2a11711e673e226e68f3bb059265944d,"In open learning environments students are confronted with complex tasks. Learners have control over the environment and decide themselves over the use of support tools. However, research indicates that merely providing students with these tools does not result in their actual use. In this article possibilities of animated pedagogical agents to enhance the use of support tools are explored. First, a typology is constructed to describe and compare the different pedagogical agents from an instructional design perspective. Second currently available pedagogical agents are analyzed, and finally empirical research on pedagogical agents in educational settings is reviewed. The conclusion discusses future research perspectives. ********** The use of open learning environments is advocated when learning complex problem-solving skills (Jonassen, 1997). These environments are characterized by at least three features. First, students receive a complex task that has to be examined from different perspectives to generate a suitable solution (Spiro, Feltovich, Jacobson, & Coulson, 1991). Second, support tools are embedded in the environment. Their use may help to solve the problem by structuring the problem-solving process or by providing problem-solving tools. And third, learners are responsible for their own learning and decide, themselves, on the use of these supportive elements. In other words, there is a large amount of learner control (Hannafin, 1995). In spite of strong theoretical arguments in favor of open learning environments, research demonstrates that students in open learning environments do not optimally use accessible support tools (Clarebout, Elen, Lowyck, Van den Ende, & Lagana, 2000; Crooks, Klein, Jones, & Dwyer, 1996, Land, 2000). Students seem not always capable to make the appropriate choices (Clark, 1991; Large, 1996; Hill & Hannafin, 2001; Lee & Lehman, 1993; Shaw, Johnson, & Ganeshan, 1999). This has a negative impact on the effectiveness and efficiency of leaming in open learning environments. How to encourage learners to make more ample and deliberate use of support in open learning environments is therefore an important question from an instructional design perspective. Two developments seem especially relevant in this respect. The first one relates to the interaction between learner characteristics and support characteristics. Research within the aptitude-treatment-interaction tradition revealed strong interactions between individual learner characteristics and instructional interventions (Snow, 1986; Snow & Swanson, 1992). This also pertains to support devices. Clark (1991) demonstrated that both too much or not enough support may be detrimental to learning. A second development pertains to the delivery of support. Technological evolutions and especially the development of so called pedagogical agents may provide possibilities to individualize support and encourage learners to use it. Pedagogical agents are, by definition, animated characters designed to operate in an educational setting for supporting or facilitating learning (Shaw, et al., 1999). They can adapt their support to learning paths of students and provide students with nonverbal feedback (Gregoire, Zettlemoyer, & Lester, 1999; Johnson, Rickel, & Lester, 2000). Pedagogical agents have been primarily described and studied from a technological perspective (Johnson et al., 2000; Johnson, Rickel, Stiles, & Munro, 1997; Lester, Voerman, Towns, & Callaway, 1997, Graesser, Wiemer-Hastings, Wiemer-Hastings, & Kreuz, 1999). Nevertheless, studies with a more learning oriented perspective are beginning to emerge (e.g., Moreno & Mayer, 2000). To consider the use of pedagogical agents in instructional design endeavors and to study their role in encouraging students to use help functions, a common instructional typology is needed to describe these agents. …","[{'authorId': '1795235', 'name': 'G. Clarebout'}, {'authorId': '1783893', 'name': 'J. Elen'}, {'authorId': '145834585', 'name': 'W. Johnson'}, {'authorId': '143878462', 'name': 'Erin Shaw'}]",109.0,"{'bibtex': '@Article{Clarebout2002AnimatedPA,\n author = {G. Clarebout and J. Elen and W. Johnson and Erin Shaw},\n journal = {Journal of Educational Multimedia and Hypermedia},\n pages = {267-286},\n title = {Animated Pedagogical Agents: An Opportunity to be Grasped?},\n volume = {11},\n year = {2002}\n}\n'}",,"{'volume': '11', 'pages': '267-286', 'name': 'Journal of Educational Multimedia and Hypermedia'}",0.0,Animated Pedagogical Agents: An Opportunity to be Grasped?,2002.0
2144,b6980ceff33c6e114a53fa69f5ad9d9430fdabe2,,"[{'authorId': '144102217', 'name': 'A. Mehrabian'}, {'authorId': '2072471638', 'name': ""E. O'reilly""}]",82.0,"{'bibtex': ""@Article{Mehrabian1980AnalysisOP,\n author = {A. Mehrabian and E. O'reilly},\n journal = {Journal of Personality and Social Psychology},\n pages = {492-503},\n title = {Analysis of personality measures in terms of basic dimensions of temperament.},\n volume = {38},\n year = {1980}\n}\n""}",,"{'volume': '38', 'pages': '492-503', 'name': 'Journal of Personality and Social Psychology'}",36.0,Analysis of personality measures in terms of basic dimensions of temperament.,1980.0
2145,b6a266e2990aab3158a755607eba715757ab46d8,"When building conversational agents that are to take part in social interaction with humans, an important question is whether psychological concepts like emotions or personality of the agents need to be incorporated. In this chapter we argue for the integration of an emotion system 
into a conversational agent to enable the simulation of having “own emotions”. We first clarify the concept of emotions and we discuss different approaches to modeling emotions and personality in artificial systems. Drawing on our work on the multimodal conversational agent Max, we present motives for the integration of emotions as integral parts of an agent’s cognitive architecture. Our approach combines different psychological emotion theories and distinguishes between primary and secondary emotions as originating from different levels of this architecture. Exemplary application scenarios are described to show how the agent’s believability can be increased by the integration of emotions. In a cooperative setting, Max 
is employed as a virtual interactive guide in a public computer museum, where his emotion module enhances his acceptance as a coequal conversational partner. We further quote an empirical study that yields evidence that the same emotion module supports the believability 
and lifelikeness of the agent in a competitive gaming scenario.","[{'authorId': '2068695177', 'name': 'Christian Becker'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",73.0,"{'bibtex': '@Inproceedings{Becker2007WhyES,\n author = {Christian Becker and S. Kopp and I. Wachsmuth},\n pages = {49-67},\n title = {Why emotions should be integrated into conversational agents},\n year = {2007}\n}\n'}",,"{'volume': '', 'pages': '49-67', 'name': ''}",60.0,Why emotions should be integrated into conversational agents,2007.0
2146,b6b782cac09b34043cc7aafee17bcfb62dbd1a05,"Autism spectrum disorder (ASD) is often characterized by core deficits in social communication and ability to understand others’ non-verbal emotional cues. This can be attributed to their atypical eye-gaze patterns along with reduced fixation towards communicator's face during social communication. With technological progress, Virtual Reality (VR) augmented with peripherals such as, eye tracker can offer a promising complementary assistive platform for presenting various social situations to this target group along with quantification of one's task performance and measurement of gaze-related indices. This paper presents the design of a VR-based social communication platform augmented with technologically-enhanced eye-tracking facility as a proof-of-concept application. We measured one's performance score along with real-time synchronized gaze-related indices while one interacted with VR-based social tasks having both context-relevant verbal and non-verbal components of social interaction. The results of a usability study carried out in the Indian sub-continent with eight pairs of individuals with ASD and typically-developing individuals showed the potential of our system to have implications on one's task performance and gaze-related indices in response to virtual peer's emotional expressions. The implication of emotions on gaze-related behavioral and physiological indices shows the potential of using gaze-related indices as bio-markers of one's anxiety during social communication.","[{'authorId': '52161814', 'name': 'Pradeep Raj Krishnappa Babu'}, {'authorId': '144477698', 'name': 'Poojan Oza'}, {'authorId': '2393577', 'name': 'U. Lahiri'}]",28.0,"{'bibtex': '@Article{Babu2018GazeSensitiveVR,\n author = {Pradeep Raj Krishnappa Babu and Poojan Oza and U. Lahiri},\n journal = {IEEE Transactions on Affective Computing},\n pages = {450-462},\n title = {Gaze-Sensitive Virtual Reality Based Social Communication Platform for Individuals with Autism},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'pages': '450-462', 'name': 'IEEE Transactions on Affective Computing'}",47.0,Gaze-Sensitive Virtual Reality Based Social Communication Platform for Individuals with Autism,2018.0
2147,b6c236d61c5b6f93087f7804d735141725484123,"Questions have long been used as a teaching tool by teachers and preceptors to assess students’ knowledge, promote comprehension, and stimulate critical thinking. Well-crafted questions lead to new insights, generate discussion, and promote the comprehensive exploration of subject matter. Poorly constructed questions can stifle learning by creating confusion, intimidating students, and limiting creative thinking. Teachers most often ask lower-order, convergent questions that rely on students’ factual recall of prior knowledge rather than asking higher-order, divergent questions that promote deep thinking, requiring students to analyze and evaluate concepts. This review summarizes the taxonomy of questions, provides strategies for formulating effective questions, and explores practical considerations to enhance student engagement and promote critical thinking. These concepts can be applied in the classroom and in experiential learning environments.","[{'authorId': '11018594', 'name': 'T. Tofade'}, {'authorId': '2073888305', 'name': 'Jamie N. Elsner'}, {'authorId': '4304131', 'name': 'Stuart T Haines'}]",270.0,"{'bibtex': '@Article{Tofade2013BestPS,\n author = {T. Tofade and Jamie N. Elsner and Stuart T Haines},\n journal = {American Journal of Pharmaceutical Education},\n title = {Best Practice Strategies for Effective Use of Questions as a Teaching Tool},\n volume = {77},\n year = {2013}\n}\n'}",,"{'volume': '77', 'name': 'American Journal of Pharmaceutical Education'}",34.0,Best Practice Strategies for Effective Use of Questions as a Teaching Tool,2013.0
2148,b774dac218fdef760a13072aeb4ea3f15519429d,"We present a new data-driven model and algorithm to identify the perceived emotions of individuals based on their walking styles. Given an RGB video of an individual walking, we extract his/her walking gait in the form of a series of 3D poses. Our goal is to exploit the gait features to classify the emotional state of the human into one of four emotions: happy, sad, angry, or neutral. Our perceived emotion recognition approach uses deep features learned via LSTM on labeled emotion datasets. Furthermore, we combine these features with affective features computed from gaits using posture and movement cues. These features are classified using a Random Forest Classifier. We show that our mapping between the combined feature space and the perceived emotional state provides 80.07% accuracy in identifying the perceived emotions. In addition to classifying discrete categories of emotions, our algorithm also predicts the values of perceived valence and arousal from gaits. We also present an EWalk (Emotion Walk) dataset that consists of videos of walking individuals with gaits and labeled emotions. To the best of our knowledge, this is the first gait-based model to identify perceived emotions from videos of walking individuals.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '80905955', 'name': 'Kyra Kapsaskis'}, {'authorId': '50227009', 'name': 'Uttaran Bhattacharya'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",47.0,"{'bibtex': '@Article{Randhavane2019IdentifyingEF,\n author = {Tanmay Randhavane and Aniket Bera and Kyra Kapsaskis and Uttaran Bhattacharya and Kurt Gray and Dinesh Manocha},\n journal = {ArXiv},\n title = {Identifying Emotions from Walking using Affective and Deep Features},\n volume = {abs/1906.11884},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1906.11884', 'name': 'ArXiv'}",97.0,Identifying Emotions from Walking using Affective and Deep Features,2019.0
2149,b774f3fc6b05a86d966365ba46a2c6ea9612bfce,"Age differences in emotion recognition from lexical stimuli and facial expressions were examined in a cross-sectional sample of adults aged 18 to 85 (N = 357). Emotion-specific response biases differed by age: Older adults were disproportionately more likely to incorrectly label lexical stimuli as happiness, sadness, and surprise and to incorrectly label facial stimuli as disgust and fear. After these biases were controlled, findings suggested that older adults were less accurate at identifying emotions than were young adults, but the pattern differed across emotions and task types. The lexical task showed stronger age differences than the facial task, and for lexical stimuli, age groups differed in accuracy for all emotional states except fear. For facial stimuli, in contrast, age groups differed only in accuracy for anger, disgust, fear, and happiness. Implications for age-related changes in different types of emotional processing are discussed.","[{'authorId': '1919851', 'name': 'D. Isaacowitz'}, {'authorId': '2538621', 'name': 'C. Löckenhoff'}, {'authorId': '50480971', 'name': 'R. Lane'}, {'authorId': '48466798', 'name': 'R. Wright'}, {'authorId': '1808389760', 'name': 'L. Sechrest'}, {'authorId': '46559560', 'name': 'R. Riedel'}, {'authorId': '2281038', 'name': 'P. Costa'}]",315.0,"{'bibtex': '@Article{Isaacowitz2007AgeDI,\n author = {D. Isaacowitz and C. Löckenhoff and R. Lane and R. Wright and L. Sechrest and R. Riedel and P. Costa},\n journal = {Psychology and aging},\n pages = {\n          147-59\n        },\n title = {Age differences in recognition of emotion in lexical stimuli and facial expressions.},\n volume = {22 1},\n year = {2007}\n}\n'}",,"{'volume': '22 1', 'pages': '\n          147-59\n        ', 'name': 'Psychology and aging'}",60.0,Age differences in recognition of emotion in lexical stimuli and facial expressions.,2007.0
2151,b77f4487be0a8285654553de443e44015d435961,,"[{'authorId': '2834276', 'name': 'Filipa Ferrada'}, {'authorId': '1397341979', 'name': 'L. Camarinha-Matos'}]",9.0,"{'bibtex': '@Inproceedings{Ferrada2017ASD,\n author = {Filipa Ferrada and L. Camarinha-Matos},\n pages = {29-43},\n title = {A System Dynamics and Agent-Based Approach to Model Emotions in Collaborative Networks},\n year = {2017}\n}\n'}",,{'pages': '29-43'},55.0,A System Dynamics and Agent-Based Approach to Model Emotions in Collaborative Networks,2017.0
2152,b79e9ee1fa71af99f37079037cc22089195bb224,,"[{'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '145678408', 'name': 'D. DeGroot'}, {'authorId': '1680388', 'name': 'W. Kosters'}]",64.0,"{'bibtex': '@Article{Broekens2008FormalMO,\n author = {J. Broekens and D. DeGroot and W. Kosters},\n journal = {Cognitive Systems Research},\n pages = {173-197},\n title = {Formal models of appraisal: Theory, specification, and computational model},\n volume = {9},\n year = {2008}\n}\n'}",,"{'volume': '9', 'pages': '173-197', 'name': 'Cognitive Systems Research'}",41.0,"Formal models of appraisal: Theory, specification, and computational model",2008.0
2153,b7ced6ae7c3899e410852910c82b31d65a6cf565,,"[{'authorId': '145616714', 'name': 'Michael Kipp'}, {'authorId': '1934427', 'name': 'Thomas Dackweiler'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}]",244.0,"{'bibtex': '@Article{Kipp2011DesigningE,\n author = {Michael Kipp and Thomas Dackweiler and Patrick Gebhard},\n journal = {KI - Künstliche Intelligenz},\n pages = {205-211},\n title = {Designing Emotions},\n volume = {25},\n year = {2011}\n}\n'}",,"{'volume': '25', 'pages': '205-211', 'name': 'KI - Künstliche Intelligenz'}",25.0,Designing Emotions,2011.0
2154,b7e3343ec10cc5cfc83e1a238c10ac08e5d393d8,"The question whether body movements and body postures are indicative of specific emotions is a matter of debate. While some studies have found evidence for specific body movements accompanying specific emotions, others indicate that movement behavior (aside from facial expression) may be only indicative of the quantity (intensity) of emotion, but not of its quality. The study reported here is an attempt to demonstrate that body movements and postures to some degree are specific for certain emotions. A sample of 224 video takes, in which actors and actresses portrayed the emotions of elated joy, happiness, sadness, despair, fear, terror, cold anger, hot anger, disgust, contempt, shame, guilt, pride, and boredom via a scenario approach, was analyzed using coding schemata for the analysis of body movements and postures. Results indicate that some emotion-specific movement and posture characteristics seem to exist, but that for body movements differences between emotions can be partly explained by the dimension of activation. While encoder (actor) differences are rather pronounced with respect to specific movement and posture habits, these differences are largely independent from the emotion-specific differences found. The results are discussed with respect to emotion-specific discrete expression models in contrast to dimensional models of emotion encoding. Copyright © 1998 John Wiley & Sons, Ltd.","[{'authorId': '4874112', 'name': 'H. Wallbott'}]",473.0,"{'bibtex': '@Article{Wallbott1998BodilyEO,\n author = {H. Wallbott},\n journal = {European Journal of Social Psychology},\n pages = {879-896},\n title = {Bodily expression of emotion},\n volume = {28},\n year = {1998}\n}\n'}",,"{'volume': '28', 'pages': '879-896', 'name': 'European Journal of Social Psychology'}",33.0,Bodily expression of emotion,1998.0
2155,b7e94f92fb7498ac3b0cfc92f881c55a1292a02e,"A long-standing dream of artificial intelligence has been to put common sense knowledge into computers—enabling machines to reason about everyday life. Some projects, such as Cyc, have begun to amass large collections of such knowledge. However, it is widely assumed that the use of common sense in interactive applications will remain impractical for years, until these collections can be considered sufficiently complete and common sense reasoning sufficiently robust. Recently, at the MIT Media Lab, we have had some success in applying common sense knowledge in a number of intelligent Interface Agents, despite the admittedly spotty coverage and unreliable inference of today's common sense knowledge systems. This paper will survey several of these applications and reflect on interface design principles that enable successful use of common sense knowledge.","[{'authorId': '145507148', 'name': 'H. Lieberman'}, {'authorId': '2108977300', 'name': 'Hugo Liu'}, {'authorId': '2218665261', 'name': 'Push Singh'}, {'authorId': '145176221', 'name': 'Barbara Barry'}]",32.0,"{'bibtex': '@Inproceedings{Lieberman2003BeatingSC,\n author = {H. Lieberman and Hugo Liu and Push Singh and Barbara Barry},\n title = {Beating Some Common Sense into Interactive Applications},\n year = {2003}\n}\n'}",,,12.0,Beating Some Common Sense into Interactive Applications,2003.0
2156,b7f49969204503a27bd369820b17fdd652d6a1ff,,"[{'authorId': '2056019545', 'name': 'C. W. Hughes'}]",1045.0,"{'bibtex': '@Article{Hughes1982EmotionTR,\n author = {C. W. Hughes},\n journal = {Journal of Nervous and Mental Disease},\n pages = {315-316},\n title = {Emotion: Theory, Research and Experience},\n volume = {170},\n year = {1982}\n}\n'}",,"{'volume': '170', 'pages': '315-316', 'name': 'Journal of Nervous and Mental Disease'}",0.0,"Emotion: Theory, Research and Experience",1982.0
2157,b7f7283b543656db8e10fbf967f9bdeb11e9ff7f,"How do our facial expressions affect the credibility of our words? We test whether smiles, either uninhibited or inhibited, affect the credibility of a written statement. Participants viewed a confederate partner displaying a neutral expression, non-Duchenne smile, Duchenne smile, or controlled smile, paired with a written statement. Participants then made a behavioral decision based on how credible they perceived the confederate’s statement to be. Compared to a neutral expression, Experiment 1 found that participants were more likely to believe the confederate’s statement when it was paired with a deliberate Duchenne smile and less likely to believe the confederate’s statement when it was paired with a deliberate controlled smile. Experiment 2 replicated these findings with spontaneously emitted expressions. These findings provide evidence that uninhibited facial expressions can increase the credibility accompanying statements, while inhibited ones can decrease credibility.","[{'authorId': '46301322', 'name': 'L. I. Reed'}, {'authorId': '123955483', 'name': 'Rachel Stratton'}, {'authorId': '52114227', 'name': 'Jessica D Rambeas'}]",13.0,"{'bibtex': '@Article{Reed2018FaceVA,\n author = {L. I. Reed and Rachel Stratton and Jessica D Rambeas},\n journal = {Evolutionary Psychology},\n title = {Face Value and Cheap Talk: How Smiles Can Increase or Decrease the Credibility of Our Words},\n volume = {16},\n year = {2018}\n}\n'}",,"{'volume': '16', 'name': 'Evolutionary Psychology'}",62.0,Face Value and Cheap Talk: How Smiles Can Increase or Decrease the Credibility of Our Words,2018.0
2158,b801a62f039aaa432ecfa124acf191d37d57114d,"Robotic emotional expressions could benefit social communication between humans and robots, if the cues such expressions contain were to be intelligible to human observers. In this paper, we present a design framework for modelling emotionally expressive robotic movements. The framework combines approach-avoidance with Shape and Effort dimensions, derived from Laban, and makes use of anatomical body planes that are general to both humanoid and non-humanoid body forms. An experimental validation study is reported with 34 participants rating an implementation of five expressive behaviours on a non-humanoid robotic platform. The results demonstrate that such expressions can encode basic emotional information, in that the parameters of the proposed design model can convey the meaning of emotional dimensions of valence, arousal and dominance. The framework thus creates a basis for implementing a set of emotional expressions that are appropriately adapted to contexts of human-robot joint activity.","[{'authorId': '2848048', 'name': 'Jekaterina Novikova'}, {'authorId': '49409850', 'name': 'L. Watts'}]",27.0,"{'bibtex': '@Article{Novikova2014ADM,\n author = {Jekaterina Novikova and L. Watts},\n journal = {Proceedings of the second international conference on Human-agent interaction},\n title = {A design model of emotional body expressions in non-humanoid robots},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the second international conference on Human-agent interaction'},35.0,A design model of emotional body expressions in non-humanoid robots,2014.0
2159,b80a0a6d413a3a853cd647d5dc32723dfd629b60,"Anthropomorphism is a far-reaching phenomenon that incorporates ideas from social psychology, cognitive psychology, developmental psychology, and the neurosciences. Although commonly considered to be a relatively universal phenomenon with only limited importance in modern industrialized societies—more cute than critical—our research suggests precisely the opposite. In particular, we provide a measure of stable individual differences in anthropomorphism that predicts three important consequences for everyday life. This research demonstrates that individual differences in anthropomorphism predict the degree of moral care and concern afforded to an agent, the amount of responsibility and trust placed on an agent, and the extent to which an agent serves as a source of social influence on the self. These consequences have implications for disciplines outside of psychology including human–computer interaction, business (marketing and finance), and law. Concluding discussion addresses how understanding anthropomorphism not only informs the burgeoning study of nonpersons, but how it informs classic issues underlying person perception as well.","[{'authorId': '3377580', 'name': 'A. Waytz'}, {'authorId': '2285765', 'name': 'J. Cacioppo'}, {'authorId': '7007014', 'name': 'Nicholas Epley'}]",519.0,"{'bibtex': '@Article{Waytz2010WhoSH,\n author = {A. Waytz and J. Cacioppo and Nicholas Epley},\n journal = {Perspectives on Psychological Science},\n pages = {219 - 232},\n title = {Who Sees Human?},\n volume = {5},\n year = {2010}\n}\n'}",,"{'volume': '5', 'pages': '219 - 232', 'name': 'Perspectives on Psychological Science'}",115.0,Who Sees Human?,2010.0
2160,b826d6986cf8390bdd28b2be5cce91bf9471b770,"This study investigated whether certain elementary properties of the human conceptual system for categorizing emotions are pancultural or are specific to particular languages and cultures. From similarity judgments provided by native speakers, multidimensional scalings of emotion-related words in Gujarati, Croatian, Japanese, Chinese, and English provided evidence of several pancultural properties. In all five languages, emotion-related words ,fell in roughly a circular order in a space definable by two dimensions: pleasure-displeasure and arousal-sleep. Similar results were obtained from unilingual and bilingual subjects.","[{'authorId': '46367714', 'name': 'J. Russell'}]",393.0,"{'bibtex': '@Article{Russell1983PanculturalAO,\n author = {J. Russell},\n journal = {Journal of Personality and Social Psychology},\n pages = {1281-1288},\n title = {Pancultural Aspects of the Human Conceptual Organization of Emotions},\n volume = {45},\n year = {1983}\n}\n'}",,"{'volume': '45', 'pages': '1281-1288', 'name': 'Journal of Personality and Social Psychology'}",42.0,Pancultural Aspects of the Human Conceptual Organization of Emotions,1983.0
2161,b85464fc97560b8ab7ccc2324a4839082f2b2b54,"workshop call demonstrates that our field is eager to move beyond first-generation generalist projects, toward a more mature practice. To do so, we seek to set up a common set of expectations and criteria for how to judge our work. In this paper, we propose some subclasses of embodied conversational character research and design, with criteria for describing and evaluating research and design advances in each. We suggest that researchers in this field could benefit from carefully identifying their own areas of expertise and contribution, and then looking for ways to collaborate on standards and share advances within these sub- areas. Presenting results, then, would require making clear the sub-areas addressed by the particular project, with evaluations appropriate to those areas included. We believe this approach can help the research community to clarify contributions, and more easily build a common base of knowledge.","[{'authorId': '1740889', 'name': 'K. Isbister'}, {'authorId': '144164167', 'name': 'Patrick Doyle'}]",61.0,"{'bibtex': '@Inproceedings{Isbister2002DesignAE,\n author = {K. Isbister and Patrick Doyle},\n title = {Design and Evaluation of Embodied Conversational Agents: A Proposed Taxonomy},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",35.0,Design and Evaluation of Embodied Conversational Agents: A Proposed Taxonomy,2002.0
2162,b86c402ed82b2e3f66fbd7c33964f157c9f97a94,"In avatar-mediated communication, there is concern that the deference between user's and avatar's appearances detracts the sense of affinity. However, the personalized avatar did not produce the favorability against the expectation. In this study, a general deformation rule independent from the aspects of the model was discussed by comparing the sixteen cartoonish deformed portraits to the originals. An avatar personalize tool based on the averaged deformation proportions was developed. It was experimentally confirmed that the deformed-personalized avatars tend to produce the more sense","[{'authorId': '48706380', 'name': 'Masayuki Heike'}, {'authorId': '1972507', 'name': 'H. Kawasaki'}, {'authorId': '49126074', 'name': 'Takahiro Tanaka'}, {'authorId': '1733141', 'name': 'K. Fujita'}]",6.0,"{'bibtex': '@Inproceedings{Heike2011StudyOD,\n author = {Masayuki Heike and H. Kawasaki and Takahiro Tanaka and K. Fujita},\n pages = {243-254},\n title = {Study on Deformation Rule of Personalized-Avatar for Producing Sense of Affinity},\n volume = {13},\n year = {2011}\n}\n'}",,"{'volume': '13', 'pages': '243-254', 'name': ''}",0.0,Study on Deformation Rule of Personalized-Avatar for Producing Sense of Affinity,2011.0
2163,b8a63ed6c02b5382930faa500ee6a891c0223e26,,"[{'authorId': '2710492', 'name': 'Maike Paetzel'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '1783613', 'name': 'Ingela Nyström'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",15.0,"{'bibtex': ""@Inproceedings{Paetzel2016CongruencyM,\n author = {Maike Paetzel and Christopher E. Peters and Ingela Nyström and Ginevra Castellano},\n pages = {402-412},\n title = {Congruency Matters - How Ambiguous Gender Cues Increase a Robot's Uncanniness},\n year = {2016}\n}\n""}",,{'pages': '402-412'},16.0,Congruency Matters - How Ambiguous Gender Cues Increase a Robot's Uncanniness,2016.0
2164,b8cac158a294c11981b22db33ed554bc6a53c6dc,,"[{'authorId': '2669604', 'name': 'K. Ochsner'}, {'authorId': '1775321', 'name': 'J. Gross'}]",3977.0,"{'bibtex': '@Article{Ochsner2005TheCC,\n author = {K. Ochsner and J. Gross},\n journal = {Trends in Cognitive Sciences},\n pages = {242-249},\n title = {The cognitive control of emotion},\n volume = {9},\n year = {2005}\n}\n'}",,"{'volume': '9', 'pages': '242-249', 'name': 'Trends in Cognitive Sciences'}",85.0,The cognitive control of emotion,2005.0
2165,b8e698e8a7d968f3dba9040e2f5fafe7e5a2b095,"Defining “emotion” is a notorious problem. Without consensual conceptualization and operationalization of exactly what phenomenon is to be studied, progress in theory and research is difficult to achieve and fruitless debates are likely to proliferate. A particularly unfortunate example is William James’s asking the question “What is an emotion?” when he really meant “feeling”, a misnomer that started a debate which is still ongoing, more than a century later. This contribution attempts to sensitize researchers in the social and behavioral sciences to the importance of definitional issues and their consequences for distinguishing related but fundamentally different affective processes, states, and traits. Links between scientific and folk concepts of emotion are explored and ways to measure emotion and its components are discussed.","[{'authorId': '2462740', 'name': 'K. Scherer'}]",3456.0,"{'bibtex': '@Article{Scherer2005WhatAE,\n author = {K. Scherer},\n journal = {Social Science Information},\n pages = {695 - 729},\n title = {What are emotions? And how can they be measured?},\n volume = {44},\n year = {2005}\n}\n'}",,"{'volume': '44', 'pages': '695 - 729', 'name': 'Social Science Information'}",93.0,What are emotions? And how can they be measured?,2005.0
2166,b8f7eb74b9b35215005fc8da64ef6d2dc31aaceb,"There is increasing interest in clarifying how different face emotion expressions are perceived by people from different cultures, of different ages and sex. However, scant availability of well-controlled emotional face stimuli from non-Western populations limit the evaluation of cultural differences in face emotion perception and how this might be modulated by age and sex differences. We present a database of East Asian face expression stimuli, enacted by young and older, male and female, Taiwanese using the Facial Action Coding System (FACS). Combined with a prior database, this present database consists of 90 identities with happy, sad, angry, fearful, disgusted, surprised and neutral expressions amounting to 628 photographs. Twenty young and 24 older East Asian raters scored the photographs for intensities of multiple-dimensions of emotions and induced affect. Multivariate analyses characterized the dimensionality of perceived emotions and quantified effects of age and sex. We also applied commercial software to extract computer-based metrics of emotions in photographs. Taiwanese raters perceived happy faces as one category, sad, angry, and disgusted expressions as one category, and fearful and surprised expressions as one category. Younger females were more sensitive to face emotions than younger males. Whereas, older males showed reduced face emotion sensitivity, older female sensitivity was similar or accentuated relative to young females. Commercial software dissociated six emotions according to the FACS demonstrating that defining visual features were present. Our findings show that East Asians perceive a different dimensionality of emotions than Western-based definitions in face recognition software, regardless of age and sex. Critically, stimuli with detailed cultural norms are indispensable in interpreting neural and behavioral responses involving human facial expression processing. To this end, we add to the tools, which are available upon request, for conducting such research.","[{'authorId': '121894317', 'name': 'Yu-Zhen Tu'}, {'authorId': '2116443919', 'name': 'Dong-Wei Lin'}, {'authorId': '3147469', 'name': 'Atsunobu Suzuki'}, {'authorId': '1739475', 'name': 'J. Goh'}]",13.0,"{'bibtex': '@Article{Tu2018EastAY,\n author = {Yu-Zhen Tu and Dong-Wei Lin and Atsunobu Suzuki and J. Goh},\n journal = {Frontiers in Psychology},\n title = {East Asian Young and Older Adult Perceptions of Emotional Faces From an Age- and Sex-Fair East Asian Facial Expression Database},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'name': 'Frontiers in Psychology'}",95.0,East Asian Young and Older Adult Perceptions of Emotional Faces From an Age- and Sex-Fair East Asian Facial Expression Database,2018.0
2167,b9138cc6eea1322f817e2e4f06cf204d0c212f9e,"L'A. veut montrer que, dans la production de la L2, le fait de prendre conscience d'un probleme linguistique peut amener les apprenants a modifier leur production. Ce faisant, ils sont obliges d'utiliser un mode de traitement plus syntaxique, qui apparait dans la comprehension. Ce qu'il advient entre la production originale et la production finale modifiee par la prise en compte du probleme, fait partie du processus d'acquisition d'une L2","[{'authorId': '2027127', 'name': 'M. Swain'}, {'authorId': '5965726', 'name': 'Sharon Lapkin'}]",1453.0,"{'bibtex': '@Article{Swain1995ProblemsIO,\n author = {M. Swain and Sharon Lapkin},\n journal = {Applied Linguistics},\n pages = {371-391},\n title = {Problems in Output and the Cognitive Processes They Generate: A Step Towards Second Language Learning},\n volume = {16},\n year = {1995}\n}\n'}",,"{'volume': '16', 'pages': '371-391', 'name': 'Applied Linguistics'}",29.0,Problems in Output and the Cognitive Processes They Generate: A Step Towards Second Language Learning,1995.0
2168,b91db89ddd3a42918403aeb80788be682d24a3a8,"Recent research in perception and theory of mind reveals that people show different behavior and lower activation of brain regions associated with mentalizing (i.e., the inference of other's mental states) when engaged in decision making with computers, when compared to humans. These findings are important for affective computing because they suggest people's decisions might be influenced differently according to whether they believe emotional expressions shown in computers are being generated by algorithms or humans. To test this, we had people engage in a social dilemma (Experiment 1) or negotiation (Experiment 2) with virtual humans that were either perceived to be agents (i.e., controlled by computers) or avatars (i.e., controlled by humans). The results showed that such perceptions have a deep impact on people's decisions: in Experiment 1, people cooperated more with virtual humans that showed cooperative facial displays (e.g., joy after mutual cooperation) than competitive displays (e.g., joy when the participant was exploited) but, the effect was stronger with avatars (d = .601) than with agents (d = .360); in Experiment 2, people conceded more to angry than neutral virtual humans but, again, the effect was much stronger with avatars (d = 1.162) than with agents (d = .066). Participants also showed less anger towards avatars and formed more positive impressions of avatars when compared to agents.","[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '48755211', 'name': 'P. Carnevale'}]",63.0,"{'bibtex': ""@Article{Melo2015HumansVC,\n author = {C. D. Melo and J. Gratch and P. Carnevale},\n journal = {IEEE Transactions on Affective Computing},\n pages = {127-136},\n title = {Humans versus Computers: Impact of Emotion Expressions on People's Decision Making},\n volume = {6},\n year = {2015}\n}\n""}",,"{'volume': '6', 'pages': '127-136', 'name': 'IEEE Transactions on Affective Computing'}",75.0,Humans versus Computers: Impact of Emotion Expressions on People's Decision Making,2015.0
2169,b92e09bbe6e176e07a995a0aee510ddc3961ce04,1. A glimpse of the material 2. Motivation and linguistic theory 3. Iconicity defined and demonstrated 4. The analogue-building model of linguistic iconicity 5. Survey of iconicity in signed and spoken languages 6. Metaphor in American Sign Language: the double mapping 7. Many metaphors in a single sign 8. The vertical scale as source domain 9. Verb agreement paths in American Sign Language 10. Complex superposition of metaphors in an American Sign Language poem 11. The future of signed-language research Appendices References Index.,"[{'authorId': '34649792', 'name': 'S. Taub'}]",469.0,"{'bibtex': '@Inproceedings{Taub2001LanguageFT,\n author = {S. Taub},\n title = {Language from the Body: Iconicity and Metaphor in American Sign Language},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Language from the Body: Iconicity and Metaphor in American Sign Language,2001.0
2170,b95b9c94864db735fcac1cd0aad2f335feaf2eb1,"The Influence of Virtual Agents’ Gender and Rapport on Enhancing Math Performance Bilge Karacora 1 (bilge.karacora@stud.uni-due.de), Morteza Dehghani 2 (morteza@ict.usc.edu), Nicole Kramer-Mertens 1 (nicole.kraemer@uni-due.de), Jonathan Gratch 2 (gratch@ict.usc.edu) Department of Social Psychology, University of Duisburg-Essen, Forsthausweg 2, 47057 Duisburg, Germany Institute for Creative Technologies, University of Southern California, 12015 Waterfront Dr., Playa Vista, CA 90094-2536, USA Rapport has been shown as an effective way to create behavioral realism in virtual agents. In social psychology, rapport is described as the establishment of a positive relationship among interaction partners by rapidly detecting and responding to each other’s nonverbal behavior (Gratch et al., 2007a). This includes displaying behaviors that indicate positive emotions (such as head nods and smiles), showing mutual attentiveness (such as mutual gaze) and certain coordination behaviors (such as postural mimicry and synchronized movement) (Tickle-Degnan & Rosenthal, 1990). Niewiadomski et al. (2010) reports that when an agent displays appropriate and socially adapted emotional expressions, he is perceived as more human-like than an agent that shows human expressions which are inappropriate or not socially adapted. Garau et al. (2005) conducted a study showing that only participants who interacted with an agent that was responsive to their movements, experienced a sense of personal contact with the agent which influenced them to behave more socially considerate as opposed to interacting with a static or moving but unresponsive agent. This indicates that rapport is an important feature in order for the agent to be perceived as human-like and for any social effects, such as social facilitation, to occur. Previous research on social facilitation/inhibition illustrates how the presence of others affects an individuals’ task performance either positively or negatively (Guerin & Innes, 1982; Zanjonc, 1965; Sanders, Baron & Moore, 1978). Whether or not similar facilitation/inhibition effects occur in presence of virtual agents has been subject to several studies. Rickenberg and Reeves (2000) found that tasks are facilitated or inhibited by the “social” presence of a virtual agent. A study by Zanbaka et al. (2004) indicates that when asked to perform a task, participants reacted similarly to the presence of a virtual agent as they would have in the presence of another human. A follow-up study by Zanbaka et al. (2007) demonstrates that the presence of a virtual agent inhibits the performance of participants on a mathematical task. The limitation of this study was that the sample consisted of only female participants being confronted with an agent of matching gender. Hayes et al. (2010) found a similar decrease in performance with regard Abstract The purpose of the present research is to investigate whether virtual agents can help enhance participants’ performance, effort and motivation in mathematics. We hypothesize that a minimal amount behavioral realism induced by display of rapport is necessary for any social effects to occur in human- computer interaction. Further, we examine whether social facilitation effects occur depending on the gender of the participants and the interacting virtual agents. In a 2x2 between subjects design, participants interacted with a male or female virtual agent that either displayed rapport or no rapport. Our results confirm that gender plays a role when interacting with virtual agents that are capable of establishing rapport. Participants’ performance and effort were significantly enhanced when interacting with an agent of opposite gender that displayed rapport . Our results have implications on designing agents for education and training purposes. Keywords: social facilitation, STEM, rapport, virtual agents Introduction There is considerable interest in factors that enhance science and math performance. Recently, there has been an upsurge of interest in educational technology that exploits social and motivational factors that enhance math performance in general, and reduce gender inequality in particular (Kim, 2004; Baylor & Ryu, 2003). This work builds on the phenomena that people often treat computers as social actors. Therefore, psychological factors that improve people's performance in traditional face-to-face settings can be simulated by technologies in form of virtual learning companions or virtual instructors. In this paper, we seek to address two related goals. First, we aim to show that certain social psychological phenomena can enhance math performance in a human-computer setting. Specifically, we show that a form of social facilitation can improve performance on standardized math tests. Second, we seek to provide further evidence that people do treat computers as social actors and help elucidate the design principles that foster this effect. We specifically demonstrate that virtual agents must possess a minimum level of behavioral realism to achieve any social effects.","[{'authorId': '3364569', 'name': 'Bilge Karacora'}, {'authorId': '145707560', 'name': 'Morteza Dehghani'}, {'authorId': '1455003730', 'name': 'Nicole Krämer-Mertens'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",21.0,"{'bibtex': ""@Article{Karacora2012TheIO,\n author = {Bilge Karacora and Morteza Dehghani and Nicole Krämer-Mertens and J. Gratch},\n journal = {Cognitive Science},\n title = {The Influence of Virtual Agents' Gender and Rapport on Enhancing Math Performance},\n volume = {34},\n year = {2012}\n}\n""}",,"{'volume': '34', 'name': 'Cognitive Science'}",22.0,The Influence of Virtual Agents' Gender and Rapport on Enhancing Math Performance,2012.0
2171,b989c68b9a6f8bb1bb849cdc7594427d47720c85,"This paper presents the SEMAINE API, an open source framework for building emotion-oriented systems. By encouraging and simplifying the use of standard representation formats, the framework aims to contribute to interoperability and reuse of system components in the research community. By providing a Java and C++ wrapper around a message-oriented middleware, the API makes it easy to integrate components running on different operating systems and written in different programming languages. The SEMAINE system 1.0 is presented as an example of a full-scale system built on top of the SEMAINE API. Three small example systems are described in detail to illustrate how integration between existing and new components is realised with minimal effort.","[{'authorId': '144951065', 'name': 'M. Schröder'}]",124.0,"{'bibtex': '@Article{Schröder2010TheSA,\n author = {M. Schröder},\n journal = {Adv. Hum. Comput. Interact.},\n pages = {319406:1-319406:21},\n title = {The SEMAINE API: Towards a Standards-Based Framework for Building Emotion-Oriented Systems},\n volume = {2010},\n year = {2010}\n}\n'}",,"{'volume': '2010', 'pages': '319406:1-319406:21', 'name': 'Adv. Hum. Comput. Interact.'}",66.0,The SEMAINE API: Towards a Standards-Based Framework for Building Emotion-Oriented Systems,2010.0
2172,b9cd58409bb62aeb7d99e0bffd32492ea01f7541,"The current research is the first investigation of how the effects of expressing discrete emotions in negotiations vary across cultures. In a hypothetical negotiation scenario (Study 1) and a computer-mediated negotiation simulation (Study 2), expressing anger (relative to not expressing anger) elicited larger concessions from European American negotiators, but smaller concessions from Asian and Asian American negotiators. A third study provided evidence that this effect is due to different cultural norms about the appropriateness of anger expressions in negotiations: When we explicitly manipulated anger expressions to be appropriate, Asian and Asian American negotiators made larger concessions to the angry opponent, and their concessions were as large as was typical for European American negotiators; when we explicitly manipulated anger expressions to be inappropriate, European American negotiators made smaller concessions to the angry opponent, and their concessions were as small as was typical for Asian and Asian American negotiators. Implications for current understanding of culture, emotions, and negotiations are discussed.","[{'authorId': '3826921', 'name': 'Hajo Adam'}, {'authorId': '5732160', 'name': 'Aiwa Shirako'}, {'authorId': '5989771', 'name': 'William W Maddux'}]",116.0,"{'bibtex': '@Article{Adam2010CulturalVI,\n author = {Hajo Adam and Aiwa Shirako and William W Maddux},\n journal = {Psychological Science},\n pages = {882 - 889},\n title = {Cultural Variance in the Interpersonal Effects of Anger in Negotiations},\n volume = {21},\n year = {2010}\n}\n'}",,"{'volume': '21', 'pages': '882 - 889', 'name': 'Psychological Science'}",44.0,Cultural Variance in the Interpersonal Effects of Anger in Negotiations,2010.0
2173,b9d245563e5bca974b39b36730cc8431e0e130d2,"During dyadic interactions, participants adjust their behavior and give feedback continuously in response to the behavior of their interlocutors and the interaction context. In this paper, we study how a participant in a dyadic interaction adapts his/her body language to the behavior of the interlocutor, given the interaction goals and context. We apply a variety of psychology-inspired body language features to describe body motion and posture. We first examine the coordination between the dyad's behavior for two interaction stances: friendly and conflictive. The analysis empirically reveals the dyad's behavior coordination, and helps identify informative interlocutor features with respect to the participant's target body language features. The coordination patterns between the dyad's behavior are found to depend on the interaction stances assumed. We apply a Gaussian-Mixture-Model-based (GMM) statistical mapping in combination with a Fisher kernel framework for automatically predicting the body language of an interacting participant from the speech and gesture behavior of an interlocutor. The experimental results show that the Fisher kernel-based approach outperforms methods using only the GMM-based mapping, and using the support vector regression, in terms of correlation coefficient and RMSE. These results suggest a significant level of predictability of body language behavior from interlocutor cues.","[{'authorId': '3161887', 'name': 'Zhaojun Yang'}, {'authorId': '47851995', 'name': 'A. Metallinou'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",30.0,"{'bibtex': '@Article{Yang2014AnalysisAP,\n author = {Zhaojun Yang and A. Metallinou and Shrikanth S. Narayanan},\n journal = {IEEE Transactions on Multimedia},\n pages = {1766-1778},\n title = {Analysis and Predictive Modeling of Body Language Behavior in Dyadic Interactions From Multimodal Interlocutor Cues},\n volume = {16},\n year = {2014}\n}\n'}",,"{'volume': '16', 'pages': '1766-1778', 'name': 'IEEE Transactions on Multimedia'}",54.0,Analysis and Predictive Modeling of Body Language Behavior in Dyadic Interactions From Multimodal Interlocutor Cues,2014.0
2174,b9d25e86ea5ede85bff1633618843b9226a2d919,,"[{'authorId': '1787101', 'name': 'L. Vardoulakis'}, {'authorId': '2880118', 'name': 'Lazlo Ring'}, {'authorId': '150931679', 'name': 'Barbara Barry'}, {'authorId': '2668280', 'name': 'C. Sidner'}, {'authorId': '1690448', 'name': 'T. Bickmore'}]",150.0,"{'bibtex': '@Inproceedings{Vardoulakis2012DesigningRA,\n author = {L. Vardoulakis and Lazlo Ring and Barbara Barry and C. Sidner and T. Bickmore},\n pages = {289-302},\n title = {Designing Relational Agents as Long Term Social Companions for Older Adults},\n year = {2012}\n}\n'}",,{'pages': '289-302'},27.0,Designing Relational Agents as Long Term Social Companions for Older Adults,2012.0
2176,ba050913a3a7e958980b7ff0780d75980dbfbffc,,"[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '1388284460', 'name': 'W. Friesen'}]",282.0,"{'bibtex': '@Article{Ekman1967HeadAB,\n author = {P. Ekman and W. Friesen},\n journal = {Perceptual and motor skills},\n pages = {\n          711-24\n        },\n title = {Head and body cues in the judgment of emotion: a reformulation.},\n volume = {24 3},\n year = {1967}\n}\n'}",,"{'volume': '24 3', 'pages': '\n          711-24\n        ', 'name': 'Perceptual and motor skills'}",18.0,Head and body cues in the judgment of emotion: a reformulation.,1967.0
2178,ba0f61a54fd4fc3aa3e0654e823719d21c0f2e89,"The concept of empathy—the intellectual or imaginative apprehension of another's condition or state of mind— is central for understanding a broad range of social phenomena including, in particular, moral development. Within this latter context, an empathic disposition can be regarded as the capacity to adopt a broad moral perspective, that is, to take ""the moral point of view."" This paper discusses the development of a 64-item self-report measure of empathy, constructed by comparing the responses of groups with high- and low-rated empathy, using the combined MMPI-CPI item pool. After providing evidence concerning the scale's reliability and validity, an attempt is made to show its relevance for specifically moral conduct by relating empathy scale scores to real life indexes of socially appropriate behavior and to certain previously wellvalidated measures of personality. Some form of empathic disposition, roletaking ability, or social sensitivity is assumed by all approaches to personality which take the interpersonal situation as a major focus of concern. Accordingly, most writers in the role-theoretical tradition (Cottrell, 1942; Gough, 1948; Mead, 1934; Sarbin, 1968) have given careful attention to this aspect of social functioning. Mead, for example, has argued that role-taking ability is the key variable in social and moral development; extending this line of reasoning he equates the ""g"" factor in intelligence with social sensitivity, the origins of which can be found in the central nervous system. In a similar vein, Cottrell and Dymond (1949) also maintained that empathy is the basic process in all social interaction. Empathy, seen as an everyday manifestation of the disposition to adopt a broad moral","[{'authorId': '143977043', 'name': 'R. Hogan'}]",1405.0,"{'bibtex': '@Article{Hogan1969DevelopmentOA,\n author = {R. Hogan},\n journal = {Journal of consulting and clinical psychology},\n pages = {\n          307-16\n        },\n title = {Development of an empathy scale.},\n volume = {33 3},\n year = {1969}\n}\n'}",,"{'volume': '33 3', 'pages': '\n          307-16\n        ', 'name': 'Journal of consulting and clinical psychology'}",23.0,Development of an empathy scale.,1969.0
2179,ba2e37336fa1ed165a032bec01367501a5a4e05a,"While the cognitive disturbances that frequently follow severe traumatic brain injury (TBI) are relatively well understood, the ways in which these affect the psychosocial functioning of people with TBI are yet to be determined and have thus received little attention in treatment research. Growing evidence indicates that a significant proportion of individuals with TBI demonstrate an inability to recognize affective information from the face, voice, bodily movement, and posture. Because accurate interpretation of emotion in others is critical for the successful negotiation of social interactions, effective treatments are necessary. Until recently, however, there have been no rehabilitation efforts in this area. The present review examines the literature on emotion perception deficits in TBI and presents a theoretical rationale for targeted intervention. Several lines of research relevant to the remediation of emotion perception in people with TBI are considered. These include work on emotion perception remediation with other cognitively impaired populations, current neuropsychological models of emotion perception and underlying neural systems, and recent conceptualizations of remediation processes. The article concludes with a discussion of the importance of carrying out efforts to improve emotion perception within a contextualized framework in which the day-to-day relevance of training is clear to all recipients. (JINS, 2008, 14, 511–525.)","[{'authorId': '6577871', 'name': 'C. Bornhofen'}, {'authorId': '143982280', 'name': 'S. McDonald'}]",154.0,"{'bibtex': '@Article{Bornhofen2008EmotionPD,\n author = {C. Bornhofen and S. McDonald},\n journal = {Journal of the International Neuropsychological Society},\n pages = {511 - 525},\n title = {Emotion perception deficits following traumatic brain injury: A review of the evidence and rationale for intervention},\n volume = {14},\n year = {2008}\n}\n'}",,"{'volume': '14', 'pages': '511 - 525', 'name': 'Journal of the International Neuropsychological Society'}",233.0,Emotion perception deficits following traumatic brain injury: A review of the evidence and rationale for intervention,2008.0
2180,ba2e73a3f5f717ba36cb44fd572c2248cf169aee,"1. Human Emotions 2. Why did Humans Become so Emotional? 3. Social Structure, Culture, and Emotions 4. Emotional Arousal: Basic Principles 5. Transactional Needs and Emotional Arousal 6. Social Structure and Emotional Arousal 7. Culture and Emotional Arousal 8. Emotions and Social Change 9. The Theory Reviewed","[{'authorId': '118688586', 'name': 'J. Turner'}]",772.0,"{'bibtex': '@Inproceedings{Turner2007HumanEA,\n author = {J. Turner},\n title = {Human Emotions: A Sociological Theory},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",117.0,Human Emotions: A Sociological Theory,2007.0
2181,ba30358d8eb578e12024ec1c62e2dcdd0273b0b6,"We present a parametric, computational model of head-eye coordination that can be used in the animation of directed gaze shifts for virtual characters. The model is based on research in human neurophysiology. It incorporates control parameters that allow for adapting gaze shifts to the characteristics of the environment, the gaze targets, and the idiosyncratic behavioral attributes of the virtual character. A user study confirms that the model communicates gaze targets as effectively as real humans do, while being preferred subjectively to state-of-the-art models.","[{'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '2633572', 'name': 'T. Pejsa'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}]",23.0,"{'bibtex': '@Inproceedings{Andrist2012AHC,\n author = {Sean Andrist and T. Pejsa and Bilge Mutlu and Michael Gleicher},\n pages = {4:1-4:6},\n title = {A head-eye coordination model for animating gaze shifts of virtual characters},\n year = {2012}\n}\n'}",,{'pages': '4:1-4:6'},24.0,A head-eye coordination model for animating gaze shifts of virtual characters,2012.0
2182,ba364694e7cc582ecd0134991e791492a98bffba,"Trust lies at the foundation of nearly all major theories of interpersonal relationships. Despite its great theoretical importance, a limited amount of research has examined how and why trust develops, is maintained, and occasionally unravels in relationships. Following a brief overview of theoretical and empirical milestones in the interpersonal-trust literature, an integrative process model of trust in dyadic relationships is presented.","[{'authorId': '144652931', 'name': 'J. Simpson'}]",431.0,"{'bibtex': '@Article{Simpson2007PsychologicalFO,\n author = {J. Simpson},\n journal = {Current Directions in Psychological Science},\n pages = {264 - 268},\n title = {Psychological Foundations of Trust},\n volume = {16},\n year = {2007}\n}\n'}",,"{'volume': '16', 'pages': '264 - 268', 'name': 'Current Directions in Psychological Science'}",21.0,Psychological Foundations of Trust,2007.0
2183,ba36dccf2d43bb8c91990c0bf479814c883a06e7,,"[{'authorId': '3191663', 'name': 'I. Couzin'}]",770.0,"{'bibtex': '@Article{Couzin2009CollectiveCI,\n author = {I. Couzin},\n journal = {Trends in Cognitive Sciences},\n pages = {36-43},\n title = {Collective cognition in animal groups},\n volume = {13},\n year = {2009}\n}\n'}",,"{'volume': '13', 'pages': '36-43', 'name': 'Trends in Cognitive Sciences'}",74.0,Collective cognition in animal groups,2009.0
2184,ba502e59fb58310a3e8f8889f02ed9e782e27c75,,"[{'authorId': '2321433', 'name': 'I. Hupont'}, {'authorId': '1787072', 'name': 'S. Baldassarri'}, {'authorId': '144046205', 'name': 'E. Cerezo'}]",25.0,"{'bibtex': '@Article{Hupont2013FacialEC,\n author = {I. Hupont and S. Baldassarri and E. Cerezo},\n journal = {Pattern Analysis and Applications},\n pages = {41-54},\n title = {Facial emotional classification: from a discrete perspective to a continuous emotional space},\n volume = {16},\n year = {2013}\n}\n'}",,"{'volume': '16', 'pages': '41-54', 'name': 'Pattern Analysis and Applications'}",44.0,Facial emotional classification: from a discrete perspective to a continuous emotional space,2013.0
2185,ba8459c5fa5be65cf1a520bd2d00e72592c71607,"A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts.","[{'authorId': '2214185', 'name': 'Asma Ghandeharioun'}, {'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '1817251', 'name': 'M. Czerwinski'}, {'authorId': '36516124', 'name': 'Kael Rowan'}]",34.0,"{'bibtex': '@Article{Ghandeharioun2019TowardsUE,\n author = {Asma Ghandeharioun and Daniel J. McDuff and M. Czerwinski and Kael Rowan},\n journal = {2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {8-14},\n title = {Towards Understanding Emotional Intelligence for Behavior Change Chatbots},\n year = {2019}\n}\n'}",,"{'pages': '8-14', 'name': '2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)'}",32.0,Towards Understanding Emotional Intelligence for Behavior Change Chatbots,2019.0
2186,bac377d3a051899dbe0d7249ed5d3d0b22d57310,"We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.","[{'authorId': '2273228', 'name': 'Catalin Ionescu'}, {'authorId': '2781306', 'name': 'Dragos Papava'}, {'authorId': '46752821', 'name': 'Vlad Olaru'}, {'authorId': '1781120', 'name': 'C. Sminchisescu'}]",2340.0,"{'bibtex': '@Article{Ionescu2014Human36MLS,\n author = {Catalin Ionescu and Dragos Papava and Vlad Olaru and C. Sminchisescu},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1325-1339},\n title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},\n volume = {36},\n year = {2014}\n}\n'}",,"{'volume': '36', 'pages': '1325-1339', 'name': 'IEEE Transactions on Pattern Analysis and Machine Intelligence'}",67.0,Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,2014.0
2188,baf65f9a9ca292d8115333a873d9e05ae2d4e6a3,"The inconsistent definition of empathy has had a negative impact on both research and practice. The aim of this article is to review and critically appraise a range of definitions of empathy and, through considered analysis, to develop a new conceptualisation. From the examination of 43 discrete definitions, 8 themes relating to the nature of empathy emerged: “distinguishing empathy from other concepts”; “cognitive or affective?”; “congruent or incongruent?”; “subject to other stimuli?”; “self/other distinction or merging?”; “trait or state influences?”; “has a behavioural outcome?”; and “automatic or controlled?” The relevance and validity of each theme is assessed and a new conceptualisation of empathy is offered. The benefits of employing a more consistent and complete definition of empathy are discussed.","[{'authorId': '117787690', 'name': 'B. Cuff'}, {'authorId': '28835463', 'name': 'Sarah D. Brown'}, {'authorId': '3795722', 'name': 'Laura K. Taylor'}, {'authorId': '34906389', 'name': 'D. Howat'}]",703.0,"{'bibtex': '@Article{Cuff2016EmpathyAR,\n author = {B. Cuff and Sarah D. Brown and Laura K. Taylor and D. Howat},\n journal = {Emotion Review},\n pages = {144 - 153},\n title = {Empathy: A Review of the Concept},\n volume = {8},\n year = {2016}\n}\n'}",,"{'volume': '8', 'pages': '144 - 153', 'name': 'Emotion Review'}",111.0,Empathy: A Review of the Concept,2016.0
2189,baf822e8e772853e88170a37f1c3d32018adf7f1,"The emerging field of emotion regulation studies how individuals influence which emotions they have, when they have them, and how they experience and express them. This review takes an evolutionary perspective and characterizes emotion in terms of response tendencies. Emotion regulation is defined and distinguished from coping, mood regulation, defense, and affect regulation. In the increasingly specialized discipline of psychology, the field of emotion regulation cuts across traditional boundaries and provides common ground. According to a process model of emotion regulation, emotion may be regulated at five points in the emotion generative process: (a) selection of the situation, (b) modification of the situation, (c) deployment of attention, (d) change of cognitions, and (e) modulation of responses. The field of emotion regulation promises new insights into age-old questions about how people manage their emotions.","[{'authorId': '1775321', 'name': 'J. Gross'}]",7257.0,"{'bibtex': '@Article{Gross1998TheEF,\n author = {J. Gross},\n journal = {Review of General Psychology},\n pages = {271 - 299},\n title = {The Emerging Field of Emotion Regulation: An Integrative Review},\n volume = {2},\n year = {1998}\n}\n'}",,"{'volume': '2', 'pages': '271 - 299', 'name': 'Review of General Psychology'}",382.0,The Emerging Field of Emotion Regulation: An Integrative Review,1998.0
2191,baf9e0badb6576f2bae6f1e03d6e04c652e9b94b,"Nightmares are common, occurring weekly in 4%-10% of the population, and are associated with female gender, younger age, increased stress, psychopathology, and dispositional traits. Nightmare pathogenesis remains unexplained, as do differences between nontraumatic and posttraumatic nightmares (for those with or without posttraumatic stress disorder) and relations with waking functioning. No models adequately explain nightmares nor have they been reconciled with recent developments in cognitive neuroscience, fear acquisition, and emotional memory. The authors review the recent literature and propose a conceptual framework for understanding a spectrum of dysphoric dreaming. Central to this is the notion that variations in nightmare prevalence, frequency, severity, and psychopathological comorbidity reflect the influence of both affect load, a consequence of daily variations in emotional pressure, and affect distress, a disposition to experience events with distressing, highly reactive emotions. In a cross-state, multilevel model of dream function and nightmare production, the authors integrate findings on emotional memory structures and the brain correlates of emotion.","[{'authorId': '117883465', 'name': 'R. Levin'}, {'authorId': '144235536', 'name': 'T. Nielsen'}]",487.0,"{'bibtex': '@Article{Levin2007DisturbedDP,\n author = {R. Levin and T. Nielsen},\n journal = {Psychological bulletin},\n pages = {\n          482-528\n        },\n title = {Disturbed dreaming, posttraumatic stress disorder, and affect distress: a review and neurocognitive model.},\n volume = {133 3},\n year = {2007}\n}\n'}",,"{'volume': '133 3', 'pages': '\n          482-528\n        ', 'name': 'Psychological bulletin'}",566.0,"Disturbed dreaming, posttraumatic stress disorder, and affect distress: a review and neurocognitive model.",2007.0
2192,bb01a43c2fc553582e3c34ed5fc249670c849dd8,"This research investigates the meaning of “human-computer relationship” and presents techniques for constructing, maintaining, and evaluating such relationships, based on research in social psychology, sociolinguistics, communication and other social sciences. Contexts in which relationships are particularly important are described, together with specific benefits (like trust) and task outcomes (like improved learning) known to be associated with relationship quality. We especially consider the problem of designing for long-term interaction, and define relational agents as computational artifacts designed to establish and maintain long-term social-emotional relationships with their users. We construct the first such agent, and evaluate it in a controlled experiment with 101 users who were asked to interact daily with an exercise adoption system for a month. Compared to an equivalent task-oriented agent without any deliberate social-emotional or relationship-building skills, the relational agent was respected more, liked more, and trusted more, even after four weeks of interaction. Additionally, users expressed a significantly greater desire to continue working with the relational agent after the termination of the study. We conclude by discussing future directions for this research together with ethical and other ramifications of this work for HCI designers.","[{'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",1053.0,"{'bibtex': '@Article{Bickmore2005EstablishingAM,\n author = {T. Bickmore and Rosalind W. Picard},\n journal = {ACM Trans. Comput. Hum. Interact.},\n pages = {293-327},\n title = {Establishing and maintaining long-term human-computer relationships},\n volume = {12},\n year = {2005}\n}\n'}",,"{'volume': '12', 'pages': '293-327', 'name': 'ACM Trans. Comput. Hum. Interact.'}",111.0,Establishing and maintaining long-term human-computer relationships,2005.0
2195,bb1917efc4fcd0e660ca3789f706c51ad213f1f6,,"[{'authorId': '2454798', 'name': 'Eduardo M. Eisman'}, {'authorId': '2061877452', 'name': 'Víctor López'}, {'authorId': '1772700', 'name': 'J. Castro'}]",10.0,"{'bibtex': '@Article{Eisman2009ControllingTE,\n author = {Eduardo M. Eisman and Víctor López and J. Castro},\n journal = {Expert Syst. Appl.},\n pages = {9698-9708},\n title = {Controlling the emotional state of an embodied conversationalagent with a dynamic probabilistic fuzzy rules based system},\n volume = {36},\n year = {2009}\n}\n'}",,"{'volume': '36', 'pages': '9698-9708', 'name': 'Expert Syst. Appl.'}",12.0,Controlling the emotional state of an embodied conversationalagent with a dynamic probabilistic fuzzy rules based system,2009.0
2196,bb448ba5947470cf1e5ae964036ebd16d45fd757,"Purpose – The purpose of this article is to describe multi‐tasking behaviour in the workplace; to link its cause to the increasing prevalence of low‐cost information and communications technologies and to the changing organizational structures that have evolved to meet the demands and opportunities of these technologies.Design/methodology/approach – This article is a presentation of the current literature on multi‐tasking behaviour among knowledge workers with a selective bibliography addressing empirical research into the behavioural, managerial and technological aspects of this phenomenon. It then expands to comprehensive coverage of the literature on past and current thinking about task structuring, strategies for coping in a multi‐tasking environment and the changing nature of work and organizations, which fuels the need to multi‐task in response to these changes.Findings – Among knowledge workers, multi‐tasking behaviour appears to be an inevitable consequence of the presence of increasingly easy acc...","[{'authorId': '33345348', 'name': 'S. H. Appelbaum'}, {'authorId': '152381722', 'name': 'A. Marchionni'}, {'authorId': '2115427436', 'name': 'A. Fernández'}]",130.0,"{'bibtex': '@Article{Appelbaum2008TheMP,\n author = {S. H. Appelbaum and A. Marchionni and A. Fernández},\n journal = {Management Decision},\n pages = {1313-1325},\n title = {The multi‐tasking paradox: perceptions, problems and strategies},\n volume = {46},\n year = {2008}\n}\n'}",,"{'volume': '46', 'pages': '1313-1325', 'name': 'Management Decision'}",30.0,"The multi‐tasking paradox: perceptions, problems and strategies",2008.0
2197,bb4c4c2482e543dcb67708f3ad775f1e3fa2ab8f,"The concept of personal space is a key element of social interactions. As such, it is a recurring subject of investigations in the context of research on proxemics. Using virtual-reality-based experiments, we contribute to this area by evaluating the direct effects of emotional expressions of an approaching virtual agent on an individual’s behavioral and physiological responses. As a pilot study focusing on the emotion expressed solely by facial expressions gave promising results, we now present a study design to gain more insight.","[{'authorId': '144483066', 'name': 'T. Kuhlen'}, {'authorId': '3249697', 'name': 'A. Bönsch'}, {'authorId': '39812907', 'name': 'J. Wendt'}, {'authorId': '47973447', 'name': 'H. Overath'}, {'authorId': '66809640', 'name': 'Ö. Gürerk'}, {'authorId': '2536104', 'name': 'C. Harbring'}, {'authorId': '3354454', 'name': 'C. Grund'}, {'authorId': '2856539', 'name': 'T. Kittsteiner'}, {'authorId': '1862105', 'name': 'B. McFadyen'}, {'authorId': '1902889', 'name': 'T. Iachini'}, {'authorId': '3200187', 'name': 'Y. Coello'}, {'authorId': '2903600', 'name': 'F. Frassinetti'}, {'authorId': '1414692301', 'name': 'V. Senese'}, {'authorId': '2505736', 'name': 'M. Rinck'}, {'authorId': '113703375', 'name': 'T. Rörtgen'}, {'authorId': '39854264', 'name': 'D. Wigboldus'}, {'authorId': '2891686', 'name': 'B. Spanlang'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '2262597', 'name': 'N. Thomas'}, {'authorId': '46582609', 'name': 'C. Spence'}, {'authorId': '3500182', 'name': 'T. Tavares'}, {'authorId': '2928107', 'name': 'D. Mitchell'}]",1.0,"{'bibtex': '@Inproceedings{Kuhlen2018TowardsUT,\n author = {T. Kuhlen and A. Bönsch and J. Wendt and H. Overath and Ö. Gürerk and C. Harbring and C. Grund and T. Kittsteiner and B. McFadyen and T. Iachini and Y. Coello and F. Frassinetti and V. Senese and M. Rinck and T. Rörtgen and D. Wigboldus and B. Spanlang and M. Slater and N. Thomas and C. Spence and T. Tavares and D. Mitchell},\n title = {Towards Understanding the Influence of a Virtual Agent ’ s Emotional Expression on Personal Space},\n year = {2018}\n}\n'}","[{'paperId': '528836373159b455f16b6dc57fd98439dd5f9cee', 'title': 'Investigating the Influence of an Approaching Virtual Agent ’ s Emotional Expression on a User ’ s Personal Space Preferences'}]",,25.0,Towards Understanding the Influence of a Virtual Agent ’ s Emotional Expression on Personal Space,2018.0
2198,bb659c68814ff3a847b571337cb5fbcd41a81ea6,"Recent fMRI evidence has detected increased medial prefrontal activation during contemplation of personal moral dilemmas compared to impersonal ones, which suggests that this cortical region plays a role in personal moral judgment. However, functional imaging results cannot definitively establish that a brain area is necessary for a particular cognitive process. This requires evidence from lesion techniques, such as studies of human patients with focal brain damage. Here, we tested 7 patients with lesions in the ventromedial prefrontal cortex and 12 healthy individuals in personal moral dilemmas, impersonal moral dilemmas and non-moral dilemmas. Compared to normal controls, patients were more willing to judge personal moral violations as acceptable behaviors in personal moral dilemmas, and they did so more quickly. In contrast, their performance in impersonal and non-moral dilemmas was comparable to that of controls. These results indicate that the ventromedial prefrontal cortex is necessary to oppose personal moral violations, possibly by mediating anticipatory, self-focused, emotional reactions that may exert strong influence on moral choice and behavior.","[{'authorId': '3341935', 'name': 'E. Ciaramelli'}, {'authorId': '4889272', 'name': 'Michela Muccioli'}, {'authorId': '2688861', 'name': 'E. Làdavas'}, {'authorId': '2697421', 'name': 'G. di Pellegrino'}]",409.0,"{'bibtex': '@Article{Ciaramelli2007SelectiveDI,\n author = {E. Ciaramelli and Michela Muccioli and E. Làdavas and G. di Pellegrino},\n journal = {Social cognitive and affective neuroscience},\n pages = {\n          84-92\n        },\n title = {Selective deficit in personal moral judgment following damage to ventromedial prefrontal cortex.},\n volume = {2 2},\n year = {2007}\n}\n'}",,"{'volume': '2 2', 'pages': '\n          84-92\n        ', 'name': 'Social cognitive and affective neuroscience'}",59.0,Selective deficit in personal moral judgment following damage to ventromedial prefrontal cortex.,2007.0
2199,bb95f4bb91bffa9a3524b5e12519831be539d357,"Abstract We first review the main points in the dispute about whether emotion is primary and independent of cognition (Zajonc), or secondary and always dependent upon cognition (Lazarus), and suggest that the dispute is largely one of definition. Because definitional disputes seldom clarify substantive, theoretical points, we suggest a variety of questions regarding cognition-emotion interaction. To stimulate discussion of these issues, we propose a componential model in which emotions are seen to develop from simpler, reflex-like forms (“wired-in” sensory-motor processes) to complex cognitive-emotional patterns that result from the participation of at least two distinct levels of memory and information processing, a schematic and a conceptual level. These systems are typically activated by a continuous stimulus check process which evaluates five environment-organism attributes: novelty; pleasantness; goal conductiveness; coping potential; and consistency with social norms and self-relevant values. Questi...","[{'authorId': '2059373265', 'name': 'H. Leventhal'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",797.0,"{'bibtex': '@Article{Leventhal1987TheRO,\n author = {H. Leventhal and K. Scherer},\n journal = {Cognition & Emotion},\n pages = {3-28},\n title = {The Relationship of Emotion to Cognition: A Functional Approach to a Semantic Controversy},\n volume = {1},\n year = {1987}\n}\n'}",,"{'volume': '1', 'pages': '3-28', 'name': 'Cognition & Emotion'}",52.0,The Relationship of Emotion to Cognition: A Functional Approach to a Semantic Controversy,1987.0
2200,bba9c82d63a83900e7fca252c8b9e9676b95b234,"In this article we discuss the role of emotions in artificial agent design, and the use of logic in reasoning about the emotional or affective states an agent can reside in. We do so by extending the KARO framework for reasoning about rational agents appropriately. In particular, we formalize in this framework how emotions are related to the action monitoring capabilities of an agent. © 2006 Wiley Periodicals, Inc. Int J Int Syst 21: 601–619, 2006.","[{'authorId': '1691228', 'name': 'J. Meyer'}]",120.0,"{'bibtex': '@Article{Meyer2004ReasoningAE,\n author = {J. Meyer},\n journal = {International Journal of Intelligent Systems},\n title = {Reasoning about emotional agents},\n volume = {21},\n year = {2004}\n}\n'}",,"{'volume': '21', 'name': 'International Journal of Intelligent Systems'}",50.0,Reasoning about emotional agents,2004.0
2201,bbaae3188aed31df77272c9f30d2b8faaa407939,"For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.","[{'authorId': '145215929', 'name': 'Youngwoo Yoon'}, {'authorId': '2068564225', 'name': 'Bok Cha'}, {'authorId': '2200538568', 'name': 'Joo-Haeng Lee'}, {'authorId': '145416765', 'name': 'Minsu Jang'}, {'authorId': '2108383739', 'name': 'Jaeyeon Lee'}, {'authorId': '1684726', 'name': 'Jaehong Kim'}, {'authorId': '80502761', 'name': 'Geehyuk Lee'}]",147.0,"{'bibtex': '@Article{Yoon2020SpeechGG,\n author = {Youngwoo Yoon and Bok Cha and Joo-Haeng Lee and Minsu Jang and Jaeyeon Lee and Jaehong Kim and Geehyuk Lee},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 16},\n title = {Speech gesture generation from the trimodal context of text, audio, and speaker identity},\n volume = {39},\n year = {2020}\n}\n'}",,"{'volume': '39', 'pages': '1 - 16', 'name': 'ACM Transactions on Graphics (TOG)'}",70.0,"Speech gesture generation from the trimodal context of text, audio, and speaker identity",2020.0
2202,bbb4858ded5c5e270883c8c105bc2b66cd4f4045,"This study explored how rapidly emotion specific facial muscle reactions were elicited when subjects were exposed to pictures of angry and happy facial expressions. In three separate experiments, it was found that distinctive facial electromyographic reactions, i.e., greater Zygomaticus major muscle activity in response to happy than to angry stimuli and greater Corrugator supercilii muscle activity in response to angry stimuli, were detectable after only 300-400 ms of exposure. These findings demonstrate that facial reactions are quickly elicited, indicating that expressive emotional reactions can be very rapidly manifested and are perhaps controlled by fast operating facial affect programs.","[{'authorId': '4583182', 'name': 'U. Dimberg'}, {'authorId': '3924673', 'name': 'M. Thunberg'}]",429.0,"{'bibtex': '@Article{Dimberg1998RapidFR,\n author = {U. Dimberg and M. Thunberg},\n journal = {Scandinavian journal of psychology},\n pages = {\n          39-45\n        },\n title = {Rapid facial reactions to emotional facial expressions.},\n volume = {39 1},\n year = {1998}\n}\n'}",,"{'volume': '39 1', 'pages': '\n          39-45\n        ', 'name': 'Scandinavian journal of psychology'}",0.0,Rapid facial reactions to emotional facial expressions.,1998.0
2203,bbcac391233fefb0ada47de6399196bb55e02956,"The evolutionary justification by LeDoux (1996) for his dual-route model of fear processing was analyzed computationally by applying genetic algorithms to artificial neural networks. The evolution was simulated of a neural network controlling an agent that gathered food in an artificial world and that was occasionally menaced by a predator. Connections could not change in the agent's lifetime, so there was no learning in the simulations. Only if the smells of food and predator were hard to distinguish and the fitness reflected time pressures in escaping from the predator did the type of dual processing postulated by LeDoux emerge in the surviving agents. Processing in the quick and dirty pathway of the fear system ensured avoidance of both predators and food, but a distinction between food and predator was made only in the long pathway. Elaborate processing inhibited the avoidance reaction and reversed it into an approach reaction to food, but strengthened the avoidance reaction to predators (and more finely tuned the direction of escape). It is suggested that computational neuroethology (Beer, 1990) may help constrain reasoning in evolutionary psychology, particularly when applied to specific neurobiological models, and in the future may even generate new hypotheses for cognitive neuroscience.","[{'authorId': '5927512', 'name': 'P. D. Dulk'}, {'authorId': '3122838', 'name': 'B. T. Heerebout'}, {'authorId': '2545816', 'name': 'R. Phaf'}]",32.0,"{'bibtex': '@Article{Dulk2003ACS,\n author = {P. D. Dulk and B. T. Heerebout and R. Phaf},\n journal = {Journal of Cognitive Neuroscience},\n pages = {194-208},\n title = {A Computational Study into the Evolution of Dual-Route Dynamics for Affective Processing},\n volume = {15},\n year = {2003}\n}\n'}",,"{'volume': '15', 'pages': '194-208', 'name': 'Journal of Cognitive Neuroscience'}",35.0,A Computational Study into the Evolution of Dual-Route Dynamics for Affective Processing,2003.0
2204,bbe8b6e541c473bfe3f2e20aa009ab1e0dba3220,,"[{'authorId': '48230544', 'name': 'U. Pfeiffer'}, {'authorId': '2127424', 'name': 'L. Schilbach'}, {'authorId': '1918177', 'name': 'Bert Timmermans'}, {'authorId': '144082475', 'name': 'B. Kuzmanovic'}, {'authorId': '2989516', 'name': 'A. Georgescu'}, {'authorId': '2487649', 'name': 'G. Bente'}, {'authorId': '2051580', 'name': 'K. Vogeley'}]",122.0,"{'bibtex': '@Article{Pfeiffer2014WhyWI,\n author = {U. Pfeiffer and L. Schilbach and Bert Timmermans and B. Kuzmanovic and A. Georgescu and G. Bente and K. Vogeley},\n journal = {NeuroImage},\n pages = {124-137},\n title = {Why we interact: On the functional role of the striatum in the subjective experience of social interaction},\n volume = {101},\n year = {2014}\n}\n'}",,"{'volume': '101', 'pages': '124-137', 'name': 'NeuroImage'}",100.0,Why we interact: On the functional role of the striatum in the subjective experience of social interaction,2014.0
2206,bbf45171545b5b05511bac696797764d15fc28e9,"We propose a new technique to simulate dynamic patterns of crowd behaviors using stress modeling. Our model accounts for permanent, stable disposition and the dynamic nature of human behaviors that change in response to the situation. The resulting approach accounts for changes in behavior in response to external stressors based on well-known theories in psychology. We combine this model with recent techniques on personality modeling for multi-agent simulations to capture a wide variety of behavioral changes and stressors. The overall formulation allows different stressors, expressed as functions of space and time, including time pressure, positional stressors, area stressors and inter-personal stressors. This model can be used to simulate dynamic crowd behaviors at interactive rates, including walking at variable speeds, breaking lane-formation over time, and cutting through a normal flow. We also perform qualitative and quantitative comparisons between our simulation results and real-world observations.","[{'authorId': '52162164', 'name': 'Sujeong Kim'}, {'authorId': '35170565', 'name': 'S. Guy'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}, {'authorId': '144247566', 'name': 'M. Lin'}]",96.0,"{'bibtex': '@Inproceedings{Kim2012InteractiveSO,\n author = {Sujeong Kim and S. Guy and Dinesh Manocha and M. Lin},\n pages = {55-62},\n title = {Interactive simulation of dynamic crowd behaviors using general adaptation syndrome theory},\n year = {2012}\n}\n'}",,{'pages': '55-62'},35.0,Interactive simulation of dynamic crowd behaviors using general adaptation syndrome theory,2012.0
2207,bc1745fb575dc9ca2d08bd334ba470515c862044,,"[{'authorId': '144464279', 'name': 'A. Ricci'}, {'authorId': '1831718', 'name': 'Michele Piunti'}, {'authorId': '1728548', 'name': 'Mirko Viroli'}, {'authorId': '3119182', 'name': 'Andrea Omicini'}]",150.0,"{'bibtex': '@Inproceedings{Ricci2009EnvironmentPI,\n author = {A. Ricci and Michele Piunti and Mirko Viroli and Andrea Omicini},\n pages = {259-288},\n title = {Environment Programming in CArtAgO},\n year = {2009}\n}\n'}",,{'pages': '259-288'},35.0,Environment Programming in CArtAgO,2009.0
2208,bc38f00bc19d6b43ad1e3b3737dd791c9cb782fc,"INTRODUCTION Computers are as ubiquitous as automobiles and toasters, but exploiting their capabilities still seems to require the training of a supersonic test pilot. VCR displays blinking a constant 12 noon around the world testi~ to this conunclrum. As interactive television, palmtop diaries and “smart” credit cards proliferate, the gap between millions of untrained users and an equal number of sophisticated microprocessors will become even more sharply apparent. With people spending a growing proportion of their lives in front ctf computer screens--informing and entertaining one ancltier, exchanging correspondence, working, shopping and falling in love—some accommodation must be found between limited human attention spans and increasingly complex collections of software and data. ,","[{'authorId': '1701876', 'name': 'P. Maes'}]",121.0,"{'bibtex': '@Inproceedings{Maes1997IntelligentS,\n author = {P. Maes},\n pages = {41-43},\n title = {Intelligent software},\n year = {1997}\n}\n'}",,{'pages': '41-43'},4.0,Intelligent software,1997.0
2209,bc5252525dd0c29324f4e45f274711f1b3665b0b,"Social robots working in public space often stimulate children's curiosity. However, sometimes children also show abusive behavior toward robots. In our case studies, we observed in many cases that children persistently obstruct the robot's activity. Some actually abused the robot by saying bad things, and at times even kicking or punching the robot. We developed a statistical model of occurrence of children's abuse. Using this model together with a simulator of pedestrian behavior, we enabled the robot to predict the possibility of an abuse situation and escape before it happens. We demonstrated that with the model the robot successfully lowered the occurrence of abuse in a real shopping mall. Categories and Subject Descriptors H.5.2 [Information Interfaces and Presentation]: User Interfaces - Interaction styles; I.2.9 [Artificial Intelligence]: Robotics, General Terms Design, Experimentation, Human Factors.","[{'authorId': '2203507', 'name': 'D. Brscic'}, {'authorId': '2064478755', 'name': 'Hiroyuki Kidokoro'}, {'authorId': '1814875', 'name': 'Yoshitaka Suehiro'}, {'authorId': '48309591', 'name': 'T. Kanda'}]",162.0,"{'bibtex': '@Article{Brscic2015EscapingFC,\n author = {D. Brscic and Hiroyuki Kidokoro and Yoshitaka Suehiro and T. Kanda},\n journal = {2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {59-66},\n title = {Escaping from Children’s Abuse of Social Robots},\n year = {2015}\n}\n'}",,"{'pages': '59-66', 'name': '2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",26.0,Escaping from Children’s Abuse of Social Robots,2015.0
2210,bc585466b504d24dd940342d2af488519aac9dbe,"The fifth Audio-Visual Emotion Challenge and workshop AVEC 2015 was held in conjunction ACM Multimedia'15. Like the previous editions of AVEC, the workshop/challenge addresses the detection of affective signals represented in audio-visual data in terms of high-level continuous dimensions. A major novelty was further introduced this year by the inclusion of the physiological modality - along with the audio and the video modalities - in the dataset. In this summary, we mainly describe participation and its conditions.","[{'authorId': '2124680', 'name': 'F. Ringeval'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",53.0,"{'bibtex': '@Article{Ringeval2015AVEC2T,\n author = {F. Ringeval and Björn Schuller and M. Valstar and R. Cowie and M. Pantic},\n journal = {Proceedings of the 23rd ACM international conference on Multimedia},\n title = {AVEC 2015: The 5th International Audio/Visual Emotion Challenge and Workshop},\n year = {2015}\n}\n'}",,{'name': 'Proceedings of the 23rd ACM international conference on Multimedia'},12.0,AVEC 2015: The 5th International Audio/Visual Emotion Challenge and Workshop,2015.0
2211,bc595fd0f97ad98663d4d681746879fe9df06045,,"[{'authorId': '8443268', 'name': 'Xuguang Zhang'}, {'authorId': '107928381', 'name': 'Xiaohu Shu'}, {'authorId': '2116778579', 'name': 'Zhen He'}]",25.0,"{'bibtex': '@Article{Zhang2019CrowdPS,\n author = {Xuguang Zhang and Xiaohu Shu and Zhen He},\n journal = {Physica A: Statistical Mechanics and its Applications},\n title = {Crowd panic state detection using entropy of the distribution of enthalpy},\n year = {2019}\n}\n'}",,{'name': 'Physica A: Statistical Mechanics and its Applications'},29.0,Crowd panic state detection using entropy of the distribution of enthalpy,2019.0
2212,bc6dff14a130c57a91d5a21339c23471faf1d46f,"disasters. Plenum, 2001. 11. Haley R, Thomas L, Hom J. Is there a Gulf War Syndrome? Searching for syndromes by factor analysis of symptoms. JAMA 1997;277:215–22. 12. Fukuda K, Nisenbaum R, Stewart G, et al. Chronic multi-symptom illness affecting Air Force veterans of the Gulf War. JAMA 1998;280:981–8. 13. Ismail K, Everitt B, Blatchley N, et al. Is there a Gulf War Syndrome? Lancet 1999;353:179–82. 14. Shapiro S, Lasarev M, McCauley L. Factor analysis of Gulf War illness: what does it add to our understanding of possible health effects of deployment. Am J Epidemiol 2002;156:578–85. 15. Doebbeling B, Clarke W, Watson D, et al. Is there a Persian Gulf War Syndrome? Evidence from a large population-based survey of veterans and nondeployed controls. Am J Med 2000;108:695–704. 16. Knoke J, Smith T, Gray G, et al. Factor analysis of self reported symptoms: Does it identify a Gulf War Syndrome? Am J Epidemiol 2000;152:379–88. 17. Kang H, Mahan C, Lee K, et al. Evidence for a deployment-related Gulf War syndrome by factor analysis. Arch Environ Health 2002;57:61–8.","[{'authorId': '2059358552', 'name': 'P. Cochat'}, {'authorId': '13267685', 'name': 'L. Vaucoret'}, {'authorId': '2097644863', 'name': 'J. Sarles'}]",68766.0,"{'bibtex': '@Article{Cochat2008EtA,\n author = {P. Cochat and L. Vaucoret and J. Sarles},\n journal = {Evidence Based Mental Health},\n pages = {102 - 104},\n title = {Et al},\n volume = {11},\n year = {2008}\n}\n'}",,"{'volume': '11', 'pages': '102 - 104', 'name': 'Evidence Based Mental Health'}",33.0,Et al,2008.0
2213,bca2e6c3e1728cf856de492605631c49e562bcf0,"The study of stress and coping points to two concepts central to an understanding of the response to trauma: approach and avoidance. This pair of concepts refers to two basic modes of coping with stress. Approach and avoidance are simply metaphors for cognitive and emotional activity that is oriented either toward or away from threat. An approach-avoidance model of coping is presented in the context of contemporary theoretical ap- proaches to coping. The research literature on coping ef- fectiveness, including evidence from our laboratory, is dis- cussed, and speculations are made about the implications for future research. The study of stress and coping has become quite popular in recent years, particularly in regard to traumatic life events. Although the area is broad and the coping process is complex, there is a striking coherence in much of the literature. This coherence is based on two concepts central to an understanding of coping with trauma: approach and avoidance. In its simplest form, this pair of concepts refers to two basic orientations toward stressful infor- mation, or two basic modes of coping with stress. Ap- proach and avoidance are shorthand terms for the cog- nitive and emotional activity that is oriented either toward or away from threat. In this article we will present the case for utilizing the concepts of approach and avoidance to provide a co- herent theoretical structure to our understanding of cop- ing with stress. Several different formulations of the ap- proach-avoidance dimension will be reviewed, followed by a brief review of the coping effectiveness literature. Several studies from our laboratory will be used to illus- trate the relationship between coping and outcome. Fi- nally, a general approach-avoidance model of coping will be presented, with suggestions for further research to cor- roborate or extend the theory. The study of coping with stress has been split into two areas: anticipation of future stressful events and re- covery from trauma. These areas have been kept re- markably distinct in both theory and research on coping. Although there are clearly important differences between the two cases, we have chosen not to emphasize this dis- tinction. For any given stress, anticipation and recovery are not always clearly separable; dealing with a trauma involves coming to terms with the event itself and with the threat of recurrence in the future. More important, Correspondence concerning this article should be sent to Susan","[{'authorId': '152368844', 'name': 'S. Roth'}, {'authorId': '144994495', 'name': 'L. Cohen'}]",1746.0,"{'bibtex': '@Article{Roth1986ApproachAA,\n author = {S. Roth and L. Cohen},\n journal = {The American psychologist},\n pages = {\n          813-9\n        },\n title = {Approach, avoidance, and coping with stress.},\n volume = {41 7},\n year = {1986}\n}\n'}",,"{'volume': '41 7', 'pages': '\n          813-9\n        ', 'name': 'The American psychologist'}",35.0,"Approach, avoidance, and coping with stress.",1986.0
2214,bca946bac5a5015291802e3e07c12236c60c3f3e,,"[{'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '3346592', 'name': 'A. Raouzaiou'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '2001300', 'name': 'G. Caridakis'}, {'authorId': '1715144', 'name': 'K. Karpouzis'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '2169958', 'name': 'M. Mancini'}]",13.0,"{'bibtex': '@Inproceedings{Bevacqua2006MultimodalSI,\n author = {Elisabetta Bevacqua and A. Raouzaiou and Christopher E. Peters and G. Caridakis and K. Karpouzis and C. Pelachaud and M. Mancini},\n pages = {164-174},\n title = {Multimodal Sensing, Interpretation and Copying of Movements by a Virtual Agent},\n year = {2006}\n}\n'}",,{'pages': '164-174'},30.0,"Multimodal Sensing, Interpretation and Copying of Movements by a Virtual Agent",2006.0
2215,bcb35a728b196dc7bda1b1c57b4cfaa2080fcc7e,,"[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",22.0,"{'bibtex': '@Inproceedings{Hudlicka2016VirtualAA,\n author = {E. Hudlicka},\n pages = {81-115},\n title = {Virtual Affective Agents and Therapeutic Games},\n year = {2016}\n}\n'}",,"{'volume': '', 'pages': '81-115', 'name': ''}",83.0,Virtual Affective Agents and Therapeutic Games,2016.0
2216,bce809c25d22a601cb1e708a7ffb16dea58651fb,"Background: Virtual reality (VR) is increasingly used to study and treat psychiatric disorders. Its fidelity depends in part on the extent to which the VR environment provides a convincing simulation, for example whether a putatively stressful VR situation actually produces a stress response. Methods: We studied the stress response in 28 healthy men exposed either to a stressor VR elevator (which simulated travelling up the outside of a tall building and culminated in the participant being asked to step off the elevator platform), or to a control elevator. We measured psychological and physiological (salivary cortisol and alpha-amylase, blood pressure, pulse, skin conductance) stress indices. We also measured subsequent performance on the N-back task because acute stress has been reported to impact on working memory. Results: Compared to participants in the control elevator, those in the external elevator had increases in skin conductance, pulse and subjective stress and anxiety ratings, altered heart rate variability, and a delayed rise in cortisol. N-back performance was unaffected. Conclusions: A putatively stressful VR elevator produces a physiological as well as a psychological stress response, supporting its use in the investigation and treatment of stress-related disorders, and its potential value as an experimental laboratory stressor.","[{'authorId': '2057229443', 'name': 'M. Martens'}, {'authorId': '1705895', 'name': 'Angus Antley'}, {'authorId': '145331574', 'name': 'D. Freeman'}, {'authorId': '144931212', 'name': 'M. Slater'}, {'authorId': '40306765', 'name': 'Paul J. Harrison'}, {'authorId': '2661182', 'name': 'E. Tunbridge'}]",63.0,"{'bibtex': '@Article{Martens2019ItFR,\n author = {M. Martens and Angus Antley and D. Freeman and M. Slater and Paul J. Harrison and E. Tunbridge},\n journal = {Journal of Psychopharmacology (Oxford, England)},\n pages = {1264 - 1273},\n title = {It feels real: physiological responses to a stressful virtual reality environment and its impact on working memory},\n volume = {33},\n year = {2019}\n}\n'}",,"{'volume': '33', 'pages': '1264 - 1273', 'name': 'Journal of Psychopharmacology (Oxford, England)'}",53.0,It feels real: physiological responses to a stressful virtual reality environment and its impact on working memory,2019.0
2218,bcf1f1fb2fbceb6e4edb895c2ca6d1665c2aa62e,"Isaac is learning ways to complete hisnightly bedtime routine; Omar is fol -lowing cues to control his disruptiveand impulsive behavior; Peter isbecoming more independent withlibrary routines; and Adriana hasbenefited from visual support withlearning hand-washing and toiletingroutines.Most of us use visual supports tonavigate our days (calendars, maps,watches, to-do lists—even high-techversions of these supports), so why notprovide similar supports to young chil -dren? We must particularly be sensi -tive to the needs of children who canbenefit greatly from them: young child-ren with autism spectrum disorder.There are many helpful kinds of visualsupports teachers can use in the class -room every day, modifying andenhancing them as children becomemore and more independent. Indeed,both parents and teachers have usedmany of these tools successfully with","[{'authorId': '50265822', 'name': 'H. Meadan'}, {'authorId': '31843910', 'name': 'M. Ostrosky'}, {'authorId': '117520017', 'name': 'Brooke Triplett'}, {'authorId': '117898575', 'name': 'Amanda Michna'}, {'authorId': '37703791', 'name': 'Angel Fettig'}]",95.0,"{'bibtex': '@Article{Meadan2011UsingVS,\n author = {H. Meadan and M. Ostrosky and Brooke Triplett and Amanda Michna and Angel Fettig},\n journal = {TEACHING Exceptional Children},\n pages = {28 - 35},\n title = {Using Visual Supports with Young Children with Autism Spectrum Disorder},\n volume = {43},\n year = {2011}\n}\n'}",,"{'volume': '43', 'pages': '28 - 35', 'name': 'TEACHING Exceptional Children'}",17.0,Using Visual Supports with Young Children with Autism Spectrum Disorder,2011.0
2219,bd2986fbbc9465e8f55fa7f7d1beffa1505862f9,,"[{'authorId': '40557756', 'name': 'Jamy J. Li'}]",392.0,"{'bibtex': '@Article{Li2015TheBO,\n author = {Jamy J. Li},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {23-37},\n title = {The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents},\n volume = {77},\n year = {2015}\n}\n'}",,"{'volume': '77', 'pages': '23-37', 'name': 'Int. J. Hum. Comput. Stud.'}",76.0,"The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents",2015.0
2221,bd779b36941b5573aeacf8a77157b43fa1ace466,,"[{'authorId': '1390848733', 'name': 'Yan Mao'}, {'authorId': '49970067', 'name': 'Zuning Li'}, {'authorId': '2110516274', 'name': 'Yongjian Li'}, {'authorId': '51125889', 'name': 'Wu He'}]",28.0,"{'bibtex': '@Article{Mao2018EmotionbasedDC,\n author = {Yan Mao and Zuning Li and Yongjian Li and Wu He},\n journal = {The Visual Computer},\n pages = {1-15},\n title = {Emotion-based diversity crowd behavior simulation in public emergency},\n year = {2018}\n}\n'}",,"{'volume': '', 'pages': '1-15', 'name': 'The Visual Computer'}",32.0,Emotion-based diversity crowd behavior simulation in public emergency,2018.0
2222,bdc192a3dac4f62a0ebc5dd824d36fcb0128a536,,"[{'authorId': '6244428', 'name': 'D. Schofield'}, {'authorId': '6441370', 'name': 'R. Shrestha'}, {'authorId': '2359876', 'name': 'Richard Percival'}, {'authorId': '5820454', 'name': 'M. Passey'}, {'authorId': '5170062', 'name': 'E. Callander'}, {'authorId': '5455965', 'name': 'S. Kelly'}]",70.0,"{'bibtex': '@Article{Schofield2011ThePA,\n author = {D. Schofield and R. Shrestha and Richard Percival and M. Passey and E. Callander and S. Kelly},\n journal = {BMC Psychiatry},\n pages = {72 - 72},\n title = {The personal and national costs of mental health conditions: impacts on income, taxes, government support payments due to lost labour force participation},\n volume = {11},\n year = {2011}\n}\n'}",,"{'volume': '11', 'pages': '72 - 72', 'name': 'BMC Psychiatry'}",61.0,"The personal and national costs of mental health conditions: impacts on income, taxes, government support payments due to lost labour force participation",2011.0
2223,bdc464624f572b9c5cfe0be420ee5b827c61107e,,"[{'authorId': '39841151', 'name': 'Lauren D. Berkovits'}, {'authorId': '50171038', 'name': 'A. Eisenhower'}, {'authorId': '32569768', 'name': 'J. Blacher'}]",135.0,"{'bibtex': '@Article{Berkovits2016EmotionRI,\n author = {Lauren D. Berkovits and A. Eisenhower and J. Blacher},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {68 - 79},\n title = {Emotion Regulation in Young Children with Autism Spectrum Disorders},\n volume = {47},\n year = {2016}\n}\n'}",,"{'volume': '47', 'pages': '68 - 79', 'name': 'Journal of Autism and Developmental Disorders'}",44.0,Emotion Regulation in Young Children with Autism Spectrum Disorders,2016.0
2224,bdcc68811729510aec15f41d56aab8b200e6f372,"We present Cathexis, a distributed, computational model which offers an alternative approach to model the dynamic nature of different affective phenomena, such as emotions, moods and temperaments, and provides a flexible way of modeling their influence on the behavior of synthetic autonomous agents. The model has been implemented as part of an extensible, object-oriented framework which provides enough functionality for agent developers to design emotional agents that can be used in a variety of applications including entertainment (e.g. synthetic agents for interactive drama, video games, etc.), education (e.g. Intelligent Tutoring Systems), and human-computer interfaces.","[{'authorId': '2095517457', 'name': 'J. Velásquez'}]",365.0,"{'bibtex': '@Inproceedings{Velásquez1997ModelingEA,\n author = {J. Velásquez},\n pages = {10-15},\n title = {Modeling Emotions and Other Motivations in Synthetic Agents},\n year = {1997}\n}\n'}",,{'pages': '10-15'},28.0,Modeling Emotions and Other Motivations in Synthetic Agents,1997.0
2225,bdf19700fd64de2d5ed7dfde5e95c7d0965b87a8,"Currently, state of the art virtual agents lack the ability to display emotion as seen in actual humans, or even in hand-animated characters. One reason for the emotional inexpressiveness of virtual agents is the lack of emotionally expressive gaze manner. For virtual agents to express emotion that observers can empathize with, they need to generate gaze - including eye, head, and torso movement - to arbitrary targets, while displaying arbitrary emotional states. Our previous work [18] describes the Gaze Warping Transformation, a method of generating emotionally expressive head and torso movement during gaze shifts that is derived from human movement data. Through an evaluation, it was shown that applying different transformations to the same gaze shift could modify the affective state perceived when the transformed gaze shift was viewed by a human observer. In this paper we propose a model of realistic, emotionally expressive gaze that builds upon the Gaze Warping Transformation by improving the transformation implementation, and by adding a model of eye movement drawn from the visual neuroscience literature. We describe how to generate a gaze to an arbitrary target, while displaying an arbitrary emotional behavior. Finally, we propose an evaluation to determine what emotions human observers will attribute to the generated gaze shifts. Once this work is completed, virtual agents will have access to a new channel for emotionally expressive behavior.","[{'authorId': '145417478', 'name': 'Brent Lance'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",44.0,"{'bibtex': '@Inproceedings{Lance2008AMO,\n author = {Brent Lance and S. Marsella},\n pages = {199-206},\n title = {A model of gaze for the purpose of emotional expression in virtual embodied agents},\n year = {2008}\n}\n'}",,{'pages': '199-206'},42.0,A model of gaze for the purpose of emotional expression in virtual embodied agents,2008.0
2227,bdf77a44438f1f44f8abfc66076c658a303add74,"Permission to make digital or hard copies of part or all of this wor k for per sonal or classr oom use is granted without fee provided that copies are not made or distributed for commer cial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this wor k owned by other s than ACM must be honored. Abstracting with cr edit is permitted. To copy other wise, to republish, to post on ser ver s, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Abstract Crowd simulation techniques have frequently been used to animate a large group of virtual humans in computer graphics applications. We present a data-driven method of simulating a crowd of virtual humans that exhibit behaviors imitating real human crowds. To do so, we record the motion of a human crowd from an aerial view using a camcorder, extract the two-dimensional moving trajectories of each individual in the crowd, and then learn an agent model from observed trajectories. The agent model decides each agent's actions based on features of the environment and the motion of nearby agents in the crowd. Once the agent model is learned, we can simulate a virtual crowd that behaves similarly to the real crowd in the video. The versatility and flexibility of our approach is demonstrated through examples in which various characteristics of group behaviors are captured and reproduced in simulated crowds.","[{'authorId': '48351473', 'name': 'Dimitris N. Metaxas'}, {'authorId': '145492783', 'name': 'J. Popović'}, {'authorId': '2231643408', 'name': 'Hoon Kang'}, {'authorId': '2231648428', 'name': 'Myung Geol Lee'}, {'authorId': '2230811017', 'name': 'Qyoun Choi'}, {'authorId': '2231614594', 'name': 'Jehee Hong'}, {'authorId': '2230734554', 'name': 'Lee'}]",310.0,"{'bibtex': '@Misc{None,\n author = {Dimitris N. Metaxas and J. Popović and Hoon Kang and Myung Geol Lee and Qyoun Choi and Jehee Hong and Lee},\n title = {Eurographics/ Acm Siggraph Symposium on Computer Animation (2007) Group Behavior from Video: a Data-driven Approach to Crowd Simulation}\n}\n'}",,,16.0,Eurographics/ Acm Siggraph Symposium on Computer Animation (2007) Group Behavior from Video: a Data-driven Approach to Crowd Simulation,
2228,be0e29291ae7c887249fcb1fbd106b3790637497,,"[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",29.0,"{'bibtex': '@Inproceedings{Hudlicka2007ReasonsFE,\n author = {E. Hudlicka},\n pages = {263-278},\n title = {Reasons for Emotions},\n year = {2007}\n}\n'}",,"{'volume': '', 'pages': '263-278', 'name': ''}",0.0,Reasons for Emotions,2007.0
2229,be1891dd9ec6aa78267ecc1649812274456a3783,"A model of P300 amplitude is proposed that reduces the many hypothetical constructs invoked to explain variations in P300 amplitude to three dimensions: 1) Subjective Probability, 2) Stimulus Meaning, and 3) Information Transmission. Evidence is presented to support the assertion that variables on the subjective probability and stimulus meaning dimensions have independent and additive contributions to overall P300 amplitude. The amplitude contributions of both of these dimensions, however, are modulated by a multiplicative relation with the proportion of transmitted stimulus information. Within each dimension, the fundamental experimental variables and their interrelations are specified. An example is presented to show how, by using an additive factors method, the respective amplitude effects of the probability and stimulus meaning dimensions can be separated. Supporting data are presented to show that the proposed model provides a reasonable and testable framework in which to conceptualize P300 results.","[{'authorId': '145207176', 'name': 'Ray Johnson'}]",661.0,"{'bibtex': '@Article{Johnson1986ForDE,\n author = {Ray Johnson},\n journal = {Psychophysiology},\n pages = {367-384},\n title = {For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1985},\n volume = {23},\n year = {1986}\n}\n'}",,"{'volume': '23', 'pages': '367-384', 'name': 'Psychophysiology'}",62.0,"For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1985",1986.0
2230,be26cdf0573bc8eee1119802e548dfaa2e401bdb,"In recent years, deep learning has achieved great success in many fields, such as computer vision and natural language processing. Compared to traditional machine learning methods, deep learning has a strong learning ability and can make better use of datasets for feature extraction. Because of its practicability, deep learning becomes more and more popular for many researchers to do research works. In this paper, we mainly introduce some advanced neural networks of deep learning and their applications. Besides, we also discuss the limitations and prospects of deep learning.","[{'authorId': '31254825', 'name': 'Xuedan Du'}, {'authorId': '2230979', 'name': 'Yinghao Cai'}, {'authorId': '2117010531', 'name': 'Shuo Wang'}, {'authorId': '2107887550', 'name': 'Leijie Zhang'}]",229.0,"{'bibtex': '@Article{Du2016OverviewOD,\n author = {Xuedan Du and Yinghao Cai and Shuo Wang and Leijie Zhang},\n journal = {2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC)},\n pages = {159-164},\n title = {Overview of deep learning},\n year = {2016}\n}\n'}",,"{'pages': '159-164', 'name': '2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC)'}",38.0,Overview of deep learning,2016.0
2231,be615a8505971a979d5fa45ee36a5e451301a41e,"This paper brings together speech recognition, emotion inference and virtual agents to implement a system for student interaction in an educational environment. By analyzing the capture speech, we can perceive an indication of the emotion status of the target student. Using the inference results, an agent can choose the suitable dialogue to interact with the student. Our experiments indicate that there is a lot of potential for such a system to be applied in other situations as well.","[{'authorId': '37462750', 'name': 'I-Hen Tsai'}, {'authorId': '31173026', 'name': 'Koong Hao-Chiang Lin'}, {'authorId': '3300322', 'name': 'Rui-Ting Sun'}, {'authorId': '2478797', 'name': 'Ren-Ying Fang'}, {'authorId': '71563124', 'name': 'Jhing-Fa Wang'}, {'authorId': '2784332', 'name': 'Yan-You Chen'}, {'authorId': '6502517', 'name': 'Chu-Chuan Huang'}, {'authorId': '119582117', 'name': 'Jiun-Sheng Li'}]",5.0,"{'bibtex': '@Article{Tsai2010ApplicationOE,\n author = {I-Hen Tsai and Koong Hao-Chiang Lin and Rui-Ting Sun and Ren-Ying Fang and Jhing-Fa Wang and Yan-You Chen and Chu-Chuan Huang and Jiun-Sheng Li},\n booktitle = {2010 Third IEEE International Conference on Digital Game and Intelligent Toy Enhanced Learning},\n journal = {2010 Third IEEE International Conference on Digital Game and Intelligent Toy Enhanced Learning},\n pages = {129-133},\n title = {Application of Educational Emotion Inference via Speech and Agent Interaction},\n year = {2010}\n}\n'}","[{'paperId': '2b1782b7d4e1beddc96bdf7b588349d9e20260ab', 'title': ""Customized Attendance System for Students' Sensibility Monitoring and Counseling""}, {'paperId': '73cea4af6bc4429f04d10d88ae56b9551b11b7a3', 'title': 'Speech based boredom verification approach for modern education system'}, {'paperId': '3d29e7cce70a3892286f8d6fc3d1188d238acc31', 'title': 'An ontology-based affective tutoring system on digital arts'}, {'paperId': '4ea336562801cad257a183d70db86802d2c2398d', 'title': 'Emotional Intelligence for Artificial Intelligence : A Review'}, {'paperId': 'ac6ec6a266df0d8245959319d55caa1c133adf7e', 'title': 'Emotional agents - state of the art and applications'}]","{'name': '2010 Third IEEE International Conference on Digital Game and Intelligent Toy Enhanced Learning', 'pages': '129-133'}",18.0,Application of Educational Emotion Inference via Speech and Agent Interaction,2010.0
2232,be921ace702e263fa25e2d5adda6909d69fdb2b7,,"[{'authorId': '2117694340', 'name': 'Y. Mao'}, {'authorId': '50341299', 'name': 'S. Li'}, {'authorId': '143628350', 'name': 'J.-X. Wang'}, {'authorId': '2463381', 'name': 'P. Jia'}, {'authorId': '2149234483', 'name': 'Zhenglin Yang'}, {'authorId': '98642924', 'name': 'Zhengyue Qiu'}]",1.0,"{'bibtex': '@Article{Mao2008ReinforcementLF,\n author = {Y. Mao and S. Li and J.-X. Wang and P. Jia and Zhenglin Yang and Zhengyue Qiu},\n journal = {Scopus},\n title = {Reinforcement learning for passive dynamic walking robot},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': 'Scopus'}",0.0,Reinforcement learning for passive dynamic walking robot,2008.0
2233,be9a4798022f2785ba914bd3ed2227618cc761a4,"Successful collaboration relies on the coordination and alignment of communicative cues. In this paper, we present mechanisms of bidirectional gaze - the coordinated production and detection of gaze cues - by which a virtual character can coordinate its gaze cues with those of its human user. We implement these mechanisms in a hybrid stochastic/heuristic model synthesized from data collected in human-human interactions. In three lab studies wherein a virtual character instructs participants in a sandwich-making task, we demonstrate how bidirectional gaze can lead to positive outcomes in error rate, completion time, and the agent's ability to produce quick, effective nonverbal references. The first study involved an on-screen agent and the participant wearing eye-tracking glasses. The second study demonstrates that these positive outcomes can be achieved using head-pose estimation in place of full eye tracking. The third study demonstrates that these effects also transfer into virtual-reality interactions.","[{'authorId': '2211183', 'name': 'Sean Andrist'}, {'authorId': '1776507', 'name': 'Michael Gleicher'}, {'authorId': '145656551', 'name': 'Bilge Mutlu'}]",69.0,"{'bibtex': '@Article{Andrist2017LookingCB,\n author = {Sean Andrist and Michael Gleicher and Bilge Mutlu},\n journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},\n title = {Looking Coordinated: Bidirectional Gaze Mechanisms for Collaborative Interaction with Virtual Characters},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems'},55.0,Looking Coordinated: Bidirectional Gaze Mechanisms for Collaborative Interaction with Virtual Characters,2017.0
2234,bebca9208e8d3c47f477bb349613e6a4e05b42c1,"This article presents an overview of a relatively recent cognitive architecture, and its internal control structures, that is, its motivational and metacognitive mechanisms. The chapter starts with a look at some general ideas underlying this cognitive architecture and the relevance of these ideas to cognitive modeling of agents. It then presents a sketch of some details of the architecture and their uses in cognitive modeling of specific tasks.","[{'authorId': '145966408', 'name': 'R. Sun'}]",33.0,"{'bibtex': '@Inproceedings{Sun2007TheMA,\n author = {R. Sun},\n pages = {63-75},\n title = {The Motivational and Metacognitive Control in CLARION},\n year = {2007}\n}\n'}",,{'pages': '63-75'},25.0,The Motivational and Metacognitive Control in CLARION,2007.0
2235,bec00f4474bf4128bd19ba2c266a7b5ffcdb72a0,"Five high school students with ASD (autistic spectrum disorder) participating in the Excel/Autism study were able to demonstrate mastery of a set of Excel topics. The Excel curriculum covered approximately the same topics as are covered in the Excel portion of Computer Business Applications, a class for regular education students at Fox Chapel Area High School, a high school in suburban Pittsburgh. The students with ASD were provided with one-on-one tutoring support. Two of the five ASD participants self-initiated activities and engaged in generative thinking to a substantial degree over the course of the eight instructional sessions for which data was recorded. Two others demonstrated lesser amounts of this behavior, and one participant did not demonstrate any. The ASD experimental participants, as compared to a treatment group of three students with ASD who did not receive instruction in Excel, demonstrated improvement in a multi-step planning task which was significant.","[{'authorId': '144973594', 'name': 'M. Hart'}]",59.0,"{'bibtex': '@Inproceedings{Hart2005AutismexcelS,\n author = {M. Hart},\n pages = {136-141},\n title = {Autism/excel study},\n year = {2005}\n}\n'}",,{'pages': '136-141'},7.0,Autism/excel study,2005.0
2236,bed413b4da1fd24e59e2c33ffd758ab140d52189,"The Tactical Language and Culture Training System (TLCTS) helps people quickly acquire communicative skills in foreign languages and cultures. More than 40,000 learners worldwide have used TLCTS courses. TLCTS utilizes artificial intelligence technologies during the authoring process, and at run time to process learner speech, engage in dialog, and evaluate and assess learner performance. This paper describes the architecture of TLCTS and the artificial intelligence technologies that it employs, and presents results from multiple evaluation studies that demonstrate the benefits of learning foreign language and culture using this approach.","[{'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '2713763', 'name': 'A. Valente'}]",111.0,"{'bibtex': '@Article{Johnson2009TacticalLA,\n author = {W. Johnson and A. Valente},\n journal = {AI Mag.},\n pages = {72-83},\n title = {Tactical Language and Culture Training Systems: Using AI to Teach Foreign Languages and Cultures},\n volume = {30},\n year = {2009}\n}\n'}",,"{'volume': '30', 'pages': '72-83', 'name': 'AI Mag.'}",14.0,Tactical Language and Culture Training Systems: Using AI to Teach Foreign Languages and Cultures,2009.0
2238,bed7551bb0f97ff991b82d0aecedaa14a1ceca24,"The successful adoption of technology is becoming increasingly important to functional independence. The present article reports findings from the Center for Research and Education on Aging and Technology Enhancement (CREATE) on the use of technology among community-dwelling adults. The sample included 1,204 individuals ranging in age from 18-91 years. All participants completed a battery that included measures of demographic characteristics, self-rated health, experience with technology, attitudes toward computers, and component cognitive abilities. Findings indicate that the older adults were less likely than younger adults to use technology in general, computers, and the World Wide Web. The results also indicate that computer anxiety, fluid intelligence, and crystallized intelligence were important predictors of the use of technology. The relationship between age and adoption of technology was mediated by cognitive abilities, computer self-efficacy, and computer anxiety. These findings are discussed in terms of training strategies to promote technology adoption.","[{'authorId': '1748503', 'name': 'S. Czaja'}, {'authorId': '7232085', 'name': 'N. Charness'}, {'authorId': '1689705', 'name': 'A. D. Fisk'}, {'authorId': '51895118', 'name': 'C. Hertzog'}, {'authorId': '2838714', 'name': 'S. Nair'}, {'authorId': '145912604', 'name': 'W. Rogers'}, {'authorId': '3238695', 'name': 'J. Sharit'}]",1649.0,"{'bibtex': '@Article{Czaja2006FactorsPT,\n author = {S. Czaja and N. Charness and A. D. Fisk and C. Hertzog and S. Nair and W. Rogers and J. Sharit},\n journal = {Psychology and aging},\n pages = {\n          333-52\n        },\n title = {Factors predicting the use of technology: findings from the Center for Research and Education on Aging and Technology Enhancement (CREATE).},\n volume = {21 2},\n year = {2006}\n}\n'}",,"{'volume': '21 2', 'pages': '\n          333-52\n        ', 'name': 'Psychology and aging'}",71.0,Factors predicting the use of technology: findings from the Center for Research and Education on Aging and Technology Enhancement (CREATE).,2006.0
2239,bf4045307607f783ae8e22e805eed62c1c4e9b84,"This paper describes the basic principles of traditional 2D hand drawn animation and their application to 3D computer animation. After describing how these principles evolved, the individual principles are detailed, addressing their meanings in 2D hand drawn animation and their application to 3D computer animation. This should demonstrate the importance of these principles to quality 3D computer animation.","[{'authorId': '31781220', 'name': 'J. Lasseter'}]",709.0,"{'bibtex': '@Article{Lasseter1987PrinciplesOT,\n author = {J. Lasseter},\n journal = {Proceedings of the 14th annual conference on Computer graphics and interactive techniques},\n title = {Principles of traditional animation applied to 3D computer animation},\n year = {1987}\n}\n'}",,{'name': 'Proceedings of the 14th annual conference on Computer graphics and interactive techniques'},28.0,Principles of traditional animation applied to 3D computer animation,1987.0
2240,bf80f74b38018431ce2e68f6af0f032cc4722b6b,,"[{'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '2724558', 'name': 'Dieta Kuchenbrandt'}]",4.0,"{'bibtex': '@Inproceedings{Eyssel2011MyRI,\n author = {F. Eyssel and Dieta Kuchenbrandt},\n title = {My robot is more human than yours: Effects of group membership on anthropomorphic judgments of social robots},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,My robot is more human than yours: Effects of group membership on anthropomorphic judgments of social robots,2011.0
2241,bfa6eb0241c6968f178f00a35677fa3df30706a7,This paper explores how interaction with systems using touchless gestures can be made intuitive and natural. Analysis of 912 video clips of gesture production from a user study of 16 subjects communicating transitive actions (manipulation of objects with or without external tools) indicated that 1) dynamic pantomimic gestures where imagined tool/object is explicitly held are performed more intuitively and easily than gestures where a body part is used to represent the tool/object or compared to static hand poses and 2) gesturing while communicating the transitive action as how the user habitually performs the action (pantomimic action) is perceived to be easier and more natural than gesturing while communicating it as an instruction. These findings provide guidelines for the characteristics of gestures and user mental models one must consciously be concerned with when designing and implementing gesture vocabularies of touchless interaction.,"[{'authorId': '2246337', 'name': 'Sukeshini A. Grandhi'}, {'authorId': '2988256', 'name': 'Gina Joue'}, {'authorId': '2076370', 'name': 'I. Mittelberg'}]",126.0,"{'bibtex': '@Article{Grandhi2011UnderstandingNA,\n author = {Sukeshini A. Grandhi and Gina Joue and I. Mittelberg},\n journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n title = {Understanding naturalness and intuitiveness in gesture production: insights for touchless gestural interfaces},\n year = {2011}\n}\n'}",,{'name': 'Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'},23.0,Understanding naturalness and intuitiveness in gesture production: insights for touchless gestural interfaces,2011.0
2242,bfc96faaf8d52d5c96684dfc9ed7189596508018,"ABSTRACT Background Emotion regulation difficulties have been associated with depression and anxiety in typically developing individuals. However, until recently, the impact of emotion regulation difficulties for adolescents and young adults with autism spectrum disorder (ASD) has received little attention. We investigated emotion regulation difficulties from the perspective of those who would experience the sequelae. This included parents, teachers, and psychologists. Method Seven focus groups with parents, teachers, and psychologists, and 7 interviews with adolescents and young adults with ASD were conducted. Results Across the groups, participants discussed their triggers of distressing emotions, difficulties with emotional awareness, emotion regulation strategies, and the consequences of their distressing emotions. Both depression and anxiety were perceived as the most experienced distressing issues with the greatest consequences. Conclusions The implications of the themes revealed in the interviews and focus groups are discussed in light in previous literature and may help to inform future interventions.","[{'authorId': '6731644', 'name': 'D. Santomauro'}, {'authorId': '5316023', 'name': 'J. Sheffield'}, {'authorId': '4384768', 'name': 'K. Sofronoff'}]",15.0,"{'bibtex': '@Article{Santomauro2017InvestigationsIE,\n author = {D. Santomauro and J. Sheffield and K. Sofronoff},\n journal = {Journal of Intellectual & Developmental Disability},\n pages = {275 - 284},\n title = {Investigations into emotion regulation difficulties among adolescents and young adults with autism spectrum disorder: A qualitative study},\n volume = {42},\n year = {2017}\n}\n'}",,"{'volume': '42', 'pages': '275 - 284', 'name': 'Journal of Intellectual & Developmental Disability'}",59.0,Investigations into emotion regulation difficulties among adolescents and young adults with autism spectrum disorder: A qualitative study,2017.0
2243,bfd10454e369ac9451889485690c1013993604e7,"Objective: To analyze gait patterns associated with sadness and depression. Embodiment theories suggest a reciprocal relationship between bodily expression and the way in which emotions are processed. Methods: In Study 1, the gait patterns of 14 inpatients suffering from major depression were compared with those of matched never-depressed participants. In Study 2, we employed musical mood induction to induce sad and positive mood in a sample of 23 undergraduates. A Fourier-based description of walking data served as the basis for the computation of linear classifiers and for the analysis of gait parameters. Results: Gait patterns associated with sadness and depression are characterized by reduced walking speed, arm swing, and vertical head movements. Moreover, depressed and sad walkers displayed larger lateral swaying movements of the upper body and a more slumped posture. Conclusion: The results of the present study indicate that a specific gait pattern characterizes individuals in dysphoric mood. ANCOVA = analysis of covariance; BDI = Beck Depression Inventory; DSM = Diagnostic and Statistical Manual of Mental Disorders; MANOVA = multivariate analysis of variance; MDD = major depressive disorder; SCID = Structured Clinical Interview for DSM-IV; SNRI = serotonin noradrenalin reuptake inhibitors; SSRI = selective serotonin reuptake inhibitors.","[{'authorId': '38575741', 'name': 'J. Michalak'}, {'authorId': '2932365', 'name': 'N. Troje'}, {'authorId': '2087114312', 'name': 'J. Fischer'}, {'authorId': '5643524', 'name': 'P. Vollmar'}, {'authorId': '2675262', 'name': 'T. Heidenreich'}, {'authorId': '153024184', 'name': 'D. Schulte'}]",347.0,"{'bibtex': '@Article{Michalak2009EmbodimentOS,\n author = {J. Michalak and N. Troje and J. Fischer and P. Vollmar and T. Heidenreich and D. Schulte},\n journal = {Psychosomatic Medicine},\n pages = {580-587},\n title = {Embodiment of Sadness and Depression—Gait Patterns Associated With Dysphoric Mood},\n volume = {71},\n year = {2009}\n}\n'}",,"{'volume': '71', 'pages': '580-587', 'name': 'Psychosomatic Medicine'}",42.0,Embodiment of Sadness and Depression—Gait Patterns Associated With Dysphoric Mood,2009.0
2244,bfd6b70fa91024a3ea9ba390f57fe903e42b5183,"Research show that teacher's nonverbal immediacy can have a positive impact on student's cognitive learning and affect [3]. This paper investigates the effectiveness of nonverbal immediacy using a virtual human. The virtual human attempts to use immediacy feedback to create rapport with the learner. Results show that the virtual human established rapport with learners but did not help them achieve better learning results. The results also suggest that creating rapport is related to higher self-efficacy, and self-efficacy is related to better learning results.","[{'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",18.0,"{'bibtex': '@Inproceedings{Wang2009CanVH,\n author = {Ning Wang and J. Gratch},\n pages = {737-739},\n title = {Can Virtual Human Build Rapport and Promote Learning?},\n year = {2009}\n}\n'}",,{'pages': '737-739'},56.0,Can Virtual Human Build Rapport and Promote Learning?,2009.0
2247,bfdbfe3bf703594b884ae69f505f94ce7e98141e,"Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each...","[{'authorId': '21451088', 'name': 'P. Ekman'}]",7591.0,"{'bibtex': '@Article{Ekman1992AnAF,\n author = {P. Ekman},\n journal = {Cognition & Emotion},\n pages = {169-200},\n title = {An argument for basic emotions},\n volume = {6},\n year = {1992}\n}\n'}",,"{'volume': '6', 'pages': '169-200', 'name': 'Cognition & Emotion'}",94.0,An argument for basic emotions,1992.0
2248,bff42ea2806f231fd3da19abae8291469e63246b,"Research has shown that virtual agents can be effective tools for teaching negotiation. Virtual agents provide an opportuni-ty for students to practice their negotiation skills which leads to better outcomes. However, these negotiation training agents often lack the ability to understand the errors students make when negotiating, thus limiting their effectiveness as training tools. In this article, we argue that automated opponent-modeling techniques serve as effective methods for diagnos-ing important negotiation mistakes. To demonstrate this, we analyze a large number of participant traces generated while negotiating with a set of automated opponents. We show that negotiators' performance is closely tied to their understanding of an opponent's preferences. We further show that opponent modeling techniques can diagnose specific errors includ-ing: failure to elicit diagnostic information from an opponent, failure to utilize the information that was elicited, and failure to understand the transparency of an opponent. These results show that opponent modeling techniques can be effective methods for diagnosing and potentially correcting crucial ne-gotiation errors.","[{'authorId': '144173470', 'name': 'Emmanuel Johnson'}, {'authorId': '50653207', 'name': 'Sarah Roediger'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",7.0,"{'bibtex': '@Article{Johnson2019AssessingCE,\n author = {Emmanuel Johnson and Sarah Roediger and Gale M. Lucas and J. Gratch},\n journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},\n title = {Assessing Common Errors Students Make When Negotiating},\n year = {2019}\n}\n'}",,{'name': 'Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents'},25.0,Assessing Common Errors Students Make When Negotiating,2019.0
2249,c0292ef46a4bed4f8146a654983683a206295f1d,"We have designed, developed, and tested an Immersive virtual reality (VR) platform to practice the protocols of Self-attachment psychotherapy. We made use of customized photorealistic avatars for the implementation of both the high-end version (based on Facebook’s Oculus) and the low-end version (based on Google’s cardboard) of our platform. Under the Selfattachment therapeutic framework, the causes of mental disorders such as chronic anxiety and depression are traced back to the individual’s insecure attachment with their primary caregiver during childhood and their subsequent problems in affect regulation. The conventional approach (without VR) to Selfattachment requires that the individual uses their childhood photographs to recall their childhood memories and then imagine that the child that they were is present with them. They thus establish a compassionate relationship with their childhood self and then, using love songs and dancing, create an affectional bond with them. Their adult self subsequently role plays a good parent and interacts with their imagined childhood self to perform various developmental and re-parenting activities. The goal is to enhance their capacities for self-regulation of emotion, which can lead them into earning secure attachment. It is hypothesized that our immersive virtual reality platform – which enables the users to interact with their customized 3D photorealistic childhood avatar - offers either a better alternative or at least a complementary visual tool to the conventional imaginal approach to Self-attachment. The platform was developed in Unity 3D, a cross-platform game engine, and takes advantage of the itSeez3D Avatar SDK for generating a customized photorealistic 3D avatar head from a 2D childhood image of the user. The platform also offers facial and body animations for some of the basic emotional states such as Happy, Sad, Scared and Joyful and it allows modifications to the avatar body (height/ width) and clothing color. A study to compare the use of the avatar-based approach (VR) to Self-attachment with the conventional photo-based approach showed promising results. Almost 85% of the participants reported that their photorealistic childhood avatar in VR was more relatable than their childhood photos. Both low-end and high-end VR based approaches were unanimously reported to be more effective than the conventional imaginal approach. Participants reported that the high-end version of the VR platform was more realistic and immersive than the low-end mobile VR version.","[{'authorId': '2485535', 'name': 'I. Ghaznavi'}, {'authorId': '146493111', 'name': 'Duncan Gillies'}, {'authorId': '40054719', 'name': 'D. Nicholls'}, {'authorId': '1694989', 'name': 'A. Edalat'}]",2.0,"{'bibtex': '@Article{Ghaznavi2020PhotorealisticAT,\n author = {I. Ghaznavi and Duncan Gillies and D. Nicholls and A. Edalat},\n journal = {2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)},\n pages = {60-67},\n title = {Photorealistic avatars to enhance the efficacy of Selfattachment psychotherapy},\n year = {2020}\n}\n'}",,"{'pages': '60-67', 'name': '2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)'}",42.0,Photorealistic avatars to enhance the efficacy of Selfattachment psychotherapy,2020.0
2250,c0373426c8e5579dcff60cc0bd930277822edc7d,"A theory of analogy must describe how the meaning of on analogy is derived from the meonings of its parts. In the structure-mapplng theory, the interpretation rules ore characterized OS implicit rules for mapping knowledge about a base domain into a torget domain. Two important features of the theory are (a) the rules depend only on syntactic properties of the knowledge representation, and not on the specific content of the domoins; ond (b) the theoretical fromework allows analogies to be distinguished o ond (b) The particular relations mapped ore determined by systemaficity. OS defined by the existence of higher-order relations.","[{'authorId': '1704065', 'name': 'D. Gentner'}]",5174.0,"{'bibtex': '@Article{Gentner1983StructureMappingAT,\n author = {D. Gentner},\n journal = {Cogn. Sci.},\n pages = {155-170},\n title = {Structure-Mapping: A Theoretical Framework for Analogy},\n volume = {7},\n year = {1983}\n}\n'}",,"{'volume': '7', 'pages': '155-170', 'name': 'Cogn. Sci.'}",58.0,Structure-Mapping: A Theoretical Framework for Analogy,1983.0
2251,c048797031d2fe53e4f8c9af8ce46952bed284cb,,"[{'authorId': '114421967', 'name': 'Catherine R Hurd'}]",3.0,"{'bibtex': '@Inproceedings{Hurd2017EmotionRA,\n author = {Catherine R Hurd},\n title = {Emotion Regulation and Autism Spectrum Disorders: A Literature Review},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",21.0,Emotion Regulation and Autism Spectrum Disorders: A Literature Review,2017.0
2252,c05d2a7ed16e47f2ab2dd18435b7a0380b5041dd,"To create socially aware virtual agents, we conduct research along two main research directions: 1) develop richer models of multimodal behaviors for the agent; 2) make the agent a more socially competent interlocutor.","[{'authorId': '1703084', 'name': 'C. Pelachaud'}]",7.0,"{'bibtex': '@Article{Pelachaud2017GretaAC,\n author = {C. Pelachaud},\n journal = {Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents},\n title = {Greta: a conversing socio-emotional agent},\n year = {2017}\n}\n'}",,{'name': 'Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents'},19.0,Greta: a conversing socio-emotional agent,2017.0
2253,c066c57bc239fcf0829cc73ae25935334ba06acc,"This paper provides a logical analysis of the concept of intention as composed of two more basic concepts, choice (or goal) and commitment. By making explicit the conditions under which an agent can drop her goals, i.e., by specifying how the agent is committed to her goals, the formalism provides analyses for Bratman's three characteristic functional roles played by intentions [Bratman, 1986], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent's intentions in terms of beliefs about another agent's intentions (or beliefs), we derive a preliminary account of interpersonal commitments.","[{'authorId': '2305444', 'name': 'Philip R. Cohen'}, {'authorId': '143634377', 'name': 'H. Levesque'}]",2270.0,"{'bibtex': '@Inproceedings{Cohen1987IntentionC,\n author = {Philip R. Cohen and H. Levesque},\n pages = {410-415},\n title = {Intention = Choice + Commitment},\n year = {1987}\n}\n'}",,{'pages': '410-415'},18.0,Intention = Choice + Commitment,1987.0
2254,c08b5a8338cdceae6d78b7108086e17d58c20b62,,"[{'authorId': '1809740', 'name': 'Jenay M. Beer'}, {'authorId': '1963261', 'name': 'Cory-Ann Smarr'}, {'authorId': '1689705', 'name': 'A. D. Fisk'}, {'authorId': '145912604', 'name': 'W. Rogers'}]",27.0,"{'bibtex': ""@Article{Beer2015YoungerAO,\n author = {Jenay M. Beer and Cory-Ann Smarr and A. D. Fisk and W. Rogers},\n journal = {International journal of human-computer studies},\n pages = {\n          1-20\n        },\n title = {Younger and older users' recognition of virtual agent facial expressions},\n volume = {75},\n year = {2015}\n}\n""}",,"{'volume': '75', 'pages': '\n          1-20\n        ', 'name': 'International journal of human-computer studies'}",91.0,Younger and older users' recognition of virtual agent facial expressions,2015.0
2256,c0c2ca0c2c2597116121ef86d3d62dabcad7fefa,"The design of an affect recognition system for socially perceptive robots relies on representative data: human-robot interaction in naturalistic settings requires an affect recognition system to be trained and validated with contextualised affective expressions, that is, expressions that emerge in the same interaction scenario of the target application. In this paper we propose an initial computational model to automatically analyse human postures and body motion to detect engagement of children playing chess with an iCat robot that acts as a game companion. Our approach is based on vision-based automatic extraction of expressive postural features from videos capturing the behaviour of the children from a lateral view. An initial evaluation, conducted by training several recognition models with contextualised affective postural expressions, suggests that patterns of postural behaviour can be used to accurately predict the engagement of the children with the robot, thus making our approach suitable for integration into an affect recognition system for a game companion in a real world scenario.","[{'authorId': '40469173', 'name': 'Jyotirmay Sanghvi'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '39799707', 'name': 'Iolanda Leite'}, {'authorId': '11845717', 'name': 'André Pereira'}, {'authorId': '2803283', 'name': 'P. McOwan'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",267.0,"{'bibtex': '@Article{Sanghvi2011AutomaticAO,\n author = {Jyotirmay Sanghvi and Ginevra Castellano and Iolanda Leite and André Pereira and P. McOwan and Ana Paiva},\n journal = {2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {305-311},\n title = {Automatic analysis of affective postures and body motion to detect engagement with a game companion},\n year = {2011}\n}\n'}",,"{'pages': '305-311', 'name': '2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",34.0,Automatic analysis of affective postures and body motion to detect engagement with a game companion,2011.0
2257,c0d7e39bc35f2cc22ba27a2f1f749a8f0212bcdf,"Since the early twentieth century, psychologists have known that there is consensus in attributing social and personality characteristics from facial appearance. Recent studies have shown that surprisingly little time and effort are needed to arrive at this consensus. Here we review recent research on social attributions from faces. Section I outlines data-driven methods capable of identifying the perceptual basis of consensus in social attributions from faces (e.g., What makes a face look threatening?). Section II describes nonperceptual determinants of social attributions (e.g., person knowledge and incidental associations). Section III discusses evidence that attributions from faces predict important social outcomes in diverse domains (e.g., investment decisions and leader selection). In Section IV, we argue that the diagnostic validity of these attributions has been greatly overstated in the literature. In the final section, we offer an account of the functional significance of these attributions.","[{'authorId': '145441940', 'name': 'A. Todorov'}, {'authorId': '3448518', 'name': 'Christopher Y. Olivola'}, {'authorId': '2365875', 'name': 'R. Dotsch'}, {'authorId': '1403899502', 'name': 'P. Mende-Siedlecki'}]",680.0,"{'bibtex': '@Article{Todorov2015SocialAF,\n author = {A. Todorov and Christopher Y. Olivola and R. Dotsch and P. Mende-Siedlecki},\n journal = {Annual review of psychology},\n pages = {\n          519-45\n        },\n title = {Social attributions from faces: determinants, consequences, accuracy, and functional significance.},\n volume = {66},\n year = {2015}\n}\n'}",,"{'volume': '66', 'pages': '\n          519-45\n        ', 'name': 'Annual review of psychology'}",187.0,"Social attributions from faces: determinants, consequences, accuracy, and functional significance.",2015.0
2258,c0f2bdd626ce66f91e14a6af604f9a1f3355a6c1,,"[{'authorId': '50419262', 'name': 'S. Pulman'}]",44.0,"{'bibtex': '@Inproceedings{Pulman1997ConversationalGB,\n author = {S. Pulman},\n title = {Conversational Games‚ Belief Revision and Bayesian Networks},\n year = {1997}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Conversational Games‚ Belief Revision and Bayesian Networks,1997.0
2259,c0f98b13fb62fcd49a389de541870d15bd3247be,"Part I: BACKGROUND: About emotion Issues of research, classification and measurements Part II: THE COGNITIVE-MOTIVATIONAL-RELATIONAL THEORY: The person-environment relationship: motivation and coping Cognition and emotion Issues of causality Part III: INDIVIDUAL EMOTIONS: Goal incongruent (negative) emotions Goal congruent (positive) and problematic emotions Part IV: EMOTIONAL DEVELOPMENT: Individual development Social influence Part V: PRACTICAL APPLICATIONS: Emotions and health Implications for research, assessment, treatment and disease prevention References Index.","[{'authorId': '5628684', 'name': 'R. Lazarus'}]",5248.0,"{'bibtex': '@Inproceedings{Lazarus1991EmotionAA,\n author = {R. Lazarus},\n title = {Emotion and Adaptation},\n year = {1991}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotion and Adaptation,1991.0
2263,c10141ed6b04b5eea3da602e75e3ad1a08262ba1,"Clinical anxiety disorders and elevated levels of anxiety vulnerability are characterized by cognitive biases, and this processing selectivity has been implicated in theoretical accounts of these conditions. We review research that has sought to evaluate the causal contributions such biases make to anxiety dysfunction and to therapeutically alleviate anxiety using cognitive-bias modification (CBM) procedures. After considering the purpose and nature of CBM methodologies, we show that variants designed to modify selective attention (CBM-A) or interpretation (CBM-I) have proven capable of reducing anxiety vulnerability and ameliorating dysfunctional anxiety. In addition to supporting the causal role of cognitive bias in anxiety vulnerability and dysfunction and illuminating the mechanisms that underpin such bias, the findings suggest that CBM procedures may have therapeutic promise within clinical settings. We discuss key issues within this burgeoning field of research and suggest future directions CBM research should take to maximize its theoretical and applied value.","[{'authorId': '145934733', 'name': 'C. MacLeod'}, {'authorId': '1808296', 'name': 'A. Mathews'}]",521.0,"{'bibtex': '@Article{MacLeod2012CognitiveBM,\n author = {C. MacLeod and A. Mathews},\n journal = {Annual review of clinical psychology},\n pages = {\n          189-217\n        },\n title = {Cognitive bias modification approaches to anxiety.},\n volume = {8},\n year = {2012}\n}\n'}",,"{'volume': '8', 'pages': '\n          189-217\n        ', 'name': 'Annual review of clinical psychology'}",125.0,Cognitive bias modification approaches to anxiety.,2012.0
2264,c10d2d7d1fd68e6517a7360fdfca72b787405b67,,"[{'authorId': '2870739', 'name': 'A. Kleinsmith'}, {'authorId': '2223722359', 'name': 'P. Ravindra De Silva'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}]",192.0,"{'bibtex': '@Article{Kleinsmith2006CrossculturalDI,\n author = {A. Kleinsmith and P. Ravindra De Silva and N. Bianchi-Berthouze},\n journal = {Interact. Comput.},\n pages = {1371-1389},\n title = {Cross-cultural differences in recognizing affect from body posture},\n volume = {18},\n year = {2006}\n}\n'}",,"{'volume': '18', 'pages': '1371-1389', 'name': 'Interact. Comput.'}",45.0,Cross-cultural differences in recognizing affect from body posture,2006.0
2266,c113f8a477b2348b4b163a9a6fd420b8a826a435,"Individuals with Autism are characterized by deficits in socialization and communication. In recent years several assistive technologies, e.g., Virtual Reality (VR), have been investigated to address the socialization deficits in these individuals. Presently available VR-based systems address various aspects of social communication in an isolated manner and without monitoring one's affective state such as, anxiety. However, in conventional observation-based therapy, a therapist adjusts the intervention paradigm by monitoring one's anxiety level. But, often these individuals have an inherent inability to explicitly express their anxiety thereby inducing limitations on conventional techniques. Physiological signals being continuously available and not directly impacted by these communication difficulties can be alternatively used as markers of one's anxiety level. In our research we aim at designing a Virtual-reality bAsed Social-communication Task (VAST) system that can address the various aspects of social communication, e.g., social context, subtle social cues, emotional expression, etc., in a cumulative and structured way. In addition, we augment this with a capability to use one's physiological signals as markers of one's anxiety level. In our preliminary feasibility study we investigate the potential of VAST to cause variations in one's performance and anxiety level that can be mapped from one's physiological indices.","[{'authorId': '2388380', 'name': 'Selvia Kuriakose'}, {'authorId': '2393577', 'name': 'U. Lahiri'}]",52.0,"{'bibtex': '@Article{Kuriakose2015UnderstandingTP,\n author = {Selvia Kuriakose and U. Lahiri},\n journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},\n pages = {665-675},\n title = {Understanding the Psycho-Physiological Implications of Interaction With a Virtual Reality-Based System in Adolescents With Autism: A Feasibility Study},\n volume = {23},\n year = {2015}\n}\n'}",,"{'volume': '23', 'pages': '665-675', 'name': 'IEEE Transactions on Neural Systems and Rehabilitation Engineering'}",28.0,Understanding the Psycho-Physiological Implications of Interaction With a Virtual Reality-Based System in Adolescents With Autism: A Feasibility Study,2015.0
2267,c130cd3c5708df9034e4748742d944183b642595,"We are seeking to outline a framework to create embodied agents with consistency both in terms of human actions and communications in general and individual humans in particular. Our goal is to drive this consistent behavior from internal or cognitive models of the agents. First, we describe channels of non-verbal communication and related research in embodied agents. We then describe cognitive processes that can be used to coordinate these channels of communication and create consistent behavior. Comments Postprint version. Presented at Workshop on Non-Verbal and Verbal Communicative Acts to Achieve Contextual Embodied Agents, Autonomous Agents 2001, 6 pages. This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/hms/85 Consistent Communication with Control Jan M. Allbeck and Norman I. Badler Center for Human Modeling and Simulation University of Pennsylvania 200 S. 33rd St., Philadelphia, PA 19104-6389 allbeck@graphics.cis.upenn.edu","[{'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '1699200', 'name': 'N. Badler'}]",15.0,"{'bibtex': '@Inproceedings{Allbeck2001ConsistentCW,\n author = {J. Allbeck and N. Badler},\n title = {Consistent Communication with Control},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",30.0,Consistent Communication with Control,2001.0
2268,c134bb64f1547baa693fd6593a8ae77bb5289b6a,"setting that mirrors a real-life situation, but with, for example, the experimental design manipulating particular factors of interest in exposures of different groups of subjects to the experience. Here we argue that immersive virtual reality is especially interesting for the study of how people respond to violent incidents where a perpetrator attacks a victim. We are interested in the circumstances under which bystanders are likely to intervene in order to prevent harm to the victim. Immersive virtual reality provides an ecologically valid setting in which to study this issue while at the same time removing the problem of physical danger, and overcoming the many ethical issues involved in the study of violence. In the rest of this paper we fi rst briefl y review some of the literature relating to responses to violence in desktop based systems such as video games, before going on to describe what we mean by immersive virtual reality, and how this is profoundly different with respect to engaging people in realistic responses to virtual situations. Next we review one experiment where participants infl ict violence on virtual characters in immersive virtual reality, a reprise of the Stanley Milgram obedience experiments. Next we apply these ideas to new research that is using virtual reality to study bystander responses to violent incidents, and we describe some qualitative results from ongoing pilot studies. Finally we discuss recommendations for the use of virtual reality in the study of violence. Our experiments discussed in this paper have been approved by the UCL Research Ethics Committee.",[],92.0,"{'bibtex': '@Misc{None,\n title = {Behavioral Neuroscience Methods Article}\n}\n'}",,,51.0,Behavioral Neuroscience Methods Article,
2270,c151f144c2c0e8d3b176edaf2ce5369c7707bd31,"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","[{'authorId': '1745524', 'name': 'Tim Althoff'}, {'authorId': '144358401', 'name': 'Kevin Clark'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]",220.0,"{'bibtex': '@Article{Althoff2016LargescaleAO,\n author = {Tim Althoff and Kevin Clark and J. Leskovec},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {463 - 476},\n title = {Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health},\n volume = {4},\n year = {2016}\n}\n'}",,"{'volume': '4', 'pages': '463 - 476', 'name': 'Transactions of the Association for Computational Linguistics'}",40.0,Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health,2016.0
2271,c16f03480935ca2eb4cb55df1a06b071f32959f8,,"[{'authorId': '2548703', 'name': 'M. Floater'}]",1096.0,"{'bibtex': '@Article{Floater2003MeanVC,\n author = {M. Floater},\n journal = {Comput. Aided Geom. Des.},\n pages = {19-27},\n title = {Mean value coordinates},\n volume = {20},\n year = {2003}\n}\n'}",,"{'volume': '20', 'pages': '19-27', 'name': 'Comput. Aided Geom. Des.'}",17.0,Mean value coordinates,2003.0
2272,c1ac0c8e52cad06a91c783606771ac320211a0a0,,"[{'authorId': '2622833', 'name': 'Salvatore Parise'}, {'authorId': '47198673', 'name': 'S. Kiesler'}, {'authorId': '2198165', 'name': 'L. Sproull'}, {'authorId': '150179849', 'name': 'Keith Waters'}]",142.0,"{'bibtex': '@Article{Parise1999CooperatingWL,\n author = {Salvatore Parise and S. Kiesler and L. Sproull and Keith Waters},\n journal = {Computers in Human Behavior},\n pages = {123-142},\n title = {Cooperating with life-like interface agents},\n volume = {15},\n year = {1999}\n}\n'}",,"{'volume': '15', 'pages': '123-142', 'name': 'Computers in Human Behavior'}",27.0,Cooperating with life-like interface agents,1999.0
2273,c1f0eee9e0ef6052beb5e7e2b3aede2a4437bf52,"Natural language generation (NLG) is an important component of question answering(QA) systems which has a significant impact on system quality. Most tranditional QA systems based on templates or rules tend to generate rigid and stylised responses without the natural variation of human language. Furthermore, such methods need an amount of work to generate the templates or rules. To address this problem, we propose a Context-Aware LSTM model for NLG. The model is completely driven by data without manual designed templates or rules. In addition, the context information, including the question to be answered, semantic values to be addressed in the response, and the dialogue act type during interaction, are well approached in the neural network model, which enables the model to produce variant and informative responses. The quantitative evaluation and human evaluation show that CA-LSTM obtains state-of-the-art performance.","[{'authorId': '144751955', 'name': 'Hao Zhou'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}]",23.0,"{'bibtex': '@Inproceedings{Zhou2016ContextawareNL,\n author = {Hao Zhou and Minlie Huang and Xiaoyan Zhu},\n pages = {2032-2041},\n title = {Context-aware Natural Language Generation for Spoken Dialogue Systems},\n year = {2016}\n}\n'}",,{'pages': '2032-2041'},22.0,Context-aware Natural Language Generation for Spoken Dialogue Systems,2016.0
2274,c1faf90821d6d9c3a445d472c145f13c1e19143f,"If the theory advanced by Watson and Morgan (in 'Emotional Reactions and Psychological Experimentation,' American Journal of Psychology, April, 1917, Vol. 28, pp. 163-174) to the effect that in infancy the original emotional reaction patterns are few, consisting so far as observed of fear, rage and love, then there must be some simple method by means of which the range of stimuli which can call out these emotions and their compounds is greatly increased. Otherwise, complexity in adult response could not be accounted for. These authors without adequate experimental evidence advanced the view that this range was increased by means of conditioned reflex factors. It was suggested there that the early home life of the child furnishes a laboratory situation for establishing conditioned emotional responses. The present authors present their experimental findings of conditioned fear responses in a male infant beginning at 11 months of age. (PsycINFO Database Record (c) 2012 APA, all rights reserved)","[{'authorId': '145762127', 'name': 'J. Watson'}, {'authorId': '144984026', 'name': 'R. Rayner'}]",1742.0,"{'bibtex': '@Article{Watson1920ConditionedER,\n author = {J. Watson and R. Rayner},\n journal = {Journal of Experimental Psychology},\n pages = {1-14},\n title = {Conditioned emotional reactions},\n volume = {3},\n year = {1920}\n}\n'}",,"{'volume': '3', 'pages': '1-14', 'name': 'Journal of Experimental Psychology'}",0.0,Conditioned emotional reactions,1920.0
2275,c22c01aa1deff3ce97180fe697b243cda3d3b60f,,"[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '25550816', 'name': 'A. L. Baylor'}]",115.0,"{'bibtex': '@Article{Kim2016ResearchBasedDO,\n author = {Yanghee Kim and A. L. Baylor},\n journal = {International Journal of Artificial Intelligence in Education},\n pages = {160-169},\n title = {Research-Based Design of Pedagogical Agent Roles: a Review, Progress, and Recommendations},\n volume = {26},\n year = {2016}\n}\n'}",,"{'volume': '26', 'pages': '160-169', 'name': 'International Journal of Artificial Intelligence in Education'}",68.0,"Research-Based Design of Pedagogical Agent Roles: a Review, Progress, and Recommendations",2016.0
2277,c23dbcf0fa401d387246d8cd7f227b992c9ca2ec,"The computation by which our brain elaborates fast responses to emotional expressions is currently an active field of brain studies. Previous studies have focused on stimuli taken from everyday life. Here, we investigated event-related potentials in response to happy vs neutral stimuli of human and non-humanoid robots. At the behavioural level, emotion shortened reaction times similarly for robotic and human stimuli. Early P1 wave was enhanced in response to happy compared to neutral expressions for robotic as well as for human stimuli, suggesting that emotion from robots is encoded as early as human emotion expression. Congruent with their lower faceness properties compared to human stimuli, robots elicited a later and lower N170 component than human stimuli. These findings challenge the claim that robots need to present an anthropomorphic aspect to interact with humans. Taken together, such results suggest that the early brain processing of emotional expressions is not bounded to human-like arrangements embodying emotion.","[{'authorId': '3440983', 'name': 'S. Dubal'}, {'authorId': '2078664017', 'name': 'A. Foucher'}, {'authorId': '3198831', 'name': 'R. Jouvent'}, {'authorId': '2433022', 'name': 'J. Nadel'}]",68.0,"{'bibtex': '@Article{Dubal2011HumanBS,\n author = {S. Dubal and A. Foucher and R. Jouvent and J. Nadel},\n journal = {Social cognitive and affective neuroscience},\n pages = {\n          90-7\n        },\n title = {Human brain spots emotion in non humanoid robots.},\n volume = {6 1},\n year = {2011}\n}\n'}",,"{'volume': '6 1', 'pages': '\n          90-7\n        ', 'name': 'Social cognitive and affective neuroscience'}",73.0,Human brain spots emotion in non humanoid robots.,2011.0
2278,c25526ad368ac02386f028962d8a00f8e94f956a,"Comments that cross-lagged correlation (CLC) is not a useful procedure for the analysis of longitudinal panel data. In particular, the difference between CLCs is not a sound basis for causal inference. Demonstrations of the failure of CLC are based mainly on results for the 2-wave, 2-variable longit","[{'authorId': '32127380', 'name': 'D. Rogosa'}]",624.0,"{'bibtex': '@Article{Rogosa1980ACO,\n author = {D. Rogosa},\n journal = {Psychological Bulletin},\n pages = {245-258},\n title = {A critique of cross-lagged correlation},\n volume = {88},\n year = {1980}\n}\n'}",,"{'volume': '88', 'pages': '245-258', 'name': 'Psychological Bulletin'}",33.0,A critique of cross-lagged correlation,1980.0
2279,c2617b8b6632e8f3349e90bc37c2f2cc9a192190,"We investigated whether the dynamics of head and facial movements apart from specific facial expressions communicate affect in infants. Age-appropriate tasks were used to elicit positive and negative affect in 28 ethnically diverse 12-month-old infants. 3D head and facial movements were tracked from 2D video. Strong effects were found for both head and facial movements. For head movement, angular velocity and angular acceleration of pitch, yaw, and roll were higher during negative relative to positive affect. For facial movement, displacement, velocity, and acceleration also increased during negative relative to positive affect. Our results suggest that the dynamics of head and facial movements communicate affect at ages as young as 12 months. These findings deepen our understanding of emotion communication and provide a basis for studying individual differences in emotion in socio-emotional development.","[{'authorId': '1785007', 'name': 'Z. Hammal'}, {'authorId': '1737918', 'name': 'J. Cohn'}, {'authorId': '1811221', 'name': 'C. Heike'}, {'authorId': '2706570', 'name': 'M. Speltz'}]",15.0,"{'bibtex': '@Article{Hammal2015WhatCH,\n author = {Z. Hammal and J. Cohn and C. Heike and M. Speltz},\n journal = {2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {281-287},\n title = {What can head and facial movements convey about positive and negative affect?},\n year = {2015}\n}\n'}",,"{'pages': '281-287', 'name': '2015 International Conference on Affective Computing and Intelligent Interaction (ACII)'}",35.0,What can head and facial movements convey about positive and negative affect?,2015.0
2280,c263083aa7d9e95e4be451b9c221162aa922bf4a,"Building a long-term relationship between human and virtual agent remains a challenge. This paper explores the influence of intimate behavioral cues and the impact of different interaction modalities (voice, text, gesture) on our perception of intimacy. We built a virtual Tourism Information (TI) counselor capable of intimate behaviors based on grounded theories of intimacy developed in psychology. We studied how external observers perceive the social behaviors of the agent during its interactions with a human tourist. Our originality is to evaluate the perception of social skills across the interaction. Our results show that nonverbal behaviors of the virtual agent reinforces the impact of verbal cues on human perception on virtual intimacy. Findings also suggest that textual medium of communication has a negative impact on the perception of virtual intimacy, whatever the level of animation and intimacy exhibited by the TI counselor.","[{'authorId': '51898243', 'name': 'D. Potdevin'}, {'authorId': '1724799', 'name': 'C. Clavel'}, {'authorId': '1731432', 'name': 'N. Sabouret'}]",15.0,"{'bibtex': '@Article{Potdevin2018VirtualIT,\n author = {D. Potdevin and C. Clavel and N. Sabouret},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {Virtual Intimacy, this little something between us: A study about Human perception of intimate behaviors in Embodied Conversational Agents},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},27.0,"Virtual Intimacy, this little something between us: A study about Human perception of intimate behaviors in Embodied Conversational Agents",2018.0
2281,c28bd8193f89554205c887443d7836130d48e244,"With the rapid development of computer technology, a new form of media – interactive narrative has received increasing attention. Interactive narrative allows the user to participate in a dynamically unfolding story, by playing a character or by exerting directorial control. By allowing the user to interact, interactive narrative provides a richer and potentially more engaging experience than traditional narrative. Moreover, because different choices of the user lead to different paths through the story, the author of interactive narrative can tailor the experience for the user or user groups. 
The design of interactive narrative faces many challenges. The central challenge comes from the integration of interactivity into the narrative. Instead of presenting one well-crafted static story, the author has to design the characters' behaviors along many paths through the story in response to possible user interactions. The amount of work can easily overwhelm an author. 
In this thesis, I present a multi-agent approach to modeling and simulating interactive narrative, implemented as the Thespian framework. Thespian utilizes a two-layer runtime system to drive the characters' interactions with the user. At the base is a multi-agent system comprised of goal-oriented autonomous agents that realize the characters in the story. Above this layer is a proactive director agent that continuously monitors the progress of the story and directs the characters toward the author's plot design goals. In addition to the two-layer runtime system, Thespian contains offline authoring procedures to facilitate the author in configuring the characters. 
The evaluation of the Thespian framework has been performed at different levels. Various components within Thespian have been individually evaluated or validated. In addition, Thespian's generality in practice for authoring a range of stories has been demonstrated through its many applications in different domains.","[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '33432486', 'name': 'Mei Si'}]",11.0,"{'bibtex': '@Inproceedings{Marsella2010ThespianAD,\n author = {S. Marsella and Mei Si},\n title = {Thespian: a decision-theoretic framework for interactive narratives},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Thespian: a decision-theoretic framework for interactive narratives,2010.0
2282,c2b0304936bf260113f48c1675f02db12a5f9a1c,"We present a data-driven algorithm for generating gaits of virtual characters with varying dominance traits. Our formulation utilizes a user study to establish a data-driven dominance mapping between gaits and dominance labels. We use our dominance mapping to generate walking gaits for virtual characters that exhibit a variety of dominance traits while interacting with the user. Furthermore, we extract gait features based on known criteria in visual perception and psychology literature that can be used to identify the dominance levels of any walking gait. We validate our mapping and the perceived dominance traits by a second user study in an immersive virtual environment. Our gait dominance classification algorithm can classify the dominance traits of gaits with ˜73 percent accuracy. We also present an application of our approach that simulates interpersonal relationships between virtual characters. To the best of our knowledge, ours is the first practical approach to classifying gait dominance and generate dominance traits in virtual characters.","[{'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '2718563', 'name': 'Aniket Bera'}, {'authorId': '40894651', 'name': 'Emily Kubin'}, {'authorId': '144470585', 'name': 'Kurt Gray'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",16.0,"{'bibtex': '@Article{Randhavane2019ModelingDD,\n author = {Tanmay Randhavane and Aniket Bera and Emily Kubin and Kurt Gray and Dinesh Manocha},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {2967-2979},\n title = {Modeling Data-Driven Dominance Traits for Virtual Characters Using Gait Analysis},\n volume = {27},\n year = {2019}\n}\n'}",,"{'volume': '27', 'pages': '2967-2979', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",78.0,Modeling Data-Driven Dominance Traits for Virtual Characters Using Gait Analysis,2019.0
2283,c2bcef09c15dc2b9df40630bf8af2ab387cec598,,"[{'authorId': '2264449', 'name': 'O. Oguz'}, {'authorId': '50495312', 'name': 'A. Akaydin'}, {'authorId': '2060192299', 'name': 'T. Yilmaz'}, {'authorId': '1746035', 'name': 'U. Güdükbay'}]",38.0,"{'bibtex': '@Article{Oguz2010EmergencyCS,\n author = {O. Oguz and A. Akaydin and T. Yilmaz and U. Güdükbay},\n journal = {Comput. Graph.},\n pages = {136-144},\n title = {Emergency crowd simulation for outdoor environments},\n volume = {34},\n year = {2010}\n}\n'}",,"{'volume': '34', 'pages': '136-144', 'name': 'Comput. Graph.'}",25.0,Emergency crowd simulation for outdoor environments,2010.0
2284,c2f441a578c1f2a5d147d3ac378454839a6cb217,"Ortony and Turner's (1990) arguments against those who adopt the view that there are basic emotions are challenged. The evidence on universals in expression and in physiology strongly suggests that there is a biological basis to the emotions that have been studied. Ortony and Turner's reviews of this literature are faulted, and their alternative theoretical explanations do not fit the evidence. The utility of the basic emotions approach is also shown in terms of the research it has generated.","[{'authorId': '21451088', 'name': 'P. Ekman'}]",1804.0,"{'bibtex': '@Article{Ekman1992AreTB,\n author = {P. Ekman},\n journal = {Psychological review},\n pages = {\n          550-3\n        },\n title = {Are there basic emotions?},\n volume = {99 3},\n year = {1992}\n}\n'}",,"{'volume': '99 3', 'pages': '\n          550-3\n        ', 'name': 'Psychological review'}",35.0,Are there basic emotions?,1992.0
2285,c3251e77adef7cb777c070e3cd0ce067ab17cce3,,"[{'authorId': '2064713461', 'name': 'Jonathan Gratch'}, {'authorId': '2079253023', 'name': 'Anya Okhmatovskaia'}, {'authorId': '2058678290', 'name': 'Francois Lamothe'}, {'authorId': '2066069570', 'name': 'Stacy Marsella'}, {'authorId': '2067580497', 'name': 'Mathieu Morales'}, {'authorId': '2097196392', 'name': 'Rick J. van der Werf'}, {'authorId': '2065275646', 'name': 'Louis-Philippe Morency'}]",252.0,"{'bibtex': '@Inproceedings{Gratch2006VirtualR,\n author = {Jonathan Gratch and Anya Okhmatovskaia and Francois Lamothe and Stacy Marsella and Mathieu Morales and Rick J. van der Werf and Louis-Philippe Morency},\n pages = {14-27},\n title = {Virtual Rapport},\n year = {2006}\n}\n'}",,{'pages': '14-27'},28.0,Virtual Rapport,2006.0
2287,c328d6681aa7b44fe0f3a7a2f40e591e347b68c5,"In our natural world, a face is usually encountered not as an isolated object but as an integrated part of a whole body. The face and the body both normally contribute in conveying the emotional state of the individual. Here we show that observers judging a facial expression are strongly influenced by emotional body language. Photographs of fearful and angry faces and bodies were used to create face-body compound images, with either matched or mismatched emotional expressions. When face and body convey conflicting emotional information, judgment of facial expression is hampered and becomes biased toward the emotion expressed by the body. Electrical brain activity was recorded from the scalp while subjects attended to the face and judged its emotional expression. An enhancement of the occipital P1 component as early as 115 ms after presentation onset points to the existence of a rapid neural mechanism sensitive to the degree of agreement between simultaneously presented facial and bodily emotional expressions, even when the latter are unattended.","[{'authorId': '6702631', 'name': 'H. Meeren'}, {'authorId': '4002418', 'name': 'Corné C R J van Heijnsbergen'}, {'authorId': '4628064', 'name': 'B. de Gelder'}]",644.0,"{'bibtex': '@Article{Meeren2005RapidPI,\n author = {H. Meeren and Corné C R J van Heijnsbergen and B. de Gelder},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n pages = {\n          16518-23\n        },\n title = {Rapid perceptual integration of facial expression and emotional body language.},\n volume = {102 45},\n year = {2005}\n}\n'}",,"{'volume': '102 45', 'pages': '\n          16518-23\n        ', 'name': 'Proceedings of the National Academy of Sciences of the United States of America'}",56.0,Rapid perceptual integration of facial expression and emotional body language.,2005.0
2291,c33864707af93c08fcb446e70fb414d8ef91847b,Interactions such as discussion and negotiation in face-toface work contexts strongly rely on non-verbal feedback. Such feedback provides indications of important negotiation states such as agreement or disagreement and understanding or confusion. The increasing popularity of groupware and its use by virtual teams for collaborative remote work necessitates the development of appropriate tools to manage the reality of distributed and remote work. Such remote collaboration is often hampered by a lack of social cohesion and such phenomena as participant multi-tasking. This paper outlines the experimental assessment of a proof of concept AI based software agent (Emotion Tracking Agent or ETA) for the real-time tracking of groupware user’s facial expressions of emotion during virtual meetings. The software agent is designed as a novel approach to the removal of negative or unwanted effects of user multitasking and attention distracting behaviours in virtual collaboration and meeting environments.,"[{'authorId': '2118148111', 'name': 'Paul Smith'}, {'authorId': '46549484', 'name': 'S. Redfern'}]",0.0,"{'bibtex': '@Inproceedings{Smith2011ExperimentalAO,\n author = {Paul Smith and S. Redfern},\n title = {Experimental Assessment of an Emotion Tracking Software Agent (ETA) for Assisting Communicative Interactions of Multitasking Users in Groupware},\n year = {2011}\n}\n'}",[],"{'name': '', 'volume': ''}",17.0,Experimental Assessment of an Emotion Tracking Software Agent (ETA) for Assisting Communicative Interactions of Multitasking Users in Groupware,2011.0
2292,c364b62928190e6c29266abcb40ce9f9dd86c1b9,"
 
 We present an approach to incorporate interesting and compelling characters in planning-based narrative generation. The approach is based on a computational model that utilizes character actions to portray these as having distinct and well-defined personalities.
 
","[{'authorId': '2842986', 'name': 'Julio César Bahamón'}, {'authorId': '2003204', 'name': 'Camille Barot'}, {'authorId': '145513579', 'name': 'R. Young'}]",12.0,"{'bibtex': '@Inproceedings{Bahamón2015AGM,\n author = {Julio César Bahamón and Camille Barot and R. Young},\n pages = {4142-4143},\n title = {A Goal-Based Model of Personality for Planning-Based Narrative Generation},\n year = {2015}\n}\n'}",,{'pages': '4142-4143'},10.0,A Goal-Based Model of Personality for Planning-Based Narrative Generation,2015.0
2293,c3e104240e03dda9cb5f2514f123e6ef0b30e942,"Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection.","[{'authorId': '2265542260', 'name': 'Samarth Tripathi'}, {'authorId': '144396601', 'name': 'H. Beigi'}]",2.0,"{'bibtex': '@Article{Tripathi2018MultiModalER,\n author = {Samarth Tripathi and H. Beigi},\n journal = {ArXiv},\n title = {Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning},\n volume = {abs/1804.05788},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1804.05788', 'name': 'ArXiv'}",22.0,Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning,2018.0
2294,c43025c429b1fbf6f1379f61801a1b40834d62e7,"Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ""features"")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.","[{'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '2256269', 'name': 'C. Farabet'}]",1841.0,"{'bibtex': '@Article{LeCun2010ConvolutionalNA,\n author = {Yann LeCun and K. Kavukcuoglu and C. Farabet},\n journal = {Proceedings of 2010 IEEE International Symposium on Circuits and Systems},\n pages = {253-256},\n title = {Convolutional networks and applications in vision},\n year = {2010}\n}\n'}",,"{'pages': '253-256', 'name': 'Proceedings of 2010 IEEE International Symposium on Circuits and Systems'}",55.0,Convolutional networks and applications in vision,2010.0
2295,c432b9a9307212f85672bb3981fa0ef429928d24,,"[{'authorId': '3106683', 'name': 'Natasha Jaques'}, {'authorId': '1801452', 'name': 'Daniel J. McDuff'}, {'authorId': '2117904670', 'name': 'Y. Kim'}, {'authorId': '1719389', 'name': 'Rosalind W. Picard'}]",35.0,"{'bibtex': '@Inproceedings{Jaques2016UnderstandingAP,\n author = {Natasha Jaques and Daniel J. McDuff and Y. Kim and Rosalind W. Picard},\n pages = {64-74},\n title = {Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language},\n year = {2016}\n}\n'}",,{'pages': '64-74'},22.0,Understanding and Predicting Bonding in Conversations Using Thin Slices of Facial Expressions and Body Language,2016.0
2297,c51027f7e93ea54102130a8394f863c1a80a7918,"The creation of a believable agent depends on several factors, one of the most important is personality. This paper presents a proposal for the development of a computational metaphor for autonomous digital actors that includes a personality component, in order to make their behavior individualized and coherent. Through the study of personality theories and their assessment instruments, we propose a model that considers personality, both in terms of its description and its dynamics. Using this model, it is expected that digital actors react in a coherent and individualized way to a given dramatic situation.","[{'authorId': '113583485', 'name': 'Ricardo Filipe Pereira Ramos'}, {'authorId': '115488349', 'name': 'Juliane Cristine'}, {'authorId': '115834192', 'name': 'Koerber Reis'}]",2.0,"{'bibtex': '@Inproceedings{Ramos2012APM,\n author = {Ricardo Filipe Pereira Ramos and Juliane Cristine and Koerber Reis},\n title = {A Personality Model based on Reiss Motivational Profile f or Autonomous Digital Actors},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",15.0,A Personality Model based on Reiss Motivational Profile f or Autonomous Digital Actors,2012.0
2298,c537775d51d8d63d8b36e9f0c8baad9bf3e86b10,"This article describes a novel emergent emotion model for a context-aware “Affective Mobile Guide with Attitude,” a guide with emotions and personality that accompanies visitors touring an outdoor attraction. The interest lies in modelling the conditions to the emergence of emotions instead of prescripting emotions. The emotion model regulates the guide's emotions, behavior and beliefs, allowing the guide to adapt to its interaction environment flexibly. The guide adopts improvisational storytelling techniques taking into consideration the user's interests, its own interests, and its current memory activation in narration. It has emotional memory that stores its previous experiences allowing it to express its perspectives while narrating the stories. The guide's internal state is expressed through a 2-dimensional face where different facial features vary from one state to another along the arousal-pleasure emotional space. The guide exhibits potential in improving overall tour experience verified by a user evaluation.","[{'authorId': '1783919', 'name': 'M. Lim'}, {'authorId': '1732377', 'name': 'R. Aylett'}]",19.0,"{'bibtex': '@Article{Lim2009ANEE,\n author = {M. Lim and R. Aylett},\n journal = {Applied Artificial Intelligence},\n pages = {835 - 854},\n title = {AN EMERGENT EMOTION MODEL FOR AN AFFECTIVE MOBILE GUIDE WITH ATTITUDE},\n volume = {23},\n year = {2009}\n}\n'}",,"{'volume': '23', 'pages': '835 - 854', 'name': 'Applied Artificial Intelligence'}",42.0,AN EMERGENT EMOTION MODEL FOR AN AFFECTIVE MOBILE GUIDE WITH ATTITUDE,2009.0
2299,c539cba11a68789d9c12a2245ac6a5cc81b53e31,,"[{'authorId': '1824649', 'name': 'C. Kerdvibulvech'}]",8.0,"{'bibtex': '@Inproceedings{Kerdvibulvech2016ANI,\n author = {C. Kerdvibulvech},\n pages = {509-518},\n title = {A Novel Integrated System of Visual Communication and Touch Technology for People with Disabilities},\n year = {2016}\n}\n'}",,{'pages': '509-518'},16.0,A Novel Integrated System of Visual Communication and Touch Technology for People with Disabilities,2016.0
2300,c547e1f79e6039d05c5ae433a36612d7f8e4d3f5,,"[{'authorId': '1748956', 'name': 'R. Fikes'}, {'authorId': '144497046', 'name': 'N. Nilsson'}]",6070.0,"{'bibtex': '@Article{Fikes1971STRIPSAN,\n author = {R. Fikes and N. Nilsson},\n journal = {Artif. Intell.},\n pages = {189-208},\n title = {STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving},\n volume = {2},\n year = {1971}\n}\n'}",,"{'volume': '2', 'pages': '189-208', 'name': 'Artif. Intell.'}",21.0,STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving,1971.0
2301,c556ed80eb0453ba3001469f70e738bc541fc727,,"[{'authorId': '1583801483', 'name': 'Kiyomoto Shinsuke'}, {'authorId': '66161924', 'name': 'Takashio Kazunori'}]",0.0,"{'bibtex': '@Inproceedings{Shinsuke2019VideoGW,\n author = {Kiyomoto Shinsuke and Takashio Kazunori},\n pages = {79-80},\n title = {Video Gaming with Emotion-Expressive Virtual Rival Agent},\n volume = {119},\n year = {2019}\n}\n'}",[],"{'name': '', 'pages': '79-80', 'volume': '119'}",0.0,Video Gaming with Emotion-Expressive Virtual Rival Agent,2019.0
2302,c59a66e3f5a284d7ecfb9a8304aca6de5dd9ea56,"This study compared the effects of a social story-only intervention with the effects of a social story-plus practices session intervention as implemented with preschool age children with disabilities (n = 16) and without disabilities (n = 16) in an inclusive preschool setting. The social story interventions were implemented with groups of four children in order to examine the impact of the intervention on the children's prosocial and antisocial skills. The findings of the study differ from previous research in that the social story interventions were not found to be an effective intervention. The significance of these findings, limitations of the study, and future research suggestions are discussed.","[{'authorId': '113642907', 'name': 'Cori M. More'}, {'authorId': '66258304', 'name': 'Nancy M. Sileo'}, {'authorId': '48127974', 'name': 'Kyle Higgins'}, {'authorId': '40012209', 'name': 'R. Tandy'}, {'authorId': '118026986', 'name': 'Michelle T. Tannock'}]",18.0,"{'bibtex': '@Article{More2013TheEO,\n author = {Cori M. More and Nancy M. Sileo and Kyle Higgins and R. Tandy and Michelle T. Tannock},\n journal = {Early Child Development and Care},\n pages = {1 - 16},\n title = {The effects of social story interventions on preschool age children with and without disabilities},\n volume = {183},\n year = {2013}\n}\n'}",,"{'volume': '183', 'pages': '1 - 16', 'name': 'Early Child Development and Care'}",84.0,The effects of social story interventions on preschool age children with and without disabilities,2013.0
2303,c5bbbd24da123f3eb4f00bcfa7fcc0ad11ace39f,"Natural head motion is an indispensable part of realistic facial animation. This paper presents a novel approach to synthesize natural head motion automatically based on grammatical and prosodic features, which are extracted by the text analysis part of a Chinese Text-to-Speech (TTS) system. A two-layer clustering method is proposed to determine elementary head motion patterns from a multimodal database which covers six emotional states. The mapping problem between textual information and elementary head motion patterns is modeled by Classification and Regression Trees (CART). With the emotional state specified by users, results from text analysis are utilized to drive corresponding CART model to create emotional head motion sequence. Then, the generated sequence is interpolated by spineand us ed to drive a Chinese text-driven avatar. The comparison experiment indicates that this approach provides a better head motion and an engaging human-computer comparing to random or none head motion.","[{'authorId': '3295988', 'name': 'Kaihui Mu'}, {'authorId': '37670752', 'name': 'J. Tao'}, {'authorId': '2061534506', 'name': 'Jianfeng Che'}, {'authorId': '2740129', 'name': 'Minghao Yang'}]",7.0,"{'bibtex': '@Inproceedings{Mu2010MoodAA,\n author = {Kaihui Mu and J. Tao and Jianfeng Che and Minghao Yang},\n pages = {37:1-37:4},\n title = {Mood avatar: automatic text-driven head motion synthesis},\n year = {2010}\n}\n'}",,{'pages': '37:1-37:4'},8.0,Mood avatar: automatic text-driven head motion synthesis,2010.0
2304,c5bc0e4faf7050465063c4831f9a4521b2a97bb6,"In this paper, a novel neural network approach to real-time collision-free path planning of robot manipulators in a nonstationary environment is proposed, which is based on a biologically inspired neural network model for dynamic trajectory generation of a point mobile robot. The state space of the proposed neural network is the joint space of the robot manipulators, where the dynamics of each neuron is characterized by a shunting equation or an additive equation. The real-time robot path is planned through the varying neural activity landscape that represents the dynamic environment. The proposed model for robot path planning with safety consideration is capable of planning a real-time “comfortable” path without suffering from the “too close” nor “too far” problems. The model algorithm is computationally efficient. The computational complexity is linearly dependent on the neural network size. The effectiveness and efficiency are demonstrated through simulation studies.","[{'authorId': '98726631', 'name': 'Simon X. Yang'}, {'authorId': '144517997', 'name': 'M. Meng'}]",44.0,"{'bibtex': '@Article{Yang1999RealtimeCP,\n author = {Simon X. Yang and M. Meng},\n journal = {Autonomous Robots},\n pages = {27-39},\n title = {Real-time Collision-free Path Planning of Robot Manipulators using Neural Network Approaches},\n volume = {9},\n year = {1999}\n}\n'}",,"{'volume': '9', 'pages': '27-39', 'name': 'Autonomous Robots'}",42.0,Real-time Collision-free Path Planning of Robot Manipulators using Neural Network Approaches,1999.0
2305,c5bd9b18841df795fb1ec0404a34c5332ca6fb8b,,"[{'authorId': '1703084', 'name': 'C. Pelachaud'}]",151.0,"{'bibtex': '@Article{Pelachaud2009StudiesOG,\n author = {C. Pelachaud},\n journal = {Speech Commun.},\n pages = {630-639},\n title = {Studies on gesture expressivity for a virtual agent},\n volume = {51},\n year = {2009}\n}\n'}",,"{'volume': '51', 'pages': '630-639', 'name': 'Speech Commun.'}",59.0,Studies on gesture expressivity for a virtual agent,2009.0
2307,c5cf3f1ad2fe912dfd36297475608099ad605988,"Using meta-analysis, we find a consistent positive correlation between emotion recognition accuracy (ERA) and goal-oriented performance. However, this existing research relies primarily on subjective perceptions of performance. The current study tested the impact of ERA on objective performance in a mixed-motive buyer-seller negotiation exercise. Greater recognition of posed facial expressions predicted better objective outcomes for participants from Singapore playing the role of seller, both in terms of creating value and claiming a greater share for themselves. The present study is distinct from past research on the effects of individual differences on negotiation outcomes in that it uses a performance-based test rather than self-reported measure. These results add to evidence for the predictive validity of emotion recognition measures on practical outcomes.","[{'authorId': '2924324', 'name': 'Hillary Anger Elfenbein'}, {'authorId': '70162857', 'name': 'M. Foo'}, {'authorId': '153272073', 'name': 'Judith B. White'}, {'authorId': '92540362', 'name': 'H. Tan'}, {'authorId': '122035442', 'name': 'V. C. Aik'}]",301.0,"{'bibtex': '@Article{Elfenbein2006ReadingYC,\n author = {Hillary Anger Elfenbein and M. Foo and Judith B. White and H. Tan and V. C. Aik},\n journal = {Journal of Nonverbal Behavior},\n pages = {205-223},\n title = {Reading your Counterpart: The Benefit of Emotion Recognition Accuracy for Effectiveness in Negotiation},\n volume = {31},\n year = {2006}\n}\n'}",,"{'volume': '31', 'pages': '205-223', 'name': 'Journal of Nonverbal Behavior'}",76.0,Reading your Counterpart: The Benefit of Emotion Recognition Accuracy for Effectiveness in Negotiation,2006.0
2308,c5ddf22606164a2bdab431046217da70033bcea4,"Abstract Severe traumatic brain injury (TBI) leads to physical, neuropsychological, and emotional deficits that interfere with the individual's capacity to return to his or her former lifestyle. This review focuses on social cognition, that is, the capacity to attend to, recognize and interpret interpersonal cues that guide social behavior. Social cognition entails “hot” processes, that is, emotion perception and emotional empathy and “cold” processes, that is, the ability to infer the beliefs, feelings, and intentions of others (theory of mind: ToM) to see their point of view (cognitive empathy) and what they mean when communicating (pragmatic inference). This review critically examines research attesting to deficits in each of these domains and also examines evidence for theorized mechanisms including specific neural networks, the role of simulation, and non-social cognition. Current research is hampered by small, heterogeneous samples and the inherent complexity of TBI pathology. Nevertheless, there is evidence that facets of social cognition are impaired in this population. New assessment tools to measure social cognition following TBI are required that predict everyday social functioning. In addition, research into remediation needs to be guided by the growing empirical base for understanding social cognition that may yet reveal how deficits dissociate following TBI. (JINS, 2013, 19, 1–16)","[{'authorId': '143982280', 'name': 'S. McDonald'}]",192.0,"{'bibtex': '@Article{McDonald2013ImpairmentsIS,\n author = {S. McDonald},\n journal = {Journal of the International Neuropsychological Society},\n pages = {231 - 246},\n title = {Impairments in Social Cognition Following Severe Traumatic Brain Injury},\n volume = {19},\n year = {2013}\n}\n'}",,"{'volume': '19', 'pages': '231 - 246', 'name': 'Journal of the International Neuropsychological Society'}",227.0,Impairments in Social Cognition Following Severe Traumatic Brain Injury,2013.0
2309,c5eed27acb27c0cd98cfe410eb0e168ab7f89305,"The authors have been developing humanoid robots in order to develop new mechanisms and functions for a humanoid robot that has the ability to communicate naturally with a human by expressing human-like emotion. In 2004, we developed the emotion expression humanoid robot WE-4RII (Waseda Eye No.4 Refined II) by integrating the new humanoid robot hands RCH-I (RoboCasa Hand No.1) into the emotion expression humanoid robot WE-4R. We confirmed that WE-4RII can effectively express its emotion.","[{'authorId': '46677342', 'name': 'K. Itoh'}, {'authorId': '2065285579', 'name': 'H. Miwa'}, {'authorId': '2149824050', 'name': 'M. Matsumoto'}, {'authorId': '144810580', 'name': 'M. Zecca'}, {'authorId': '1678499', 'name': 'H. Takanobu'}, {'authorId': '2852132', 'name': 'S. Roccella'}, {'authorId': '1798096', 'name': 'M. Carrozza'}, {'authorId': '145745294', 'name': 'P. Dario'}, {'authorId': '1737432', 'name': 'A. Takanishi'}]",59.0,"{'bibtex': '@Article{Itoh2004VariousEE,\n author = {K. Itoh and H. Miwa and M. Matsumoto and M. Zecca and H. Takanobu and S. Roccella and M. Carrozza and P. Dario and A. Takanishi},\n journal = {IEEE Conference on Robotics and Automation, 2004. TExCRA Technical Exhibition Based.},\n pages = {35-36},\n title = {Various emotional expressions with emotion expression humanoid robot WE-4RII},\n year = {2004}\n}\n'}",,"{'pages': '35-36', 'name': 'IEEE Conference on Robotics and Automation, 2004. TExCRA Technical Exhibition Based.'}",4.0,Various emotional expressions with emotion expression humanoid robot WE-4RII,2004.0
2310,c5fa83d7fb825eebad2c88c4a61b14634c51ba47,"The Continuous Double Auction (CDA) is the dominant market institution for real-world trading of equities, commodities, derivatives, etc. We describe a series of laboratory experiments that, for the first time, allow human subjects to interact with software bidding agents in a CDA. Our bidding agents use strategies based on extensions of the Gjerstad-Dickhaut and Zero-Intelligence-Plus algorithms. We find that agents consistently obtain significantly larger gains from trade than their human counterparts. This was unexpected because both humans and agents have approached theoretically perfect efficiency in prior all-human or allagent CDA experiments. Another unexpected finding is persistent far-from-equilibrium trading, in sharp contrast to the robust convergence observed in previous all-human or all-agent experiments. We consider possible explanations for our empirical findings, and speculate on the implications for future agent-human interactions in electronic markets.","[{'authorId': '143863023', 'name': 'Rajarshi Das'}, {'authorId': '144353288', 'name': 'James E. Hanson'}, {'authorId': '1721327', 'name': 'J. Kephart'}, {'authorId': '1699108', 'name': 'G. Tesauro'}]",280.0,"{'bibtex': '@Inproceedings{Das2001AgentHumanII,\n author = {Rajarshi Das and James E. Hanson and J. Kephart and G. Tesauro},\n pages = {1169-1187},\n title = {Agent-Human Interactions in the Continuous Double Auction},\n year = {2001}\n}\n'}",,{'pages': '1169-1187'},10.0,Agent-Human Interactions in the Continuous Double Auction,2001.0
2311,c5fe1b3e15c245da94102e2389b76451d78e9d69,"Human object recognition and tracking is important in robotics and automation. The Kinect sensor and its SDK have provided a reliable human tracking solution where a constant line of sight is maintained. However, if the human object is lost from sight during the tracking, the existing method cannot recover and resume tracking the previous object correctly. In this paper, a human recognition method is developed based on colour and depth information that is provided from any RGB-D sensor. In particular, the method firstly introduces a mask based on the depth information of the sensor to segment the shirt from the image (shirt segmentation); it then extracts the colour information of the shirt for recognition (shirt recognition). As the shirt segmentation is only based on depth information, it is light invariant compared to colour-based segmentation methods. The proposed colour recognition method introduces a confidence-based ruling method to classify matches. The proposed shirt segmentation and colour recognition method is tested using a variety of shirts with the tracked human at standstill or moving in varying lighting conditions. Experiments show that the method can recognize shirts of varying colours and patterns robustly.","[{'authorId': '35571088', 'name': 'Benjamin J. Southwell'}, {'authorId': '2057272394', 'name': 'G. Fang'}]",351.0,"{'bibtex': '@Article{Southwell2013HumanOR,\n author = {Benjamin J. Southwell and G. Fang},\n journal = {International Journal of Advanced Robotic Systems},\n title = {Human Object Recognition Using Colour and Depth Information from an RGB-D Kinect Sensor},\n volume = {10},\n year = {2013}\n}\n'}",,"{'volume': '10', 'name': 'International Journal of Advanced Robotic Systems'}",22.0,Human Object Recognition Using Colour and Depth Information from an RGB-D Kinect Sensor,2013.0
2312,c605011eeffc9fa32bdb34e16a82b72e17c3be3d,"To increase the believability and life-likeness of Embodied Conversational Agents (ECAs), we introduce a behavior synthesis technique for the generation of expressive gesturing. A small set of dimensions of expressivity is used to characterize individual variability of movement. We empirically evaluate our implementation in two separate user studies. The results suggest that our approach works well for a subset of expressive behavior. However, animation fidelity is not high enough to realize subtle changes. Interaction effects between different parameters need to be studied further.","[{'authorId': '28226629', 'name': 'Bjoern Hartmann'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1742939', 'name': 'S. Buisine'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",85.0,"{'bibtex': '@Inproceedings{Hartmann2005DesignAE,\n author = {Bjoern Hartmann and M. Mancini and S. Buisine and C. Pelachaud},\n pages = {1095-1096},\n title = {Design and evaluation of expressive gesture synthesis for embodied conversational agents},\n year = {2005}\n}\n'}",,{'pages': '1095-1096'},8.0,Design and evaluation of expressive gesture synthesis for embodied conversational agents,2005.0
2313,c6624273dd9d38832c35d55b0018c6aca9a3000d,"This paper contributes to the study of interaction between groups of people and groups of robots by examining the effect of group size on people's attitudes and behaviors toward robots as interaction partners. Our work is motivated by psychological research on human intergroup dynamics, particularly the interindividual-intergroup discontinuity effect, which suggest that interactions among groups are more competitive than interactions among individuals. To test the discontinuity effect in the context of human-robot interaction, we conducted a between-subjects experiment with four conditions, derived by differentiating the ratio of humans to robots in the interaction (one or two humans interacting with one or two robots). Participants played a game with robots in which they were given a chance to exhibit competitive and cooperative behaviors, which we tracked. We also measured changes in participants' attitudes toward robots following gameplay. Our results show that people playing in groups behave more competitively towards the robots than individual human players. However, participants' attitudes toward robots did not change after the short-term interaction.","[{'authorId': '1736515', 'name': 'W. Chang'}, {'authorId': '2111279798', 'name': 'Jeremy P. White'}, {'authorId': '2116650164', 'name': 'Joohyun Park'}, {'authorId': '2063978778', 'name': 'Anna Holm'}, {'authorId': '1746316', 'name': 'S. Šabanović'}]",41.0,"{'bibtex': ""@Article{Chang2012TheEO,\n author = {W. Chang and Jeremy P. White and Joohyun Park and Anna Holm and S. Šabanović},\n journal = {2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication},\n pages = {845-850},\n title = {The effect of group size on people's attitudes and cooperative behaviors toward robots in interactive gameplay},\n year = {2012}\n}\n""}",,"{'pages': '845-850', 'name': '2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication'}",28.0,The effect of group size on people's attitudes and cooperative behaviors toward robots in interactive gameplay,2012.0
2314,c6804ce868469199527927144542302a6cf26627,"Dopamine (DA) acts as a key neurotransmitter in the brain. Numerous studies have shown its regulatory role in motor and cognitive function. However, the impairment of emotional processes in neurologic and psychiatric pathologies involving the dopaminergic system (Parkinson disease, schizophrenia, autism, Attention Deficit Hyperactivity Disorder, Huntington disease, frontal lobe lesions), as well as the influence that administration of dopaminergic agonists/antagonists exert on the processing of emotion, suggest a role for DA in emotional processes. Moreover, emotional processes are dependent upon a variety of structures, the majority of which form part of the limbic system and are subject to DA innervation. In reviewing the literature, the amygdala emerges as a brain structure critical for emotional processing. It may also be implicated in deficits in emotional recognition found in two major disorders where DA's implication is clear: Parkinson disease and schizophrenia. In addition, the amygdala's response to emotional tasks is likely to be altered by the administration of both agonist and antagonist dopaminergic drugs. Experimental studies reinforce the idea of a dopaminergic contribution to emotional response, as suggested by biochemical, pharmacologic, and lesion experiments. Although the implication of the dopaminergic system in emotional processing appears to be clearly documented, the contribution of specific DA receptor subtypes, or of the DA cotransmitters cholecystokinin and neurotensin, or even glutamate, is, however, still unclear. Altogether, these observations suggest that DA has, undoubtedly, a direct and/or indirect role in the full emotional process.","[{'authorId': '1401031672', 'name': 'P. Salgado-Pineda'}, {'authorId': '48851106', 'name': 'P. Delaveau'}, {'authorId': '144256377', 'name': 'O. Blin'}, {'authorId': '122226418', 'name': 'A. Nieoullon'}]",138.0,"{'bibtex': '@Article{Salgado-Pineda2005DopaminergicCT,\n author = {P. Salgado-Pineda and P. Delaveau and O. Blin and A. Nieoullon},\n journal = {Clinical Neuropharmacology},\n pages = {228-237},\n title = {Dopaminergic Contribution to the Regulation of Emotional Perception},\n volume = {28},\n year = {2005}\n}\n'}",,"{'volume': '28', 'pages': '228-237', 'name': 'Clinical Neuropharmacology'}",110.0,Dopaminergic Contribution to the Regulation of Emotional Perception,2005.0
2315,c681b4dbd705db71d9668e32e471eeeadfa1b939,"Group emotional contagion, the transfer of moods among people in a group, and its influence on work group dynamics was examined in a laboratory study of managerial decision making using multiple, convergent measures of mood, individual attitudes, behavior, and group-level dynamics. Using a 2 times 2 experimental design, with a trained confederate enacting mood conditions, the predicted effect of emotional contagion was found among group members, using both outside coders' ratings of participants' mood and participants' self-reported mood. No hypothesized differences in contagion effects due to the degree of pleasantness of the mood expressed and the energy level with which it was conveyed were found. There was a significant influence of emotional contagion on individual-level attitudes and group processes. As predicted, the positive emotional contagion group members experienced improved cooperation, decreased conflict, and increased perceived task performance. Theoretical implications and practical ramifications of emotional contagion in groups and organizations are discussed.","[{'authorId': '1838029', 'name': 'Sigal G. Barsade'}]",2860.0,"{'bibtex': '@Article{Barsade2002TheRE,\n author = {Sigal G. Barsade},\n journal = {Administrative Science Quarterly},\n pages = {644 - 675},\n title = {The Ripple Effect: Emotional Contagion and its Influence on Group Behavior},\n volume = {47},\n year = {2002}\n}\n'}",,"{'volume': '47', 'pages': '644 - 675', 'name': 'Administrative Science Quarterly'}",173.0,The Ripple Effect: Emotional Contagion and its Influence on Group Behavior,2002.0
2318,c69b28b27cb231a24afe44268cdea0e427ae1e53,"Interpersonal Effects of Emotions in Morally-charged Negotiations Morteza Dehghani (morteza@ict.usc.edu), Jonathan Gratch (gratch@ict.usc.edu) Institute for Creative Technologies, University of Southern California, 1205 Waterfront Dr., Playa Vista, CA 90094-2536, USA Peter J. Carnevale (peter.carnevale@marshall.usc.edu) Marshall School of Business, University of Southern California, 3670 Trousdale Parkway, Los Angeles, CA 90089-0808 Abstract The majority of research on emotion and moral decision- making has focused on the intrapersonal effects of emotion. However, witnessing and displaying emotional expressions is also known to play a significant role in the facilitation and coordination of our social interactions. In this work, we hypothesize that interpersonal emotions effect moral appraisals by prioritizing different moral concerns. We investigate the impact of facial displays of discrete emotions, specifically anger and sadness, in a morally charged multi- item negotiation task. The results of our experiment support our hypothesis that moral appraisals can be strongly affected by interpersonal emotional expressions. We show that displays of anger may backfire if one of the parties associates moral significance to the objects of the negotiation, whereas displays of sadness promote higher concession-making. Overall, we argue that emotional expressions can shift moral concerns within a negotiation in ways that can promote cooperation. Keywords: Emotion, Sacred Values, Moral Decision- Making, Negotiations Introduction Recent research into emotion and moral decision-making reveals a consistent pattern. When confronted with possible threats to moral or sacred concerns, people tend to become emotional (e.g. Ginges et al., 2007), uncompromising (e.g. Tetlock, 2003), and act in ways contrary to traditional formalizations of rational self-interest (e.g. Atran, 2010). This article adds a bit of hope to this otherwise gloomy picture. Building on findings from both moral decision- making and the interpersonal effects of emotion, we show how emotion can sometimes foster cooperation rather than conflict. Our findings have potential important implications for negotiation and conflict resolution in sacred domains. In this article, we build on the social-functional framework of emotions (Keltner & Haidt, 1999; Frijda & Masquita, 1994) which claims that different sociomoral concerns are prioritized based on the distinct emotions that are experienced (Horbeg, Oveis & Keltner, 2011). This theory argues that our perception of the permissibility of actions in moral situations is affected by the emotions experienced, as different emotions heighten the salience of different moral domains. For example, disgust has been linked to violations of purity-sanctity (Rozin, et al., 1999), and research shows that experimentally predisposing individuals to disgust increases their tendency to focus on purity related issues (e.g. sexuality) as opposed to other moral concerns such as justice (e.g. Tapias, Glaser, Keltner, Vasquez & Wickens, 2007). However, the research on emotion-related moral appraisals has been mostly limited to the intrapersonal effects of emotion in decision-making. In contrast to research on moral decision-making, research on negotiation and conflict resolution has largely emphasized both the intrapersonal as well as interpersonal effects of emotion (Carnevale, 2008; Forgas, 1998). Work on interpersonal aspects of emotion argues that emotional expressions by one party can change how the other party construes and reacts to a situation. The evidence from this line of research suggests that cognitive emotional appraisals are not only antecedents of emotions experienced, but they may also follow from the perception of emotional expressions in others (Lerner, Han, & Keltner, 2007). For example, it has been widely documented that perceiving expressed anger during a course of a negotiation can elicit more concessions compared to other emotions (such as happiness) or no emotions at all (e.g. Van Kleef, De Dreu & Manstead, 2004a, 2004b, 2010; Sinaceur & Tiedens, 2006). These authors argue that anger communicates that a party has high aspirations and that concessions would be required from the other party to reach an agreement. The majority of these findings, however, rely on negotiations only involving issues that might be of interest to people but have no sentimental or moral significance to them (e.g. negotiating over the price of a cellphone and its duration of service). In this article, we build on these findings and show that interpersonal effects of emotion unfold somewhat differently in moral contexts. Here, we investigate the impact of facial displays of discrete emotions, specifically anger and sadness, in a morally-charged multi-item negotiation task. We hypothesize that perceiving different emotional expressions in others can influence moral cognition by prioritizing different moral domains and shifting interpretive-frames. In other words, our interpretation of a moral issue can be subjected to the emotional expressions conveyed by other individuals involved in the negotiation. Anger is connotated with the prioritization of ethics of autonomy concerned with rights and justice (Rozin, et al., 1999), and sadness is linked to eliciting sympathy and heightening the salience of need, weakness and harm/care (Horbeg, Oveis & Keltner, 2011). We predict that when an object is perceived as a sacred (or protected) value (with intrinsic moral significance) (Tetlock,","[{'authorId': '145707560', 'name': 'Morteza Dehghani'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '48755211', 'name': 'P. Carnevale'}]",8.0,"{'bibtex': '@Article{Dehghani2012InterpersonalEO,\n author = {Morteza Dehghani and J. Gratch and P. Carnevale},\n journal = {Cognitive Science},\n title = {Interpersonal Effects of Emotions in Morally-charged Negotiations},\n volume = {34},\n year = {2012}\n}\n'}",,"{'volume': '34', 'name': 'Cognitive Science'}",26.0,Interpersonal Effects of Emotions in Morally-charged Negotiations,2012.0
2319,c6a1534ccada6f09e0b5ad29241da555574aee48,"Emotional experiences can be described by two factors: valence (how negative or positive) and arousal (how calming or exciting). Although both dimensions appear to influence memory, they may do so via distinct mechanisms. The amygdala likely plays a specific role in modulating memory for arousing experiences, whereas non-amygdalar networks may be instrumental in enhancing memory for non-arousing positive or negative events.","[{'authorId': '2922446', 'name': 'E. Kensinger'}]",487.0,"{'bibtex': '@Article{Kensinger2004RememberingEE,\n author = {E. Kensinger},\n journal = {Reviews in the Neurosciences},\n pages = {241 - 252},\n title = {Remembering Emotional Experiences: The Contribution of Valence and Arousal},\n volume = {15},\n year = {2004}\n}\n'}",,"{'volume': '15', 'pages': '241 - 252', 'name': 'Reviews in the Neurosciences'}",84.0,Remembering Emotional Experiences: The Contribution of Valence and Arousal,2004.0
2320,c6a2c40110efa0ab53f1762f47ef5779d85b7a9a,"This paper presents findings from an evaluation of an autonomous agent populated virtual environment called PUPPET, that allows children to play multiple roles in the creation of an interactive narrative: audience, actor, scriptwriter, and editor. The speech of children playing with the PUPPET system was analysed to determine their ability to understand the behaviour and motives of the autonomous agents, to take on the role of an avatar character, to script dialogue for the characters, and to reflect upon and edit the recorded dialogue.","[{'authorId': '143679313', 'name': 'P. Marshall'}, {'authorId': '1685816', 'name': 'Y. Rogers'}, {'authorId': '118334943', 'name': 'M. Scaife'}]",42.0,"{'bibtex': '@Inproceedings{Marshall2002PUPPETAV,\n author = {P. Marshall and Y. Rogers and M. Scaife},\n title = {PUPPET: a virtual environment for children to act and direct interactive narratives},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",5.0,PUPPET: a virtual environment for children to act and direct interactive narratives,2002.0
2321,c6b773631a8bfb9bb7d5c39e9af66bce3b488376,"A growing body of literature points to the important role that context plays in emotion regulation. One dimension of context that has significance for emotion regulation is the nature of the relationship between interactive partners. This review provides an organized account of existing empirical evidence assessing emotion regulation within close relationships across the life span. Specifically, the reviewed research includes studies examining parent-child relationships, sibling relationships, friendships, and romantic relationships in relation to emotion regulation. The review highlights evidence concerning how relationship processes influence emotion regulation. Based on the current state of the literature, future directions for research in this area are recommended. This review seeks to advance a more nuanced approach to the study of the social processes associated with emotion regulation. An argument is made for how building upon research concerning the relationship context as a basis for emotion regulation can further elucidate theorizing on the determinants of emotion regulation. (PsycINFO Database Record (c) 2020 APA, all rights reserved).","[{'authorId': '7020927', 'name': 'Eric W. Lindsey'}]",24.0,"{'bibtex': '@Article{Lindsey2020RelationshipCA,\n author = {Eric W. Lindsey},\n journal = {Emotion},\n pages = {\n          59-62\n        },\n title = {Relationship context and emotion regulation across the life span.},\n volume = {20 1},\n year = {2020}\n}\n'}",,"{'volume': '20 1', 'pages': '\n          59-62\n        ', 'name': 'Emotion'}",30.0,Relationship context and emotion regulation across the life span.,2020.0
2322,c6b7dbb4356a7ad20dd17906ed321bd021950500,,"[{'authorId': '144827818', 'name': 'M. Gross'}, {'authorId': '144942021', 'name': 'Elizabeth A. Crane'}, {'authorId': '1892780', 'name': 'B. Fredrickson'}]",147.0,"{'bibtex': '@Article{Gross2010MethodologyFA,\n author = {M. Gross and Elizabeth A. Crane and B. Fredrickson},\n journal = {Journal of Nonverbal Behavior},\n pages = {223-248},\n title = {Methodology for Assessing Bodily Expression of Emotion},\n volume = {34},\n year = {2010}\n}\n'}",,"{'volume': '34', 'pages': '223-248', 'name': 'Journal of Nonverbal Behavior'}",37.0,Methodology for Assessing Bodily Expression of Emotion,2010.0
2323,c6bf764d9c80c0273ec5ff3a2f0fa152f5616d2a,"Affective support can play a central role in adaptive learning environments. Although virtual human tutors hold significant promise for providing affective support, a key open question is how a tutor's facial expressions can influence learners' performance. In this paper, we report on a study to examine the influence of a human tutor agent's facial expressions on learners' performance and emotions during learning. Results from the study suggest that learners' performance is significantly better when a human tutor agent facially expresses emotions that are congruent with the content relevancy. Results also suggest that learners facially express significantly more confusion when the human tutor agent provides incongruent facial expressions. These results can inform the design of virtual humans as pedagogical agents can inform the design of virtual humans as pedagogical agents and designing intelligent learner-agent interactions.","[{'authorId': '3408438', 'name': 'Nicholas V. Mudrick'}, {'authorId': '37057683', 'name': 'M. Taub'}, {'authorId': '145394858', 'name': 'R. Azevedo'}, {'authorId': '34971293', 'name': 'Jonathan P. Rowe'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",10.0,"{'bibtex': '@Article{Mudrick2017TowardAV,\n author = {Nicholas V. Mudrick and M. Taub and R. Azevedo and Jonathan P. Rowe and James C. Lester},\n booktitle = {Affective Computing and Intelligent Interaction},\n journal = {2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {184-189},\n title = {Toward affect-sensitive virtual human tutors: The influence of facial expressions on learning and emotion},\n year = {2017}\n}\n'}","[{'paperId': '7031d1857789ee4ace8d5c564cffc34b069806c8', 'title': 'A cautionary tale of side-by-side evaluations while developing emotional expression for intelligent virtual agents'}, {'paperId': 'f59fa7f67b569caac5ec56451d2cd025ed2ba83b', 'title': 'Exploring the Role of Visual Design in Digital Public Health Safety Education'}, {'paperId': 'df9de08ad5a6438de6e38737ae70a6a7276584a1', 'title': 'Building an Emotionally Responsive Avatar with Dynamic Facial Expressions in Human - Computer Interactions'}, {'paperId': '55e8096617ef7ed237f6da2ff3765a827d571903', 'title': 'Designing a Multimodal Emotional Interface in the Context of Negotiation'}, {'paperId': '05c8d82810df9398972e1c91a3d828938bdeedff', 'title': 'Associating Facial Expressions and Upper-Body Gestures with Learning Tasks for Enhancing Intelligent Tutoring Systems'}, {'paperId': '31b577a0660056bd0d10714e4468fd21b54d215b', 'title': 'Expressão de Afetividade por Agente Pedagógico em Ambiente Virtual de Ensino e Aprendizagem: uma Revisão Sistemática da Literatura'}, {'paperId': '66b4b44d75c465e31f43927af3a0ab86e5f9825e', 'title': 'Affect-Based Early Prediction of Player Mental Demand and Engagement for Educational Games'}, {'paperId': '4a66c038ce5cab5bfe896bde8432e3dcc03b977f', 'title': 'Pedagogical Agents: Back to the Future'}, {'paperId': '2011d3bd464c442f25149d7091a4b82ae122589c', 'title': ""Evaluating Adaptive Pedagogical Agents' Prompting Strategies Effect on Students' Emotions""}, {'paperId': 'e98fa4c67162f6666a8a560a4d72d0cf4a1e4af3', 'title': 'Design, User Experience, and Usability. Interaction Design: 9th International Conference, DUXU 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part I'}]","{'name': '2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)', 'pages': '184-189'}",14.0,Toward affect-sensitive virtual human tutors: The influence of facial expressions on learning and emotion,2017.0
2324,c6c0a69c9cee330093b0aae014ceaca9ba7eff15,,"[{'authorId': '2252548890', 'name': 'Alan Lipman'}, {'authorId': '1422357475', 'name': 'E. Hall'}]",6859.0,"{'bibtex': '@Article{Lipman1970TheHD,\n author = {Alan Lipman and E. Hall},\n journal = {British Journal of Sociology},\n pages = {353},\n title = {The Hidden Dimension},\n volume = {21},\n year = {1970}\n}\n'}",,"{'volume': '21', 'pages': '353', 'name': 'British Journal of Sociology'}",0.0,The Hidden Dimension,1970.0
2327,c6e17abd83bf8ca7f66b7da5e72c514d07fe6194,"Abstract Objective: Immersive virtual reality (VR) distraction provides clinically effective pain relief and increases subjective reports of “fun” in medical settings of procedural pain. The goal of this study was to better describe the variable of “fun” associated with VR distraction analgesia using the circumplex model (pleasure/arousal) of affect. Materials and Methods: Seventy-four healthy volunteers (mean age, 29 years; 37 females) received a standardized, 18-minute, multimodal pain sequence (alternating thermal heat and electrical stimulation to distal extremities) while receiving immersive, interactive VR distraction. Subjects rated both their subjective pain intensity and fun using 0–10 Graphic Rating Scales, as well as the pleasantness of their emotional valence and their state of arousal on 9-point scales. Results: Compared with pain stimulation in the control (baseline, no VR) condition, immersive VR distraction significantly reduced subjective pain intensity (P < 0.001). During VR distraction,...","[{'authorId': '116158613', 'name': 'R. ShararSam'}, {'authorId': '117253791', 'name': 'AlamdariAva'}, {'authorId': '115722920', 'name': 'HofferChristine'}, {'authorId': '114941212', 'name': 'G. HoffmanHunter'}, {'authorId': '115699001', 'name': 'P. JensenMark'}, {'authorId': '115522050', 'name': 'R. PattersonDavid'}]",43.0,"{'bibtex': '@Article{ShararSam2016CircumplexMO,\n author = {R. ShararSam and AlamdariAva and HofferChristine and G. HoffmanHunter and P. JensenMark and R. PattersonDavid},\n journal = {Games for health journal},\n title = {Circumplex Model of Affect: A Measure of Pleasure and Arousal During Virtual Reality Distraction Analgesia},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': 'Games for health journal'}",31.0,Circumplex Model of Affect: A Measure of Pleasure and Arousal During Virtual Reality Distraction Analgesia,2016.0
2328,c6e3c927978d546519d79c1db81618d783fdd63c,"The conveyance and recognition of affect and emotion partially determine how people interact with others and how they carry out and perform in their day-to-day activities. Hence, it is becoming necessary to endow technology with the ability to recognize users' affective states to increase the technologies' effectiveness. This paper makes three contributions to this research area. First, we demonstrate recognition models that automatically recognize affective states and affective dimensions from non-acted body postures instead of acted postures. The scenario selected for the training and testing of the automatic recognition models is a body-movement-based video game. Second, when attributing affective labels and dimension levels to the postures represented as faceless avatars, the level of agreement for observers was above chance level. Finally, with the use of the labels and affective dimension levels assigned by the observers as ground truth and the observers' level of agreement as base rate, automatic recognition models grounded on low-level posture descriptions were built and tested for their ability to generalize to new observers and postures using random repeated subsampling validation. The automatic recognition models achieve recognition percentages comparable to the human base rates as hypothesized.","[{'authorId': '2870739', 'name': 'A. Kleinsmith'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}, {'authorId': '143903462', 'name': 'A. Steed'}]",189.0,"{'bibtex': '@Article{Kleinsmith2011AutomaticRO,\n author = {A. Kleinsmith and N. Bianchi-Berthouze and A. Steed},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {1027-1038},\n title = {Automatic Recognition of Non-Acted Affective Postures},\n volume = {41},\n year = {2011}\n}\n'}",,"{'volume': '41', 'pages': '1027-1038', 'name': 'IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)'}",58.0,Automatic Recognition of Non-Acted Affective Postures,2011.0
2329,c6f1e684a49a32cfe1db8906fe8e3299a8024a25,,"[{'authorId': '2844803', 'name': 'Felix Kistler'}, {'authorId': '2158172', 'name': 'Birgit Lugrin'}, {'authorId': '3048626', 'name': 'Ionut Damian'}, {'authorId': '2040306', 'name': 'C. Dang'}, {'authorId': '1742930', 'name': 'E. André'}]",69.0,"{'bibtex': '@Article{Kistler2012NaturalIW,\n author = {Felix Kistler and Birgit Lugrin and Ionut Damian and C. Dang and E. André},\n journal = {Journal on Multimodal User Interfaces},\n pages = {39-47},\n title = {Natural interaction with culturally adaptive virtual characters},\n volume = {6},\n year = {2012}\n}\n'}",,"{'volume': '6', 'pages': '39-47', 'name': 'Journal on Multimodal User Interfaces'}",30.0,Natural interaction with culturally adaptive virtual characters,2012.0
2330,c6ff186ed2952d35a74d22a4abc4448dd7869e89,"Social decisions are heavily influenced by emotion. For decades, the dominant research paradigm has been characterized by a focus on the decision maker’s own positive or negative mood. We argue that a full understanding of the role of emotion in social decision making requires a complementary focus on interpersonal effects (i.e., the effects of one individual’s emotions on the other’s behavior); a focus on discrete emotions rather than general mood states; and a distinction between cooperative and competitive settings. To advance insight into these issues, we present the Emotions as Social Information (EASI) model. The model is grounded in two basic assumptions, namely that individuals use others’ emotions to make sense of ambiguous situations, and that the effects of others’ emotions and the processes that drive them depend critically on the cooperative or competitive nature of the situation. A review of recent research supports our analysis. We demonstrate that the interpersonal effects of emotions are pervasive and can be better understood in terms of the unique social functions of each emotion than in terms of valence. Effects in cooperative settings are best explained in terms of affective reactions (i.e., emotional contagion, affect infusion, and mood management), whereas effects in competitive contexts are better understood in terms of the strategic inferences Emotions as Social Information 47 individuals draw from other’s emotions. We conclude by discussing the implications of our model and highlighting avenues for future research. Would you trust a salesperson who smiles at you, or give money to a grumpy beggar muttering angry phrases? How would you react to a friend who arrives late for an appointment looking guilty and ashamed? Would you work harder when your team leader appears upbeat and cheerful or rather irritated and moody? Reversing the roles, would you show that you are angry or happy about your negotiation partner’s insultingly low or surprisingly generous offer? If you wanted to solicit help, which emotions would you show? Diverse as they may seem, all these questions revolve around one fundamental issue: How do emotional expressions shape social interactions and decisions? This is the focus of the chapter. Emotions have long been regarded as disruptive forces that interfere with rational decision making. Increasingly, however, this view has given way to a functional perspective on emotions. Contemporary scholars share the idea that emotions signal the importance of events to relevant concerns, help prioritize goals, and generate a state of action readiness that prepares the individual to respond to changes in the environment (see, e.g., Frijda, 1986). This not only applies to individual goals and actions but also, and perhaps especially, to social interaction and decision making—situations in which one’s own behavior influences and is influenced by one or more others. Indeed, social interactions are among themost commonly reported antecedents of emotions (Anderson& Guerrero, 1998; Shaver et al., 1992), and emotions have plenty of potential to shape behavior (Frijda, 1986; Manstead, 1991; Van Kleef, 2009). Although the last decades have witnessed a growing attention to the role of emotion in social decision making, we believe that this research has not done justice to the inherently social nature of emotion. The reason is that most research has addressed the intrapersonal effects of affective states (especially positive vs. negative moods), demonstrating that individuals’ judgments and decisions are influenced by their mood state (for reviews, see e.g., Ashby et al., 1999; Forgas, 1995; Isen, 1987; Schwarz et al., 1991; Wyer et al., 1999). Although much of this work involved social situations, the role of emotion itself has been studied in a rather a-social way, ignoring the role of the interaction partner. In this article, we advocate a more social approach, advancing a new model of the interpersonal effects of emotion in social decision making. 1. Concerns with Past Research and Aims of the Present Article The chapter—and much of the research reviewed in it—is motivated by three key concerns about the dominant research focus. The first concern is the prevailing focus on intrapersonal effects. We contend that the role of 48 Gerben A. Van Kleef et al. emotion in social decision making cannot be fully understood by merely considering the decision maker’s own emotions. We don’t just feel our emotions; we also express them in social interaction. This means that other people may observe our emotions, and may be influenced by them. Parkinson’s (1996) article, entitled ‘‘Emotions are social,’’ aptly notes that our emotions are not only often evoked by social interaction; but they also influence the behavior of social interaction partners by serving as a form of communication (Ekman, 1993; Fridlund, 1994; Frijda, 1986; Knutson, 1996). A true understanding of the role of emotion in social decision making therefore requires an additional focus on the interpersonal effects of emotions—the effects of one individual’s emotions on the other’s social decisions and behavior. Despite the growing popularity of social-functional approaches to emotion (e.g., Elfenbein, 2007; Fischer & Manstead, 2008; Frijda & Mesquita, 1994; Keltner & Haidt, 1999; Morris & Keltner, 2000; Oatley & Jenkins, 1992; Van Kleef, 2009), a systematic analysis in the context of social decision making is missing. Accordingly, our first objective is to fill this void. Our second concern is with the tendency to focus on positive versus negative mood as opposed to discrete emotions. We argue that this emphasis on diffuse mood states blurs our understanding of the multifaceted role of emotion in social decision making. As explained in more detail below, each discrete emotion has its own antecedents, appraisal components, relational themes, and action tendencies (Frijda et al., 1989; Lazarus, 1991; Manstead & Tetlock, 1989; Roseman et al., 1994; Smith et al., 1993). Emotions therefore provide more differentiated information and carry more clearcut behavioral implications than moods (Weiner, 1986). Our second goal, accordingly, is to highlight these specific functions of discrete emotions in the context of social decision making. We will show that discrete emotions (e.g., happiness, anger, sadness, guilt) have differential effects on social behavior that cannot be understood in terms of valence. Our final concern is the relative neglect of the social context within which emotions are expressed and behavioral actions emerge. Although emotions have been studied in various domains of social decision making, context has seldom been varied within studies, and it has not been common to compare the effects of emotions across different social settings. We argue that it is important to consider the social context. In particular, we propose that the same emotions may have entirely different effects depending on whether they are expressed in a predominantly cooperative or competitive setting. Cooperative settings are generally characterized by higher levels of trust and benevolence and greater motivation to work together; competitive settings typically breed distrust, more selfish motivations, and strategic behavior (De Dreu et al., 2007). It stands to reason that people respond differently to a smile or frown from a partner in a cooperative setting than from an adversary in a competitive situation (Lanzetta & Englis, 1989). Emotions as Social Information 49 Motivated by these observations, we present the Emotions as Social Information (EASI) model (see also Van Kleef, 2009) to enhance understanding of the interpersonal effects of emotions in social decision making. The model is grounded in two basic assumptions. The first is that social decision-making situations are fuzzy and are characterized by insufficient information about interdependent others’ goals, desires, and intentions. In such an uncertain environment, people rely on additional cues to make sense of the situation. We propose that people use their partner’s emotions to disambiguate the situation and to inform their social decisions. Our second assumption is that the nature of the situation fundamentally shapes the interpersonal effects of emotions. In predominantly cooperative situations, where parties’ goals are aligned and trust is high, people are likely to assimilate to the emotions of their partner, and their social decisions are likely to be influenced by their resulting emotional state. In more competitive settings, in contrast, where parties’ goals conflict and trust tends to be low, individuals use their counterpart’s emotions as strategic information to inform their behavior. This distinction has important implications for our understanding of the interpersonal effects of emotions in social decision making, as we demonstrate below. This model helps us to generate and address new questions about the role of emotion in social life. How do the emotional expressions of one party influence the social decisions of interdependent others?What are the distinct social signals that are conveyed by discrete emotions, and how do individuals respond to those signals? When do emotions spread from one individual to the other, and how does this affect social decision making? What strategic information do people distill from their counterpart’s emotional expressions, and how do they act on those inferences? We rely on the EASI model and recent empirical work to provide preliminary answers to these and other questions. To do so, we proceed as follows. We first build on the aforementioned assumptions to develop our model. Next, empirical support for the main propositions of themodel is reviewed by considering research evidence from our own and others’ laboratories. We close with a section summarizing our main conclusions and their implications, highlighting some key gaps in our knowledge, and offeri","[{'authorId': '148298087', 'name': 'van Kleef'}]",467.0,"{'bibtex': '@Article{Kleef2010AnIA,\n author = {van Kleef},\n journal = {Circulation-arrhythmia and Electrophysiology},\n title = {An interpersonal approach to emotion in social decision making: the emotions as social information model},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': 'Circulation-arrhythmia and Electrophysiology'}",193.0,An interpersonal approach to emotion in social decision making: the emotions as social information model,2010.0
2332,c717eb4e913c3249eac18d0fba13a1aa02d60dad,"The development of a multidimensional individual difference measure of empathy is described. The final version of the instrument consists of four seven-item subscales, each of which taps a separate aspect of the global concept ""empathy."" One scale, the perspective-taking scale, contains items which assess spontaneous attempts to adopt the perspectives of other people and see things from their point of view. Items on the fantasy scale measure the tendency to identify with characters in movies, novels, plays and other fictional situations. The other two subscales explicitly tap respondents' chronic emotional reactions to the negative experiences of others. The empathic concern scale inquires about respondents' feelings of warmth, compassion, and concern for others, while the personal distress scale measures the personal feelings of anxiety and discomfort that result from observing another's negative experience. The factor structure underlying these scales is the same for both sexes, and emerged in two independent samples. Test-retest and internal reliabilities of all four scales were substantial. The pattern of sex differences and the intercorrelations of these four scales are discussed in terms of recent theoretical treatments of the development of empathy (Hoffman, 1976). It is concluded that the new measure has considerable potential for investigations of the multidimensional nature of empathy.","[{'authorId': '47994338', 'name': 'Mark H. Davis'}, {'authorId': '1575959158', 'name': 'Miles P. Davis'}, {'authorId': '2107723533', 'name': 'M. Davis'}, {'authorId': '2110853467', 'name': 'Matthew Davis'}, {'authorId': '2110853313', 'name': 'Mark Davis'}, {'authorId': '2110745785', 'name': 'Mm Davis'}, {'authorId': '2107723533', 'name': 'M. Davis'}, {'authorId': '145544155', 'name': 'F. Davis'}, {'authorId': '47215691', 'name': 'H. Davis'}, {'authorId': '118016670', 'name': 'I. W. Davis'}]",4988.0,"{'bibtex': '@Inproceedings{Davis1980AMA,\n author = {Mark H. Davis and Miles P. Davis and M. Davis and Matthew Davis and Mark Davis and Mm Davis and M. Davis and F. Davis and H. Davis and I. W. Davis},\n title = {A Multidimensional Approach to Individual Differences in Empathy},\n year = {1980}\n}\n'}",,"{'volume': '', 'name': ''}",28.0,A Multidimensional Approach to Individual Differences in Empathy,1980.0
2333,c74084d7a77a7732115dcab68b3c2dc9b100748a,"emotion regulation and present a common framework for understanding emotion regulation, the process model of emoHandbook of emotion regulation. 2nd. problems with emotion and emotion regulation (estimates range from 40% to more 2007, in Handbook of Emotion Regulation (p. 10), J. J. Gross (Ed.), New. Reviews the book, The Handbook of Emotion Regulation (2nd Ed.) edited by James J. Gross (see record 2013-44085-000 ). The scientific field of emotion. Adapted from Gross &Thompson. Handbook of Emotion Regulation. Emotions... • ...arise when one attends to situations relevant (meaning) to one's goals. ABSTRACT Emotion regulation is not always deliberate, but can also operate on nonconscious or implicit levels. From an action control perspective, there.","[{'authorId': '34961032', 'name': 'K. Lambert'}]",1216.0,"{'bibtex': '@Article{Lambert2007HandbookOE,\n author = {K. Lambert},\n journal = {JAMA},\n pages = {1805-1810},\n title = {Handbook of Emotion Regulation},\n volume = {298},\n year = {2007}\n}\n'}",,"{'volume': '298', 'pages': '1805-1810', 'name': 'JAMA'}",0.0,Handbook of Emotion Regulation,2007.0
2334,c777e0a136369181dd5bd11e4a3e0a27e7a529b9,"Audio-visual emotion expression by synthetic agents is widely employed in research, industrial, and commercial applications. However, the mechanism through which people judge the multimodal emotional display of these agents is not yet well understood. This study is an attempt to provide a better understanding of the interaction between video and audio channels through the use of a continuous dimensional evaluation framework of valence, activation, and dominance. The results indicate that the congruent audio-visual presentation contains information allowing users to differentiate between happy and angry emotional expressions to a greater degree than either of the two channels individually. Interestingly, however, sad and neutral emotions which exhibit a lesser degree of activation show more confusion when presented using both channels. Furthermore, when faced with a conflicting emotional presentation, users predominantly attended to the vocal channel. It is speculated that this is most likely due to the limited level of facial emotion expression inherent in the current animated face. The results also indicate that there is no clear integration of audio and visual channels in emotion perception as in speech perception indicated by the McGurk effect. The final judgments were biased toward the modality with stronger expression power.","[{'authorId': '2523983', 'name': 'E. Provost'}, {'authorId': '2108057415', 'name': 'Sungbok Lee'}, {'authorId': '1742183', 'name': 'M. Matarić'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",16.0,"{'bibtex': '@Article{Provost2008HumanPO,\n author = {E. Provost and Sungbok Lee and M. Matarić and Shrikanth S. Narayanan},\n journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {2201-2204},\n title = {Human perception of synthetic character emotions in the presence of conflicting and congruent vocal and facial expressions},\n year = {2008}\n}\n'}",,"{'pages': '2201-2204', 'name': '2008 IEEE International Conference on Acoustics, Speech and Signal Processing'}",11.0,Human perception of synthetic character emotions in the presence of conflicting and congruent vocal and facial expressions,2008.0
2335,c784671a78d78090192e14f9961bcf96be991c79,"Eye gaze offers several key cues regarding conversational discourse during face-to-face interaction between people. While a large body of research results exist to document the use of gaze in human-to-human interaction, and in animating realistic embodied avatars, recognition of conversational eye gestures - distinct eye movement patterns relevant to discourse - has received less attention. We analyze eye gestures during interaction with an animated embodied agent and propose a non-intrusive vision-based approach to estimate eye gaze and recognize eye gestures. In our user study, human participants avert their gaze (i.e. with ""look-away"" or ""thinking"" gestures) during periods of cognitive load. Using our approach, an agent can visually differentiate whether a user is thinking about a response or is waiting for the agent or robot to take its turn.","[{'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '1761495', 'name': 'C. M. Christoudias'}, {'authorId': '1753210', 'name': 'Trevor Darrell'}]",57.0,"{'bibtex': '@Inproceedings{Morency2006RecognizingGA,\n author = {Louis-Philippe Morency and C. M. Christoudias and Trevor Darrell},\n pages = {287-294},\n title = {Recognizing gaze aversion gestures in embodied conversational discourse},\n year = {2006}\n}\n'}",,{'pages': '287-294'},39.0,Recognizing gaze aversion gestures in embodied conversational discourse,2006.0
2336,c78475b9450c42764c94bf28abfbace01893ef6a,"Chapter Objectives ✓ To learn about the concepts of data mining. ✓ To understand the need for, and the applications of data mining ✓ To differentiate between data mining and machine learning ✓ To understand the process of data mining. ✓ To understand the difference between data mining and machine learning. Introduction to Data Mining In the age of information, an enormous amount of data is available in different industries and organizations. The availability of this massive data is of no use unless it is transformed into valuable information. Otherwise, we are sinking in data, but starving for knowledge. The solution to this problem is data mining which is the extraction of useful information from the huge amount of data that is available. Data mining is defined as follows: ‘Data mining is a collection of techniques for efficient automated discovery of previously unknown, valid, novel, useful and understandable patterns in large databases. The patterns must be actionable so they may be used in an enterprise's decision making.’ From this definition, the important take aways are: • Data mining is a process of automated discovery of previously unknown patterns in large volumes of data. • This large volume of data is usually the historical data of an organization known as the data warehouse. • Data mining deals with large volumes of data, in Gigabytes or Terabytes of data and sometimes as much as Zetabytes of data (in case of big data). • Patterns must be valid, novel, useful and understandable. • Data mining allows businesses to determine historical patterns to predict future behaviour. • Although data mining is possible with smaller amounts of data, the bigger the data the better the accuracy in prediction. • There is considerable hype about data mining at present, and the Gartner Group has listed data mining as one of the top ten technologies to watch. Need of Data Mining Data mining is a recent buzz word in the field of Computer Science. It is a computing process that uses intelligent mathematical algorithms to extract the relevant data and computes the probability of future actions. It is also known as Knowledge Discovery in Data (KDD).","[{'authorId': '2944823', 'name': 'D. Larose'}]",4907.0,"{'bibtex': '@Article{Larose2005IntroductionTD,\n author = {D. Larose},\n journal = {Principles of Data Mining},\n title = {Introduction to Data Mining},\n year = {2005}\n}\n'}",,{'name': 'Principles of Data Mining'},71.0,Introduction to Data Mining,2005.0
2337,c787a3156bc3d20d4d505a831d463892d764f537,,"[{'authorId': '2059629695', 'name': 'Paula M. Ellis'}, {'authorId': '145315445', 'name': 'J. Bryson'}]",14.0,"{'bibtex': '@Inproceedings{Ellis2005TheSO,\n author = {Paula M. Ellis and J. Bryson},\n pages = {394-404},\n title = {The Significance of Textures for Affective Interfaces},\n year = {2005}\n}\n'}",,{'pages': '394-404'},35.0,The Significance of Textures for Affective Interfaces,2005.0
2338,c799058a7105b0bd4ff6046ccafab766f7753c24,"This paper presents an ontological approach to the domain of drama. After a description of the drama domain in a cross- cultural and media setting, we introduce the ontology Drammar. Drammar consists of two components, encoding respectively the conceptual model and the SWRL rules. The conceptual model, mainly grounding in AI theories, represents the major concepts of drama, such as agents, actions, plans, units, emotions and values. Then, the paper focuses on the rule component that augments the representation by mapping the intentions of the characters onto the actions actually performed and by appraising the emotion felt by the characters in the drama. To illustrate the functioning of the ontology we introduce a running example from an excerpt of the drama Hamlet. Finally, we carry out an evaluation of the approach on an annotation task that is relevant for drama studies research and teaching. In particular, the emotion appraisal is tested on the main characters of four dramas of different nature, by computing precision and recall results with respect to a human annotator.","[{'authorId': '152862473', 'name': 'V. Lombardo'}, {'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '1944743919', 'name': 'Antonio Pizzo'}, {'authorId': '144411873', 'name': 'R. Damiano'}, {'authorId': '1792687', 'name': 'Antonio Lieto'}]",28.0,"{'bibtex': '@Article{Lombardo2015CouplingCM,\n author = {V. Lombardo and C. Battaglino and Antonio Pizzo and R. Damiano and Antonio Lieto},\n journal = {Semantic Web},\n pages = {503-534},\n title = {Coupling conceptual modeling and rules for the annotation of dramatic media},\n volume = {6},\n year = {2015}\n}\n'}",,"{'volume': '6', 'pages': '503-534', 'name': 'Semantic Web'}",134.0,Coupling conceptual modeling and rules for the annotation of dramatic media,2015.0
2339,c7ec687b34759b189251d8fd46aabf3e826de3cf,"With the rising interest in Virtual Reality and the fast development and improvement of available devices, new features of interactions are becoming available. One of them that is becoming very popular is hand tracking, as the idea to replace controllers for interactions in virtual worlds. This experiment aims to compare different interaction types in VR using either controllers or hand tracking. Participants had to play two simple VR games with various types of tasks in those games — grabbing objects or typing numbers. While playing, they were using interactions with different visualizations of hands and controllers. The focus of this study was to investigate user experience of varying interactions (controller vs. hand tracking) for those two simple tasks. Results show that different interaction types statistically significantly influence reported emotions with Self-Assessment Manikin (SAM), where for hand tracking participants were feeling higher valence, but lower arousal and dominance. Additionally, task type of grabbing was reported to be more realistic, and participants experienced a higher presence. Surprisingly, participants rated the interaction type with controllers where both where hands and controllers were visualized as statistically most preferred. Finally, hand tracking for both tasks was rated with the System Usability Scale (SUS) scale, and hand tracking for the task typing was rated as statistically significantly more usable. These results can drive further research and, in the long term, contribute to help selecting the most matching interaction modality for a task.","[{'authorId': '1390123460', 'name': 'Jan-Niklas Voigt-Antons'}, {'authorId': '51293010', 'name': 'Tanja Kojić'}, {'authorId': '2064469770', 'name': 'Danish Ali'}, {'authorId': '145733288', 'name': 'S. Möller'}]",35.0,"{'bibtex': '@Article{Voigt-Antons2020InfluenceOH,\n author = {Jan-Niklas Voigt-Antons and Tanja Kojić and Danish Ali and S. Möller},\n journal = {2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)},\n pages = {1-4},\n title = {Influence of Hand Tracking as a Way of Interaction in Virtual Reality on User Experience},\n year = {2020}\n}\n'}",,"{'pages': '1-4', 'name': '2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)'}",17.0,Influence of Hand Tracking as a Way of Interaction in Virtual Reality on User Experience,2020.0
2341,c8089943935fcdbd486279f3205914ddbe4fe396,"This paper describes the design and evaluation of a robotic TV assistant that helps users find a TV-programme that fits their interests. Questions that were addressed include: What personality do users prefer for the robotic TV-assistant? What level of control do they prefer? How do personality and the level of control relate to each other? Four prototypes were developed by combining two personalities and two levels of user control. In the high control condition, a speech-based command-and-control interaction style was used, whereas the interaction style in the low control condition consisted of speech-based system-initiative natural language dialogue. The results demonstrated an interaction between the effects of personality and level of control on user preferences. Overall, the most preferred combination was an extravert and friendly personality with low user control. Additionally, it was found that perceived level of control was influenced by the robot's personality. This suggests that the robot's personality can be used as a means to increase the amount of control that users perceive","[{'authorId': '1729155', 'name': 'B. Meerbeek'}, {'authorId': '2163126', 'name': 'J. Hoonhout'}, {'authorId': '49317857', 'name': 'P. Bingley'}, {'authorId': '1745774', 'name': 'J. Terken'}]",17.0,"{'bibtex': '@Article{Meerbeek2006InvestigatingTR,\n author = {B. Meerbeek and J. Hoonhout and P. Bingley and J. Terken},\n journal = {ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication},\n pages = {404-410},\n title = {Investigating the relationship between the personality of a robotic TV assistant and the level of user control},\n year = {2006}\n}\n'}",,"{'pages': '404-410', 'name': 'ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication'}",28.0,Investigating the relationship between the personality of a robotic TV assistant and the level of user control,2006.0
2342,c83153089aded7999c4979958f8aae27dfef7ab3,"Future Intelligent Environments will feature fixed and mobile display screens with socially-aware virtual embodied agents. This paper addresses the potential of virtual embodied agents to generate a social connection with the inhabitants of Intelligent Environments. The goal of the study is to explore to what extent users mimic the behavior of an interactive agent. An experiment is conducted in which participants interact verbally with a virtual embodied agent. During the interaction, both the vocal pitch and the affective facial expressions of the agent are locally manipulated and the consecutive vocal and facial expressions of the participants registered. Computational analyses of the recorded expressions reveal vocal and facial mimicry as a sign of unconscious affect recognition and social connection. The implications of these results regarding the design of future Intelligent Environments are discussed.","[{'authorId': '153696301', 'name': 'R.J.H. Mattheij'}, {'authorId': '36365492', 'name': 'M. Postma'}, {'authorId': '1729457', 'name': 'E. Postma'}]",10.0,"{'bibtex': '@Article{Mattheij2015MirrorMO,\n author = {R.J.H. Mattheij and M. Postma and E. Postma},\n journal = {J. Ambient Intell. Smart Environ.},\n pages = {121-132},\n title = {Mirror mirror on the wall: Is there mimicry in you all?},\n volume = {7},\n year = {2015}\n}\n'}",,"{'volume': '7', 'pages': '121-132', 'name': 'J. Ambient Intell. Smart Environ.'}",46.0,Mirror mirror on the wall: Is there mimicry in you all?,2015.0
2343,c882fe3b02e84462292517b1f824ce2a7b6c2525,,"[{'authorId': '68990793', 'name': 'A. Byrns'}, {'authorId': '28987363', 'name': 'H. Abdessalem'}, {'authorId': '47258254', 'name': 'M. Cuesta'}, {'authorId': '34277926', 'name': 'M. Bruneau'}, {'authorId': '145580293', 'name': 'S. Belleville'}, {'authorId': '1788058', 'name': 'C. Frasson'}]",8.0,"{'bibtex': ""@Inproceedings{Byrns2020AdaptiveMT,\n author = {A. Byrns and H. Abdessalem and M. Cuesta and M. Bruneau and S. Belleville and C. Frasson},\n pages = {214-219},\n title = {Adaptive Music Therapy for Alzheimer's Disease Using Virtual Reality},\n year = {2020}\n}\n""}",,{'pages': '214-219'},17.0,Adaptive Music Therapy for Alzheimer's Disease Using Virtual Reality,2020.0
2344,c89c71dbe5617bea44383585b58cd0cbc37bf79a,"A general game playing system is one that can accept a formal description of a game and play the game effectively without human intervention. Unlike specialized game players, such as Deep Blue, general game players do not rely on algorithms designed in advance for specific games; and, unlike Deep Blue, they are able to play different kinds of games. In order to promote work in this area, the AAAI is sponsoring an open competition at this summer's Twentieth National Conference on Artificial Intelligence. This article is an overview of the technical issues and logistics associated with this summer's competition, as well as the relevance of general game playing to the long range-goals of artificial intelligence.","[{'authorId': '4940444', 'name': 'M. Genesereth'}, {'authorId': '38372561', 'name': 'Nathaniel Love'}, {'authorId': '145044563', 'name': 'B. Pell'}]",552.0,"{'bibtex': '@Article{Genesereth2005GeneralGP,\n author = {M. Genesereth and Nathaniel Love and B. Pell},\n journal = {AI Mag.},\n pages = {62-72},\n title = {General Game Playing: Overview of the AAAI Competition},\n volume = {26},\n year = {2005}\n}\n'}",,"{'volume': '26', 'pages': '62-72', 'name': 'AI Mag.'}",6.0,General Game Playing: Overview of the AAAI Competition,2005.0
2345,c89ca850c24675d83386c345c32e5fbbe74c78ea,"To engage their human users, socially interactive virtual agents must be equipped with the ability to communicate emotions using facial expressions. Therefore, a main goal is to build a generative model that can produce the range of realistic dynamic facial expressions of emotion that occur across social life. We contribute to this goal by building a psychologically valid generative model of facial expressions directly from subjective human perception using a novel psychology-based approach. First, we build a valence-arousal space of face movements by identifying the specific face movements that convey valence (positive/negative) and arousal (excited/calm) in 40 individual participants. We then tested the capacity of the valence-arousal space to generate a broad range of facial expressions of emotion including the six classic emotions and complex emotions. By cross-correlating a large set of facial expressions of basic and complex emotions with the valence-arousal space, we show that our model can successfully represent a wide range of emotions. We anticipate that our psychologically valid facial expression generation model will enhance the emotion signalling capabilities of virtual agents.","[{'authorId': None, 'name': 'Meng Liu'}, {'authorId': '2341660', 'name': 'Y. Duan'}, {'authorId': '2054557', 'name': 'Robin A. A. Ince'}, {'authorId': '6416348', 'name': 'Chaona Chen'}, {'authorId': '48522841', 'name': 'Oliver G. B. Garrod'}, {'authorId': '2287417', 'name': 'P. Schyns'}, {'authorId': '2143019', 'name': 'Rachael E. Jack'}]",3.0,"{'bibtex': '@Article{Liu2020BuildingAG,\n author = {Meng Liu and Y. Duan and Robin A. A. Ince and Chaona Chen and Oliver G. B. Garrod and P. Schyns and Rachael E. Jack},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Building a Generative Space of Facial Expressions of Emotions Using Psychological Data-driven Methods},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},22.0,Building a Generative Space of Facial Expressions of Emotions Using Psychological Data-driven Methods,2020.0
2346,c8cc1836d64ea3ae0d31f7d3bbad5e574bdd4450,"Despite the apparent sociability of human kind, immoral behaviour is ever present in society. The term 'immoral behaviour' represents a complex array of conduct, ranging from insensitivity to topics of conversation through to violent assault and murder. To better understand the neuroscience of immoral behaviour, this review investigates two clinical populations that commonly present with changes in moral behaviour - behavioural-variant frontotemporal dementia and acquired brain injuries. Based on evidence from these groups, it is argued that rather than a single underlying cause, immoral behaviour can result from three distinct types of cognitive failure: (1) problems understanding others; (2) difficulties controlling behaviour; or (3) deficits in the capacity to make appropriate emotional contributions. Each of these failures is associated with damage to different brain regions. A more nuanced approach to the neuroscience of immoral behaviour has important implications for our understanding of immoral behaviour in a wide range of clinical groups, as well as human society more broadly.","[{'authorId': '48872234', 'name': 'S. Roberts'}, {'authorId': '2149900272', 'name': 'J. Henry'}, {'authorId': '2932333', 'name': 'P. Molenberghs'}]",17.0,"{'bibtex': '@Article{Roberts2019ImmoralBF,\n author = {S. Roberts and J. Henry and P. Molenberghs},\n journal = {Journal of neuropsychology},\n title = {Immoral behaviour following brain damage: A review.},\n year = {2019}\n}\n'}",,{'name': 'Journal of neuropsychology'},181.0,Immoral behaviour following brain damage: A review.,2019.0
2347,c8d756dab66e1f07bb83821a2c992e84da6fd137,"In order to move toward a more accurate, complete, and integrative theory of the causes of emotions, empirical evidence relevant to a recently proposed appraisal theory was examined, and hypotheses from several alternative appraisal theories were compared and tested. Given questions that focused on the cognitive causes of emotions rather than their phenomenological contents, 182 subjects rated the appraisal determinants of emotion experiences that they recalled. Results suggest that appraisals of unexpectedness (not unexpected/unexpected), situational state (motive-inconsistent/motiveconsistent), motivational state (aversive/appetitive), probability (uncertain/ certain), control potential (low/high), problem source (non-characterological/characterological factors), and agency (circumstances/other person/self), differentiate a large number of widely-discussed emotions. These results are used to formulate a revised, empirically grounded, and more comprehensive model that specifies which appraisals cause 17 ...","[{'authorId': '6784016', 'name': 'Ira J. Roseman'}]",921.0,"{'bibtex': '@Article{Roseman1996AppraisalDO,\n author = {Ira J. Roseman},\n journal = {Cognition & Emotion},\n pages = {241-278},\n title = {Appraisal Determinants of Emotions: Constructing a More Accurate and Comprehensive Theory},\n volume = {10},\n year = {1996}\n}\n'}",,"{'volume': '10', 'pages': '241-278', 'name': 'Cognition & Emotion'}",52.0,Appraisal Determinants of Emotions: Constructing a More Accurate and Comprehensive Theory,1996.0
2348,c90b4d970b6c036f28e66a2cb2fe129fe0705063,,"[{'authorId': '1384137264', 'name': 'Erick J. Rodríguez-Seda'}, {'authorId': '2377539', 'name': 'D. Stipanović'}, {'authorId': '145963824', 'name': 'M. Spong'}]",30.0,"{'bibtex': '@Article{Rodríguez-Seda2016GuaranteedCA,\n author = {Erick J. Rodríguez-Seda and D. Stipanović and M. Spong},\n journal = {Journal of Optimization Theory and Applications},\n pages = {1014-1038},\n title = {Guaranteed Collision Avoidance for Autonomous Systems with Acceleration Constraints and Sensing Uncertainties},\n volume = {168},\n year = {2016}\n}\n'}",,"{'volume': '168', 'pages': '1014-1038', 'name': 'Journal of Optimization Theory and Applications'}",45.0,Guaranteed Collision Avoidance for Autonomous Systems with Acceleration Constraints and Sensing Uncertainties,2016.0
2349,c939317b385303018ae0cce13988347ba1a26535,"Anthropomorphic virtual agents can serve as powerful technological mediators to impact motivational outcomes such as self-efficacy and attitude change. Such anthropomorphic agents can be designed as simulated social models in the Bandurian sense, providing social influence as virtual ‘role models’. Of particular value is the capacity for designing such agents as optimized social models for a target audience and context. Importantly, the visual presence and appearance of such agents can have a major impact on motivation and affect regardless of the underlying technical sophistication. Empirical results of different instantiations of agent presence and appearance are reviewed for both autonomous virtual agents and avatars that represent a user.","[{'authorId': '25550816', 'name': 'A. L. Baylor'}]",223.0,"{'bibtex': '@Article{Baylor2009PromotingMW,\n author = {A. L. Baylor},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n pages = {3559 - 3565},\n title = {Promoting motivation with virtual agents and avatars: role of visual presence and appearance},\n volume = {364},\n year = {2009}\n}\n'}",,"{'volume': '364', 'pages': '3559 - 3565', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",54.0,Promoting motivation with virtual agents and avatars: role of visual presence and appearance,2009.0
2351,c9442568ff8d64c4d55ad770abbf8d658002a862,"Abstract:Yaezawa and Yoshida's (1981) ﬁndings on the effects of an intrusion on personal space were reinvestigated. Thirty-ﬁve female students were confronted with the approach of a male stranger, and blood pressure (BP) and heart rate (HR) were monitored. Throughout the model's approach, HR showed a signiﬁcant triphasic change (an initial decrease, a subsequent increase, and then a secondary decrease), whereas subjective feelings of anxiety and tension showed signiﬁcant, gradual increases. These trends were similar to those of Yaezawa and Yoshida’s. Nonetheless, their explanation that the triphasic change in HR reﬂected once hightened and then relieved tension, which was incongruent with the subjective ratings, seemed questionable. As the BP elevated to a moderate degree in spite of the modest HR changes, total peripheral resistance must have been increasing during the model's approach. Blood pressure elevations via this sort of hemodynamic pressor mechanisms have often been reported when a person can only tolerate passively during exposure to stress. This seems to be the case in an intrusion on personal space.","[{'authorId': '49770907', 'name': 'Y. Sawada'}]",25.0,"{'bibtex': '@Article{Sawada2003BloodPA,\n author = {Y. Sawada},\n journal = {Japanese Psychological Research},\n pages = {115-121},\n title = {Blood pressure and heart rate responses to an intrusion on personal space},\n volume = {45},\n year = {2003}\n}\n'}",,"{'volume': '45', 'pages': '115-121', 'name': 'Japanese Psychological Research'}",15.0,Blood pressure and heart rate responses to an intrusion on personal space,2003.0
2352,c993b1752b6057f561a58b330c69232a82c2bc7a,,"[{'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1802126', 'name': 'I. Poggi'}]",127.0,"{'bibtex': '@Inproceedings{Peters2005AMO,\n author = {Christopher E. Peters and C. Pelachaud and Elisabetta Bevacqua and M. Mancini and I. Poggi},\n pages = {229-240},\n title = {A Model of Attention and Interest Using Gaze Behavior},\n year = {2005}\n}\n'}",,{'pages': '229-240'},31.0,A Model of Attention and Interest Using Gaze Behavior,2005.0
2353,c9b5d487e0aff4a25de0c73276a7584beba53f40,,"[{'authorId': '2047241817', 'name': 'Maike Paetzel-Prüsmann'}, {'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",27.0,"{'bibtex': '@Article{Paetzel-Prüsmann2021TheIO,\n author = {Maike Paetzel-Prüsmann and G. Perugia and Ginevra Castellano},\n journal = {Comput. Hum. Behav.},\n pages = {106756},\n title = {The Influence of robot personality on the development of uncanny feelings},\n volume = {120},\n year = {2021}\n}\n'}",,"{'volume': '120', 'pages': '106756', 'name': 'Comput. Hum. Behav.'}",79.0,The Influence of robot personality on the development of uncanny feelings,2021.0
2354,c9b95398d3afc6c32a56f9b0b0648d2ed91ed90f,"Standardized metrics for assessing the success of robots is a necessity for a research field to compare and validate results. The Godspeed Questionnaire Series (GQS) is one of the most frequently used questionnaires in the field of Human-Robot Interaction (HRI) with over 160 citations as of October 2014. In this paper, we present a meta analysis of studies that used the GQS. The HRI community uses a large variety of robotic platforms and only the NAO robot seems to be used by multiple research groups. A qualitative meta analysis of 18 NAO studies reveals accumulated findings on perceived intelligence, likability, and anthropomorphism, but also reveals contradictions on how the robot's behaviour and task context impact GQS ratings. The paper closes with a reflection on how added value of data analysis and presentation could be achieved for the HRI community in future.","[{'authorId': '37585849', 'name': 'A. Weiss'}, {'authorId': '1728894', 'name': 'C. Bartneck'}]",92.0,"{'bibtex': '@Article{Weiss2015MetaAO,\n author = {A. Weiss and C. Bartneck},\n journal = {2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {381-388},\n title = {Meta analysis of the usage of the Godspeed Questionnaire Series},\n year = {2015}\n}\n'}",,"{'pages': '381-388', 'name': '2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)'}",42.0,Meta analysis of the usage of the Godspeed Questionnaire Series,2015.0
2355,c9bb0b4917ab6748aaf24a97582e11574decdf14,,"[{'authorId': '1682667', 'name': 'Lars Omlor'}, {'authorId': '33365402', 'name': 'M. Giese'}]",50.0,"{'bibtex': '@Article{Omlor2006ExtractionOS,\n author = {Lars Omlor and M. Giese},\n journal = {Neurocomputing},\n pages = {1938-1942},\n title = {Extraction of spatio-temporal primitives of emotional body expressions},\n volume = {70},\n year = {2006}\n}\n'}",,"{'volume': '70', 'pages': '1938-1942', 'name': 'Neurocomputing'}",16.0,Extraction of spatio-temporal primitives of emotional body expressions,2006.0
2356,c9c5b4e22be1af958adb9fe7445ddc9d039bfad3,,"[{'authorId': '1694989', 'name': 'A. Edalat'}]",22.0,"{'bibtex': '@Inproceedings{Edalat2017SelfattachmentAH,\n author = {A. Edalat},\n pages = {273-314},\n title = {Self-attachment: A Holistic Approach to Computational Psychiatry},\n year = {2017}\n}\n'}",,"{'volume': '', 'pages': '273-314', 'name': ''}",109.0,Self-attachment: A Holistic Approach to Computational Psychiatry,2017.0
2359,c9ec92da00669e9a1fa1dc33cebd1420c11cb768,"This paper presents a system designed to create crisis-simulation that shows the emergence of crowd behavior from individual behaviors, based on the emotional contagion phenomenon. Many studies from psychology and sociology mention this phenomenon but very little was found about what parameters determine it. This paper proposes a model of emotional contagion based on individual personality and relationships. A key contribution of this work is describing a computational mapping from OCEAN personality traits to the strength of, and susceptibility to, emotional contagion. This model is implemented into a multi-agent system where everyone is represented by an emotional, social and cognitive agent.","[{'authorId': '1930380', 'name': 'Margot Lhommet'}, {'authorId': '1790872', 'name': 'D. Lourdeaux'}, {'authorId': '2084784', 'name': 'J. Barthès'}]",25.0,"{'bibtex': '@Article{Lhommet2011NeverAI,\n author = {Margot Lhommet and D. Lourdeaux and J. Barthès},\n journal = {2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology},\n pages = {89-92},\n title = {Never Alone in the Crowd: A Microscopic Crowd Model Based on Emotional Contagion},\n volume = {2},\n year = {2011}\n}\n'}",,"{'volume': '2', 'pages': '89-92', 'name': '2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology'}",18.0,Never Alone in the Crowd: A Microscopic Crowd Model Based on Emotional Contagion,2011.0
2360,ca20b05926f4f754912dbbfa79aa3de38640c652,"Perception and emotion interact, as is borne out by studies of how people recognize emotion from facial expressions. Psychological and neurological research has elucidated the processes, and the brain structures, that participate in facial emotion recognition. Studies have shown that emotional reactions to viewing faces can be very rapid and that these reactions may, in turn, be used to judge the emotion shown in the face. Recent experiments have argued that people actively explore facial expressions in order to recognize the emotion, a mechanism that emphasizes the instrumental nature of social cognition.","[{'authorId': '46306086', 'name': 'R. Adolphs'}]",49.0,"{'bibtex': '@Article{Adolphs2006PerceptionAE,\n author = {R. Adolphs},\n journal = {Current Directions in Psychological Science},\n pages = {222 - 226},\n title = {Perception and Emotion},\n volume = {15},\n year = {2006}\n}\n'}",,"{'volume': '15', 'pages': '222 - 226', 'name': 'Current Directions in Psychological Science'}",18.0,Perception and Emotion,2006.0
2361,ca43ed7c7fb969d034eb6d13af3d290f1cd7dd5b,"Emotional expression is a key requirement for intelligent virtual agents. In order for an agent to produce dynamic spoken content speech synthesis is required. However, despite substantial work with pre-recorded prompts, very little work has explored the combined effect of high quality emotional speech synthesis and facial expression. In this paper we offer a baseline evaluation of the naturalness and emotional range available by combining the freely available SmartBody component of the Virtual Human Toolkit (VHTK) with CereVoice text to speech (TTS) system. Results echo previous work using pre-recorded prompts, the visual modality is dominant and the modalities do not interact. This allows the speech synthesis to add gradual changes to the perceived emotion both in terms of valence and activation. The naturalness reported is good, 3.54 on a 5 point MOS scale.","[{'authorId': '1762616', 'name': 'B. Potard'}, {'authorId': '1702205', 'name': 'M. Aylett'}, {'authorId': '48317415', 'name': 'David A. Baude'}]",75.0,"{'bibtex': '@Inproceedings{Potard2008IntelligentVA,\n author = {B. Potard and M. Aylett and David A. Baude},\n title = {Intelligent Virtual Agents},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': ''}",13.0,Intelligent Virtual Agents,2008.0
2362,ca642f0448a305dc35071e1160b5b7952ba081c7,We examined how the presence and nonverbal communication of an animated pedagogical agent affects students’ perceptions and learning. College students learned about astronomy either without an agen...,"[{'authorId': '2445038', 'name': 'Casey Frechette'}, {'authorId': '144006457', 'name': 'R. Moreno'}]",66.0,"{'bibtex': ""@Article{Frechette2010TheRO,\n author = {Casey Frechette and R. Moreno},\n journal = {J. Media Psychol. Theor. Methods Appl.},\n pages = {61-72},\n title = {The Roles of Animated Pedagogical Agents' Presence and Nonverbal Communication in Multimedia Learning Environments},\n volume = {22},\n year = {2010}\n}\n""}",,"{'volume': '22', 'pages': '61-72', 'name': 'J. Media Psychol. Theor. Methods Appl.'}",50.0,The Roles of Animated Pedagogical Agents' Presence and Nonverbal Communication in Multimedia Learning Environments,2010.0
2363,ca6587d319c317cef0753546e9d738022c551cd2,"Psychologists have long assumed that the motivation for all intentional action, including all action intended to benefit others, is egoistic. People benefit others because, ultimately, to do so benefits themselves. The empathy-altruism hypothesis challenges this assumption. It claims that empathic emotion evokes truly altruistic motivation, motivation with an ultimate goal of benefiting not the self but the person for whom empathy is felt. Logical and psychological distinctions between egoism and altruism are reviewed, providing a conceptualframeworkfor empirical tests for the existence of altruism. Results of empirical tests to date are summarized; these results provide impressive support for the empathy-altruism hypothesis. We conclude that the popular and parsimonious explanation of prosocial motivation in terms of universal egoism must give way to a pluralistic explanation that includes altruism as well as egoism. Implications of such a pluralism are briefly noted, not only for our understanding of prosocial motivation but also for our understanding of human nature and of the emotion-motivation link. We humans devote much time and energy to helping others. We send money to rescue famine victims halfway around the world. We work to save whales. We stay up all night to comfort a friend who has just suffered a broken relationship. We stop on a busy highway to help a stranded motorist change a flat.","[{'authorId': '32526876', 'name': 'C. Batson'}, {'authorId': '145911430', 'name': 'Laura L. Shaw'}]",1077.0,"{'bibtex': '@Article{Batson1991EvidenceFA,\n author = {C. Batson and Laura L. Shaw},\n journal = {Psychological Inquiry},\n pages = {107-122},\n title = {Evidence for Altruism: Toward a Pluralism of Prosocial Motives},\n volume = {2},\n year = {1991}\n}\n'}",,"{'volume': '2', 'pages': '107-122', 'name': 'Psychological Inquiry'}",100.0,Evidence for Altruism: Toward a Pluralism of Prosocial Motives,1991.0
2364,ca6b3dda45a82093453aa4d3ea0b3293b59bfad9,"Much theory and research on emotion are based on the facial expressions of amateurs asked to pose for still photographs. The theory of facial affect programs (FAPs; P. Ekman, 1972) was proposed to account for the resulting expressions, most of which are patterns consisting of distinguishab le parts. In the present study, 4 Hollywood films noted for fine acting and realism were examined for the facial expressions that accompany a basic emotion. In keeping with the theory of FAPs, professional actors judged as happy were found smiling in 97% (Duchenne smiling in 74%) of cases. In contrast, actors judged as surprised, afraid, angry, disgusted, or sad rarely showed the predicted pattern (found in 0 to 31% of cases). Typically, they used one or two parts from the full pattern. If these films represent real life, these findings favor a theory that assumes separable parts (e.g., components theory) over the older theory of FAPs.","[{'authorId': '3024196', 'name': 'James M. Carroll'}, {'authorId': '46367714', 'name': 'J. Russell'}]",195.0,"{'bibtex': ""@Article{Carroll1997FacialEI,\n author = {James M. Carroll and J. Russell},\n journal = {Journal of Personality and Social Psychology},\n pages = {164-176},\n title = {Facial Expressions in Hollywood's Portrayal of Emotion},\n volume = {72},\n year = {1997}\n}\n""}",,"{'volume': '72', 'pages': '164-176', 'name': 'Journal of Personality and Social Psychology'}",54.0,Facial Expressions in Hollywood's Portrayal of Emotion,1997.0
2365,ca9c2a8dd5bdc4ea258357faafcb1e69a0c1c116,"To create a robot with a mind of its own, we extended a formalized version of a model that explains affect-driven interaction with mechanisms for goal-directed behavior. We ran simulation experiments with intelligent software agents and found that agents preferred affect-driven decision options to rational decision options in situations where choices for low expected utility are irrational. This behavior counters current models in decision making, which generally have a hedonic bias and always select the option with the highest expected utility.","[{'authorId': '71825175', 'name': 'J. Hoorn'}, {'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '3347943', 'name': 'G. F. Siddiqui'}]",21.0,"{'bibtex': '@Article{Hoorn2008WhenTU,\n author = {J. Hoorn and M. Pontier and G. F. Siddiqui},\n journal = {2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},\n pages = {296-301},\n title = {When the User Is Instrumental to Robot Goals: First Try – Agent Uses Agent},\n volume = {2},\n year = {2008}\n}\n'}",,"{'volume': '2', 'pages': '296-301', 'name': '2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology'}",19.0,When the User Is Instrumental to Robot Goals: First Try – Agent Uses Agent,2008.0
2366,cb2ad000afe1c69fee69fc1bacfcdf37dd055184,"The possible modulatory influence of motivations and emotions is of great interest in designing robotic adaptive systems. In this paper, an attempt is made to connect the concept of periodic behaviour activations to emotional modulation, in order to link the variability of behaviours to the circumstances in which they are activated. The impact of emotion is studied, described as timed controlled structures, on simple but conflicting reactive behaviours. Through this approach it is shown that the introduction of such asynchronies in the robot control system may lead to an adaptation in the emergent behaviour without having an explicit action selection mechanism. The emergent behaviours of a simple robot designed with both a parallel and a hierarchical architecture are evaluated and compared.","[{'authorId': '1686116', 'name': 'E. Burattini'}, {'authorId': '145005536', 'name': 'Silvia Rossi'}]",12.0,"{'bibtex': '@Article{Burattini2010PeriodicAO,\n author = {E. Burattini and Silvia Rossi},\n journal = {Connection Science},\n pages = {197 - 213},\n title = {Periodic activations of behaviours and emotional adaptation in behaviour-based robotics},\n volume = {22},\n year = {2010}\n}\n'}",,"{'volume': '22', 'pages': '197 - 213', 'name': 'Connection Science'}",46.0,Periodic activations of behaviours and emotional adaptation in behaviour-based robotics,2010.0
2367,cb3dd836eb28a064896e13fd4a2f0d2aeccede5e,"Virtual Reality (VR) is the state-of-the-art human-computer interface; it uses computer graphics to create a realistic-looking virtual world that the user can interact with in real-time. Recent advances in VR have shown promise in the pursuit of devising new techniques to combat mental disorder(s). Har-nessing the power of VR, we have developed a customised immersive virtual reality platform to practise protocols of self-attachment psychotherapy. Consumer-level VR is a recent phenomenon; for wide scale adaptation of such platforms it is important that they are built taking into account usability engineering principles speciﬁc to virtual environments(VE). In this work, we share our experience applying systematic heuristic and formative evaluations to our VR platform to make it more usable. The participants who evaluated our platform were asked (via a follow-up questionnaire) to rate their level-of-immersion, learn-ability and overall usability of the platform. Insights from our usability evaluation could help developers build better and more usable psycho-therapeutic VR platforms in the future.","[{'authorId': '2485535', 'name': 'I. Ghaznavi'}, {'authorId': '2101444684', 'name': 'Usman Jehanzeb'}, {'authorId': '146493111', 'name': 'Duncan Gillies'}]",7.0,"{'bibtex': '@Inproceedings{Ghaznavi2019UsabilityEO,\n author = {I. Ghaznavi and Usman Jehanzeb and Duncan Gillies},\n title = {Usability evaluation of an immersive virtual reality platform for self-attachment psychotherapy},\n year = {2019}\n}\n'}",,,17.0,Usability evaluation of an immersive virtual reality platform for self-attachment psychotherapy,2019.0
2369,cb8ca0655b0ab1ef3f05d80e7260b9c361dc126a,"In this paper we describe a wearable system that allows people to place and interact with 3D virtual tags placed around them. This uses two wearable technologies: a head-worn wearable computer (Google Glass) and a chest-worn depth sensor (Tango). The Google Glass is used to generate and display virtual information to the user, while the Tango is used to provide robust indoor position tracking for the Glass. The Tango enables spatial awareness of the surrounding world using various motion sensors including 3D depth sensing, an accelerometer and a motion tracking camera. Using these systems together allows users to create a virtual tag via voice input and then register this tag to a physical object or position in 3D space as an augmented annotation. We describe the design and implementation of the system, user feedback, research implications, and directions for future work.","[{'authorId': '2363053', 'name': 'Alaeddin Nassani'}, {'authorId': '1759034', 'name': 'Huidong Bai'}, {'authorId': '48534381', 'name': 'Gun A. Lee'}, {'authorId': '1684805', 'name': 'M. Billinghurst'}]",28.0,"{'bibtex': '@Article{Nassani2015TagIA,\n author = {Alaeddin Nassani and Huidong Bai and Gun A. Lee and M. Billinghurst},\n journal = {SIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications},\n title = {Tag it!: AR annotation using wearable sensors},\n year = {2015}\n}\n'}",,{'name': 'SIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications'},4.0,Tag it!: AR annotation using wearable sensors,2015.0
2370,cb976cb512a5a073ddb6a215f3b4fd2dadbb1a29,In the present article a model for memory retrieval of personal experiences for virtual agents is presented. It builds upon previous work and focuses on the effect memory retrieval can have on the agent’s emotional state. Memory retrieval is defined as an emotional re-appraisal of past experiences. The variation of intensity of such reexperience is explored by modeling two phenomena: downward spirals and habituation. Downward spirals consist of situations in which recurrent retrieval of negatively charged past memories increases the recall intensity of such memories in depressed individuals. Habituation is the reduction of recall intensity caused by re-experiencing past experiences in a context perceived by the individual as safe.,"[{'authorId': '1784861', 'name': 'P. Gomes'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '145813496', 'name': 'C. Martinho'}]",0.0,"{'bibtex': '@Inproceedings{Gomes2011BetweenDS,\n author = {P. Gomes and Ana Paiva and C. Martinho},\n title = {Between Downward Spirals and Habituation : Emotion Intensity in Virtual Agents ’ Memory Retrieval},\n year = {2011}\n}\n'}",[],,23.0,Between Downward Spirals and Habituation : Emotion Intensity in Virtual Agents ’ Memory Retrieval,2011.0
2371,cbb29e3008cba2266a3a3e017fab60000878ab84,"Facial emotion recognition will become vitally important in future multi-cultural visual communication systems, for emotion translation between cultures, which may be considered analogous to speech translation. However so far the recognition of facial emotions is mainly addressed by computer vision researchers, based on facial display. Also detection of vocal expressions of emotions can be found in research work done by acoustic researchers. Most of these research paradigms are devoted purely to visual or purely to auditory human emotion detection. However we found that it is very interesting to consider both these auditory and visual information together, for processing, since we hope this kind of multi-modal information processing will become a datum of information processing in future multimedia era. By several intensive subjective evaluation studies we found that human beings recognise anger, happiness, surprise and dislike by their visual appearance, compared to voice only detection. When the audio track of each emotion clip is dubbed with a different type of auditory emotional expression, still anger, happiness and surprise were video dominant. However the dislike emotion gave mixed responses to different speakers. In both studies we found that sadness and fear emotions were audio dominant. As a conclusion, we propose a method of facial emotion detection by using a hybrid approach, which uses multi-modal information for facial emotion recognition.","[{'authorId': '2220211943', 'name': 'L. C. D. Silva'}, {'authorId': '2407080', 'name': 'T. Miyasato'}, {'authorId': '2458123', 'name': 'R. Nakatsu'}]",233.0,"{'bibtex': '@Article{Silva1997FacialER,\n author = {L. C. D. Silva and T. Miyasato and R. Nakatsu},\n journal = {Proceedings of ICICS, 1997 International Conference on Information, Communications and Signal Processing. Theme: Trends in Information Systems Engineering and Wireless Multimedia Communications (Cat.},\n pages = {397-401 vol.1},\n title = {Facial emotion recognition using multi-modal information},\n volume = {1},\n year = {1997}\n}\n'}",,"{'volume': '1', 'pages': '397-401 vol.1', 'name': 'Proceedings of ICICS, 1997 International Conference on Information, Communications and Signal Processing. Theme: Trends in Information Systems Engineering and Wireless Multimedia Communications (Cat.'}",12.0,Facial emotion recognition using multi-modal information,1997.0
2372,cbf4a4bbfadf0c8321da579075f14b997b1354ad,"Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Annotations should include Action Units (AUs) and their intensities as well as emotion category. This goal cannot be readily achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of one million images of facial expressions of emotion in the wild (i.e., face images downloaded from the Internet). First, we show that this newly proposed algorithm can recognize AUs and their intensities reliably across databases. To our knowledge, this is the first published algorithm to achieve highly-accurate results in the recognition of AUs and their intensities across multiple databases. Our algorithm also runs in real-time (>30 images/second), allowing it to work with large numbers of images and video sequences. Second, we use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. These images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, affective computing, social and cognitive psychology and neuroscience, e.g., ""show me all the images with happy faces"" or ""all images with AU 1 at intensity c"".","[{'authorId': '1413851634', 'name': 'C. F. Benitez-Quiroz'}, {'authorId': '8038057', 'name': 'R. Srinivasan'}, {'authorId': '1384255355', 'name': 'Aleix M. Martinez'}]",453.0,"{'bibtex': '@Article{Benitez-Quiroz2016EmotioNetAA,\n author = {C. F. Benitez-Quiroz and R. Srinivasan and Aleix M. Martinez},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5562-5570},\n title = {EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild},\n year = {2016}\n}\n'}",,"{'pages': '5562-5570', 'name': '2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'}",36.0,"EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild",2016.0
2373,cc11380ded972a25976e184e3d6d434a16612a40,"Research on virtual characters has been ongoing for the past 20 years. Early efforts focused mostly on making the characters move and speak that is, on body and facial animation. Simultaneously, researchers worked on making characters look convincing by adding animation and rendering hair, clothes, and muscles. The next step was to increase artists' interactive control over characters so that it was easier to create convincing video games and cinema.","[{'authorId': '2671891', 'name': 'Zerrin Kasap'}, {'authorId': '2497958', 'name': 'M. B. Moussa'}, {'authorId': '32319566', 'name': 'P. Chaudhuri'}, {'authorId': '1387241200', 'name': 'N. Magnenat-Thalmann'}]",105.0,"{'bibtex': '@Article{Kasap2009MakingTR,\n author = {Zerrin Kasap and M. B. Moussa and P. Chaudhuri and N. Magnenat-Thalmann},\n journal = {IEEE Computer Graphics and Applications},\n pages = {20-29},\n title = {Making Them Remember—Emotional Virtual Characters with Memory},\n volume = {29},\n year = {2009}\n}\n'}",,"{'volume': '29', 'pages': '20-29', 'name': 'IEEE Computer Graphics and Applications'}",20.0,Making Them Remember—Emotional Virtual Characters with Memory,2009.0
2374,cc3b557cd80a9c36083769c4e370a4b10567c073,"Anvil is a tool for the annotation of audiovisual material containing multimodal dialogue. Annotation takes place on freely definable, multiple layers (tracks) by inserting time-anchored elements that hold a number of typed attribute-value pairs. Higher-level elements (suprasegmental) consist of a sequence of elements. Attributes contain symbols or cross-level links to arbitrary other elements. Anvil is highly generic (usable with different annotation schemes), platform-independent, XMLbased and fitted with an intuitive graphical user interface. For project integration, Anvil offers the import of speech transcription and export of text and table data for further statistical processing.","[{'authorId': '145616714', 'name': 'Michael Kipp'}]",651.0,"{'bibtex': '@Inproceedings{Kipp2001ANVILA,\n author = {Michael Kipp},\n pages = {1367-1370},\n title = {ANVIL - a generic annotation tool for multimodal dialogue},\n year = {2001}\n}\n'}",,{'pages': '1367-1370'},11.0,ANVIL - a generic annotation tool for multimodal dialogue,2001.0
2375,cc4c0205dafea88e7b8349ed26cefca95970da8a,"In the recent years, engagement modeling has gained increasing attention due the important role it plays in human-agent interaction. The agent should be able to detect, in real time, the engagement level of the user in order to react accordingly. In this context, our goal is to develop a computational model to predict engagement level of the user in real time. Relying on previous findings, we use facial expressions, head movements and gaze direction as predictive features. Moreover, engagement is not only measured from single cues, but from the combination of several cues that arise over a certain time window. Thus, for better engagement prediction, we consider the variation of multimodal behaviors over time. To this end, we rely on LSTM that can jointly model the temporality and the sequentiality of multimodal behaviors.","[{'authorId': '8447202', 'name': 'Soumia Dermouche'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",23.0,"{'bibtex': '@Article{Dermouche2019EngagementMI,\n author = {Soumia Dermouche and C. Pelachaud},\n journal = {2019 International Conference on Multimodal Interaction},\n title = {Engagement Modeling in Dyadic Interaction},\n year = {2019}\n}\n'}",,{'name': '2019 International Conference on Multimodal Interaction'},26.0,Engagement Modeling in Dyadic Interaction,2019.0
2376,cc6f1228a42e005777f0cb05c010916f96b70535,,"[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '1742554', 'name': 'Arvid Kappas'}, {'authorId': '1732377', 'name': 'R. Aylett'}, {'authorId': '1691444', 'name': 'H. Hastie'}, {'authorId': '2431058', 'name': 'W. Barendregt'}, {'authorId': '3191258', 'name': 'F. Nabais'}, {'authorId': '145681472', 'name': 'S. Bull'}]",92.0,"{'bibtex': '@Inproceedings{Castellano2013TowardsEV,\n author = {Ginevra Castellano and Ana Paiva and Arvid Kappas and R. Aylett and H. Hastie and W. Barendregt and F. Nabais and S. Bull},\n pages = {733-736},\n title = {Towards Empathic Virtual and Robotic Tutors},\n year = {2013}\n}\n'}",,{'pages': '733-736'},16.0,Towards Empathic Virtual and Robotic Tutors,2013.0
2377,ccab4cc81387d1b1f095ce4fe74aad5ed4f73305,"Motivating Japanese EFL (English as a foreign language) learners is essential in order for them to learn English conversation effectively. We developed an English conversation mobile learning system based on a heuristic model of variables influencing the concept of “willing to communicate” by MacIntyre et al. The system has three features: an application that learners can use anywhere and anytime, topics based on the interests and lives of learners, and pseudo-interactive and agreeable English conversation. The results of an experiment and analysis revealed that the topics based on the interests and lives of learners might be effective for talking in English with relaxed from the beginning and for talking with motivation, while a pseudo-interactive and agreeable English conversation style might work well in helping learners evaluate themselves to speak more fluently, express themselves better, and speak in a more relaxed manner as they continue to practice speaking.","[{'authorId': '7545906', 'name': 'Kae Nakaya'}, {'authorId': '1775359', 'name': 'M. Murota'}]",9.0,"{'bibtex': '@Inproceedings{Nakaya2013DEVELOPMENTAE,\n author = {Kae Nakaya and M. Murota},\n title = {DEVELOPMENT AND EVALUATION OF AN INTERACTIVE ENGLISH CONVERSATION LEARNING SYSTEM WITH A MOBILE DEVICE USING TOPICS BASED ON THE LIFE OF THE LEARNER},\n year = {2013}\n}\n'}",,,26.0,DEVELOPMENT AND EVALUATION OF AN INTERACTIVE ENGLISH CONVERSATION LEARNING SYSTEM WITH A MOBILE DEVICE USING TOPICS BASED ON THE LIFE OF THE LEARNER,2013.0
2378,ccb03a53ce4a6384a75cfb623a2ac9e8cd0a0b21,"Collaborative creation of knowledge is an approach which has been successfully demonstrated by crowdsourcing project like Wikipedia. Similar techniques have recently been adopted for the creation of collaboratively generated Knowledge Graphs like, for example, Wikidata. While such an approach enables the creation of high quality structured content, it also comes with the challenge of introducing contributors’ implicit bias in the generated Knowledge Graph. In this paper, we investigate how paid crowdsourcing can be used to understand contributor bias for controversial facts to be included into collaborative Knowledge Graphs. We propose methods to trace the provenance of crowdsourced fact checking thus enabling bias transparency rather than aiming at eliminating bias from the Knowledge Graph.","[{'authorId': '1694274', 'name': 'Gianluca Demartini'}]",17.0,"{'bibtex': '@Article{Demartini2019ImplicitBI,\n author = {Gianluca Demartini},\n journal = {Companion Proceedings of The 2019 World Wide Web Conference},\n title = {Implicit Bias in Crowdsourced Knowledge Graphs},\n year = {2019}\n}\n'}",,{'name': 'Companion Proceedings of The 2019 World Wide Web Conference'},24.0,Implicit Bias in Crowdsourced Knowledge Graphs,2019.0
2379,ccdabcf81bab920e16e84163ba92507eb7cb7bf9,"Implementing and fleshing out a number of psychological and neuroscience theories of cognition, the LIDA conceptual model aims at being a cognitive “theory of everything.” With modules or processes for perception, working memory, episodic memories, “consciousness,” procedural memory, action selection, perceptual learning, episodic learning, deliberation, volition, and non-routine problem solving, the LIDA model is ideally suited to provide a working ontology that would allow for the discussion, design, and comparison of AGI systems. The LIDA architecture is based on the LIDA cognitive cycle, a sort of “cognitive atom.” The more elementary cognitive modules and processes play a role in each cognitive cycle. Higher-level processes are performed over multiple cycles. In addition to giving a quick overview of the LIDA conceptual model, and its underlying computational technology, we argue for the LIDA architecture's role as a foundational architecture for an AGI. Finally, lessons For AGI researchers drawn from the model and its architecture are discussed.","[{'authorId': '145796793', 'name': 'S. Franklin'}]",62.0,"{'bibtex': '@Inproceedings{Franklin2007AFA,\n author = {S. Franklin},\n pages = {36-54},\n title = {A Foundational Architecture for Artificial General Intelligence},\n year = {2007}\n}\n'}",,{'pages': '36-54'},72.0,A Foundational Architecture for Artificial General Intelligence,2007.0
2380,ccdbc666f2b82c8fb4530db1a9df247f06c6655d,"This study used longitudinal data collected from two trauma-exposed samples, survivors of community violence (N = 294) and wildfire evacuees (N = 234), to examine a key claim underlying a proposed reformulation of the symptom structure of posttraumatic stress disorder (PTSD). This theory, which we term the PTSD-dysphoria model, posits that 8 of 17 symptoms of PTSD reflect dysphoria or general psychological distress and might be deemphasized to improve the utility of the PTSD construct (Simms, Watson, & Doebbeling, 2002). For each sample, we analyzed PTSD symptoms and measures of general distress administered at 2 time points. A consistent pattern of findings was observed across assessments for each sample: All 17 PTSD symptoms were highly associated with measures of general distress. Moreover, we found no evidence that dysphoria symptoms were more highly correlated than PTSD-specific symptoms with general distress. Results call into question both the conceptual basis and the clinical utility of differentiating between symptoms that appear to be relatively specific to PTSD and those that seem more broadly characteristic of general psychological distress.","[{'authorId': '35204425', 'name': 'G. Marshall'}, {'authorId': '10712652', 'name': 'Terry L. Schell'}, {'authorId': '7792934', 'name': 'J. Miles'}]",91.0,"{'bibtex': '@Article{Marshall2010AllPS,\n author = {G. Marshall and Terry L. Schell and J. Miles},\n journal = {Journal of abnormal psychology},\n pages = {\n          126-35\n        },\n title = {All PTSD symptoms are highly associated with general distress: ramifications for the dysphoria symptom cluster.},\n volume = {119 1},\n year = {2010}\n}\n'}",,"{'volume': '119 1', 'pages': '\n          126-35\n        ', 'name': 'Journal of abnormal psychology'}",48.0,All PTSD symptoms are highly associated with general distress: ramifications for the dysphoria symptom cluster.,2010.0
2381,ccfa7e011aa23b2477d11f6c4f779f998db382ac,"Successful human interaction commonly involves prototypical exchanges where interactors are engaged, synchronized, and harmonious in their behaviors. The copying of aspects of the other's behavior, at different levels, seems central to establishing and maintaining such empathic connections. Yet, many questions remain unanswered, particularly how it is possible to reflect the same affective content back to the other when the actual motion itself is not exactly the same as theirs. This paper presents a perceptual study in which emotional gestures conducted by an actor were mapped onto synthesized versions generated by an embodied virtual agent. Copying is at the expressive level, where qualities such as the fluidity or expansiveness of gestures are considered, rather than exact low-level motion matching. Participants were later asked to rate the emotional content of video recordings of both the original and the synthesized gestures. A statistical analysis shows that, in most cases, participants associated the emotional content of the agent's gestures with that intended to be expressed by the original actor. The results suggest that a combination of the type of movement performed and its quality is important for successfully communicating emotions.","[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '2803283', 'name': 'P. McOwan'}]",52.0,"{'bibtex': '@Article{Castellano2012ExpressiveCB,\n author = {Ginevra Castellano and M. Mancini and Christopher E. Peters and P. McOwan},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},\n pages = {776-783},\n title = {Expressive Copying Behavior for Social Agents: A Perceptual Analysis},\n volume = {42},\n year = {2012}\n}\n'}",,"{'volume': '42', 'pages': '776-783', 'name': 'IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans'}",55.0,Expressive Copying Behavior for Social Agents: A Perceptual Analysis,2012.0
2383,cd0a0648572ccf2d350a8cb9a0bf9aacaa12942c,,"[{'authorId': '1715438', 'name': 'J. Laird'}, {'authorId': '48603437', 'name': 'A. Newell'}, {'authorId': '1749322', 'name': 'P. Rosenbloom'}]",2952.0,"{'bibtex': '@Article{Laird1987SOARAA,\n author = {J. Laird and A. Newell and P. Rosenbloom},\n journal = {Artif. Intell.},\n pages = {1-64},\n title = {SOAR: An Architecture for General Intelligence},\n volume = {33},\n year = {1987}\n}\n'}",,"{'volume': '33', 'pages': '1-64', 'name': 'Artif. Intell.'}",97.0,SOAR: An Architecture for General Intelligence,1987.0
2384,cd2dd4c17e42c3de1681a1c9834803e99740db1e,,"[{'authorId': '9174234', 'name': 'Jina Lee'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",239.0,"{'bibtex': '@Inproceedings{Lee2006NonverbalBG,\n author = {Jina Lee and S. Marsella},\n pages = {243-255},\n title = {Nonverbal Behavior Generator for Embodied Conversational Agents},\n year = {2006}\n}\n'}",,{'pages': '243-255'},22.0,Nonverbal Behavior Generator for Embodied Conversational Agents,2006.0
2387,cd2e1ac003d8b7904cde6a647c6110324e73df0e,,"[{'authorId': '2737576', 'name': 'M. Schurz'}, {'authorId': '2811620', 'name': 'J. Raduà'}, {'authorId': '1946019', 'name': 'M. Aichhorn'}, {'authorId': '2511893', 'name': 'F. Richlan'}, {'authorId': '2274473', 'name': 'J. Perner'}]",1183.0,"{'bibtex': '@Article{Schurz2014FractionatingTO,\n author = {M. Schurz and J. Raduà and M. Aichhorn and F. Richlan and J. Perner},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {9-34},\n title = {Fractionating theory of mind: A meta-analysis of functional brain imaging studies},\n volume = {42},\n year = {2014}\n}\n'}",,"{'volume': '42', 'pages': '9-34', 'name': 'Neuroscience & Biobehavioral Reviews'}",209.0,Fractionating theory of mind: A meta-analysis of functional brain imaging studies,2014.0
2388,cd94d69c7f30ab3add8ae62ab753c18cf60c9d1b,,"[{'authorId': '1398030059', 'name': 'J. Gutiérrez-Maldonado'}, {'authorId': '1404488371', 'name': 'M. Rus-Calafell'}, {'authorId': '1406405598', 'name': 'Joan González-Conde'}]",36.0,"{'bibtex': '@Article{Gutiérrez-Maldonado2014CreationOA,\n author = {J. Gutiérrez-Maldonado and M. Rus-Calafell and Joan González-Conde},\n journal = {Virtual Reality},\n pages = {61-71},\n title = {Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability},\n volume = {18},\n year = {2014}\n}\n'}",,"{'volume': '18', 'pages': '61-71', 'name': 'Virtual Reality'}",57.0,Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability,2014.0
2389,cda4cfadf25c6bef41d9db0a3a98001c1b42493b,"The argument of this original and difficult book is that “gestures are an integral part of language as much as are words, phrases and sentences-gestures and language are one system” (p. 2) . Gestures are instantaneous, imagistic, analog, holistic expressions of the same thought that speech renders in hierarchical, linear, digital, analytic form. David McNeill credits Adam Kendon (1972, 1980) with discovering the link between, and essential unity of, speech sounds and gestural movements; his own work elaborates this insight at the higher linguistic levels of semantics and pragmatics. The topic of the book, then, is gestures that accompany speech, the left-hand end of what McNeill calls “ K e n h i ’ s coiitiiiiiiim: Gesticulation + Language-like gestures + Pantomimes 3 Emblems + Sign languages” (p. 37). The continuum ranges from the informal, spontaneous, idiosyncratic movements of the hands and arms that often accompany speech, to the socially-regulated, standardized, linguistic forms of a sign language, with its arbitrary (non-iconic) lexicon. Between these poles the obligatory presence of speech declines and the linguistic properties of gestures increase. “Language-like gestures” are grammatically integrated into an utterance, as when a speaker, asked about the weather on his vacation, replies: “Well, it was [oscillating hand gesture]”, where the “so-so” gesture replaces an adjectival predicate. “Pantomime” conveys its full meaning in silence or, at most, with inarticulate onomatopoeia; also, in pantomime, sequences of gestures can form a unit, as they can in a sign language, but cannot in gesticulation. “Emblems” conform to standards of wellformedness, a language-like property that gesticulation and pantomime lack: in England, the palm-front V-sign is Churchill’s “Victory!”, the palm-back V-sign is a sexual insult. (For an amusing cross-class confusion in emblem dialects, see Collett, Marsh, and O’Shaughnessy, 1979, p. 229, where Margaret Thatcher appears in an Associated Press Photo, making the palm-back V-sign at a moment of electoral triumph.) The contrast between the two ends of Kendon’s continuum, between spontaneous gesture and conventional sign, epitomizes McNeill’s notion of the process by which an utterance evolves in a speaker’s mind. Spontaneous gesture reveals the primitive stage of an utterance, global, unsegmented, non-hierarchical, from which its conventional representation in speech unfolds: hierarchical, segmented, linear. The inner symbols of the primitive stage are private, idiosyncratic, closed to social influence; the end stage is public, grammatical, socially regulated. McNeill supposes that the primitive","[{'authorId': '1403941699', 'name': 'M. Studdert-Kennedy'}]",2688.0,"{'bibtex': '@Article{Studdert-Kennedy1994HandAM,\n author = {M. Studdert-Kennedy},\n journal = {Language and Speech},\n pages = {203 - 209},\n title = {Hand and Mind: What Gestures Reveal About Thought.},\n volume = {37},\n year = {1994}\n}\n'}",,"{'volume': '37', 'pages': '203 - 209', 'name': 'Language and Speech'}",10.0,Hand and Mind: What Gestures Reveal About Thought.,1994.0
2391,cdcee1355981de99b93608ab61b09a249da728b6,"With the increasing number of emergencies, the crowd simulation technology has attracted wide attention in the recent years. Existing emergencies have shown that individuals are easy to be influenced by others' emotion during the evacuation. This will make it easier for people to aggregate together and increase security risks. Some of the existing evacuation models without considering emotion are therefore not suitable for describing crowd behaviors in emergencies. We propose a perception‐based emotion contagion model and use multiagent technology to simulate crowd behaviors. Navigation points are introduced to guide the movement of the agents. Based on the proposed model, a prototype simulation system for crowd emotion contagion is developed. The comparative simulation experiments verify that the model can effectively deduct the evacuation time and crowd emotion contagion. The proposed model could be an assistant analysis method for crowd management in emergencies.","[{'authorId': '2109341502', 'name': 'Zhen Liu'}, {'authorId': '91436120', 'name': 'Tingting Liu'}, {'authorId': '145877966', 'name': 'M. Ma'}, {'authorId': '1679560', 'name': 'Hui-Huang Hsu'}, {'authorId': '115478092', 'name': 'Zhongrui Ni'}, {'authorId': '9398597', 'name': 'Yanjie Chai'}]",18.0,"{'bibtex': '@Article{Liu2018APE,\n author = {Zhen Liu and Tingting Liu and M. Ma and Hui-Huang Hsu and Zhongrui Ni and Yanjie Chai},\n journal = {Computer Animation and Virtual Worlds},\n title = {A perception‐based emotion contagion model in crowd emergent evacuation simulation},\n volume = {29},\n year = {2018}\n}\n'}",,"{'volume': '29', 'name': 'Computer Animation and Virtual Worlds'}",34.0,A perception‐based emotion contagion model in crowd emergent evacuation simulation,2018.0
2393,ce19ccea3c4d8f4ba794d96d5e3f2e35029d4984,,"[{'authorId': '40428623', 'name': 'Luis-Felipe Rodríguez'}, {'authorId': '145956015', 'name': 'Félix F. Ramos'}]",2.0,"{'bibtex': '@Article{Rodríguez2014DevelopmentOC,\n author = {Luis-Felipe Rodríguez and Félix F. Ramos},\n journal = {Cognitive Computation},\n pages = {351 - 375},\n title = {Development of Computational Models of Emotions for Autonomous Agents: A Review},\n volume = {6},\n year = {2014}\n}\n'}",,"{'volume': '6', 'pages': '351 - 375', 'name': 'Cognitive Computation'}",0.0,Development of Computational Models of Emotions for Autonomous Agents: A Review,2014.0
2394,ce91afdf69694bf661ad55f85f2911461ef8e932,"Interactive storytelling is a privileged application of intelligent visual actor technology. The authors introduce their character-based interactive storytelling prototype that uses hierarchical task network planning techniques, which support story generation and any-time user intervention.","[{'authorId': '1696638', 'name': 'M. Cavazza'}, {'authorId': '144553087', 'name': 'Fred Charles'}, {'authorId': '1788038', 'name': 'Steven J. Mead'}]",387.0,"{'bibtex': '@Article{Cavazza2002CharacterBasedIS,\n author = {M. Cavazza and Fred Charles and Steven J. Mead},\n journal = {IEEE Intell. Syst.},\n pages = {17-24},\n title = {Character-Based Interactive Storytelling},\n volume = {17},\n year = {2002}\n}\n'}",,"{'volume': '17', 'pages': '17-24', 'name': 'IEEE Intell. Syst.'}",14.0,Character-Based Interactive Storytelling,2002.0
2395,cea4fb5e46aee36ff77ac5d4f0014cd8cb1bee30,"People often draw trait inferences from the facial appearance of other people. We investigated the minimal conditions under which people make such inferences. In five experiments, each focusing on a specific trait judgment, we manipulated the exposure time of unfamiliar faces. Judgments made after a 100-ms exposure correlated highly with judgments made in the absence of time constraints, suggesting that this exposure time was sufficient for participants to form an impression. In fact, for all judgments—attractiveness, likeability, trustworthiness, competence, and aggressiveness—increased exposure time did not significantly increase the correlations. When exposure time increased from 100 to 500 ms, participants' judgments became more negative, response times for judgments decreased, and confidence in judgments increased. When exposure time increased from 500 to 1,000 ms, trait judgments and response times did not change significantly (with one exception), but confidence increased for some of the judgments; this result suggests that additional time may simply boost confidence in judgments. However, increased exposure time led to more differentiated person impressions.","[{'authorId': '46716544', 'name': 'Janine Willis'}, {'authorId': '145441940', 'name': 'A. Todorov'}]",1573.0,"{'bibtex': '@Article{Willis2006FirstI,\n author = {Janine Willis and A. Todorov},\n journal = {Psychological Science},\n pages = {592 - 598},\n title = {First Impressions},\n volume = {17},\n year = {2006}\n}\n'}",,"{'volume': '17', 'pages': '592 - 598', 'name': 'Psychological Science'}",21.0,First Impressions,2006.0
2396,cea58679f2294c19f3f6b37823e1250a6f1cb015,"Social Stories are becoming a popular intervention used to improve the social skills of children with disabilities. This article examines the use of Social Stories with young children with disabilities. Social Stories are described, creation guidelines are recommended, and strategies for Social Story implementation in the classroom are discussed.","[{'authorId': '113642907', 'name': 'Cori M. More'}]",16.0,"{'bibtex': '@Article{More2012SocialSA,\n author = {Cori M. More},\n journal = {Intervention in School and Clinic},\n pages = {167 - 174},\n title = {Social Stories™ and Young Children},\n volume = {47},\n year = {2012}\n}\n'}",,"{'volume': '47', 'pages': '167 - 174', 'name': 'Intervention in School and Clinic'}",28.0,Social Stories™ and Young Children,2012.0
2397,cea690881d7ac0f222e83348525f52a20bc53308,,"[{'authorId': '143880621', 'name': 'Saif M. Mohammad'}, {'authorId': '1854999', 'name': 'Xiao-Dan Zhu'}, {'authorId': '2886725', 'name': 'S. Kiritchenko'}, {'authorId': '145900862', 'name': 'Joel D. Martin'}]",234.0,"{'bibtex': '@Article{Mohammad2015SentimentEP,\n author = {Saif M. Mohammad and Xiao-Dan Zhu and S. Kiritchenko and Joel D. Martin},\n journal = {Inf. Process. Manag.},\n pages = {480-499},\n title = {Sentiment, emotion, purpose, and style in electoral tweets},\n volume = {51},\n year = {2015}\n}\n'}",,"{'volume': '51', 'pages': '480-499', 'name': 'Inf. Process. Manag.'}",89.0,"Sentiment, emotion, purpose, and style in electoral tweets",2015.0
2398,cf08954f2433149a6e198423597ec8add84d278c,,"[{'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145109163', 'name': 'Ari Shapiro'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '3201827', 'name': 'A. Leuski'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",190.0,"{'bibtex': '@Article{Hartholt2013AllTN,\n author = {Arno Hartholt and D. Traum and S. Marsella and Ari Shapiro and Giota Stratou and A. Leuski and Louis-Philippe Morency and J. Gratch},\n journal = {Nature},\n pages = {\n          465\n        },\n title = {All Together Now - Introducing the Virtual Human Toolkit},\n volume = {418 6897},\n year = {2013}\n}\n'}",,"{'volume': '418 6897', 'pages': '\n          465\n        ', 'name': 'Nature'}",52.0,All Together Now - Introducing the Virtual Human Toolkit,2013.0
2400,cf1f0dddacd263c62dc079b9ef09478098c2d6f6,"In recent years 3D virtual characters have become more common in desktop interfaces, particularly in gaming and entertainment applications. In this paper we describe how augmented reality (AR) technology can be used to bring virtual characters into the real world and compare AR characters to other types of virtual characters. We have developed a handheld AR educational application in which a virtual character teaches users about art history. We present results from a user study that explores how realistic the character needs to be for it to be an effective and engaging educational tool and if augmented reality offers benefits for this type of application.","[{'authorId': '144184131', 'name': 'Daniel Wagner'}, {'authorId': '1684805', 'name': 'M. Billinghurst'}, {'authorId': '1742819', 'name': 'D. Schmalstieg'}]",69.0,"{'bibtex': '@Inproceedings{Wagner2006HowRS,\n author = {Daniel Wagner and M. Billinghurst and D. Schmalstieg},\n pages = {57},\n title = {How real should virtual characters be?},\n year = {2006}\n}\n'}",,{'pages': '57'},23.0,How real should virtual characters be?,2006.0
2401,cf5cea3fabefca82ce2f3a618307324d37550ae4,"Notwithstanding the significant role that human–robot interactions (HRI) will play in the near future, limited research has explored the neural correlates of feeling eerie in response to social robots. To address this empirical lacuna, the current investigation examined brain activity using functional magnetic resonance imaging while a group of participants (n = 26) viewed a series of human–human interactions (HHI) and HRI. Although brain sites constituting the mentalizing network were found to respond to both types of interactions, systematic neural variation across sites signaled diverging social-cognitive strategies during HHI and HRI processing. Specifically, HHI elicited increased activity in the left temporal–parietal junction indicative of situation-specific mental state attributions, whereas HRI recruited the precuneus and the ventromedial prefrontal cortex (VMPFC) suggestive of script-based social reasoning. Activity in the VMPFC also tracked feelings of eeriness towards HRI in a parametric manner, revealing a potential neural correlate for a phenomenon known as the uncanny valley. By demonstrating how understanding social interactions depends on the kind of agents involved, this study highlights pivotal sub-routes of impression formation and identifies prominent challenges in the use of humanoid robots.","[{'authorId': '113310304', 'name': 'Y. Wang'}, {'authorId': '1849354', 'name': 'S. Quadflieg'}]",55.0,"{'bibtex': '@Article{Wang2015InOO,\n author = {Y. Wang and S. Quadflieg},\n journal = {Social Cognitive and Affective Neuroscience},\n pages = {1515 - 1524},\n title = {In our own image? Emotional and neural processing differences when observing human–human vs human–robot interactions},\n volume = {10},\n year = {2015}\n}\n'}",,"{'volume': '10', 'pages': '1515 - 1524', 'name': 'Social Cognitive and Affective Neuroscience'}",96.0,In our own image? Emotional and neural processing differences when observing human–human vs human–robot interactions,2015.0
2402,cf91085c85202178f1e3b5d821c5b0e6562d7862,,"[{'authorId': '2713204', 'name': 'K. Dalton'}, {'authorId': '2878453', 'name': 'Brendon M. Nacewicz'}, {'authorId': '30361732', 'name': 'T. Johnstone'}, {'authorId': '34897071', 'name': 'Hillary S. Schaefer'}, {'authorId': '2087502', 'name': 'M. Gernsbacher'}, {'authorId': '2252509287', 'name': 'H. Goldsmith'}, {'authorId': '2238962699', 'name': 'Andrew L. Alexander'}, {'authorId': '2247343381', 'name': 'Richard J. Davidson'}]",1486.0,"{'bibtex': '@Article{Dalton2005GazeFA,\n author = {K. Dalton and Brendon M. Nacewicz and T. Johnstone and Hillary S. Schaefer and M. Gernsbacher and H. Goldsmith and Andrew L. Alexander and Richard J. Davidson},\n journal = {Nature Neuroscience},\n pages = {519-526},\n title = {Gaze fixation and the neural circuitry of face processing in autism},\n volume = {8},\n year = {2005}\n}\n'}",,"{'volume': '8', 'pages': '519-526', 'name': 'Nature Neuroscience'}",24.0,Gaze fixation and the neural circuitry of face processing in autism,2005.0
2403,cfcb33daf43d8b66b46753a467c6b44553f801b5,"Issues and outcomes in negotiation and related settings strategies and tactics in negotiation determinants of demands, concessions and contentious behaviour reactions to the other party's behaviour cognitive and decision processes in negotiation the dual concern model and the determinants of problem solving social norms and their impact on negotiation relationships among the negotiating parties the group context of negotiation mediation choices among procedures in social conflict.","[{'authorId': '48681011', 'name': 'D. G. Pruitt'}, {'authorId': '48755211', 'name': 'P. Carnevale'}]",1023.0,"{'bibtex': '@Inproceedings{Pruitt1993NegotiationIS,\n author = {D. G. Pruitt and P. Carnevale},\n title = {Negotiation in Social Conflict},\n year = {1993}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Negotiation in Social Conflict,1993.0
2404,cfef9aac22acd2c7c932a93736cdac2e4aed1949,"The goal of this work is to build a real-time emotion detection system which utilizes multi-modal fusion of different timescale features of speech. Conventional spectral and prosody features are used for intra-frame and supra-frame features respectively, and a new information fusion algorithm which takes care of the characteristics of each machine learning algorithm is introduced. In this framework, the proposed system can be associated with additional features, such as lexical or discourse information, in later steps. To verify the realtime system performance, binary decision tasks on angry and neutral emotion are performed using concatenated speech signal simulating realtime conditions.","[{'authorId': '2110026741', 'name': 'Samuel Kim'}, {'authorId': '1765829', 'name': 'P. Georgiou'}, {'authorId': '2108057415', 'name': 'Sungbok Lee'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",102.0,"{'bibtex': '@Article{Kim2007RealtimeED,\n author = {Samuel Kim and P. Georgiou and Sungbok Lee and Shrikanth S. Narayanan},\n journal = {2007 IEEE 9th Workshop on Multimedia Signal Processing},\n pages = {48-51},\n title = {Real-time Emotion Detection System using Speech: Multi-modal Fusion of Different Timescale Features},\n year = {2007}\n}\n'}",,"{'pages': '48-51', 'name': '2007 IEEE 9th Workshop on Multimedia Signal Processing'}",19.0,Real-time Emotion Detection System using Speech: Multi-modal Fusion of Different Timescale Features,2007.0
2405,d019b9ce4e86234898a23389e9bc3ad96f585728,"We present ADAPT, a flexible platform for designing and authoring functional, purposeful human characters in a rich virtual environment. Our framework incorporates character animation, navigation, and behavior with modular interchangeable components to produce narrative scenes. The animation system provides locomotion, reaching, gaze tracking, gesturing, sitting, and reactions to external physical forces, and can easily be extended with more functionality due to a decoupled, modular structure. The navigation component allows characters to maneuver through a complex environment with predictive steering for dynamic obstacle avoidance. Finally, our behavior framework allows a user to fully leverage a character's animation and navigation capabilities when authoring both individual decision-making and complex interactions between actors using a centralized, event-driven model.","[{'authorId': '2483632', 'name': 'Alexander Shoulson'}, {'authorId': '2463776', 'name': 'N. Marshak'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}, {'authorId': '1699200', 'name': 'N. Badler'}]",60.0,"{'bibtex': '@Article{Shoulson2013ADAPTTA,\n author = {Alexander Shoulson and N. Marshak and Mubbasir Kapadia and N. Badler},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {1035-1047},\n title = {ADAPT: The Agent Developmentand Prototyping Testbed},\n volume = {20},\n year = {2013}\n}\n'}",,"{'volume': '20', 'pages': '1035-1047', 'name': 'IEEE Transactions on Visualization and Computer Graphics'}",65.0,ADAPT: The Agent Developmentand Prototyping Testbed,2013.0
2406,d033a28bedf5b41a4a2e80778f6f790f9a0c16e9,"The premise of this paper is that agent technology in collaborative virtual environments (CVEs) may be enriched by incorporating an emotional channel alongside the conventional informational content, and that this would be best achieved through an associated visual human embodiment or avatar. Since humans express emotion in face-to-face encounters primarily through facial expression, an investigation was undertaken in order to establish how such expressions might be effectively and efficiently captured and represented in an avatar. The study involved consulting socio-psychological research relating to face-to-face encounters, followed by a controlled experiment to investigate user ability to interpret the faces of the avatars pre-prepared to express specific emotions. Effectiveness was demonstrated through good recognition rates for all but one of the emotion categories, and efficiency was established since a reduced feature set was found to be sufficient to build the successfully recognised core set of avatar facial expressions","[{'authorId': '2263196603', 'name': 'Fabri M'}, {'authorId': '2223875267', 'name': 'Moore Dj'}, {'authorId': '81839805', 'name': 'Hobbs Dj'}]",41.0,"{'bibtex': '@Inproceedings{M2002ExpressiveAN,\n author = {Fabri M and Moore Dj and Hobbs Dj},\n title = {Expressive Agents: Non-verbal Communication in Collaborative Virtual Environments},\n year = {2002}\n}\n'}",,,46.0,Expressive Agents: Non-verbal Communication in Collaborative Virtual Environments,2002.0
2407,d0573284bcb9287947edd6806d6dda88b4a78bd8,"Emotional mimicry is the imitation of the emotional expressions of others. According to the classic view on emotional mimicry (the Matched Motor Hypothesis), people mimic the specific facial movements that comprise a discrete emotional expression. However, little evidence exists for the mimicry of discrete emotions; rather, the extant evidence supports only valence-based mimicry. We propose an alternative Emotion Mimicry in Context view according to which emotional mimicry is not based on mere perception but rather on the interpretation of signals as emotional intentions in a specific context. We present evidence for the idea that people mimic contextualized emotions rather than simply expressive muscle movements. Our model postulates that (implicit or explicit) contextual information is needed for emotional mimicry to take place. It takes into account the relationship between observer and expresser, and suggests that emotional mimicry depends on this relationship and functions as a social regulator.","[{'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '7444483', 'name': 'A. Fischer'}]",456.0,"{'bibtex': '@Article{Hess2013EmotionalMA,\n author = {U. Hess and A. Fischer},\n journal = {Personality and Social Psychology Review},\n pages = {142 - 157},\n title = {Emotional Mimicry as Social Regulation},\n volume = {17},\n year = {2013}\n}\n'}",,"{'volume': '17', 'pages': '142 - 157', 'name': 'Personality and Social Psychology Review'}",131.0,Emotional Mimicry as Social Regulation,2013.0
2408,d0593a60df1b2f647fd11226b32e3c808c1bc37d,"The ERP component N170 is face-sensitive, yet its specificity for faces is controversial. We recorded ERPs while subjects viewed upright and inverted faces and seven object categories. Peak, topography and segmentation analyses were performed. N170 was earlier and larger to faces than to all objects. The classic increase in amplitude and latency was found for inverted faces on N170 but also on P1. Segmentation analyses revealed an extra map found only for faces, reflecting an extra cluster of activity compared to objects. While the N1 for objects seems to reflect the return to baseline from the P1, the N170 for faces reflects a supplement activity. The electrophysiological 'specificity' of faces could lie in the involvement of extra generators for face processing compared to objects and the N170 for faces seems qualitatively different from the N1 for objects. Object and face processing also differed as early as 120 ms.","[{'authorId': '3332602', 'name': 'R. Itier'}, {'authorId': '31880105', 'name': 'Margot J. Taylor'}]",627.0,"{'bibtex': '@Article{Itier2004N170ON,\n author = {R. Itier and Margot J. Taylor},\n journal = {Cerebral cortex},\n pages = {\n          132-42\n        },\n title = {N170 or N1? Spatiotemporal differences between object and face processing using ERPs.},\n volume = {14 2},\n year = {2004}\n}\n'}",,"{'volume': '14 2', 'pages': '\n          132-42\n        ', 'name': 'Cerebral cortex'}",84.0,N170 or N1? Spatiotemporal differences between object and face processing using ERPs.,2004.0
2409,d05a81be26726604844009f2e3129d40933351a8,,"[{'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '3865216', 'name': 'Alice M. Isen'}]",748.0,"{'bibtex': '@Article{Carnevale1986TheIO,\n author = {P. Carnevale and Alice M. Isen},\n journal = {Organizational Behavior and Human Decision Processes},\n pages = {1-13},\n title = {The Influence of Positive Affect and Visual Access on the Discovery of Integrative Solutions in Bilateral Negotiation},\n volume = {37},\n year = {1986}\n}\n'}",,"{'volume': '37', 'pages': '1-13', 'name': 'Organizational Behavior and Human Decision Processes'}",25.0,The Influence of Positive Affect and Visual Access on the Discovery of Integrative Solutions in Bilateral Negotiation,1986.0
2410,d09cfa55f26bafaad715a779ba9fe9b0e30fdb85,,"[{'authorId': '3018957', 'name': 'C. Reeck'}, {'authorId': '20618378', 'name': 'D. Ames'}, {'authorId': '2669604', 'name': 'K. Ochsner'}]",185.0,"{'bibtex': '@Article{Reeck2016TheSR,\n author = {C. Reeck and D. Ames and K. Ochsner},\n journal = {Trends in Cognitive Sciences},\n pages = {47-63},\n title = {The Social Regulation of Emotion: An Integrative, Cross-Disciplinary Model},\n volume = {20},\n year = {2016}\n}\n'}",,"{'volume': '20', 'pages': '47-63', 'name': 'Trends in Cognitive Sciences'}",149.0,"The Social Regulation of Emotion: An Integrative, Cross-Disciplinary Model",2016.0
2411,d0b20f03ef64bda936df2e2a1961ef7d213b842e,"Emotional signaling plays an important role in negotiations and other social decision-making tasks as it can signal intention and shape joint decisions. Specifically it has been shown to influence cooperation or competition. This has been shown in previous studies for scripted interactions that control emotion signaling and rely on manual coding of affect. In this work we examine face-to-face interactions in an iterative social dilemma task (prisoner's dilemma) via an automatic framework for facial expression analysis. We explore if automatic analysis of emotion can give insight into the social function of emotion in face-to-face interactions. Our analysis suggests that positive and negative displays of emotion are associated with more prosocial and proself game acts respectively. Moreover signaling cooperative intentions to the opponent via positivity can leave participants more open to exploitation, whereas signaling a more tough stance via negativity seems to discourage exploitation. However, the benefit of negative affect is short-term and both players do worse over time if they show negative emotions.","[{'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '2065815350', 'name': 'Rens Hoegen'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",10.0,"{'bibtex': '@Article{Stratou2015EmotionalSI,\n author = {Giota Stratou and Rens Hoegen and Gale M. Lucas and J. Gratch},\n journal = {2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {180-186},\n title = {Emotional signaling in a social dilemma: An automatic analysis},\n year = {2015}\n}\n'}",,"{'pages': '180-186', 'name': '2015 International Conference on Affective Computing and Intelligent Interaction (ACII)'}",20.0,Emotional signaling in a social dilemma: An automatic analysis,2015.0
2412,d0cbfa1bc7e6ccf1f289b7210d5d6180f2e72350,"
 
 We present the SimSensei system, a fully automatic virtual agent that conducts interviews to assess indicators of psychological distress. We emphasize on the perception part of the system, a multimodal framework which captures and analyzes user state for both behavioral understanding and interactional purposes.
 
","[{'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '1930380', 'name': 'Margot Lhommet'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '3194430', 'name': 'Kallirroi Georgila'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '29861580', 'name': 'A. Rizzo'}]",32.0,"{'bibtex': '@Inproceedings{Morency2015SimSenseiDA,\n author = {Louis-Philippe Morency and Giota Stratou and D. DeVault and Arno Hartholt and Margot Lhommet and Gale M. Lucas and Fabrizio Morbini and Kallirroi Georgila and Stefan Scherer and J. Gratch and S. Marsella and D. Traum and A. Rizzo},\n pages = {4307-4308},\n title = {SimSensei Demonstration: A Perceptive Virtual Human Interviewer for Healthcare Applications},\n year = {2015}\n}\n'}",,{'pages': '4307-4308'},10.0,SimSensei Demonstration: A Perceptive Virtual Human Interviewer for Healthcare Applications,2015.0
2413,d0ce831d9a56928fa88b810c4d1f68a6c432509c,"In this paper we briefly examine the architecture of augmented reality in smartphone applications. While the architecture is fairly consistent across AR browsers / SDKs, there are some important differences. Appreciating these differences will help developers choose a framework or browser implementation that best meets their needs. Understanding the architecture of these systems will also expose the gaps in current implementations and help us to think clearly about the likely evolution of augmented reality browsers over the next few years. Thomas Reicher's "" Framework For Dynamically Adaptable Augmented Reality Systems "" thesis [1, 2] provides a reference architecture for comparing different augmented reality frameworks and this is a good starting point for our discussion. An illustration of Reicher's reference architecture is shown below. (from Asa MacWilliams[4]) In this model, the augmented reality system is organized into six logical subsystems. A thorough explanation of these subsystems is given by MacWillaims [4]. For convenience below is a brief summary.","[{'authorId': '2032112', 'name': 'Ben Butchart'}]",12.0,"{'bibtex': '@Misc{None,\n author = {Ben Butchart},\n title = {Architectural Styles for Augmented Reality in Smartphones (}\n}\n'}",,,6.0,Architectural Styles for Augmented Reality in Smartphones (,
2414,d0cfeec38d9db31bb75af1a6308527bd9f989dd2,,"[{'authorId': '118295706', 'name': 'Betsy E. Tolstedt'}, {'authorId': '47817664', 'name': 'J. Stokes'}]",106.0,"{'bibtex': '@Article{Tolstedt1983RelationOV,\n author = {Betsy E. Tolstedt and J. Stokes},\n journal = {Journal of Counseling Psychology},\n pages = {573-580},\n title = {Relation of Verbal, Affective, and Physical Intimacy to Marital Satisfaction.},\n volume = {30},\n year = {1983}\n}\n'}",,"{'volume': '30', 'pages': '573-580', 'name': 'Journal of Counseling Psychology'}",19.0,"Relation of Verbal, Affective, and Physical Intimacy to Marital Satisfaction.",1983.0
2415,d15bdf485f3a64abb59e4d0d1d1b18a9fc652bf9,"Emotional reactions to color hue, saturation, and brightness (Munsell color system and color chips) were investigated using the Pleasure-Arousal-Dominance emotion model. Saturation (S) and brightness (B) evidenced strong and consistent effects on emotions. Regression equations for standardized variables were; Pleasure = .69B + .22S, Arousal = -.31B + .60S, Dominance = -.76B + .32S. Brightness effects were nearly the same for chromatic and achromatic colors. Blue, blue-green, green, red-purple, purple, and purple-blue were the most pleasant hues, whereas yellow and green-yellow were the least pleasant. Green-yellow, blue-green, and green were the most arousing, whereas purple-blue and yellow-red were the least arousing. Green-yellow induced greater dominance than red-purple.","[{'authorId': '1580348834', 'name': 'Patricia Valdez'}, {'authorId': '144102217', 'name': 'A. Mehrabian'}]",1174.0,"{'bibtex': '@Article{Valdez1994EffectsOC,\n author = {Patricia Valdez and A. Mehrabian},\n journal = {Journal of experimental psychology. General},\n pages = {\n          394-409\n        },\n title = {Effects of color on emotions.},\n volume = {123 4},\n year = {1994}\n}\n'}",,"{'volume': '123 4', 'pages': '\n          394-409\n        ', 'name': 'Journal of experimental psychology. General'}",45.0,Effects of color on emotions.,1994.0
2416,d167eb654abd485d9de28a6c004797c958a44e5c,,"[{'authorId': '1733226', 'name': 'R. Hughes'}]",993.0,"{'bibtex': '@Article{Hughes2002ACT,\n author = {R. Hughes},\n journal = {Transportation Research Part B-methodological},\n pages = {507-535},\n title = {A continuum theory for the flow of pedestrians},\n volume = {36},\n year = {2002}\n}\n'}",,"{'volume': '36', 'pages': '507-535', 'name': 'Transportation Research Part B-methodological'}",28.0,A continuum theory for the flow of pedestrians,2002.0
2417,d1b0e07cd93332b64541118e66e5fb32ecd214c3,"Gesture behavior is a natural part of human conversation. Much work has focused on removing the need for tedious hand-animation to create embodied conversational agents by designing speech-driven gesture generators. However, these generators often work in a black-box manner, assuming a general relationship between input speech and output motion. As their success remains limited, we investigate in more detail how speech may relate to different aspects of gesture motion. We determine a number of parameters characterizing gesture, such as speed and gesture size, and explore their relationship to the speech signal in a two-fold manner. First, we train multiple recurrent networks to predict the gesture parameters from speech to understand how well gesture attributes can be modeled from speech alone. We find that gesture parameters can be partially predicted from speech, and some parameters, such as path length, being predicted more accurately than others, like velocity. Second, we design a perceptual study to assess the importance of each gesture parameter for producing motion that people perceive as appropriate for the speech. Results show that a degradation in any parameter was viewed negatively, but some changes, such as hand shape, are more impactful than others. A video summarization can be found at https://youtu.be/aw6-_5kmLjY.","[{'authorId': '3430725', 'name': 'Ylva Ferstl'}, {'authorId': '143687087', 'name': 'Michael Neff'}, {'authorId': '145795454', 'name': 'R. Mcdonnell'}]",13.0,"{'bibtex': '@Article{Ferstl2020UnderstandingTP,\n author = {Ylva Ferstl and Michael Neff and R. Mcdonnell},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Understanding the Predictability of Gesture Parameters from Speech and their Perceptual Importance},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},41.0,Understanding the Predictability of Gesture Parameters from Speech and their Perceptual Importance,2020.0
2418,d1b6231de691383ebe2acde07328048ba3198d5b,"Because group-based emotions are rooted in the social identity of the perceiver, we propose that group-based emotions should be sensitive to changes in this social identity. In three experiments, young women reported feeling more anger, fear, and disgust toward Muslims when their identity as women had been made salient, in comparison with various control conditions where their identity as young adults, as social sciences students, their personal identity, or no identity had been made salient. These effects were mediated by appraisals of intergroup threats. In Experiment 3, the salience of the woman social identity also increased intentions to avoid Muslims.","[{'authorId': '1986623', 'name': 'T. Kuppens'}, {'authorId': '4779221', 'name': 'V. Yzerbyt'}]",41.0,"{'bibtex': '@Article{Kuppens2012GroupBasedET,\n author = {T. Kuppens and V. Yzerbyt},\n journal = {Basic and Applied Social Psychology},\n pages = {20 - 33},\n title = {Group-Based Emotions: The Impact of Social Identity on Appraisals, Emotions, and Behaviors},\n volume = {34},\n year = {2012}\n}\n'}",,"{'volume': '34', 'pages': '20 - 33', 'name': 'Basic and Applied Social Psychology'}",55.0,"Group-Based Emotions: The Impact of Social Identity on Appraisals, Emotions, and Behaviors",2012.0
2419,d1bde81d5bad2f06e0f2033ee45c50ff373a625a,"The development of children's emotion-related self-regulation appears to be related to, and likely involved in, many aspects of children's development. In this review, the distinction between effortful self-regulatory processes and those that are somewhat less voluntary is discussed, and literature on the former capacities is reviewed. Emotion-related self-regulation develops rapidly in the early years of life and improves more slowly into adulthood. Individual differences in children's self-regulation are fairly stable after the first year or two of life. Such individual differences are inversely related to at least some types of externalizing problems. Findings for internalizing problems are less consistent and robust, although emotion-related self-regulation appears to be inversely related to internalizing problems after the early years. Self-regulatory capacities have been related to both genetic and environmental factors and their interaction. Some interventions designed to foster self-regulation and, hence, reduce maladjustment, have proved to be at least partially effective.","[{'authorId': '15102546', 'name': 'N. Eisenberg'}, {'authorId': '6138798', 'name': 'Tracy L. Spinrad'}, {'authorId': '6149978', 'name': 'Natalie D. Eggum'}]",905.0,"{'bibtex': ""@Article{Eisenberg2010EmotionrelatedSA,\n author = {N. Eisenberg and Tracy L. Spinrad and Natalie D. Eggum},\n journal = {Annual review of clinical psychology},\n pages = {\n          495-525\n        },\n title = {Emotion-related self-regulation and its relation to children's maladjustment.},\n volume = {6},\n year = {2010}\n}\n""}",,"{'volume': '6', 'pages': '\n          495-525\n        ', 'name': 'Annual review of clinical psychology'}",200.0,Emotion-related self-regulation and its relation to children's maladjustment.,2010.0
2420,d20838945496292b9dd1245350f90e45900f58e5,"We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user’s during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change---the Drinker’s Check-Up (DCU)---whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users’ evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers.","[{'authorId': '1779199', 'name': 'C. Lisetti'}, {'authorId': '1809087', 'name': 'R. Amini'}, {'authorId': '2671668', 'name': 'Ugan Yasavur'}, {'authorId': '1719172', 'name': 'N. Rishe'}]",192.0,"{'bibtex': '@Article{Lisetti2013ICH,\n author = {C. Lisetti and R. Amini and Ugan Yasavur and N. Rishe},\n journal = {ACM Trans. Manag. Inf. Syst.},\n pages = {19:1-19:28},\n title = {I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions},\n volume = {4},\n year = {2013}\n}\n'}",,"{'volume': '4', 'pages': '19:1-19:28', 'name': 'ACM Trans. Manag. Inf. Syst.'}",111.0,I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions,2013.0
2421,d21792b581d5dd504c6c6344cbba417155f7aa55,"For more than a century, empathy has been a central topic in the study of human emotion. It plays a crucial role in our everyday social life, having implications for the survival of the species. In the case of agents that inhabit virtual worlds and interact socially among each other and with humans, empathy has also been considered to be an important mechanism to promote engaging and believable interactions. However, creating empathic agents, until recently, has been accomplished mostly through the implementation of specific empathic behaviors or by using domain-dependent empirical models. In this article, we propose a generic computational model of empathy that is grounded on recent psychological theories about empathy. The proposed model treats empathy as a process in which the intensity of the empathic response is modulated by a set of factors that involve the relationship between the agents of the empathic interaction, namely, the similarity and affective link, as well as some characteristics of the empathizer agent, such as mood and personality. This model was implemented into an affective agent architecture, which was then used in an evaluation that had 77 participants. The results indicate that our empathy model, when used to simulate a social scenario with a small group of agents, significantly changed the way that the users perceived and described the interactions between those agents.","[{'authorId': '2997654', 'name': 'Sérgio Hortas Rodrigues'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",32.0,"{'bibtex': '@Article{Rodrigues2015APM,\n author = {Sérgio Hortas Rodrigues and S. Mascarenhas and João Dias and Ana Paiva},\n journal = {Interact. Comput.},\n pages = {371-391},\n title = {A Process Model of Empathy For Virtual Agents},\n volume = {27},\n year = {2015}\n}\n'}",,"{'volume': '27', 'pages': '371-391', 'name': 'Interact. Comput.'}",54.0,A Process Model of Empathy For Virtual Agents,2015.0
2424,d2589e79f5e390eb2b72b0490099940bdd876722,"Acknowledgments List of Illustrations Figures Plates Preface to the Anniversary Edition by Paul Ekman Preface to the Third Edition by Paul Ekman Preface to the Second Edition by Francis Darwin Introduction to the Third Edition by Paul Ekman The Expression of the Emotions in Man and Animals Introduction to the First Edition 1. General Principles of Expression 2. General Principles of Expression -- continued 3. General Principles of Expression -- continued 4. Means of Expression in Animals 5. Special Expressions of Animals 6. Special Expressions of Man: Suffering and Weeping 7. Low Spirits, Anxiety, Grief, Dejection, Despair 8. Joy, High Spirits, Love, Tender Feelings, Devotion 9. Reflection - Meditation - Ill-temper - Sulkiness - Determination 10. Hatred and Anger 11. Disdain - Contempt - Disgust - Guilt - Pride, Etc. - Helplessness - Patience - Affirmation and Negation 12. Surprise - Astonishment - Fear - Horror 13. Self-attention - Shame - Shyness - Modesty: Blushing 14. Concluding Remarks and Summary Afterword, by Paul Ekman APPENDIX I: Charles Darwin's Obituary, by T. H. Huxley APPENDIX II: Changes to the Text, by Paul Ekman APPENDIX III: Photography and The Expression of the Emotions, by Phillip Prodger APPENDIX IV: A Note on the Orientation of the Plates, by Phillip Prodger and Paul Ekman APPENDIX V: Concordance of Illustrations, by Phillip Prodger APPENDIX VI: List of Head Words from the Index to the First Edition NOTES NOTES TO THE COMMENTARIES INDEX","[{'authorId': '115299156', 'name': 'C. Darwin'}]",8728.0,"{'bibtex': '@Inproceedings{Darwin1956TheEO,\n author = {C. Darwin},\n title = {The Expression of the Emotions in Man and Animals},\n year = {1956}\n}\n'}",,"{'volume': '', 'name': ''}",8.0,The Expression of the Emotions in Man and Animals,1956.0
2426,d25fee01197f1210c94658b39652fa55f2321e65,"Our ability to have an experience of another's pain is characteristic of empathy. Using functional imaging, we assessed brain activity while volunteers experienced a painful stimulus and compared it to that elicited when they observed a signal indicating that their loved one—present in the same room—was receiving a similar pain stimulus. Bilateral anterior insula (AI), rostral anterior cingulate cortex (ACC), brainstem, and cerebellum were activated when subjects received pain and also by a signal that a loved one experienced pain. AIand ACC activation correlated with individual empathy scores. Activity in the posterior insula/secondary somatosensory cortex, the sensorimotor cortex (SI/MI), and the caudal ACC was specific to receiving pain. Thus, a neural response in AIand rostral ACC, activated in common for “self” and “other” conditions, suggests that the neural substrate for empathic experience does not involve the entire “pain matrix.” We conclude that only that part of the pain network associated with its affective qualities, but not its sensory qualities, mediates empathy.","[{'authorId': '47272511', 'name': 'T. Singer'}, {'authorId': '145324953', 'name': 'B. Seymour'}, {'authorId': '101096038', 'name': 'J. O’Doherty'}, {'authorId': '3084123', 'name': 'H. Kaube'}, {'authorId': '2231343', 'name': 'R. Dolan'}, {'authorId': '144155759', 'name': 'C. Frith'}]",3576.0,"{'bibtex': '@Article{Singer2004EmpathyFP,\n author = {T. Singer and B. Seymour and J. O’Doherty and H. Kaube and R. Dolan and C. Frith},\n journal = {Science},\n pages = {1157 - 1162},\n title = {Empathy for Pain Involves the Affective but not Sensory Components of Pain},\n volume = {303},\n year = {2004}\n}\n'}",,"{'volume': '303', 'pages': '1157 - 1162', 'name': 'Science'}",68.0,Empathy for Pain Involves the Affective but not Sensory Components of Pain,2004.0
2427,d26770838402eb548ca6f4b2ae0b4bdff7e6f566,"We solve the mean field equations for a stochastic Hopfield network with temperature (noise) in the presence of strong, i.e., multiply stored, patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the first time a rigorous solution of the mean filed equations for the standard Hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been used hitherto for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when the sum of the squares of degrees of the patterns is negligible compared to the network size. In the case of a single strong pattern, when the ratio of the number of all stored pattens and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy.","[{'authorId': '1694989', 'name': 'A. Edalat'}]",11.0,"{'bibtex': '@Inproceedings{Edalat2013CapacityOS,\n author = {A. Edalat},\n pages = {2661-2669},\n title = {Capacity of strong attractor patterns to model behavioural and cognitive prototypes},\n year = {2013}\n}\n'}",,{'pages': '2661-2669'},35.0,Capacity of strong attractor patterns to model behavioural and cognitive prototypes,2013.0
2428,d290f540ae13ad94ef0a2fc61898c21cfdefb349,"When deployed on mobile devices, virtual agents have the potential to deliver advice regarding medical conditions, as well as provide a ubiquitous channel for health education and behavior change for a variety of chronic health conditions. We describe design guidelines for mobile agent dialogues to support chronic disease management, a general-purpose smartphone-based architecture for a conversational virtual agent that simulates face-to-face health counseling conversations with patients, and an initial agent implementation that provides counseling to patients with the chronic heart condition atrial fibrillation in conjunction with a mobile heart rhythm monitor that is attached to the back of the phone. Preliminary results from a randomized trial with 120 patients with atrial fibrillation indicate that the agent results in significant improvements in self-reported quality of life relative to a standard of care control group.","[{'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '3489930', 'name': 'Everlyne Kimani'}, {'authorId': '144902406', 'name': 'H. Trinh'}, {'authorId': '150027283', 'name': 'Alexandra M Pusateri'}, {'authorId': '1397166245', 'name': 'M. Paasche-Orlow'}, {'authorId': '3490149', 'name': 'J. Magnani'}]",35.0,"{'bibtex': '@Article{Bickmore2018ManagingCC,\n author = {T. Bickmore and Everlyne Kimani and H. Trinh and Alexandra M Pusateri and M. Paasche-Orlow and J. Magnani},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {Managing Chronic Conditions with a Smartphone-based Conversational Virtual Agent},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},21.0,Managing Chronic Conditions with a Smartphone-based Conversational Virtual Agent,2018.0
2429,d29cccbde0fed6031c48dd6e4294b3a1b46672ea,,[],82.0,"{'bibtex': '@Misc{None,\n title = {submitted): Modeling Embodied Feedback With Virtual Humans}\n}\n'}",,,0.0,submitted): Modeling Embodied Feedback With Virtual Humans,
2430,d2c267141a394777485ab96d202c939aca81df00,"TeachAR is an Augmented Reality (AR) tool for teaching English colors, shapes, and spatial relationships to young children aged 4 to 6 years old who are non-native speakers of English. TeachAR utilizes the ARToolkit plugin for the Unity game engine for square marker tracking and game development. The Microsoft Kinect's microphone and speech API is used for isolated word speech recognition, a webcam for image capturing and a desktop monitor for viewing the AR scene. Previous language learning AR applications usually use audio output, however TeachAR uses speech as input for language learning. This paper describes the TeachAR demonstration and user experience with the application.","[{'authorId': '8842641', 'name': 'C. Dalim'}, {'authorId': '152168867', 'name': 'Arindam Dey'}, {'authorId': '2297177', 'name': 'Thammathip Piumsomboon'}, {'authorId': '1684805', 'name': 'M. Billinghurst'}, {'authorId': '2036992', 'name': 'M. S. Sunar'}]",53.0,"{'bibtex': '@Article{Dalim2016TeachARAI,\n author = {C. Dalim and Arindam Dey and Thammathip Piumsomboon and M. Billinghurst and M. S. Sunar},\n journal = {2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},\n pages = {344-345},\n title = {TeachAR: An Interactive Augmented Reality Tool for Teaching Basic English to Non-native Children},\n year = {2016}\n}\n'}",,"{'pages': '344-345', 'name': '2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)'}",18.0,TeachAR: An Interactive Augmented Reality Tool for Teaching Basic English to Non-native Children,2016.0
2431,d2d886eff14325caa9adb368f2bbd8e8ed99c4a5,"unmasking the face a guide to recognizing emotions from unmasking the face a guide to recognizing emotions from >>download unmasking the face: a guide to recognizing unmasking the face: a guide to recognizing emotions from unmasking the facea guide to recognizing emotions from free download unmasking the face: a guide to recognizing unmasking the face a guide to recognizing emotions from books on nonverbal communication hrdi home paul ekman, wallace v. friesen facial expression paul ekman a new pan-cultural facial expression of emotion 1 book reviews springer unmasking the face a to recognizing emotions from facial unmasking the face a guide to recognizing emotions from unmasking the face ebookdigz media bias through facial expressions on local las vegas guide to facial micro expressions beaconac kinesics : encyclopedia of communication theory the role of the eyes and mouth in facial emotions guide to facial micro expressions ojaa lnai 6895 expressing emotions on robotic companions with environmental science chapter 8 test fbtest free download here pdfsdocuments2 expressing emotions on robotic companions with limited war at the end of world totte free kindle unmasking the face ebooks download firebase the paralanguage of behavior intelligence forecasting 6e emotional approach of foreign vowel acquisition tools and methods for pollution prevention louduk historias de ko y mana aadver how the human genome work hsandc taxonomy 1 systematics and morphology xciii darwin and facial expression: a century of research in review","[{'authorId': '21451088', 'name': 'P. Ekman'}, {'authorId': '37652085', 'name': 'Wallace V. Friesen'}]",520.0,"{'bibtex': '@Inproceedings{Ekman1975UnmaskingTF,\n author = {P. Ekman and Wallace V. Friesen},\n title = {Unmasking the Face: A Guide to Recognizing Emotions From Facial Expressions},\n year = {1975}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Unmasking the Face: A Guide to Recognizing Emotions From Facial Expressions,1975.0
2433,d32c12a11a3b5f36b85a25e3c7d1fc453097fdf4,"The present study examined whether sex differences in emotion are related to the social context and addressed differences between global, retrospective, and on-line, momentary self-descriptions of ...","[{'authorId': '1731779', 'name': 'L. F. Barrett'}, {'authorId': '153444389', 'name': 'L. Robin'}, {'authorId': '4148925', 'name': 'P. Pietromonaco'}, {'authorId': '14217652', 'name': 'Kristen M. Eyssell'}]",376.0,"{'bibtex': '@Article{Barrett1998AreWT,\n author = {L. F. Barrett and L. Robin and P. Pietromonaco and Kristen M. Eyssell},\n journal = {Cognition & Emotion},\n pages = {555-578},\n title = {Are Women the More Emotional Sex? Evidence From Emotional Experiences in Social Context},\n volume = {12},\n year = {1998}\n}\n'}",,"{'volume': '12', 'pages': '555-578', 'name': 'Cognition & Emotion'}",52.0,Are Women the More Emotional Sex? Evidence From Emotional Experiences in Social Context,1998.0
2434,d3336e951eaef94df864f6c514a7757a5a76ef41,"In a prospective cross-sectional study, we used computerized volumetry of magnetic resonance images to examine the patterns of brain aging in 148 healthy volunteers. The most substantial age-related decline was found in the volume of the prefrontal gray matter. Smaller age-related differences were observed in the volume of the fusiform, inferior temporal and superior parietal cortices. The effects of age on the hippocampal formation, the postcentral gyrus, prefrontal white matter and superior parietal white matter were even weaker. No significant age-related differences were observed in the parahippocampal and anterior cingulate gyri, inferior parietal lobule, pericalcarine gray matter, the precentral gray and white matter, postcentral white matter and inferior parietal white matter. The volume of the total brain volume and the hippocampal formation was larger in men than in women even after adjustment for height. Inferior temporal cortex showed steeper aging trend in men. Small but consistent rightward asymmetry was found in the whole cerebral hemispheres, superior parietal, fusiform and orbito-frontal cortices, postcentral and prefrontal white matter. The left side was larger than the right in the dorsolateral prefrontal, parahippocampal, inferior parietal and pericalcarine cortices, and in the parietal white matter. However, there were no significant differences in age trends between the hemispheres.","[{'authorId': '145349986', 'name': 'N. Raz'}, {'authorId': '3568698', 'name': 'F. Gunning'}, {'authorId': '2228866873', 'name': 'Denise Head'}, {'authorId': '2228182200', 'name': 'James H. Dupuis'}, {'authorId': '4019199', 'name': 'John McQuain'}, {'authorId': '4803270', 'name': 'S. Briggs'}, {'authorId': '144995692', 'name': 'W. Loken'}, {'authorId': '2228364354', 'name': 'Allen E. Thornton'}, {'authorId': '5080944', 'name': 'J. Acker'}]",1259.0,"{'bibtex': '@Article{Raz1997SelectiveAO,\n author = {N. Raz and F. Gunning and Denise Head and James H. Dupuis and John McQuain and S. Briggs and W. Loken and Allen E. Thornton and J. Acker},\n journal = {Cerebral cortex},\n pages = {\n          268-82\n        },\n title = {Selective aging of the human cerebral cortex observed in vivo: differential vulnerability of the prefrontal gray matter.},\n volume = {7 3},\n year = {1997}\n}\n'}",,"{'volume': '7 3', 'pages': '\n          268-82\n        ', 'name': 'Cerebral cortex'}",120.0,Selective aging of the human cerebral cortex observed in vivo: differential vulnerability of the prefrontal gray matter.,1997.0
2435,d34d354f7b5167a772acd68076854f3a6f4f3d95,,"[{'authorId': '1698459', 'name': 'H. Stuckenschmidt'}, {'authorId': '1725286', 'name': 'F. V. Harmelen'}]",281.0,"{'bibtex': '@Inproceedings{Stuckenschmidt2005OntologyLF,\n author = {H. Stuckenschmidt and F. V. Harmelen},\n pages = {45-61},\n title = {Ontology languages for the Semantic Web},\n year = {2005}\n}\n'}",,"{'volume': '', 'pages': '45-61', 'name': ''}",11.0,Ontology languages for the Semantic Web,2005.0
2436,d350745cd04e2f1a4d60dd045779de2d1775c5b5,"
 
 Agency - the capacity to plan and act - and experience - the capacity to sense and feel - are two critical aspects that determine whether people will perceive non-human entities, such as autonomous agents, to have a mind. There is evidence that the absence of either can reduce cooperation. We present an experiment that tests the necessity of both for cooperation with agents. In this experiment we manipulated people's perceptions about the cognitive and affective abilities of agents, when engaging in the ultimatum game. The results indicated that people offered more money to agents that were perceived to make decisions according to their intentions (high agency), rather than randomly (low agency). Additionally, the results showed that people offered more money to agents that expressed emotion (high experience), when compared to agents that did not (low experience). We discuss the implications of this agency-experience theoretical framework for the design of artificially intelligent decision makers.
 
","[{'authorId': '1977901', 'name': 'C. D. Melo'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '48755211', 'name': 'P. Carnevale'}]",20.0,"{'bibtex': '@Inproceedings{Melo2014TheIO,\n author = {C. D. Melo and J. Gratch and P. Carnevale},\n pages = {336-342},\n title = {The Importance of Cognition and Affect for Artificially Intelligent Decision Makers},\n year = {2014}\n}\n'}",,{'pages': '336-342'},35.0,The Importance of Cognition and Affect for Artificially Intelligent Decision Makers,2014.0
2437,d360dd1cae4d5ce895e01bd02913c3231990b5d1,"Besides reduction of energy consumption, which implies alternate actuation and light construction, the main research domain in automobile development in the near future is dominated by driver assistance and natural driver-car communication. The ability of a car to understand natural speech and provide a human-like driver assistance system can be expected to be a factor decisive for market success on par with automatic driving systems. Emotional factors and affective states are thereby crucial for enhanced safety and comfort. This paper gives an extensive literature overview on work related to influence of emotions on driving safety and comfort, automatic recognition, control of emotions, and improvement of in-car interfaces by affect sensitive technology. Various use-case scenarios are outlined as possible applications for emotion-oriented technology in the vehicle. The possible acceptance of such future technology by drivers is assessed in a Wizard-Of-Oz user study, and feasibility of automatically recognising various driver states is demonstrated by an example system for monitoring driver attentiveness. Thereby an accuracy of 91.3% is reported for classifying in real-time whether the driver is attentive or distracted.","[{'authorId': '1751126', 'name': 'F. Eyben'}, {'authorId': '2103575', 'name': 'M. Wöllmer'}, {'authorId': '2272779', 'name': 'T. Poitschke'}, {'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '152934979', 'name': 'C. Blaschke'}, {'authorId': '2723594', 'name': 'B. Färber'}, {'authorId': '1403875780', 'name': 'Nhu Nguyen-Thien'}]",168.0,"{'bibtex': '@Article{Eyben2010EmotionOT,\n author = {F. Eyben and M. Wöllmer and T. Poitschke and Björn Schuller and C. Blaschke and B. Färber and Nhu Nguyen-Thien},\n journal = {Adv. Hum. Comput. Interact.},\n pages = {263593:1-263593:17},\n title = {Emotion on the Road - Necessity, Acceptance, and Feasibility of Affective Computing in the Car},\n volume = {2010},\n year = {2010}\n}\n'}",,"{'volume': '2010', 'pages': '263593:1-263593:17', 'name': 'Adv. Hum. Comput. Interact.'}",96.0,"Emotion on the Road - Necessity, Acceptance, and Feasibility of Affective Computing in the Car",2010.0
2438,d36efb9ad91e00faa334b549ce989bfae7e2907a,"Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed.","[{'authorId': '35043531', 'name': 'A. Dempster'}, {'authorId': '7890796', 'name': 'N. Laird'}, {'authorId': '2235217', 'name': 'D. Rubin'}]",50943.0,"{'bibtex': '@Inproceedings{Dempster1977MaximumLF,\n author = {A. Dempster and N. Laird and D. Rubin},\n title = {Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper},\n year = {1977}\n}\n'}",,"{'volume': '', 'name': ''}",89.0,Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper,1977.0
2439,d39683d3d326d5a78bab73cc27d1767083af6d7c,"Past studies on emotion recognition and aging have found evidence of age-related decline when emotion recognition was assessed by having participants detect single emotions depicted in static images of full or partial (e.g., eye region) faces. These tests afford good experimental control but do not capture the dynamic nature of real-world emotion recognition, which is often characterized by continuous emotional judgments and dynamic multimodal stimuli. Research suggests that older adults often perform better under conditions that better mimic real-world social contexts. We assessed emotion recognition in young, middle-aged, and older adults using two traditional methods (single emotion judgments of static images of faces and eyes) and an additional method in which participants made continuous emotion judgments of dynamic, multimodal stimuli (videotaped interactions between young, middle-aged, and older couples). Results revealed an Age × Test interaction. Largely consistent with prior research, we found some evidence that older adults performed worse than young adults when judging single emotions from images of faces (for sad and disgust faces only) and eyes (for older eyes only), with middle-aged adults falling in between. In contrast, older adults did better than young adults on the test involving continuous emotion judgments of dyadic interactions, with middle-aged adults falling in between. In tests in which target stimuli differed in age, emotion recognition was not facilitated by an age match between participant and target. These findings are discussed in terms of theoretical and methodological implications for the study of aging and emotional processing.","[{'authorId': '4773810', 'name': 'Jocelyn A Sze'}, {'authorId': '25574636', 'name': 'Madeleine S. Goodkind'}, {'authorId': '6307176', 'name': 'Anett Gyurak'}, {'authorId': '2001910', 'name': 'R. Levenson'}]",98.0,"{'bibtex': '@Article{Sze2012AgingAE,\n author = {Jocelyn A Sze and Madeleine S. Goodkind and Anett Gyurak and R. Levenson},\n journal = {Psychology and aging},\n pages = {\n          940-950\n        },\n title = {Aging and emotion recognition: not just a losing matter.},\n volume = {27 4},\n year = {2012}\n}\n'}",,"{'volume': '27 4', 'pages': '\n          940-950\n        ', 'name': 'Psychology and aging'}",98.0,Aging and emotion recognition: not just a losing matter.,2012.0
2440,d39ab1a5f531068516fe093c3e524bc3ce63eed5,"Face emotion expression of virtual human is an interesting research area for many scholars. The former research laid much stress on the acquirement of human face data and creation of the local movement on face, there is no general mathematics model for emotion synthesis. First, the concept of emotion vector was proposed and the emotion state of virtual human could be transformed to an emotion vector calculated by the basic emotion vector in emotion vector space. Second, the concept of expression vector was established and the expression of virtual human could be transformed to expression vector calculated by the basic expression vector in expression vector space. Third, the emotion mapping was set up from emotion vector space to expression vector space and emotion state and the emotion mapping implemented expression animation. Finally, a preliminary experiment was carried on microcomputer and the emotion synthesis was calculated by six basic emotions.","[{'authorId': '2059085302', 'name': 'Liu Zhen'}]",4.0,"{'bibtex': '@Article{Zhen2006SynthesisOE,\n author = {Liu Zhen},\n journal = {Computer Simulation},\n title = {Synthesis of Emotion Vector and Expression Vector for Virtual Human},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': 'Computer Simulation'}",0.0,Synthesis of Emotion Vector and Expression Vector for Virtual Human,2006.0
2441,d46cc7bd8a8c1bb4b0295bf2de76dd85c9a20be6,"The general goal of our research is to develop a personal computer assistant that persuades children to adhere to a healthy lifestyle during daily activities at home. The assistant will be used in three different roles: as companion, educator and motivator. This study investigates whether the effectiveness of the computer assistant with an iCat robot embodiment, can be improved when it expresses emotions (tested for each of the three roles). It shows that emotion expressions can improve the effectiveness of the robot to achieve its role objectives. The improvements that we found are small, however, probably due to a ceiling effect: All subjective measures are rated very positively in the neutral condition, thus leaving little room for improvement. It also showed that the emotional speech was less intelligible, which may limit the robots' effectiveness.","[{'authorId': '1716216', 'name': 'J. Kessens'}, {'authorId': '1784286', 'name': 'Mark Antonius Neerincx'}, {'authorId': '1786270', 'name': 'R. Looije'}, {'authorId': '144269375', 'name': 'M. Kroes'}, {'authorId': '2622415', 'name': 'G. Bloothooft'}]",28.0,"{'bibtex': '@Article{Kessens2009FacialAV,\n author = {J. Kessens and Mark Antonius Neerincx and R. Looije and M. Kroes and G. Bloothooft},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-7},\n title = {Facial and vocal emotion expression of a personal computer assistant to engage, educate and motivate children},\n year = {2009}\n}\n'}",,"{'pages': '1-7', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",21.0,"Facial and vocal emotion expression of a personal computer assistant to engage, educate and motivate children",2009.0
2442,d46e9ea18c80b33a3edf473a035784636b2ed0b8,"In this paper, we describe our findings from research designed to explore the effect of virtual human counselors' self-disclosure using intimate human back stories on real human clients' social responses in psychological counseling sessions. To investigate this subject, we designed an experiment involving two conditions of the counselors' self-disclosure: human back stories and computer back stories. We then measured socially anxious users' verbal self-disclosure. The results demonstrated that highly anxious users revealed personal information more than less anxious users when they interacted with virtual counselors who disclosed intimate information about themselves using human back stories. Furthermore, we found that greater inclination toward facilitated self-disclosure from highly anxious users following interaction with virtual counselors who employed human back stories rather than computer back stories. In addition, a further analysis of socially anxious users' feelings of rapport demonstrated that virtual counselors elicited more rapport with highly anxious users than less anxious users when interacting with counselors who employed human back stories. This outcome was not found in the users' interactions with counselors who employed computer back stories.","[{'authorId': '34728215', 'name': 'Sin-Hwa Kang'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",35.0,"{'bibtex': '@Article{Kang2012SociallyAP,\n author = {Sin-Hwa Kang and J. Gratch},\n journal = {Studies in health technology and informatics},\n pages = {\n          202-6\n        },\n title = {Socially Anxious People Reveal More Personal Information with Virtual Counselors That Talk about Themselves using Intimate Human Back Stories},\n volume = {181},\n year = {2012}\n}\n'}",,"{'volume': '181', 'pages': '\n          202-6\n        ', 'name': 'Studies in health technology and informatics'}",13.0,Socially Anxious People Reveal More Personal Information with Virtual Counselors That Talk about Themselves using Intimate Human Back Stories,2012.0
2443,d49d9ef4be031b4c654a912717200bd8b34a426a,,"[{'authorId': '7832413', 'name': 'E. Sundstrom'}, {'authorId': '47697332', 'name': 'I. Altman'}]",128.0,"{'bibtex': '@Article{Sundstrom1976InterpersonalRA,\n author = {E. Sundstrom and I. Altman},\n journal = {Human Ecology},\n pages = {47-67},\n title = {Interpersonal relationships and personal space: Research review and theoretical model},\n volume = {4},\n year = {1976}\n}\n'}",,"{'volume': '4', 'pages': '47-67', 'name': 'Human Ecology'}",84.0,Interpersonal relationships and personal space: Research review and theoretical model,1976.0
2444,d4ad611e3a39f55c420dd843f28a23f017a639ed,"A study deployed the mental health Relational Frame Theory as grounding for an analysis of sentiment dynamics in human-language dialogs. The work takes a step towards enabling use of conversational agents in mental health settings. Sentiment tendencies and mirroring behaviors in 11k human-human dialogs were compared with behaviors when humans interacted with conversational agents in a similar-sized collection. The study finds that human sentiment-related interaction norms persist in human-agent dialogs, but that humans are twice as likely to respond negatively when faced with a negative utterance by a robot than in a comparable situation with humans. Similarly, inhibition towards use of obscenity is greatly reduced. We introduce a new Affective Neural Net implementation that specializes in analyzing sentiment in real time.","[{'authorId': '3819917', 'name': 'Adam S. Miner'}, {'authorId': '5014723', 'name': 'A. Chow'}, {'authorId': '46949721', 'name': 'Sarah Adler'}, {'authorId': '2065204644', 'name': 'Ilia Zaitsev'}, {'authorId': '113547125', 'name': 'P. Tero'}, {'authorId': '6120000', 'name': 'Alison M Darcy'}, {'authorId': '1750481', 'name': 'A. Paepcke'}]",53.0,"{'bibtex': '@Article{Miner2016ConversationalAA,\n author = {Adam S. Miner and A. Chow and Sarah Adler and Ilia Zaitsev and P. Tero and Alison M Darcy and A. Paepcke},\n journal = {Proceedings of the Fourth International Conference on Human Agent Interaction},\n title = {Conversational Agents and Mental Health: Theory-Informed Assessment of Language and Affect},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the Fourth International Conference on Human Agent Interaction'},36.0,Conversational Agents and Mental Health: Theory-Informed Assessment of Language and Affect,2016.0
2445,d4b99c2af5158ecd42d483ea4b3775a2c8e0767a,"It has long been recognized that facial expressions, body posture/gestures and vocal parameters play an important role in human communication and the implicit signalling of emotion. Recent advances in low cost computer vision and behavioral sensing technologies can now be applied to the process of making meaningful inferences as to user state when a person interacts with a computational device. Effective use of this additive information could serve to promote human interaction with virtual human (VH) agents that may enhance diagnostic assessment. The same technology could also be leveraged to improve engagement in teletherapy approaches between remote patients and care providers. This paper will focus on our current research in these areas within the DARPA-funded “Detection and Computational Analysis of Psychological Signals” project, with specific attention to the SimSensei application use case. SimSensei is a virtual human interaction platform that is able to sense and interpret real-time audiovisual behavioral signals from users interacting with the system. It is specifically designed for health care support and leverages years of virtual human research and development at USC-ICT. The platform enables an engaging face-to-face interaction where the virtual human automatically reacts to the state and inferred intent of the user through analysis of behavioral signals gleaned from facial expressions, body gestures and vocal parameters. Akin to how non-verbal behavioral signals have an impact on human to human interaction and communication, SimSensei aims to capture and infer from user non-verbal communication to improve engagement between a VH and a user. The system can also quantify and interpret sensed behavioral signals longitudinally that can be used to inform diagnostic assessment within a clinical context.","[{'authorId': '32775508', 'name': 'A. Rizzo'}, {'authorId': '2072545325', 'name': 'Stefan Scherer'}, {'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2038490', 'name': 'Ron Artstein'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '2551269', 'name': 'Angela Nazarian'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '2072346682', 'name': 'Rachel Wood'}, {'authorId': '6349590', 'name': 'Jill Boberg'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",29.0,"{'bibtex': '@Inproceedings{Rizzo2014DetectionAC,\n author = {A. Rizzo and Stefan Scherer and D. DeVault and J. Gratch and Ron Artstein and Arno Hartholt and Gale M. Lucas and S. Marsella and Fabrizio Morbini and Angela Nazarian and Giota Stratou and D. Traum and Rachel Wood and Jill Boberg and Louis-Philippe Morency},\n title = {Detection and Computational Analysis of Psychological Signals Using a Virtual Human Interviewing Agent},\n year = {2014}\n}\n'}","[{'paperId': 'b302df0e32bc45b3674d0f0337424c58d036eba0', 'title': 'The Construction of Critical Factors for Successfully Introducing Chatbots into Mental Health Services in the Army: Using a Hybrid MCDM Approach'}, {'paperId': 'ee534a991f37b24ac41c4258e1cf99c17755774c', 'title': 'The use of machine learning to support the therapeutic process – strengths and weaknesses'}, {'paperId': '0652ce2b638f8491ac460abe35d866dc6c399b11', 'title': 'Speech Behavioral Markers Align on Symptom Factors in Psychological Distress'}, {'paperId': 'e178bc2ed564f8a508868fbd1468b8d4b244cef4', 'title': 'Technology and Mental Health: State of the Art for Assessment and Treatment.'}, {'paperId': '76b9795adaaa4a5b6a3d4966e9546753a8b1c766', 'title': 'Computational Scientific Discovery in Psychology'}, {'paperId': 'b4c9c36d3150e8271383c54fac861303d555c211', 'title': 'High School Educator Training by Simulation to Address Emotional and Behavioral Concerns in School Settings: A Randomized Study'}, {'paperId': 'a2ae80799ea5dafad50d2e990911da8bc273621c', 'title': 'From Combat to COVID-19 – Managing the Impact of Trauma Using Virtual Reality'}, {'paperId': '0432724163328d57c1d08d1291278d5e45af4112', 'title': 'Integrating women into ground close combat roles: an opportunity to reflect on universal paradigms of arduous training'}, {'paperId': 'e176fa38286bb89a707d7c7d49ccf38d2364bb8e', 'title': 'Computer-Based PTSD Assessment in VR Exposure Therapy'}, {'paperId': '4e192e6b0f6b1e3be0c43bff94dd060781636797', 'title': 'Behaving socially with a virtual human role-player in a simulated counseling session'}, {'paperId': 'fd4e518b4cd5ff3a63bdba261f108ccdd1a6ea59', 'title': 'Ubiquitous Virtual Humans: A Multi-platform Framework for Embodied AI Agents in XR'}, {'paperId': '6954aaceaed9ead3c892cbcb4c1d9a0ec41cc177', 'title': 'Ethics of Immersive Technologies'}, {'paperId': 'fa93d326a8b53e14063d6251021b8a519f21268a', 'title': 'Bridging the gap between technological advance and professional psychology training: A way forward.'}, {'paperId': 'a2618504de324e8e29f6fdd1af12df00aa31c95a', 'title': 'Key Considerations for Incorporating Conversational AI in Psychotherapy'}, {'paperId': 'db32e56bfded11d59c7664d799fca6f74e8b01b5', 'title': 'Blowing in the wind: Increasing social presence with a virtual human via environmental airflow interaction in mixed reality'}, {'paperId': '7645b5e136db46a47172ea9372c2d52830df981d', 'title': 'Virtual Humans in Augmented Reality: A First Step towards Real-World Embedded Virtual Roleplayers'}, {'paperId': '8238f01bce09a45aa64b86414e4e66dda1bbaf87', 'title': 'Virtual Job Interviewing Practice for High-Anxiety Populations'}, {'paperId': 'c4422cf5d3c60b224a7f69118cac12510dc0e449', 'title': 'Ökologisch valides Motion Capture von Dirigierbewegungen mit dem kinelyze-System'}, {'paperId': 'b82e468d7de60bb6a19181004ee0168a03bb3fe2', 'title': 'Teaching communication skills with technology: Creating a virtual patient for medical students'}, {'paperId': '749675a8f3a3b7a84a11176b239893490887a334', 'title': 'PaolaChat: A Virtual Agent with Naturalistic Breathing'}, {'paperId': 'e1233264e3ff2fa6415a307ffb8b22cbea7ddbd5', 'title': 'Improving Social Presence with a Virtual Human via Multimodal Physical -- Virtual Interactivity in AR'}, {'paperId': '9e5b55d37ea21c26d5027e29cc4602a8382d3384', 'title': 'Reporting Mental Health Symptoms: Breaking Down Barriers to Care with Virtual Human Interviewers'}, {'paperId': '5717b90974f217071be0fe5d8c7af8a71a23550e', 'title': 'HCI International 2020 – Late Breaking Papers: Virtual and Augmented Reality: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings'}, {'paperId': '9d5062f09ad602b3c397aaa46a9b6241efb9c240', 'title': 'Face Smile Detection and Cavernous Biometric Prediction using Perceptual User Interfaces (PUIs)'}, {'paperId': '94fbd88c6874e07dea9cc49107ce10e2bc18701c', 'title': '""Hi, can I help?"" An exploratory study of designing a chatbot to complement school nurses in supporting youths\' mental health'}, {'paperId': 'fee6a04e51d6b9879c4d36a72c512aa45ecb947c', 'title': 'Blowing in the Wind: Increasing Copresence with a Virtual Human via Airflow Influence in Augmented Reality'}, {'paperId': 'b11890a3051c22a54d8e2ce80dbcf713266c2868', 'title': 'Virtual, Augmented and Mixed Reality: Interaction, Navigation, Visualization, Embodiment, and Simulation'}, {'paperId': '33735898c03b75598562f12b24747e76a89828bb', 'title': 'Autonomous Virtual Human Agents for Healthcare Information Support and Clinical Interviewing'}, {'paperId': '1a7e418e35c65be24f5462b3cfb4e8b733710b7b', 'title': 'Clinical interviewing by a virtual human agent with automatic behavior analysis'}]","{'name': '', 'volume': ''}",46.0,Detection and Computational Analysis of Psychological Signals Using a Virtual Human Interviewing Agent,2014.0
2446,d4f5ef88dd2e8f028d47ea89fe68e09c4d40bd6c,"Sentiment analysis is a type of opinion mining study analyzing people's thoughts, feelings, and assessments evaluations of society things such as products, services, and institutions individuals, organizations, events and etc. This paper focuses on an effective machine learning model, Recurrent Neural Network (RNN), for sentiment analysis of both Bengali and English text. For sentiment analysis of Bengali text, we construct a novel dataset accumulated from different open source sites and social media. The dataset comprises over 17,000 texts, which are labelled as neutral, positive, and negative. Our study provides a rigorous comparative analysis on the performance of RNN in terms of sentiment analysis of both raw text and corresponding translated text (for both English and Bengali language). In summary, we achieve 82.9% and 84.5% accuracies for sentiment analysis of Bengali and English text respectively using RNN.","[{'authorId': '2154835144', 'name': 'S. N. Monjoor'}, {'authorId': '12831464', 'name': 'O. Faruk'}, {'authorId': '2154837478', 'name': 'K. M. Mahmudul Haque'}, {'authorId': '2151951107', 'name': 'F. Iqbal'}, {'authorId': '2050712775', 'name': 'M. A. Mitu'}, {'authorId': '2110034858', 'name': 'M. A. Islam'}, {'authorId': '79335343', 'name': 'M. Mukta'}]",3.0,"{'bibtex': '@Article{Monjoor2021ACS,\n author = {S. N. Monjoor and O. Faruk and K. M. Mahmudul Haque and F. Iqbal and M. A. Mitu and M. A. Islam and M. Mukta},\n journal = {The 2nd International Conference on Distributed Sensing and Intelligent Systems (ICDSIS 2021)},\n pages = {220-231},\n title = {A Comparative study for sentiment analysis of raw and translated text},\n volume = {2021},\n year = {2021}\n}\n'}",,"{'volume': '2021', 'pages': '220-231', 'name': 'The 2nd International Conference on Distributed Sensing and Intelligent Systems (ICDSIS 2021)'}",15.0,A Comparative study for sentiment analysis of raw and translated text,2021.0
2447,d5020b0925ffb5a8503f8b079442eeba408b83b3,"This paper focuses on how to give the intelligent virtual agent (IVA) behaviors with personalized features. It takes full account of the impact of personality and emotion factor on IVA's behavior, and proposes a path planning algorithm influenced by personality parameters - PD* (Personality D*) algorithm, which is improved on the basis of D* algorithm. PD* algorithm set different level of dangerous coefficient which is associated with IVA's personality, and adopts Heuristic search function based on energy to realize path planning with personality characteristic. The algorithm was tested and applied to an IVA in a Smart Home.","[{'authorId': '2108833763', 'name': 'Shi Lin'}, {'authorId': '46644542', 'name': 'Li Zhigang'}]",0.0,"{'bibtex': '@Article{Lin2012APP,\n author = {Shi Lin and Li Zhigang},\n booktitle = {IEEE Symposium on Electrical & Electronics Engineering},\n journal = {2012 IEEE Symposium on Electrical & Electronics Engineering (EEESYM)},\n pages = {581-584},\n title = {A path planning algorithm for intelligent virtual agent},\n year = {2012}\n}\n'}",[],"{'name': '2012 IEEE Symposium on Electrical & Electronics Engineering (EEESYM)', 'pages': '581-584'}",11.0,A path planning algorithm for intelligent virtual agent,2012.0
2448,d50e12d5baf8ed0a3f063ad4169eddaec2e5b751,"
 
 This paper describes entries to the third Playable Experiences track to be held at the AIIDE conference. We discuss the five entries accepted to the track for 2015, as well as the ongoing development of the track as part of AIIDE.
 
","[{'authorId': '15914175', 'name': 'Nathan R Sturtevant'}, {'authorId': '2633360', 'name': 'Jeff Orkin'}, {'authorId': '2088944', 'name': 'Robert Zubek'}, {'authorId': '48878113', 'name': 'Michael Cook'}, {'authorId': '34810994', 'name': 'Stephen G. Ware'}, {'authorId': '2286384', 'name': 'Christian Stith'}, {'authorId': '145513579', 'name': 'R. Young'}, {'authorId': '2056621390', 'name': 'Phillip Wright'}, {'authorId': '1409225781', 'name': 'Squirrel Eiserloh'}, {'authorId': '152999225', 'name': 'A. Ramirez'}, {'authorId': '1884952', 'name': 'V. Bulitko'}, {'authorId': '114998911', 'name': 'Kieran Lord'}]",20.0,"{'bibtex': '@Inproceedings{Sturtevant2014PlayableEA,\n author = {Nathan R Sturtevant and Jeff Orkin and Robert Zubek and Michael Cook and Stephen G. Ware and Christian Stith and R. Young and Phillip Wright and Squirrel Eiserloh and A. Ramirez and V. Bulitko and Kieran Lord},\n pages = {308-},\n title = {Playable Experiences at AIIDE 2014},\n year = {2014}\n}\n'}",,{'pages': '308-'},29.0,Playable Experiences at AIIDE 2014,2014.0
2449,d51f4c193dc322b3cb93e227d168c63ee0637332,"In social interactions, interpersonal distance between interaction partners plays an important role in determining the status of the relationship. Interpersonal distance is an important nonverbal behavior, and is used to regulate personal space in a complex interplay with other nonverbal behaviors such as eye gaze. In social anxiety, studies regarding the impact of interpersonal distance on within-situation avoidance behavior are so far rare. Thus the present study aimed to scrutinize the relationship between gaze direction, sex, interpersonal distance, and social anxiety in social interactions. Social interactions were modeled in a virtual-reality (VR) environment, where 20 low and 19 high socially anxious women were confronted with approaching male and female characters, who stopped in front of the participant, either some distance away or close to them, and displayed either a direct or an averted gaze. Gaze and head movements, as well as heart rate, were measured as indices of avoidance behavior and fear reactions. High socially anxious participants showed a complex pattern of avoidance behavior: when the avatar was standing farther away, high socially anxious women avoided gaze contact with male avatars showing a direct gaze. Furthermore, they showed avoidance behavior (backward head movements) in response to male avatars showing a direct gaze, regardless of the interpersonal distance. Overall, the current study proved that VR social interactions might be a very useful tool for investigating avoidance behavior of socially anxious individuals in highly controlled situations. This might also be the first step in using VR social interactions in clinical protocols for the therapy of social anxiety disorder.","[{'authorId': '34439843', 'name': 'M. Wieser'}, {'authorId': '145825010', 'name': 'P. Pauli'}, {'authorId': '3020847', 'name': 'Miriam Grosseibl'}, {'authorId': '3104492', 'name': 'Ina Molzow'}, {'authorId': '1684604', 'name': 'A. Mühlberger'}]",85.0,"{'bibtex': '@Article{Wieser2010VirtualSI,\n author = {M. Wieser and P. Pauli and Miriam Grosseibl and Ina Molzow and A. Mühlberger},\n journal = {Cyberpsychology, behavior and social networking},\n pages = {\n          547-54\n        },\n title = {Virtual Social Interactions in Social Anxiety - The Impact of Sex, Gaze, and Interpersonal Distance},\n volume = {13 5},\n year = {2010}\n}\n'}",,"{'volume': '13 5', 'pages': '\n          547-54\n        ', 'name': 'Cyberpsychology, behavior and social networking'}",48.0,"Virtual Social Interactions in Social Anxiety - The Impact of Sex, Gaze, and Interpersonal Distance",2010.0
2450,d52a8469b7a720f02ec8ca4bf7539036caad885b,"The BERT2 social robot, a platform for the exploration of human-robot interaction, is currently being built at the Bristol Robotics Laboratory. This paper describes work on the robot's face, a hybrid face composed of a plastic faceplate and an LCD display, and our implementation of facial expressions on this versatile platform. We report the implementation of two representations of affect space, each of which map the space of potential emotions to specific facial feature parameters and the results of a series of human-robot interaction experiments to characterize the recognizability of the robot's archetypal facial expressions. The tested subjects' recognition of the implemented facial expressions for happy, surprised, and sad was robust (with nearly 100% recognition). Subjects, however, tended to confuse the expressions for disgusted and afraid with other expressions, with correct recognition rates of 21.1% and 52.6% respectively. Future work involves the addition of more realistic eye movements for stronger recognition of certain responses. These results demonstrate that a hybrid face with affect space facial expression implementations can provide emotive conveyance readily recognized by human beings.","[{'authorId': '3097045', 'name': 'D. Bazo'}, {'authorId': '144301847', 'name': 'R. Vaidyanathan'}, {'authorId': '34348169', 'name': 'A. Lenz'}, {'authorId': '1730409', 'name': 'C. Melhuish'}]",32.0,"{'bibtex': '@Article{Bazo2010DesignAT,\n author = {D. Bazo and R. Vaidyanathan and A. Lenz and C. Melhuish},\n journal = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n pages = {5317-5322},\n title = {Design and testing of a hybrid expressive face for a humanoid robot},\n year = {2010}\n}\n'}",,"{'pages': '5317-5322', 'name': '2010 IEEE/RSJ International Conference on Intelligent Robots and Systems'}",24.0,Design and testing of a hybrid expressive face for a humanoid robot,2010.0
2451,d53af9bdfc37b8a58ffe067b2024bb6144e72cd9,"Pedagogical agents – lifelike characters that guide users through multimedia learning environments – are intended to facilitate the learning process. According to social agency theory, the presentation of social cues (the image and voice of the agent) may prime the social interaction schema and cause the learner to deeply process the learning material. Research on pedagogical agents focuses mainly on either the role of the presence of pedagogical agents, by comparing agent and no-agent groups, or on the design of the character, by comparing different agent groups. This article takes a comprehensive view combining both approaches in order to answer the question of how pedagogical agents should be designed so as to promote learner motivation and learning. It is argued that not only the mere presence, but also the valence of the social cues presented plays a decisive role. Two experiments examine the role of the perceived appeal of a pedagogical agent’s appearance and voice. The results of Experiment 1 indic...","[{'authorId': '1932752', 'name': 'Steffi Domagk'}]",118.0,"{'bibtex': '@Article{Domagk2010DoPA,\n author = {Steffi Domagk},\n journal = {J. Media Psychol. Theor. Methods Appl.},\n pages = {84-97},\n title = {Do Pedagogical Agents Facilitate Learner Motivation and Learning Outcomes?},\n volume = {22},\n year = {2010}\n}\n'}",,"{'volume': '22', 'pages': '84-97', 'name': 'J. Media Psychol. Theor. Methods Appl.'}",56.0,Do Pedagogical Agents Facilitate Learner Motivation and Learning Outcomes?,2010.0
2452,d588bfb109693795ad4d9e3d57fa3e13f649c903,,"[{'authorId': '2071582721', 'name': 'Murat Akçay'}]",1081.0,"{'bibtex': '@Inproceedings{Akçay2017AdvantagesAC,\n author = {Murat Akçay},\n title = {Advantages and challenges associated with augmented reality for education : A systematic review of the literature},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",78.0,Advantages and challenges associated with augmented reality for education : A systematic review of the literature,2017.0
2453,d5c870988bc490dc87d4a06ee0e6342a5b165187,"We have developed a general purpose use and modular architecture of an Embodied Conversational Agent (ECA) called Greta. Our 3D agent is able to communicate using verbal and nonverbal channels like gaze, head and torso movements, facial expressions and gestures. It follows the SAIBA framework [10] and the MPEG4 [6] standards. Our system is optimized to be used in interactive applications.","[{'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",146.0,"{'bibtex': '@Inproceedings{Niewiadomski2009GretaAI,\n author = {Radoslaw Niewiadomski and Elisabetta Bevacqua and M. Mancini and C. Pelachaud},\n pages = {1399-1400},\n title = {Greta: an interactive expressive ECA system},\n year = {2009}\n}\n'}",,{'pages': '1399-1400'},10.0,Greta: an interactive expressive ECA system,2009.0
2454,d5ca5efdecba2707aab57d4cb2df008a7ecdd0dd,"Recent years have witnessed the birth of a new paradigm for learning environments: animated pedagogical agents. These lifelike autonomous characters cohabit learning environments with students to create rich, face-to-face learning interactions. This opens up exciting new possibilities; for example, agents can demonstrate complex tasks, employ locomotion and gesture to focus students' attention on the most salient aspect of the task at hand, and convey emotional responses to the tutorial situation. Animated pedagogical agents offer great promise for broadening the bandwidth of tutorial communication and increasing learning environments' ability to engage and motivate students. This article sets forth the motivations behind animated pedagogical agents, describes the key capabilities they offer, and discusses the technical issues they raise. The discussion is illustrated with descriptions of a number of animated agents that represent the current state of the art.","[{'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '1717955', 'name': 'James C. Lester'}]",1152.0,"{'bibtex': '@Inproceedings{Johnson2000AnimatedPA,\n author = {W. Johnson and J. Rickel and James C. Lester},\n title = {Animated Pedagogical Agents: Face-to-Face Interaction in Interactive Learning Environments},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",87.0,Animated Pedagogical Agents: Face-to-Face Interaction in Interactive Learning Environments,2000.0
2455,d5f799ebc283e399f1b13516f28588c1a8fadb32,,"[{'authorId': '47851995', 'name': 'A. Metallinou'}, {'authorId': '3161887', 'name': 'Zhaojun Yang'}, {'authorId': '2467369', 'name': 'Chi-Chun Lee'}, {'authorId': '2106794', 'name': 'C. Busso'}, {'authorId': '2090211', 'name': 'S. Carnicke'}, {'authorId': '145254843', 'name': 'Shrikanth S. Narayanan'}]",55.0,"{'bibtex': '@Article{Metallinou2016TheUC,\n author = {A. Metallinou and Zhaojun Yang and Chi-Chun Lee and C. Busso and S. Carnicke and Shrikanth S. Narayanan},\n journal = {Language Resources and Evaluation},\n pages = {497-521},\n title = {The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations},\n volume = {50},\n year = {2016}\n}\n'}",,"{'volume': '50', 'pages': '497-521', 'name': 'Language Resources and Evaluation'}",60.0,The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations,2016.0
2456,d621786b597687f555fae83dc1a021fd21713d90,"Abstract The concept of an agent has become important in both artificial intelligence (AT) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.","[{'authorId': '48106342', 'name': 'M. Wooldridge'}, {'authorId': '144626042', 'name': 'N. Jennings'}]",7126.0,"{'bibtex': '@Article{Wooldridge1995IntelligentAT,\n author = {M. Wooldridge and N. Jennings},\n journal = {The Knowledge Engineering Review},\n pages = {115 - 152},\n title = {Intelligent agents: theory and practice},\n volume = {10},\n year = {1995}\n}\n'}",,"{'volume': '10', 'pages': '115 - 152', 'name': 'The Knowledge Engineering Review'}",233.0,Intelligent agents: theory and practice,1995.0
2457,d62ce6d0a131781d538fa8cb2c8781a3a567a7fd,,"[{'authorId': '1683412', 'name': 'Pascale Fung'}]",18.0,"{'bibtex': '@Article{Fung2015ROBOTSWH,\n author = {Pascale Fung},\n journal = {Scientific American},\n pages = {\n          60-3\n        },\n title = {ROBOTS WITH HEART.},\n volume = {313 5},\n year = {2015}\n}\n'}",,"{'volume': '313 5', 'pages': '\n          60-3\n        ', 'name': 'Scientific American'}",0.0,ROBOTS WITH HEART.,2015.0
2458,d63addfd508a7273bb9bf98f48883256ff12b46f,"Important progress has been made in the methodology for making computer systems easier to use. Highlights are the ""Wizard-of-Oz"" technique and rapid iterative developmental testing. It is argued that more fundamental advances, inventions of truly new and useful computer-based cognitive tools, will result from deeper behavioral analysis of the capabilities and limitations of human performance. Three such analysis methods are described; failure analysis, individual difference analysis, and time profile analysis. A few dramatic success stories are recounted. Promising targets for ""synthesis by analysis"" are proposed. The reason for this meeting is a shared desire to make computers better tools for people to use in the pursuit of their goals. I 'm going to talk about how to go about doing that. First I'll review what I believe to be highlights of what's been learned so far about how to do our business. Then I'll propose some ways in which I think we can make our future efforts have even broader and more profound effects. As a group, the members of the sponsoring organizations and the audience combine psychology, the study of behavior, with computer science and design, disciplines of invention and construction. It's clear enough why both interests are engaged by the task of building computer tools for human use. But it has not always been obvious how to produce an effective merger of the two. Memory is only too fresh of the days in which programmers programmed only for other programmers, and psychologists only carped. Not long enough gone is the time when human factors specialists were called in to bless a system shortly before it was marketed, and if allowed to do an evaluation at all, took so long and learned so little as to be classified as an expense. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ©1987 ACM-0-89791-213-6/87/0004/0333 $00.75 In the last 5 years we have come a long way. There has been a sea change in attitudes. Designers and programmers have inscribed the words ""user friendly"" on their doorposts, while whole divisions of Fortune 500 companies have focused their efforts on usability. Tools ""such as user interface management systems have been developed to make it easier to focus on the issue, and a number of user-oriented inventions like windows, mice and icons have actually found their way into wide use. Methods for informing the design and development process have also improved dramatically. The most effective improvements have come from the introduction of techniques by which new ideas can be tried and tested for usability quickly, cheaply, and very early in the development cycle. In one, the ""Wizard-of-Oz"" method pioneered by John Gould (Gould, Conti and Hovanyecz, 1983), new system functions or features are evaluated before anything at all is built by having a hidden human play the role of the would-be computer. Another extremely effective methodology that is being applied with increasing frequency and benefit is rapid iterative user testing during development. Feedback from tests of small numbers of representative users suggests modifications of an early prototype which are quickly made, similarly tested, loop. An exemplary application is described by Good, Whiteside and Wixon (1984), who obtained a manyfold reduction in user difficulties by iteratively redesigning an interface on the basis of experience with a prototype installed in a shopping center test facility. Still another promising technique has been the development of approximate computational models of users that allow ""ballpark"" evaluations of certain aspects of interfaces during the design phase itself (Card, Moran and Newell, 1983; Kieras and Polson, 1985) Also, of course, there has been a considerable accumulation of art and wisdom on the part of both human factors people and designers who have been paying attention to these matters, and much of it has been capttlred in compendia of design guidelines (e.g. Smith and Mosier, 1984), and is even beginning to be frozen into requirements and standards. What's next? I believe there are important opportunities for a deeper synergy between behavioral analysis and the powerful techniques of computer science for realizing systems that help with intellectual tasks, I believe that we should broaden the conception of our role as workers","[{'authorId': '1836606', 'name': 'T. Landauer'}]",50.0,"{'bibtex': '@Inproceedings{Landauer1986PsychologyAA,\n author = {T. Landauer},\n pages = {333-335},\n title = {Psychology as a mother of invention},\n year = {1986}\n}\n'}",,{'pages': '333-335'},11.0,Psychology as a mother of invention,1986.0
2459,d65e6b99059292cd145b00ecdea947dad89dae91,"Video game non-player characters (NPCs) are a type of agent that often inherits emotion models and functions from ancestor virtual agents. Few emotion models have been designed for NPCs explicitly, and therefore do not approach the expressive possibilities available to live-action performing actors nor hand-crafted animated characters. With distinct perspectives on emotion generation from multiple fields within narratology and computational cognitive psychology, the architecture of NPC emotion systems can reflect the theories and practices of performing artists. This chapter argues that the deployment of virtual agent emotion models applied to NPCs can constrain the performative aesthetic properties of NPCs. An actor-centric emotion model can accommodate creative processes for actors and may reveal what features emotion model architectures should have that are most useful for contemporary game production of photorealistic NPCs that achieve cinematic acting styles and robust narrative design.","[{'authorId': '144812758', 'name': 'Sheldon Schiffer'}]",0.0,"{'bibtex': '@Inproceedings{Schiffer2021MultiDisciplinaryPT,\n author = {Sheldon Schiffer},\n pages = {17-42},\n title = {Multi-Disciplinary Paths to Actor-Centric Non-Player Character Emotion Models},\n year = {2021}\n}\n'}",[],"{'name': '', 'pages': '17-42', 'volume': ''}",31.0,Multi-Disciplinary Paths to Actor-Centric Non-Player Character Emotion Models,2021.0
2460,d65ff869293ba3fe5f9e460656c657417e2f660b,"Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate “face area” also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area “FF”) that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is selectively involved in the perception of faces.","[{'authorId': '1931482', 'name': 'N. Kanwisher'}, {'authorId': '46449222', 'name': 'J. McDermott'}, {'authorId': '3286262', 'name': 'M. Chun'}]",7356.0,"{'bibtex': '@Article{Kanwisher1997TheFF,\n author = {N. Kanwisher and J. McDermott and M. Chun},\n journal = {The Journal of Neuroscience},\n pages = {4302 - 4311},\n title = {The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception},\n volume = {17},\n year = {1997}\n}\n'}",,"{'volume': '17', 'pages': '4302 - 4311', 'name': 'The Journal of Neuroscience'}",42.0,The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception,1997.0
2461,d674763a31eaa14c5d0425a694ea71669fedbc4e,Contents: The Phenomenon and the Methods to Be Used in Its Investigation. Working Memory as a Potential Mediator. Processing Speed as a Potential Mediator. Summary.,"[{'authorId': '3344407', 'name': 'T. Salthouse'}]",223.0,"{'bibtex': '@Inproceedings{Salthouse1992MechanismsOA,\n author = {T. Salthouse},\n title = {Mechanisms of Age Cognition Relations in Adulthood},\n year = {1992}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Mechanisms of Age Cognition Relations in Adulthood,1992.0
2462,d68dde94572b89baad1e36c35b2cfc1d69628170,,"[{'authorId': '39540970', 'name': 'Ginevra Castellano'}, {'authorId': '2169958', 'name': 'M. Mancini'}]",13.0,"{'bibtex': '@Inproceedings{Castellano2009AnalysisOE,\n author = {Ginevra Castellano and M. Mancini},\n pages = {193-198},\n title = {Analysis of Emotional Gestures for the Generation of Expressive Copying Behaviour in an Embodied Agent},\n year = {2009}\n}\n'}",,{'pages': '193-198'},16.0,Analysis of Emotional Gestures for the Generation of Expressive Copying Behaviour in an Embodied Agent,2009.0
2463,d6c4ed037f8b0dbe14ac7dfc94ea9c9b60bc73ca,"Commercial chatbots such as Apple’s Siri, Microsoft’s XiaoIce, Amazon’s Alexa, Jingdong’s JIMI, and Alibaba’s Alime, have some great prospective in applications such as hosting programs, writing poetry, providing pre-sale consulting and after-sales service in E-commerce, and providing virtual shopping guidance. However, in most cases, existed chatbots in the world are neither designed specifically for children, nor suitable for children, especially for children with ASD (autism spectrum disorder). In order to develop chatbots that are suitable for children with ASD, the present study firstly adopted an open source chatting corpus containing more than 1.7 million question-and-answer Chinese sentences of chatting histories involving children in many cases, and screened out more than 400,000 ideal chatting sentences for model training. Then a generative-based method combing Bi-LSTM and attention mechanism with word embedding based on deep neural network was adopted to build a general Chinese chatbot. The quality evaluation results indicated that our chatbot can successfully intrigue participants’ interest and made them understand it well. The chatbot also showed its’ great potential for using in the conversation-mediated intervention for Chinese children with ASD.","[{'authorId': '2108262941', 'name': 'Xuan Li'}, {'authorId': '37234765', 'name': 'Huixin Zhong'}, {'authorId': '2119454939', 'name': 'Bin Zhang'}, {'authorId': '2108454268', 'name': 'Jiaming Zhang'}]",10.0,"{'bibtex': '@Article{Li2020AGC,\n author = {Xuan Li and Huixin Zhong and Bin Zhang and Jiaming Zhang},\n journal = {International Journal of Machine Learning and Computing},\n pages = {519-526},\n title = {A General Chinese Chatbot Based on Deep Learning and Its’ Application for Children with ASD},\n volume = {10},\n year = {2020}\n}\n'}",,"{'volume': '10', 'pages': '519-526', 'name': 'International Journal of Machine Learning and Computing'}",37.0,A General Chinese Chatbot Based on Deep Learning and Its’ Application for Children with ASD,2020.0
2464,d6d848ab41e697f09491daea1ac655aa307b631f,"The modality effect refers to a cognitive load learning effect that occurs when a mixed-mode (partly visual and partly auditory) presentation of information is more effective than a single-mode (either visual or auditory alone) presentation of the same information. For learning to occur, novel material must be organised and incorporated into long-term memory via a limited working memory. For instruction to be effective, it has to be designed in ways in which the limitations of working memory are overcome. As distraction and interference impose an additional memory load, their impact on the limited working memory system has to be taken into consideration in a multimedia context where the different formats of words and pictures allow for many possible ways of presenting information. The instructional predictions that flow from the experimental work on the modality effect are straightforward. From a practical perspective, the modality effect provides guidelines for effective instruction.","[{'authorId': '1819200', 'name': 'R. Mayer'}]",162.0,"{'bibtex': '@Inproceedings{Mayer2014TheCH,\n author = {R. Mayer},\n pages = {345-368},\n title = {The Cambridge Handbook of Multimedia Learning: Principles Based on Social Cues in Multimedia Learning: Personalization, Voice, Image, and Embodiment Principles},\n year = {2014}\n}\n'}",,"{'volume': '', 'pages': '345-368', 'name': ''}",33.0,"The Cambridge Handbook of Multimedia Learning: Principles Based on Social Cues in Multimedia Learning: Personalization, Voice, Image, and Embodiment Principles",2014.0
2465,d6df49d3fe4832ae2b1034da765b9546b489e668,,"[{'authorId': '1801981', 'name': 'C. Brom'}, {'authorId': '2547876', 'name': 'K. Pesková'}, {'authorId': '2842056', 'name': 'J. Lukavský'}]",59.0,"{'bibtex': '@Inproceedings{Brom2007WhatDY,\n author = {C. Brom and K. Pesková and J. Lukavský},\n pages = {89-101},\n title = {What Does Your Actor Remember? Towards Characters with a Full Episodic Memory},\n year = {2007}\n}\n'}",,{'pages': '89-101'},19.0,What Does Your Actor Remember? Towards Characters with a Full Episodic Memory,2007.0
2466,d7077b579a3b8352ae59d00deec94c99b64ac06b,"Empathy reflects the natural ability to perceive and be sensitive to the emotional states of others, coupled with a motivation to care for their well-being. It has evolved in the context of parental care for offspring, as well as within kinship bonds, to help facilitate group living. In this paper, we integrate the perspectives of evolution, animal behaviour, developmental psychology, and social and clinical neuroscience to elucidate our understanding of the proximate mechanisms underlying empathy. We focus, in particular, on processing of signals of distress and need, and their relation to prosocial behaviour. The ability to empathize, both in animals and humans, mediates prosocial behaviour when sensitivity to others' distress is paired with a drive towards their welfare. Disruption or atypical development of the neural circuits that process distress cues and integrate them with decision value leads to callous disregard for others, as is the case in psychopathy. The realization that basic forms of empathy exist in non-human animals is crucial for gaining new insights into the underlying neurobiological and genetic mechanisms of empathy, enabling translation towards therapeutic and pharmacological interventions.","[{'authorId': '3235030', 'name': 'J. Decety'}, {'authorId': '4141534', 'name': 'I. Bartal'}, {'authorId': '3983316', 'name': 'F. Uzefovsky'}, {'authorId': '1382527236', 'name': 'A. Knafo-Noam'}]",400.0,"{'bibtex': '@Article{Decety2016EmpathyAA,\n author = {J. Decety and I. Bartal and F. Uzefovsky and A. Knafo-Noam},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n title = {Empathy as a driver of prosocial behaviour: highly conserved neurobehavioural mechanisms across species},\n volume = {371},\n year = {2016}\n}\n'}",,"{'volume': '371', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",132.0,Empathy as a driver of prosocial behaviour: highly conserved neurobehavioural mechanisms across species,2016.0
2468,d72622e04349f847cde3888560dc7e5550ccd29a,"Background Living systems are associated with Social networks — networks made up of nodes, some of which may be more important in various aspects as compared to others. While different quantitative measures labeled as “centralities” have previously been used in the network analysis community to find out influential nodes in a network, it is debatable how valid the centrality measures actually are. In other words, the research question that remains unanswered is: how exactly do these measures perform in the real world? So, as an example, if a centrality of a particular node identifies it to be important, is the node actually important? Purpose The goal of this paper is not just to perform a traditional social network analysis but rather to evaluate different centrality measures by conducting an empirical study analyzing exactly how do network centralities correlate with data from published multidisciplinary network data sets. Method We take standard published network data sets while using a random network to establish a baseline. These data sets included the Zachary's Karate Club network, dolphin social network and a neural network of nematode Caenorhabditis elegans. Each of the data sets was analyzed in terms of different centrality measures and compared with existing knowledge from associated published articles to review the role of each centrality measure in the determination of influential nodes. Results Our empirical analysis demonstrates that in the chosen network data sets, nodes which had a high Closeness Centrality also had a high Eccentricity Centrality. Likewise high Degree Centrality also correlated closely with a high Eigenvector Centrality. Whereas Betweenness Centrality varied according to network topology and did not demonstrate any noticeable pattern. In terms of identification of key nodes, we discovered that as compared with other centrality measures, Eigenvector and Eccentricity Centralities were better able to identify important nodes.","[{'authorId': '4026257', 'name': 'Komal Batool'}, {'authorId': '1795560', 'name': 'M. Niazi'}]",87.0,"{'bibtex': '@Article{Batool2014TowardsAM,\n author = {Komal Batool and M. Niazi},\n journal = {PLoS ONE},\n title = {Towards a Methodology for Validation of Centrality Measures in Complex Networks},\n volume = {9},\n year = {2014}\n}\n'}",,"{'volume': '9', 'name': 'PLoS ONE'}",35.0,Towards a Methodology for Validation of Centrality Measures in Complex Networks,2014.0
2469,d7293f87cc24b0f5e851d0decd58e6467bc75dd9,,"[{'authorId': '4367292', 'name': 'P. Ellsworth'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",1831.0,"{'bibtex': '@Inproceedings{Ellsworth2003AppraisalPI,\n author = {P. Ellsworth and K. Scherer},\n title = {Appraisal processes in emotion.},\n year = {2003}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Appraisal processes in emotion.,2003.0
2470,d731ff4de66c38580ccb6e8a92d093766a194878,"We used functional MRI to test the hypothesis that emotional states can selectively influence cognition-related neural activity in lateral prefrontal cortex (PFC), as evidence for an integration of emotion and cognition. Participants (n = 14) watched short videos intended to induce emotional states (pleasant/approach related, unpleasant/withdrawal related, or neutral). After each video, the participants were scanned while performing a 3-back working memory task having either words or faces as stimuli. Task-related neural activity in bilateral PFC showed a predicted pattern: an Emotion × Stimulus crossover interaction, with no main effects, with activity predicting task performance. This highly specific result indicates that emotion and higher cognition can be truly integrated, i.e., at some point of processing, functional specialization is lost, and emotion and cognition conjointly and equally contribute to the control of thought and behavior. Other regions in lateral PFC showed hemispheric specialization for emotion and for stimuli separately, consistent with a hierarchical and hemisphere-based mechanism of integration.","[{'authorId': '2556954', 'name': 'J. Gray'}, {'authorId': '2723253', 'name': 'T. Braver'}, {'authorId': '2057029', 'name': 'M. Raichle'}]",759.0,"{'bibtex': '@Article{Gray2002IntegrationOE,\n author = {J. Gray and T. Braver and M. Raichle},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n pages = {4115 - 4120},\n title = {Integration of emotion and cognition in the lateral prefrontal cortex},\n volume = {99},\n year = {2002}\n}\n'}",,"{'volume': '99', 'pages': '4115 - 4120', 'name': 'Proceedings of the National Academy of Sciences of the United States of America'}",64.0,Integration of emotion and cognition in the lateral prefrontal cortex,2002.0
2471,d7482fa49be14224dbe5297ee6b428810b589741,"Embodied conversational agents are computer-generated cartoonlike characters that demonstrate many of the same properties as humans in face-to-face conversation, including the ability to produce and respond to verbal and nonverbal communication. They constitute a type of (a) multimodal interface where the modalities are those natural to human conversation: speech, facial displays, hand gestures, and body stance; (b) software agent, insofar as they represent the computer in an interaction with a human or represent their human users in a computational environment (as avatars, for example); and (c) dialogue system where both verbal and nonverbal devices advance and regulate the dialogue between the user and the computer. With an embodied conversational agent, the visual dimension of interacting with an animated character on a screen plays an intrinsic role. Not just pretty pictures, the graphics display visual features of conversation in the same way that the face and hands do in face-to-face conversation among humans.This book describes research in all aspects of the design, implementation, and evaluation of embodied conversational agents as well as details of specific working systems. Many of the chapters are written by multidisciplinary teams of psychologists, linguists, computer scientists, artists, and researchers in interface design. The authors include Elisabeth Andre, Norm Badler, Gene Ball, Justine Cassell, Elizabeth Churchill, James Lester, Dominic Massaro, Cliff Nass, Sharon Oviatt, Isabella Poggi, Jeff Rickel, and Greg Sanders.","[{'authorId': '145431806', 'name': 'Justine Cassell'}]",1042.0,"{'bibtex': '@Inproceedings{Cassell2000EmbodiedCA,\n author = {Justine Cassell},\n title = {Embodied conversational agents},\n year = {2000}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Embodied conversational agents,2000.0
2475,d778baf82269d66d9155066e916007868b132827,"In this paper, we present evidence that although current models for introduction of robotic companions stress individual encounters, a social community alternative is promising. This argument emerges from an experiment we conducted with a small interactive robot at two local nursing homes. Here we give a brief introduction to the robot and our experience at the homes. We compare the robot used to a semi-robotic toy whose use initially suggested to us the benefits of social community models in the presentation of robotics to the elderly. We find that even where individual encounters are significant, sensitivity to social dimensions improve the benefits of these encounters","[{'authorId': '34704262', 'name': 'Cory D. Kidd'}, {'authorId': '144779703', 'name': 'Will Taggart'}, {'authorId': '2264591', 'name': 'S. Turkle'}]",347.0,"{'bibtex': '@Article{Kidd2006ASR,\n author = {Cory D. Kidd and Will Taggart and S. Turkle},\n journal = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},\n pages = {3972-3976},\n title = {A sociable robot to encourage social interaction among the elderly},\n year = {2006}\n}\n'}",,"{'pages': '3972-3976', 'name': 'Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.'}",13.0,A sociable robot to encourage social interaction among the elderly,2006.0
2476,d7875618d1e8dd43158ce61ea67dc836a9b58a85,,"[{'authorId': '38761094', 'name': 'R. Frank'}, {'authorId': '6030414', 'name': 'Thomas Gilovich'}, {'authorId': '144932144', 'name': 'D. Regan'}]",346.0,"{'bibtex': '@Article{Frank1993TheEO,\n author = {R. Frank and Thomas Gilovich and D. Regan},\n journal = {Ethology and Sociobiology},\n pages = {247-256},\n title = {The evolution of one-shot cooperation: An experiment},\n volume = {14},\n year = {1993}\n}\n'}",,"{'volume': '14', 'pages': '247-256', 'name': 'Ethology and Sociobiology'}",0.0,The evolution of one-shot cooperation: An experiment,1993.0
2477,d789dd7659e43306fa9e28ab8a0b033f257d0a27,"As an increasing part of the Army’s mission involves establishing rapport with diverse populations, training interpersonal skills becomes critically important. Here we describe a “Rapport Agent” that senses and responds to a speaker’s nonverbal behavior and provide empirical evidence that it increases speaker fluency and engagement. We argue such agent technology has potential, both as a training system to enhance communication skills, and to assess the key factors that influence rapport in face-to-face interactions. We conclude by discussing ways the nonverbal correlates of rapport vary between Arabic and English speakers and discuss the potential of such technology to advance research and training into rapport in cross-cultural settings.","[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1947943', 'name': 'A. Okhmatovskaia'}, {'authorId': '144346436', 'name': 'S. Duncan'}]",3.0,"{'bibtex': '@Inproceedings{Gratch2006VirtualHF,\n author = {J. Gratch and A. Okhmatovskaia and S. Duncan},\n title = {Virtual Humans for the Study of Rapport in Cross Cultural Settings},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",41.0,Virtual Humans for the Study of Rapport in Cross Cultural Settings,2006.0
2478,d78a6acfc145c0ad599c3c6ba542657f488caa01,"The present study examined interpersonal synchrony during psychological counseling, focusing on heart rate synchrony. In psychological counseling and psychotherapy, embodied synchrony is considered an important factor related to building rapport and empathy. Recent interpersonal synchrony/coordination studies have addressed this issue, not only at the behavioral level but also at the neurological (brain activity) and physiological (cardiac activity) levels. However, there is little known literature on heart rate synchrony in a psychological counseling context. Therefore, we conducted a single exploratory case study to ascertain whether heart rate synchrony was observed in a counseling session and how it related to therapeutic processes and psychological issues. One male university student and one male clinical psychologist participated in our experiment. The student had a counseling session for 50 minutes. Video data were recorded and two wearable sensors were attached to the chests of both participants to collect heart rate data. We applied nonlinear time series analyses, based on a recurrence analysis, to the heart rate data to quantitatively assess heart rate synchrony. A qualitative analysis was also conducted by three clinical psychologists, based on video data from the viewpoints of clinical psychology and psychotherapy. The results show that the heart rate synchrony between client and therapist was observed and changed dynamically during the session. The present study suggests that heart rate synchrony may occur in some clinically important scenes and reflect psychological factors (e.g., building rapport and empathy) and social relationships (e.g., leader-follower). The present study shows the applicability of recurrence-based analyses to complex heart rate data during psychological counseling, as explored in other interpersonal synchrony studies. Further examinations using more data from multiple viewpoints are expected to support our findings and cast light on the relationship between embodied synchrony and psychological issues in the context of psychological counseling and psychotherapy.","[{'authorId': '35086651', 'name': 'Kentaro Kodama'}, {'authorId': '50053878', 'name': 'Shintaro Tanaka'}, {'authorId': '2065065', 'name': 'Daichi Shimizu'}, {'authorId': '2053009209', 'name': 'Kyoko Hori'}, {'authorId': '2018593731', 'name': 'H. Matsui'}]",19.0,"{'bibtex': '@Article{Kodama2018HeartRS,\n author = {Kentaro Kodama and Shintaro Tanaka and Daichi Shimizu and Kyoko Hori and H. Matsui},\n journal = {Psychology},\n pages = {1858-1874},\n title = {Heart Rate Synchrony in Psychological Counseling: A Case Study},\n volume = {09},\n year = {2018}\n}\n'}",,"{'volume': '09', 'pages': '1858-1874', 'name': 'Psychology'}",43.0,Heart Rate Synchrony in Psychological Counseling: A Case Study,2018.0
2479,d792562462dbb687015954805d31620240db57a1,,"[{'authorId': '2083280282', 'name': 'E. Tulving'}]",4348.0,"{'bibtex': '@Inproceedings{Tulving1972EpisodicAS,\n author = {E. Tulving},\n title = {Episodic and semantic memory},\n year = {1972}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Episodic and semantic memory,1972.0
2481,d7ae21f2f22f391e3c782fbffd5353a8f11151d4,"Affect modeling plays a vital role in faithfully simulating human emotion and in emotionally evocative technology. Current affect models are still strong simplifications compared to human affective complexity. To establish richer agent interaction, we integrated three affect-related models: CoMERG, IPEFiC ADM and EMA. These models partly overlap, and where distinct, they complement one another. The integrated model called Silicon Coppélia was implemented and simulation experiments were performed to test the behavior of the model. These experiments show that the model can simulate richer agent behaviors than any of the models could have done alone.","[{'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '3347943', 'name': 'G. F. Siddiqui'}]",10.0,"{'bibtex': '@Article{Pontier2009SiliconCI,\n author = {M. Pontier and G. F. Siddiqui},\n journal = {2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology},\n pages = {279-284},\n title = {Silicon Coppélia: Integrating Three Affect-Related Models for Establishing Richer Agent Interaction},\n volume = {2},\n year = {2009}\n}\n'}",,"{'volume': '2', 'pages': '279-284', 'name': '2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology'}",11.0,Silicon Coppélia: Integrating Three Affect-Related Models for Establishing Richer Agent Interaction,2009.0
2483,d7d0bc213119e930833a7f40005b2fd111399957,,"[{'authorId': '3259367', 'name': 'J. Albus'}, {'authorId': '145667472', 'name': 'A. Barbera'}]",73.0,"{'bibtex': '@Article{Albus2005RCSAC,\n author = {J. Albus and A. Barbera},\n journal = {Annu. Rev. Control.},\n pages = {87-99},\n title = {RCS: A cognitive architecture for intelligent multi-agent systems},\n volume = {29},\n year = {2005}\n}\n'}",,"{'volume': '29', 'pages': '87-99', 'name': 'Annu. Rev. Control.'}",32.0,RCS: A cognitive architecture for intelligent multi-agent systems,2005.0
2484,d7f05e0e7d45b539188dbf0dd1648fd03565c185,"Simon Baron-Cohen, Professor of Developmental Psychopathology at the University of Cambridge has long been known for his work with autism and, as we all know, children with autism have quite remarkable deficiencies in their ability to use or read the social cues and emotions of everyday life. In this remarkable set of CDs he has taken up the challenge of teaching these skills in a unique manner. 
 
Once installed, the child or youth (and even adults) signs on their unique identifier and enters one of three main centres: Emotions Library, Learning Center, and Games Zone. The material is graded into 6 levels of complexity, and various forms of help (e.g. a happy robot ‘Emoto’, somewhat like a talking MicroSoft Office Assistant) are easily available allowing even young children to explore on their own. 
 
In the Emotions Library, 412 emotions are arranged in 24 groups. Once the child picks a group, she or he can click on an emotion in that group and see actors (6 different ones representing various ages, cultures and both sexes) in a brief video with a narrator describing the emotion and what to watch for. In addition, there are sample story lines illustrating the emotion (at the younger levels, clicking on a line leads to the narrator speaking it), sample voice expressions of the emotion and information about the emotion (definition, whether it is seen as positive or negative, notes about how others usually react to it). Students get a chance to make and keep notes under their log-on name. 
 
In the Learning Center, the emotions groups are similar to the library but the structure is more conducive to learning and exploration, again with videos, stories and vocal expressions separately. The lessons contain test questions and give students a chance to concentrate on the beginners 100 or top 20 along with questions and rewards. The quizzes give a chance to find faces with specific emotions, match emotional statements and faces, match statements with people in a picture. 
 
Rewards are built into much of the practice and include hundreds of objects with a variety of information or movement. Objects include flags, butterflies, trains, birds, objects of the universe and more. If all the train cars are collected, it can be assembled to drive around a track. Other objects can be enlarged under a microscope, time lapse movies can be constructed and, my favourite, band instruments play–the more instruments collected, the more interesting the music that can be constructed. One reward is building up time to spend in the Games Zone. 
 
The Games Zone includes matching games, hand-eye coordination games, real world face games and the opportunity to control Daniel Radcliffe (aka Harry Potter) in a variety of emotions. 
 
It is not enough merely to teach the emotions, make the lessons fun, earn rewards and play games. Behind the whole set-up is Mind Reading Manager. This sets the parameters for a number of components of the three major sections thus allowing parents, teachers or professionals to track progress of a child, limit time in games, set limits on emotions (e.g. removing the ‘romantic’ emotions from beginner levels for children), fix levels and more. It is possible to track students’ overall engagement with different components, lists of emotions completed, average scores and more. 
 
My own delay in forwarding a book review was that I wanted a chance to test it out for real with a day treatment service (age groups 5 to 9, 10 to 12 and 13 to 18 years). Attendees included those with disruptive behavioural disorders, depression, anxiety and pervasive developmental disorders. The program was particularly suited to group work with children with developmental disorders, but our staff used it for anger management training, social skills training and reward time for children and teens. When it can be used as a reward, you can learn one more important item, this program is fun! The final word from staff and children was that they were so pleased that separate orders for additional CDs were made and for other mental health programs that came to demonstrations. 
 
The CDs or DVD can be installed on notebook computers, desk top computers, or, our favourite, connected to an LCD projector for group work. Technical support is available and it has proven helpful for us (a couple of minor questions). The cost is reasonable and the quality of the production, directing and acting is all excellent. 
 
My overall conclusion is to support the staff I work with; Simon Baron-Cohen and team have done excellent work for children, youth and mental health professionals everywhere. Thank-you Dr. Baron-Cohen!","[{'authorId': '31864525', 'name': 'W. Junek'}]",291.0,"{'bibtex': ""@Article{Junek2007MindRT,\n author = {W. Junek},\n journal = {Journal de l'Académie canadienne de psychiatrie de l'enfant et de l'adolescent},\n pages = {182-183},\n title = {Mind Reading: The Interactive Guide to Emotions},\n volume = {16},\n year = {2007}\n}\n""}",,"{'volume': '16', 'pages': '182-183', 'name': ""Journal de l'Académie canadienne de psychiatrie de l'enfant et de l'adolescent""}",0.0,Mind Reading: The Interactive Guide to Emotions,2007.0
2485,d8326c457c1036d12f2c85033ddfcff0229c6b6e,"Agent-based modeling and simulation can provide a powerful test environment for crisis management scenarios. Human agent interaction has limitations in representing norms issued by an agent to a human agent that has emotions. In this study, we present an approach to the interaction between a virtual normative agent and a human agent in an evacuation scenario. Through simulation comparisons, it is shown that the method used in this study can more fully simulate the real-life out come of an emergency situation and also improves the au thenticity of the agent interaction.","[{'authorId': '66280130', 'name': 'E. Pagou'}, {'authorId': '10394930', 'name': 'V. Kamla'}, {'authorId': '51221370', 'name': 'I. Tchappi'}, {'authorId': '2297347', 'name': 'A. Najjar'}]",0.0,"{'bibtex': '@Article{Pagou2023DevelopmentOA,\n author = {E. Pagou and V. Kamla and I. Tchappi and A. Najjar},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {16330-16331},\n title = {Development of a Human-Agent Interaction System including Norm and Emotion in an Evacuation Situation (Student Abstract)},\n year = {2023}\n}\n'}",[],{'pages': '16330-16331'},7.0,Development of a Human-Agent Interaction System including Norm and Emotion in an Evacuation Situation (Student Abstract),2023.0
2486,d8384f7ef288d2d5cb267128471c5427fc98b54b,"Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.","[{'authorId': '1743797', 'name': 'Isabelle M Guyon'}, {'authorId': '1766703', 'name': 'A. Elisseeff'}]",15328.0,"{'bibtex': '@Article{Guyon2003AnIT,\n author = {Isabelle M Guyon and A. Elisseeff},\n journal = {J. Mach. Learn. Res.},\n pages = {1157-1182},\n title = {An Introduction to Variable and Feature Selection},\n volume = {3},\n year = {2003}\n}\n'}",,"{'volume': '3', 'pages': '1157-1182', 'name': 'J. Mach. Learn. Res.'}",57.0,An Introduction to Variable and Feature Selection,2003.0
2487,d83921fab4ed59492bc34019e1e61f94bd462308,". Abstract Arti(cid:12)cial intelligence researchers attempting to create engaging, apparently living creatures may (cid:12)nd important insight in the work of artists who have explored the idea of believable character. In particular, appropriately timed and clearly expressed emotion is a central requirement for believable characters. We discuss these ideas and suggest how they may apply to believable interactive characters, which we call \believable agents."" This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laboratories. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the o(cid:14)cial policies, either expressed or implied, of any other parties.","[{'authorId': '145207410', 'name': 'J. Bates'}]",1384.0,"{'bibtex': '@Article{Bates1994TheRO,\n author = {J. Bates},\n journal = {Commun. ACM},\n pages = {122-125},\n title = {The role of emotion in believable agents},\n volume = {37},\n year = {1994}\n}\n'}",,"{'volume': '37', 'pages': '122-125', 'name': 'Commun. ACM'}",12.0,The role of emotion in believable agents,1994.0
2490,d868fbf872df338aafd3d5e282f444c5e7a94d98,,"[{'authorId': '37589220', 'name': 'E. Hess'}]",160.0,"{'bibtex': '@Article{Hess1975TheRO,\n author = {E. Hess},\n journal = {Scientific American},\n pages = {\n          110-2, 116-9\n        },\n title = {The role of pupil size in communication.},\n volume = {233 5},\n year = {1975}\n}\n'}",,"{'volume': '233 5', 'pages': '\n          110-2, 116-9\n        ', 'name': 'Scientific American'}",0.0,The role of pupil size in communication.,1975.0
2491,d87ceda3042f781c341ac17109d1e94a717f5f60,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.","[{'authorId': '1721801', 'name': 'C. Fellbaum'}]",14075.0,"{'bibtex': '@Article{Fellbaum2000WordNetA,\n author = {C. Fellbaum},\n journal = {Language},\n pages = {706},\n title = {WordNet : an electronic lexical database},\n volume = {76},\n year = {2000}\n}\n'}",,"{'volume': '76', 'pages': '706', 'name': 'Language'}",10.0,WordNet : an electronic lexical database,2000.0
2492,d87d864a05805c4eafe74df8959c1c4644f8c22e,"Thanks to the decreasing cost of whole-body sensing technology and its increasing reliability, there is an increasing interest in, and understanding of, the role played by body expressions as a powerful affective communication channel. The aim of this survey is to review the literature on affective body expression perception and recognition. One issue is whether there are universal aspects to affect expression perception and recognition models or if they are affected by human factors such as culture. Next, we discuss the difference between form and movement information as studies have shown that they are governed by separate pathways in the brain. We also review psychological studies that have investigated bodily configurations to evaluate if specific features can be identified that contribute to the recognition of specific affective states. The survey then turns to automatic affect recognition systems using body expressions as at least one input modality. The survey ends by raising open questions on data collecting, labeling, modeling, and setting benchmarks for comparing automatic recognition systems.","[{'authorId': '2870739', 'name': 'A. Kleinsmith'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}]",541.0,"{'bibtex': '@Article{Kleinsmith2013AffectiveBE,\n author = {A. Kleinsmith and N. Bianchi-Berthouze},\n journal = {IEEE Transactions on Affective Computing},\n pages = {15-33},\n title = {Affective Body Expression Perception and Recognition: A Survey},\n volume = {4},\n year = {2013}\n}\n'}",,"{'volume': '4', 'pages': '15-33', 'name': 'IEEE Transactions on Affective Computing'}",191.0,Affective Body Expression Perception and Recognition: A Survey,2013.0
2495,d88b6ae8e81e2020366204fc192a144d49cd2c89,,"[{'authorId': '3213354', 'name': 'B. Gelder'}, {'authorId': '1974324', 'name': 'R. Hortensius'}]",24.0,"{'bibtex': '@Inproceedings{Gelder2014TheMF,\n author = {B. Gelder and R. Hortensius},\n pages = {153-164},\n title = {The many faces of the emotional body},\n year = {2014}\n}\n'}",,"{'volume': '', 'pages': '153-164', 'name': ''}",66.0,The many faces of the emotional body,2014.0
2496,d8a61867fba1efa68b325536a9ac84ffeae3935e,"Rates of spontaneous mutation per genome as measured in the laboratory are remarkably similar within broad groups of organisms but differ strikingly among groups. Mutation rates in RNA viruses, whose genomes contain ca. 10(4) bases, are roughly 1 per genome per replication for lytic viruses and roughly 0.1 per genome per replication for retroviruses and a retrotransposon. Mutation rates in microbes with DNA-based chromosomes are close to 1/300 per genome per replication; in this group, therefore, rates per base pair vary inversely and hugely as genome sizes vary from 6 x 10(3) to 4 x 10(7) bases or base pairs. Mutation rates in higher eukaryotes are roughly 0.1-100 per genome per sexual generation but are currently indistinguishable from 1/300 per cell division per effective genome (which excludes the fraction of the genome in which most mutations are neutral). It is now possible to specify some of the evolutionary forces that shape these diverse mutation rates.","[{'authorId': '1916258', 'name': 'J. Drake'}, {'authorId': '4708450', 'name': 'B. Charlesworth'}, {'authorId': '5774763', 'name': 'D. Charlesworth'}, {'authorId': '12269780', 'name': 'J. Crow'}]",1865.0,"{'bibtex': '@Article{Drake1998RatesOS,\n author = {J. Drake and B. Charlesworth and D. Charlesworth and J. Crow},\n journal = {Genetics},\n pages = {\n          1667-86\n        },\n title = {Rates of spontaneous mutation.},\n volume = {148 4},\n year = {1998}\n}\n'}",,"{'volume': '148 4', 'pages': '\n          1667-86\n        ', 'name': 'Genetics'}",188.0,Rates of spontaneous mutation.,1998.0
2497,d8a77815f38bc5d21b591efbaa23cb6975e88766,"Using an affective priming procedure (S. T. Murphy & R. B. Zajonc, 1993), 7 studies examined the effects of the contextual activation of representations of attachment security (secure base schema) on the evaluation of neutral stimuli under either neutral or stressful contexts. In all the studies, participants also reported on their attachment style. Results indicated that the subliminal priming of secure base representations led to more positive affective reactions to neutral stimuli than did the subliminal priming of neutral or no pictures under both neutral and stressful contexts. Although the subliminal priming of positively valued, attachment-unrelated representations heightened positive evaluations under neutral contexts, it failed to elicit positive affect under stressful contexts. The results also revealed interesting effects of attachment style. The discussion focuses on the affective component of the secure base schema.","[{'authorId': '4021295', 'name': 'M. Mikulincer'}, {'authorId': '2367107', 'name': 'Gilad Hirschberger'}, {'authorId': '2229885721', 'name': 'Orit Nachmias'}, {'authorId': '5605945', 'name': 'Omri Gillath'}]",312.0,"{'bibtex': '@Article{Mikulincer2001TheAC,\n author = {M. Mikulincer and Gilad Hirschberger and Orit Nachmias and Omri Gillath},\n journal = {Journal of personality and social psychology},\n pages = {\n          305-21\n        },\n title = {The affective component of the secure base schema: affective priming with representations of attachment security.},\n volume = {81 2},\n year = {2001}\n}\n'}",,"{'volume': '81 2', 'pages': '\n          305-21\n        ', 'name': 'Journal of personality and social psychology'}",48.0,The affective component of the secure base schema: affective priming with representations of attachment security.,2001.0
2498,d8cf6275e610c797b7a433d5ac9acb7c4de9ec30,"Background Sleep, physical activity, and diet have been associated with mental health and well-being individually in young adults. However, which of these “big three” health behaviors most strongly predicts mental health and well-being, and their higher-order relationships in predictive models, is less known. This study investigated the differential and higher-order associations between sleep, physical activity, and dietary factors as predictors of mental health and well-being in young adults. Method In a cross-sectional survey design, 1,111 young adults (28.4% men) ages 18–25 from New Zealand and the United States answered an online survey measuring typical sleep quantity and quality; physical activity; and consumption of raw and processed fruit and vegetables, fast food, sweets, and soda, along with extensive covariates (including demographics, socioeconomic status, body mass index, alcohol use, smoking, and health conditions) and the outcome measures of depressive symptoms [measured by the Center for Epidemiological Depression Scale (CES-D)] and well-being (measured by the Flourishing Scale). Results Controlling for covariates, sleep quality was the strongest predictor of depressive symptoms and well-being, followed by sleep quantity and physical activity. Only one dietary factor—raw fruit and vegetable consumption—predicted greater well-being but not depressive symptoms when controlling for covariates. There were some higher-order interactions among health behaviors in predicting the outcomes, but these did not survive cross-validation. Conclusion Sleep quality is an important predictor of mental health and well-being in young adults, whereas physical activity and diet are secondary but still significant factors. Although strictly correlational, these patterns suggest that future interventions could prioritize sleep quality to maximize mental health and well-being in young adults.","[{'authorId': '2034055971', 'name': 'Shay-Ruby Wickham'}, {'authorId': '1485669084', 'name': 'Natasha A. Amarasekara'}, {'authorId': '1708253966', 'name': 'A. Bartonicek'}, {'authorId': '4856780', 'name': 'Tamlin S. Conner'}]",52.0,"{'bibtex': '@Article{Wickham2020TheBT,\n author = {Shay-Ruby Wickham and Natasha A. Amarasekara and A. Bartonicek and Tamlin S. Conner},\n journal = {Frontiers in Psychology},\n title = {The Big Three Health Behaviors and Mental Health and Well-Being Among Young Adults: A Cross-Sectional Investigation of Sleep, Exercise, and Diet},\n volume = {11},\n year = {2020}\n}\n'}",,"{'volume': '11', 'name': 'Frontiers in Psychology'}",70.0,"The Big Three Health Behaviors and Mental Health and Well-Being Among Young Adults: A Cross-Sectional Investigation of Sleep, Exercise, and Diet",2020.0
2499,d91c273c6b0cf2c7de5cdf85ec0fedcde300d522,"Cognitive load theory suggests that effective instructional material facilitates learning by directing cognitive resources toward activities that are relevant to learning rather than toward preliminaries to learning. One example of ineffective instruction occurs if learners unnecessarily are required to mentally integrate disparate sources of mutually referring information such as separate text and diagrams. Such split-source information may generate a heavy cognitive load, because material must be mentally integrated before learning can commence. This article reports findings from six experiments testing the consequences of split-source and integrated information using electrical engineering and biology instructional materials. Experiment 1 was designed to compare conventional instructions with integrated instructions over a period of several months in an industrial training setting. The materials chosen were unintelligible without mental integration. Results favored integrated instructions throughout th...","[{'authorId': '145760137', 'name': 'Paul Chandler'}, {'authorId': '2479443', 'name': 'J. Sweller'}]",2660.0,"{'bibtex': '@Article{Chandler1991CognitiveLT,\n author = {Paul Chandler and J. Sweller},\n journal = {Cognition and Instruction},\n pages = {293-332},\n title = {Cognitive Load Theory and the Format of Instruction},\n volume = {8},\n year = {1991}\n}\n'}",,"{'volume': '8', 'pages': '293-332', 'name': 'Cognition and Instruction'}",14.0,Cognitive Load Theory and the Format of Instruction,1991.0
2500,d997919c30fa6711bc5c25cf8c8aea34fac27b91,"Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.","[{'authorId': '1756344', 'name': 'T. Baltrušaitis'}, {'authorId': '2149814967', 'name': 'P. Robinson'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",1206.0,"{'bibtex': '@Article{Baltrušaitis2016OpenFaceAO,\n author = {T. Baltrušaitis and P. Robinson and Louis-Philippe Morency},\n journal = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n pages = {1-10},\n title = {OpenFace: An open source facial behavior analysis toolkit},\n year = {2016}\n}\n'}",,"{'pages': '1-10', 'name': '2016 IEEE Winter Conference on Applications of Computer Vision (WACV)'}",75.0,OpenFace: An open source facial behavior analysis toolkit,2016.0
2501,d9a6d795eb46a022935228e23e72a8a099cc1d2f,"Automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing. We describe a system for detecting elements of interactional style: whether a speaker is awkward, friendly, or flirtatious. We create and use a new spoken corpus of 991 4-minute speed-dates. Participants rated their interlocutors for these elements of style. Using rich dialogue, lexical, and prosodic features, we are able to detect flirtatious, awkward, and friendly styles in noisy natural conversational data with up to 75% accuracy, compared to a 50% baseline. We describe simple ways to extract relatively rich dialogue features, and analyze which features performed similarly for men and women and which were gender-specific.","[{'authorId': '1746807', 'name': 'Dan Jurafsky'}, {'authorId': '2615814', 'name': 'R. Ranganath'}, {'authorId': '100525940', 'name': 'Daniel A. McFarland'}]",90.0,"{'bibtex': '@Inproceedings{Jurafsky2009ExtractingSM,\n author = {Dan Jurafsky and R. Ranganath and Daniel A. McFarland},\n pages = {638-646},\n title = {Extracting Social Meaning: Identifying Interactional Style in Spoken Conversation},\n year = {2009}\n}\n'}",,{'pages': '638-646'},32.0,Extracting Social Meaning: Identifying Interactional Style in Spoken Conversation,2009.0
2502,d9a9e1f7fa29978890b98c11fc1f9271fa3a55ef,,"[{'authorId': '2775420', 'name': 'Thomas Janssoone'}, {'authorId': '2049106', 'name': 'C. Clavel'}, {'authorId': '2521061', 'name': 'Kévin Bailly'}, {'authorId': '145793390', 'name': 'G. Richard'}]",17.0,"{'bibtex': '@Inproceedings{Janssoone2016UsingTA,\n author = {Thomas Janssoone and C. Clavel and Kévin Bailly and G. Richard},\n pages = {175-189},\n title = {Using Temporal Association Rules for the Synthesis of Embodied Conversational Agents with a Specific Stance},\n year = {2016}\n}\n'}",,{'pages': '175-189'},31.0,Using Temporal Association Rules for the Synthesis of Embodied Conversational Agents with a Specific Stance,2016.0
2503,d9c7784977296d48a586e40b0cc9b0f5b9d65de0,,"[{'authorId': '144984483', 'name': 'T. Rist'}, {'authorId': '1742930', 'name': 'E. André'}, {'authorId': '1990470', 'name': 'Stephan Baldes'}, {'authorId': '48785659', 'name': 'Patrick Gebhard'}, {'authorId': '2922093', 'name': 'Martin Klesen'}, {'authorId': '145616714', 'name': 'Michael Kipp'}, {'authorId': '2208081', 'name': 'P. Rist'}, {'authorId': '2052615340', 'name': 'Markus Schmitt'}]",56.0,"{'bibtex': '@Inproceedings{Rist2004ARO,\n author = {T. Rist and E. André and Stephan Baldes and Patrick Gebhard and Martin Klesen and Michael Kipp and P. Rist and Markus Schmitt},\n pages = {377-404},\n title = {A Review of the Development of Embodied Presentation Agents and Their Application Fields},\n year = {2004}\n}\n'}",,{'pages': '377-404'},60.0,A Review of the Development of Embodied Presentation Agents and Their Application Fields,2004.0
2504,d9c8d4dde4772e06bdb88a2d88c28cee0533d880,"In the early days of e-Commerce, web designers follow their intuition of how the website should look like. Further development of web design have seen for instance Nielsen's heuristic, which resulted designers to focus mainly on cognitive functionality and usability. However, as technology advances and e-Commerce matures, the affective quality has become important than ever. Affect has been proven to influence human cognitive judgments, and human first impression or primary affective responses to a website is seen to influence cognitive quality and higher order affect, such as Perceived Usefulness and Perceived Ease of Use. The study focuses at human Perceived Affective Quality, which looks at both valence and arousal dimension of affect, in the measurement of affective responses to an e-Commerce website. Kansei Engineering is a technique that is seen to enable the measurement and assimilation of the affective responses into perceptual design element. This paper presents the concept and framework of Kansei Engineering in e-Commerce websites. The framework provides systematic method of evaluating consumer's subjective emotional responses and ways to determine correlation of emotional responses with website design. Further research with empirical studies will enable to formulate guideline to design affective quality website. Result of such studies will help to increase understanding between web designers and users, and improve affective quality of a website. The design of affective quality website will result a paradigm shift from WYSIWYG to WYSIWYD.","[{'authorId': '2980650', 'name': 'A. Lokman'}]",32.0,"{'bibtex': '@Inproceedings{Lokman2006KANSEIEC,\n author = {A. Lokman},\n title = {KANSEI ENGINEERING CONCEPT IN E-COMMERCE WEBSITE},\n year = {2006}\n}\n'}",,"{'volume': '', 'name': ''}",39.0,KANSEI ENGINEERING CONCEPT IN E-COMMERCE WEBSITE,2006.0
2505,d9eeabdd4a7ea23c72a9d11aa00792d16a71f76d,,"[{'authorId': '3065464', 'name': 'A. V. D. Pütten'}, {'authorId': '1750852', 'name': 'N. Krämer'}]",142.0,"{'bibtex': '@Article{Pütten2014HowDC,\n author = {A. V. D. Pütten and N. Krämer},\n journal = {Comput. Hum. Behav.},\n pages = {422-439},\n title = {How design characteristics of robots determine evaluation and uncanny valley related responses},\n volume = {36},\n year = {2014}\n}\n'}",,"{'volume': '36', 'pages': '422-439', 'name': 'Comput. Hum. Behav.'}",89.0,How design characteristics of robots determine evaluation and uncanny valley related responses,2014.0
2506,d9f01059ccc0785066c5242d0c673b41010fe034,"Abstract This paper is concerned with some limitations of the vignette methodology used in contemporary appraisal research and their implications for appraisal theory. We focus on two recent studies in which emotional manipulations were achieved using textual materials, and criticise the investigators' apparent implicit assumption that participation in everyday social reality is somehow comparable to reading a story. We take issue with three related aspects of this cognitive analogy between life and its narrative representation, by arguing that emotional reactions in real life are not necessarily mediated by symbolic processes, that people are involved participants of real life rather than neutral observers, and that in real life people's evaluations and emotions are typically part of an ongoing dialogue rather than the expression of a soliloquy. Results from these studies of emotional vignettes therefore tend to overestimate the importance of constructive, abstract, and individualistic processes in the e...","[{'authorId': '144603514', 'name': 'B. Parkinson'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",229.0,"{'bibtex': '@Article{Parkinson1993MakingSO,\n author = {B. Parkinson and A. Manstead},\n journal = {Cognition & Emotion},\n pages = {295-323},\n title = {Making Sense of Emotion in Stories and Social Life},\n volume = {7},\n year = {1993}\n}\n'}",,"{'volume': '7', 'pages': '295-323', 'name': 'Cognition & Emotion'}",29.0,Making Sense of Emotion in Stories and Social Life,1993.0
2507,da108ef0726b52c7339e2c5cfdc57e34d2bc2e34,"Based on artificial psychology and common emotion theory,combined with personality and OCC model in 3-dimension emotion space,this paper builds an emotion model and adopts the model as feeling kernel,realizes a virtual feeling robot by the application software tools of VB,ViaVoice,KeDaXunFei,TalkingShow.The virtual feeling robot not only has the ability of study and memory,but also has the ability of emotion exchange.","[{'authorId': '2054523158', 'name': 'Wang Chao'}]",4.0,"{'bibtex': '@Article{Chao2007StudyOE,\n author = {Wang Chao},\n journal = {Computer Engineering},\n title = {Study on Emotion Model Building and Virtual Feeling Robot},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': 'Computer Engineering'}",0.0,Study on Emotion Model Building and Virtual Feeling Robot,2007.0
2508,da1626cc8306c988dd2ae695233faa98db21301f,,"[{'authorId': '3424122', 'name': 'Y. Ikeda'}, {'authorId': '37310260', 'name': 'Ryota Horie'}, {'authorId': '145094033', 'name': 'Midori Sugaya'}]",35.0,"{'bibtex': '@Inproceedings{Ikeda2017EstimatingEW,\n author = {Y. Ikeda and Ryota Horie and Midori Sugaya},\n pages = {1589-1600},\n title = {Estimating Emotion with Biological Information for Robot Interaction},\n year = {2017}\n}\n'}",,{'pages': '1589-1600'},10.0,Estimating Emotion with Biological Information for Robot Interaction,2017.0
2509,da2c7189b9f0e927d97a1afcfdab5eb4109ff52a,"Sixty-seven psychology faculty, graduates and undergraduates, were presented a silent videotape film of 5 depressed and 5 nondepressed psychiatric patients and were asked to identify which patients appeared depressed on the basis of nonverbal cues alone. Ratings of frequency and duration of eye contact were made by 2 raters independent of the investigator. Results showed depressed patients maintained eye contact for only about one-fourth the time that nondepressed patients did. In addition to the identification of eye conact as a nonverbal cue in depression, the mouth and angle of neck were also identified as salient nonverbal cues. Discussion focused on the value of nonverbal cues as a source of diagnostic information.","[{'authorId': '47421976', 'name': 'P. Waxer'}]",108.0,"{'bibtex': '@Article{Waxer1974NonverbalCF,\n author = {P. Waxer},\n journal = {Journal of abnormal psychology},\n pages = {\n          319-22\n        },\n title = {Nonverbal cues for depression.},\n volume = {83 3},\n year = {1974}\n}\n'}",,"{'volume': '83 3', 'pages': '\n          319-22\n        ', 'name': 'Journal of abnormal psychology'}",7.0,Nonverbal cues for depression.,1974.0
2510,da51344c09418aa79efb70a0ed3003faf722d71f,"Effects of Moral Concerns on Negotiations Eunkyung Kim (eunkyung@usc.edu) 1 , Morteza Dehghani (mdehghan@usc.edu) 1 , Yoo Kyoung Kim (yookyouk@usc.edu) 2 , Peter J. Carnevale (peter.carnevale@marshall.usc.edu) 2 , Jonathan Gratch (gratch@ict.usc.edu) 3 Brain Creativity Institute, University of Southern California, Los Angeles, CA 90089 USA Marshall School of Business, University of Southern California, Los Angeles, CA 90089 Institute for Creative Technologies, University of Southern California, Playa Vista, CA 90094 Abstract There is now considerable evidence that emotion plays an important role in negotiation. Emotions, such as anger and happiness, affect concession-making, not only in human vs. human negotiations but also in human vs. agent negotiations. Recent research has demonstrated the impact of emotional expressions in morally-charged negotiations. Thus, taking people’s moral concerns into account is crucial for building agents that operate in morally sensitive domains. This paper explores the interplay between people’s moral concerns, emotional expressions and concession-making during a morally charged negotiation. Our results demonstrate that participants who had stronger concerns for the Individualizing foundations (Harm and Fairness) make greater concessions for sacred negotiation items when faced with a sad opponent than an angry opponent. Also, we find that participants who had high Binding foundations (In-group, Authority and Purity) are more sensitive to social status, and make greater concessions in scenarios that involve agents in a higher social status. Keywords: Emotion; Moral Foundations Theory; Sacred values; Negotiation; Agent Modeling. Introduction With the growing interest in understanding the role of emotional expressions in negotiation (e.g., Barry, Fulmer & Goates, 2006; Van Kleef et al., 2010), many studies have investigated how emotional expression affects negotiation processes and outcomes (Ames & Johar, 2009; Choi et al., 2012; de Melo et al., 2014). For instance, negotiators concede more when their opponent expresses anger instead of happiness (Van Kleef, De Dreu & Manstead, 2004a, 2004b). Sinaceur & Tiedens (2006) further reveal that the effect of anger on concession works only when anger recipients have poor alternatives. The past works on emotional expression suggest that emotion plays an important role as a signal (e.g., anger indicates a negotiator’s dissatisfaction with his opponent’s offer). Furthermore, negotiators respond to emotional expressions depending on their own conditions (e.g., alternatives). Thus, it is important to understand what moderates a negotiator’s reaction to emotional expression. Past studies have shown that positive moods in negotiation foster concession-making (e.g., Carnevale & Isen, 1986). Mood effects may be mediated by expression of positive emotion, for example, a positive-mood induction procedure may lead negotiators to smile more and this smiling may have an impact on perceptions and concession- making. Regardless, the possible interaction of emotion and other variables, for example, cognition as in decision frame (Carnevale, 2008), or motivation as in moral concerns, is a domain highly worthy of inquiry. Although some studies have tried to understand how people’s innate personality interacts with their emotion during negotiation games (Bolton, Katok, & Zwick, 1998; Batson & Moran, 1999), little research has paid attention to how moral concerns impact reactions to emotional expressions and affect concession-making. Our recent research demonstrates that emotional expressions can potentially shift moral concerns during a negotiation, such that displays of anger would backfire if the negotiator associates moral significance to the objects of the negotiation, whereas displays of sadness promote higher concession-making (Dehghani, Gratch and Carnevale, 2012). Because morality significantly influences decision- making (e.g., Sjoberg & Winroth, 1986; Gintis et al., 2003), the present research aims to examine the role of people’s moral concerns on how they react to emotional expressions and make concessions. Adapting the Moral Foundations Theory (Haidt & Graham, 2007; Haidt & Joseph, 2007; Graham, 2013), we examine effects of two different types of foundations (i.e., Individualizing foundations and Binding foundations) on concession-making. We predict that people who have stronger Individualizing foundations would react more to emotional expressions because the Individualizing foundations indicate the tendency to care about other people’s emotions (whether others are emotionally or physically suffering, or being treated fairly) and therefore, that would effect their concession-making. On the other hand, we predict that people with stronger Binding foundations be more sensitive to their negotiation partner’s social status because Binding foundations indicate concern about other people’s roles in the group (whether negotiation partner is their boss or co-worker). Understanding the interaction between moral concerns and emotion are crucial in designing autonomous decision- making agents that operate in morally sensitive domains. Progress in agent research has enabled us to work closely with software agents in morally sensitive situations where agents’ actions may lead to significant results, such as loss of life (Tambe, 2011; Dehghani et al., 2013). Therefore, it is important to better understand the interactions between people’s moral concerns, emotion and agent decision- making strategies. Our results suggest that incorporating","[{'authorId': '47056370', 'name': 'Eunkyung Kim'}, {'authorId': '145707560', 'name': 'Morteza Dehghani'}, {'authorId': '17837204', 'name': 'Y. Kim'}, {'authorId': '48755211', 'name': 'P. Carnevale'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",4.0,"{'bibtex': '@Article{Kim2014EffectsOM,\n author = {Eunkyung Kim and Morteza Dehghani and Y. Kim and P. Carnevale and J. Gratch},\n journal = {Cognitive Science},\n title = {Effects of Moral Concerns on Negotiations},\n volume = {36},\n year = {2014}\n}\n'}",,"{'volume': '36', 'name': 'Cognitive Science'}",26.0,Effects of Moral Concerns on Negotiations,2014.0
2511,da58d554b6f7cc1bbe2e081d7d050733c79a90f8,"Abstract The ability and motivation to share attention is a unique aspect of human cognition. Despite its significance, the neural basis remains elusive. To investigate the neural correlates of joint attention, we developed a novel, interactive research paradigm in which participants' gaze behavior—as measured by an eye tracking device—was used to contingently control the gaze of a computer-animated character. Instructed that the character on screen was controlled by a real person outside the scanner, 21 participants interacted with the virtual other while undergoing fMRI. Experimental variations focused on leading versus following the gaze of the character when fixating one of three objects also shown on the screen. In concordance with our hypotheses, results demonstrate, firstly, that following someone else's gaze to engage in joint attention resulted in activation of anterior portion of medial prefrontal cortex (MPFC) known to be involved in the supramodal coordination of perceptual and cognitive processes. Secondly, directing someone else's gaze toward an object activated the ventral striatum which—in light of ratings obtained from participants—appears to underlie the hedonic aspects of sharing attention. The data, therefore, support the idea that other-initiated joint attention relies upon recruitment of MPFC previously related to the “meeting of minds.” In contrast, self-initiated joint attention leads to a differential increase of neural activity in reward-related brain areas, which might contribute to the uniquely human motivation to engage in the sharing of experiences.","[{'authorId': '2127424', 'name': 'L. Schilbach'}, {'authorId': '2746496', 'name': 'M. Wilms'}, {'authorId': '1717616', 'name': 'S. Eickhoff'}, {'authorId': '2307453', 'name': 'S. Romanzetti'}, {'authorId': '1871335', 'name': 'R. Tepest'}, {'authorId': '2487649', 'name': 'G. Bente'}, {'authorId': '108357144', 'name': 'N. Shah'}, {'authorId': '38644159', 'name': 'G. Fink'}, {'authorId': '2051580', 'name': 'K. Vogeley'}]",372.0,"{'bibtex': '@Article{Schilbach2010MindsMF,\n author = {L. Schilbach and M. Wilms and S. Eickhoff and S. Romanzetti and R. Tepest and G. Bente and N. Shah and G. Fink and K. Vogeley},\n journal = {Journal of Cognitive Neuroscience},\n pages = {2702-2715},\n title = {Minds Made for Sharing: Initiating Joint Attention Recruits Reward-related Neurocircuitry},\n volume = {22},\n year = {2010}\n}\n'}",,"{'volume': '22', 'pages': '2702-2715', 'name': 'Journal of Cognitive Neuroscience'}",58.0,Minds Made for Sharing: Initiating Joint Attention Recruits Reward-related Neurocircuitry,2010.0
2512,da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63,,"[{'authorId': '13477045', 'name': 'T. Moerland'}, {'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '1689001', 'name': 'C. Jonker'}]",141.0,"{'bibtex': '@Article{Moerland2017EmotionIR,\n author = {T. Moerland and J. Broekens and C. Jonker},\n journal = {Machine Learning},\n pages = {443 - 480},\n title = {Emotion in reinforcement learning agents and robots: a survey},\n volume = {107},\n year = {2017}\n}\n'}",,"{'volume': '107', 'pages': '443 - 480', 'name': 'Machine Learning'}",147.0,Emotion in reinforcement learning agents and robots: a survey,2017.0
2513,dab735dc0c0023cdc28715ab20257970e966bb46,"Empathy is often seen as the capacity to perceive, understand and experience others' emotions. This concept has been incorporated in virtual agents to achieve better believability, social interaction and user engagement. However, this has been mostly done to achieve empathic relations with the users. Instead, in this article we focus on empathy between synthetic characters and propose an analytical approach that consists in a generic computational model of empathy, supported by recent neuropsychological studies. The proposed model of empathy was implemented into an affective agent architecture. To evaluate the implementation a small scenario was defined and we asked a group of users to visualize it with the empathy model and another group to visualize it without the model. The results obtained confirmed that our model was capable of producing significant effects in the perception of the emergent empathic responses.","[{'authorId': '2997654', 'name': 'Sérgio Hortas Rodrigues'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '2151066261', 'name': 'João Dias'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",50.0,"{'bibtex': '@Article{Rodrigues2009ICF,\n author = {Sérgio Hortas Rodrigues and S. Mascarenhas and João Dias and Ana Paiva},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-7},\n title = {“I can feel it too!”: Emergent empathic reactions between synthetic characters},\n year = {2009}\n}\n'}",,"{'pages': '1-7', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",17.0,“I can feel it too!”: Emergent empathic reactions between synthetic characters,2009.0
2514,dabee61075cc07f57458f0abfd17855ddcd55518,,"[{'authorId': '2668280', 'name': 'C. Sidner'}, {'authorId': '2115445335', 'name': 'Christopher Lee'}, {'authorId': '34704262', 'name': 'Cory D. Kidd'}, {'authorId': '3012739', 'name': 'N. Lesh'}, {'authorId': '2246518871', 'name': 'Charles Rich'}]",547.0,"{'bibtex': '@Article{Sidner2005ExplorationsIE,\n author = {C. Sidner and Christopher Lee and Cory D. Kidd and N. Lesh and Charles Rich},\n journal = {Artif. Intell.},\n pages = {140-164},\n title = {Explorations in engagement for humans and robots},\n volume = {166},\n year = {2005}\n}\n'}",,"{'volume': '166', 'pages': '140-164', 'name': 'Artif. Intell.'}",74.0,Explorations in engagement for humans and robots,2005.0
2515,dad5698954e962c010aaa837049aa4a771e78e92,"Making roads safer by avoiding road collisions is one of the main reasons for inventing Autonomous vehicles (AVs). In this context, designing agent-based collision avoidance components of AVs which truly represent human cognition and emotions look is a more feasible approach as agents can replace human drivers. However, to the best of our knowledge, very few human emotion and cognition-inspired agent-based studies have previously been conducted in this domain. Furthermore, these agent-based solutions have not been validated using any key validation technique. Keeping in view this lack of validation practices, we have selected state-of-the-art Emotion Enabled Cognitive Agent (EEC_Agent), which was proposed to avoid lateral collisions between semi-AVs. The architecture of EEC_Agent has been revised using Exploratory Agent Based Modeling (EABM) level of the Cognitive Agent Based Computing (CABC) framework and real-time fear emotion generation mechanism using the Ortony, Clore & Collins (OCC) model has also been introduced. Then the proposed fear generation mechanism has been validated using the Validated Agent Based Modeling level of CABC framework using a Virtual Overlay MultiAgent System (VOMAS). Extensive simulation and practical experiments demonstrate that the Enhanced EEC_Agent exhibits the capability to feel different levels of fear, according to different traffic situations and also needs a smaller Stopping Sight Distance (SSD) and Overtaking Sight Distance (OSD) as compared to human drivers.","[{'authorId': '40611071', 'name': 'F. Riaz'}, {'authorId': '1795560', 'name': 'M. Niazi'}]",0.0,"{'bibtex': '@Article{Riaz2017ValidationOE,\n author = {F. Riaz and M. Niazi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Validation of Enhanced Emotion Enabled Cognitive Agent Using Virtual Overlay Multi-Agent System Approach},\n volume = {abs/1708.01628},\n year = {2017}\n}\n'}",[],"{'name': 'ArXiv', 'volume': 'abs/1708.01628'}",34.0,Validation of Enhanced Emotion Enabled Cognitive Agent Using Virtual Overlay Multi-Agent System Approach,2017.0
2516,dae68fe1d3c27a8b112e5fee54f0e436ca126825,"In online shopping environments, the product-advising function originally performed by salespeople is being increasingly taken over by software-based product recommendation agents (PRAs). However, the literature has mostly focused on the functionality design and utilitarian value of such decision support systems, mostly ignoring the potential social influence they could exert on their users. The objective of this study is to apply a social relationship perspective to the design of interfaces for PRAs. We investigate the effects of applying anthropomorphic interfaces—namely, humanoid embodiment and voice output—on users' perceived social relationship with a technological and software-based artifact designed for electronic commerce contexts. The findings from a laboratory experiment indicate that using humanoid embodiment and human voice-based communication significantly influences users' perceptions of social presence, which in turn enhances users' trusting beliefs, perceptions of enjoyment, and ultimately, their intentions to use the agent as a decision aid. These results extend the applicability of theories concerning traditional shopper-salesperson relationships to customers' interactions with technological artifacts residing on Web sites—that is, the recommendation agent software—and provide practitioners with guidelines on how to design Internet stores with the goal of building social relationships with online shoppers and enhancing their overall shopping experiences.","[{'authorId': '1714338', 'name': 'Lingyun Qiu'}, {'authorId': '1732947', 'name': 'I. Benbasat'}]",500.0,"{'bibtex': '@Article{Qiu2009EvaluatingAP,\n author = {Lingyun Qiu and I. Benbasat},\n journal = {Journal of Management Information Systems},\n pages = {145 - 182},\n title = {Evaluating Anthropomorphic Product Recommendation Agents: A Social Relationship Perspective to Designing Information Systems},\n volume = {25},\n year = {2009}\n}\n'}",,"{'volume': '25', 'pages': '145 - 182', 'name': 'Journal of Management Information Systems'}",136.0,Evaluating Anthropomorphic Product Recommendation Agents: A Social Relationship Perspective to Designing Information Systems,2009.0
2518,db01283ce203a53914b833e65c8c608e17b3501e,"This paper introduces MARCO, a hybrid, chess playing agent equipped with a custom-built robotic arm and an emotionally expressive, virtual face presented on a small, servo-controlled display. MARCO was built to investigate the hypothesis that hybrid systems capable of displaying emotions make playing chess more personal and enjoyable. In addition, it is our aim to realize emotional contagion between man and machine in that the agent has the power to influence the human player on an emotional level and vice versa. The hardware components consist of eight Dynamixel servos, an Arduino-based control board, a 5.6 inch display, and a DGT chessboard. The software components run concurrently as separate processes. The main components are the virtual agent framework MARC, the WASABI Affect Simulation architecture, and the TSCP chess engine.","[{'authorId': '1403827243', 'name': 'C. Becker-Asano'}, {'authorId': '121893527', 'name': 'Eduardo A. L. Meneses'}, {'authorId': '2123331', 'name': 'Nicolas Riesterer'}, {'authorId': '2828438', 'name': 'J. Hué'}, {'authorId': '2573148', 'name': 'C. Dornhege'}, {'authorId': '145304209', 'name': 'B. Nebel'}]",8.0,"{'bibtex': '@Article{Becker-Asano2014TheHA,\n author = {C. Becker-Asano and Eduardo A. L. Meneses and Nicolas Riesterer and J. Hué and C. Dornhege and B. Nebel},\n journal = {Proceedings of the second international conference on Human-agent interaction},\n title = {The hybrid agent MARCO: a multimodal autonomous robotic chess opponent},\n year = {2014}\n}\n'}",,{'name': 'Proceedings of the second international conference on Human-agent interaction'},8.0,The hybrid agent MARCO: a multimodal autonomous robotic chess opponent,2014.0
2519,db097eac96dd0ce5b7874f9ae74306fac5b0b2df,1. Introduction 2. On the way to the planning theory 3. Plans and practical reasoning 4. Agent rationality: toward a general theory 5. Reconsideration and rationality 6. Agent rationality: the historical theory 7. Commitment revisited 8. Two faces of intention 9. Acting with an intention 10. Intention and expected side effects 11. Conclusion Bibliography Notes Index.,"[{'authorId': '49774672', 'name': 'Hugh Mccann'}, {'authorId': '70049329', 'name': 'M. Bratman'}]",2767.0,"{'bibtex': '@Inproceedings{Mccann1991IntentionPA,\n author = {Hugh Mccann and M. Bratman},\n title = {Intention, Plans, and Practical Reason},\n year = {1991}\n}\n'}",,"{'volume': '', 'name': ''}",3.0,"Intention, Plans, and Practical Reason",1991.0
2521,db20e05c8bbed7e6d1dd2220979d6b22ff5b82b4,,"[{'authorId': '2789205', 'name': 'Andry Chowanda'}, {'authorId': '1795102', 'name': 'Martin Flintham'}, {'authorId': '35896384', 'name': 'P. Blanchfield'}, {'authorId': '1795528', 'name': 'M. Valstar'}]",23.0,"{'bibtex': '@Inproceedings{Chowanda2016PlayingWS,\n author = {Andry Chowanda and Martin Flintham and P. Blanchfield and M. Valstar},\n pages = {85-95},\n title = {Playing with Social and Emotional Game Companions},\n year = {2016}\n}\n'}",,{'pages': '85-95'},15.0,Playing with Social and Emotional Game Companions,2016.0
2522,db414273c23e3663315e301b9bd5f651348d3620,"Human interactions are replete with emotional exchanges, and hence, the ability to decode others’ emotional expressions is of great importance. The present research distinguishes between the emotional signal (the intended emotion) and noise (perception of secondary emotions) in social emotion perception and investigates whether these predict the quality of social interactions. In three studies, participants completed laboratory-based assessments of emotion recognition ability and later reported their perceptions of naturally occurring social interactions. Overall, noise perception in the recognition task was associated with perceiving more negative emotions in others and perceiving interactions more negatively. Conversely, signal perception of facial emotion expressions was associated with higher quality in social interactions. These effects were moderated by relationship closeness in Greece but not in Germany. These findings suggest that emotion recognition as assessed in the laboratory is a valid predictor of social interaction quality. Thus, emotion recognition generalizes from the laboratory to everyday life.","[{'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '1870489', 'name': 'K. Kafetsios'}, {'authorId': '47371862', 'name': 'Heidi Mauersberger'}, {'authorId': '4856544', 'name': 'Christophe Blaison'}, {'authorId': '117313403', 'name': 'Carolin-Louisa Kessler'}]",24.0,"{'bibtex': '@Article{Hess2016SignalAN,\n author = {U. Hess and K. Kafetsios and Heidi Mauersberger and Christophe Blaison and Carolin-Louisa Kessler},\n journal = {Personality and Social Psychology Bulletin},\n pages = {1092 - 1110},\n title = {Signal and Noise in the Perception of Facial Emotion Expressions},\n volume = {42},\n year = {2016}\n}\n'}",,"{'volume': '42', 'pages': '1092 - 1110', 'name': 'Personality and Social Psychology Bulletin'}",62.0,Signal and Noise in the Perception of Facial Emotion Expressions,2016.0
2523,db549199bf242797dfef3d8b3c8054e904eaacb9,,"[{'authorId': '2679929', 'name': 'D. Artz'}, {'authorId': '145526918', 'name': 'Y. Gil'}]",794.0,"{'bibtex': '@Article{Artz2007ASO,\n author = {D. Artz and Y. Gil},\n journal = {J. Web Semant.},\n pages = {58-71},\n title = {A survey of trust in computer science and the Semantic Web},\n volume = {5},\n year = {2007}\n}\n'}",,"{'volume': '5', 'pages': '58-71', 'name': 'J. Web Semant.'}",141.0,A survey of trust in computer science and the Semantic Web,2007.0
2524,db6567a225f592fd88f734a5fa9a5bf6d1707cad,"er for as long as possible using noisy continuous acoustic interaction. Evolved dynamical recurrent neural networks are used as the control architecture. Acoustic coupling poses nontrivial problems like discriminating ’self’ from ’non-self’ and structuring production of signals in time so as to minimize interference. Detailed observation of the most frequently evolved behavioral strategy shows that interacting agents perform rhythmic signals leading to the coordination of movement. During coordination, signals become entrained in an anti-phase mode that resembles turn-taking. Perturbation techniques show that signalling behavior not only performs an external function, but it is also integrated into the movement of the producing agent, thus showing the difficulty of separating behavior into social and non-social classes. Structural congruence between agents is shown by exploring internal dynamics as well as the response of single agents in the presence of signalling beacons that reproduce the signal patterns of the interacting agents. Lack of entrain-","[{'authorId': '117803990', 'name': 'E. D. Di Paolo'}]",15.0,"{'bibtex': '@Article{Paolo2000BehavioralCS,\n author = {E. D. Di Paolo},\n journal = {Adaptive Behavior},\n pages = {27 - 48},\n title = {Behavioral Coordination, Structural Congruence and Entrainment in a Simulation of Acoustically Coupled Agents},\n volume = {8},\n year = {2000}\n}\n'}",,"{'volume': '8', 'pages': '27 - 48', 'name': 'Adaptive Behavior'}",38.0,"Behavioral Coordination, Structural Congruence and Entrainment in a Simulation of Acoustically Coupled Agents",2000.0
2525,db6d367f43ec2b92439aa0922cc00367f8c9bef1,"Previous work on social categorization has shown that people often use cues such as a person's gender, age, or ethnicity to categorize and form impressions of others. The present research investigated effects of social category membership on the evaluation of humanoid robots. More specifically, participants rated a humanoid robot that either belonged to their in-group or to a national out-group with regard to anthropomorphism (e.g., mind attribution, warmth), psychological closeness, contact intentions, and design. We predicted that participants would show an in-group bias towards the robot that ostensibly belonged to their in-group--as indicated by its name and location of production. In line with our hypotheses, participants not only rated the in-group robot more favourably--importantly, they also anthropomorphized it more strongly than the out-group robot. Our findings thus document that people even apply social categorization processes and subsequent differential social evaluations to robots.","[{'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '2724558', 'name': 'Dieta Kuchenbrandt'}]",245.0,"{'bibtex': '@Article{Eyssel2012SocialCO,\n author = {F. Eyssel and Dieta Kuchenbrandt},\n journal = {The British journal of social psychology},\n pages = {\n          724-31\n        },\n title = {Social categorization of social robots: anthropomorphism as a function of robot group membership.},\n volume = {51 4},\n year = {2012}\n}\n'}",,"{'volume': '51 4', 'pages': '\n          724-31\n        ', 'name': 'The British journal of social psychology'}",19.0,Social categorization of social robots: anthropomorphism as a function of robot group membership.,2012.0
2526,db9a21ad723986536e514b1c0a5aaba6a5af9a58,,"[{'authorId': '1747703', 'name': 'Matt Huenerfauth'}]",13.0,"{'bibtex': '@Inproceedings{Huenerfauth2009ImprovingSR,\n author = {Matt Huenerfauth},\n pages = {530-539},\n title = {Improving Spatial Reference in American Sign Language Animation through Data Collection from Native ASL Signers},\n year = {2009}\n}\n'}",,{'pages': '530-539'},24.0,Improving Spatial Reference in American Sign Language Animation through Data Collection from Native ASL Signers,2009.0
2527,db9bd8f9032dd4ab418199f1c137236acdb0c30d,,"[{'authorId': '48051692', 'name': 'J. Piaget'}]",86.0,"{'bibtex': '@Article{Piaget1962TheRO,\n author = {J. Piaget},\n journal = {Bulletin of the Menninger Clinic},\n pages = {\n          129-37\n        },\n title = {The relation of affectivity to intelligence in the mental development of the child.},\n volume = {26},\n year = {1962}\n}\n'}",,"{'volume': '26', 'pages': '\n          129-37\n        ', 'name': 'Bulletin of the Menninger Clinic'}",0.0,The relation of affectivity to intelligence in the mental development of the child.,1962.0
2528,dc0930139eeb93b25e03abbc04fa8d7b4127af9a,"—The pace of life is becoming faster. As a result, some people are too busy to communicate with their families. In this paper, we propose an Augmented Reality (AR) system using smartphone. The system allows us to leave emotional notes in the real environment using Virtual-Agent. The crucial part of the system focuses on the emotional short voice message exchange with the Virtual-Agents. Spatial note system is based on two parts, one is the virtual agent services and the other is the AR system. The virtual agent services allow users to make a voice message by recording user’s voice. Then, a Virtual-Agent with the appropriate facial expression is generated. User can also change the facial expressions as he likes. There are 4 emotions of the virtual agents: happy, sorrow, angry and calm. Each emotion has 4 levels to express. The AR system will detect planes from the smartphone view of the real world. Users can put the Virtual- Agent anywhere on a plane.","[{'authorId': '2181745962', 'name': 'Puxuan Qu'}, {'authorId': '144705533', 'name': 'J. Tanaka'}]",0.0,"{'bibtex': '@Inproceedings{Qu2019SpatialNS,\n author = {Puxuan Qu and J. Tanaka},\n title = {Spatial Note System Using Virtual Agent to Enhance Family Connection},\n year = {2019}\n}\n'}",[],,14.0,Spatial Note System Using Virtual Agent to Enhance Family Connection,2019.0
2529,dc2319bc1705b955e11b38e4c3100933e336d8c1,"Using a dynamic stimuli paradigm, in which faces expressed either happiness or anger, the authors tested the hypothesis that perceptions of trustworthiness are related to these expressions. Although the same emotional intensity was added to both trustworthy and untrustworthy faces, trustworthy faces who expressed happiness were perceived as happier than untrustworthy faces, and untrustworthy faces who expressed anger were perceived as angrier than trustworthy faces. The authors also manipulated changes in face trustworthiness simultaneously with the change in expression. Whereas transitions in face trustworthiness in the direction of the expressed emotion (e.g., high-to-low trustworthiness and anger) increased the perceived intensity of the emotion, transitions in the opposite direction decreased this intensity. For example, changes from high to low trustworthiness increased the intensity of perceived anger but decreased the intensity of perceived happiness. These findings support the hypothesis that changes along the trustworthiness dimension correspond to subtle changes resembling expressions signaling whether the person displaying the emotion should be avoided or approached.","[{'authorId': '1896078', 'name': 'N. Oosterhof'}, {'authorId': '145441940', 'name': 'A. Todorov'}]",292.0,"{'bibtex': '@Article{Oosterhof2009SharedPB,\n author = {N. Oosterhof and A. Todorov},\n journal = {Emotion},\n pages = {\n          128-33\n        },\n title = {Shared perceptual basis of emotional expressions and trustworthiness impressions from faces.},\n volume = {9 1},\n year = {2009}\n}\n'}",,"{'volume': '9 1', 'pages': '\n          128-33\n        ', 'name': 'Emotion'}",28.0,Shared perceptual basis of emotional expressions and trustworthiness impressions from faces.,2009.0
2530,dc30b1e64f33bf3c806eca6b2ed2bd13c72e7acd,,"[{'authorId': '2086718386', 'name': 'J. Holmes'}]",145.0,"{'bibtex': '@Article{Holmes2007DesigningAT,\n author = {J. Holmes},\n journal = {Comput. Educ.},\n pages = {523-547},\n title = {Designing agents to support learning by explaining},\n volume = {48},\n year = {2007}\n}\n'}",,"{'volume': '48', 'pages': '523-547', 'name': 'Comput. Educ.'}",42.0,Designing agents to support learning by explaining,2007.0
2531,dc6a5d898996f2c221752806aeab449c24c9b1b7,"Emotions seem to come and go as they please. However, we actually hold considerable sway over our emotions: We influence which emotions we have and how we experience and express these emotions. The process model of emotion regulation described here suggests that how we regulate our emotions matters. Regulatory strategies that act early in the emotion-generative process should have quite different outcomes than strategies that act later. This review focuses on two widely used strategies for down-regulating emotion. The first, reappraisal, comes early in the emotion-generative process. It consists of changing how we think about a situation in order to decrease its emotional impact. The second, suppression, comes later in the emotion-generative process. It involves inhibiting the outward signs of emotion. Theory and research suggest that reappraisal is more effective than suppression. Reappraisal decreases the experience and behavioral expression of emotion, and has no impact on memory. By contrast, suppression decreases behavioral expression, but fails to decrease the experience of emotion, and actually impairs memory. Suppression also increases physiological responding in both the suppressors and their social partners.","[{'authorId': '1775321', 'name': 'J. Gross'}]",1215.0,"{'bibtex': '@Article{Gross2001EmotionRI,\n author = {J. Gross},\n journal = {Current Directions in Psychological Science},\n pages = {214 - 219},\n title = {Emotion Regulation in Adulthood: Timing Is Everything},\n volume = {10},\n year = {2001}\n}\n'}",,"{'volume': '10', 'pages': '214 - 219', 'name': 'Current Directions in Psychological Science'}",20.0,Emotion Regulation in Adulthood: Timing Is Everything,2001.0
2534,dc786d29e27da3604335b04b51c42e9da92df2e0,,"[{'authorId': '145968463', 'name': 'M. S. Erden'}]",61.0,"{'bibtex': '@Article{Erden2013EmotionalPF,\n author = {M. S. Erden},\n journal = {International Journal of Social Robotics},\n pages = {441-456},\n title = {Emotional Postures for the Humanoid-Robot Nao},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '441-456', 'name': 'International Journal of Social Robotics'}",36.0,Emotional Postures for the Humanoid-Robot Nao,2013.0
2535,dcac94878ae242b5e0379cac6e85fecb07cf314c,"Working memory is one of the cognitive functions that is the most sensitive to normal and pathological age-related effects. In older individuals with a mild cognitive impairment, deficits in working memory are frequent and can precede those of episodic memory, in addition to having a strong prognostic value of evolution toward a dementia of Alzheimer type. Because of its implication in numerous cognitive and cognitive-motor tasks, working memory is called upon in a wide range of daily life activities. Impairment in working memory therefore increases the risk of a loss of autonomy. In the current review, we present different working memory training programs. We show how these training programs are associated with specific effects and to near and far transfers towards other cognitive functions in older adults without cognitive impairment or with mild cognitive impairment, as well as in patients with dementia. We show that the benefits are confirmed by neuronal modifications, suggesting an improvement in the neuronal efficiency of the targeted or related trained processes. Finally, we consider the central question of the generalization of the cognitive gains of working memory training toward ecological situations.","[{'authorId': '1753261158', 'name': 'Marine Saba'}, {'authorId': '39841320', 'name': 'S. Blanchet'}]",1.0,"{'bibtex': '@Article{Saba2020WorkingMT,\n author = {Marine Saba and S. Blanchet},\n journal = {Geriatrie et psychologie neuropsychiatrie du vieillissement},\n pages = {\n          187-195\n        },\n title = {Working memory training in normal and pathological aging: neurocognitive gains and generalization.},\n volume = {18 2},\n year = {2020}\n}\n'}",,"{'volume': '18 2', 'pages': '\n          187-195\n        ', 'name': 'Geriatrie et psychologie neuropsychiatrie du vieillissement'}",0.0,Working memory training in normal and pathological aging: neurocognitive gains and generalization.,2020.0
2536,dcd036f4f3d7279d79014f89711af63232e1ca82,"Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.","[{'authorId': '48520635', 'name': 'Xuesong Yang'}, {'authorId': '1725643', 'name': 'Yun-Nung (Vivian) Chen'}, {'authorId': '1395813836', 'name': 'Dilek Z. Hakkani-Tür'}, {'authorId': '34963487', 'name': 'Paul A. Crook'}, {'authorId': '47058148', 'name': 'Xiujun Li'}, {'authorId': '1800422', 'name': 'Jianfeng Gao'}, {'authorId': '144718788', 'name': 'L. Deng'}]",77.0,"{'bibtex': '@Article{Yang2016EndtoendJL,\n author = {Xuesong Yang and Yun-Nung (Vivian) Chen and Dilek Z. Hakkani-Tür and Paul A. Crook and Xiujun Li and Jianfeng Gao and L. Deng},\n journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {5690-5694},\n title = {End-to-end joint learning of natural language understanding and dialogue manager},\n year = {2016}\n}\n'}",,"{'pages': '5690-5694', 'name': '2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}",31.0,End-to-end joint learning of natural language understanding and dialogue manager,2016.0
2537,dce799f4f4844828f78969ef0c53d1fbbc9bb370,"People behave differently in the presence of other people than they do when they are alone. People also may behave differently when designers introduce more human-like qualities into computer interfaces. In an experimental study we demonstrate that people's responses to a talking-face interface differ from their responses to a text-display interface. They attribute some personality traits to it; they are more aroused by it; they present themselves in a more positive light. We use theories of person perception, social facilitation, and self-presentation to predict and interpret these results. We suggest that as computer interfaces become more ""human-like,"" people who use those interfaces may change their own personas in response to them.","[{'authorId': '2198165', 'name': 'L. Sproull'}, {'authorId': '2080666', 'name': 'M. Subramani'}, {'authorId': '47198673', 'name': 'S. Kiesler'}, {'authorId': '2110484995', 'name': 'Janet H. Walker'}, {'authorId': '150179849', 'name': 'Keith Waters'}]",429.0,"{'bibtex': '@Article{Sproull1996WhenTI,\n author = {L. Sproull and M. Subramani and S. Kiesler and Janet H. Walker and Keith Waters},\n journal = {Hum. Comput. Interact.},\n pages = {97-124},\n title = {When the Interface Is a Face},\n volume = {11},\n year = {1996}\n}\n'}",,"{'volume': '11', 'pages': '97-124', 'name': 'Hum. Comput. Interact.'}",54.0,When the Interface Is a Face,1996.0
2539,dd142a9b8cef8aefa7be73b2158bfb3867a60686,,"[{'authorId': '80562459', 'name': 'Bushra Kidwai'}, {'authorId': '67341321', 'name': 'R. Nadesh'}]",20.0,"{'bibtex': '@Article{Kidwai2020DesignAD,\n author = {Bushra Kidwai and R. Nadesh},\n journal = {Procedia Computer Science},\n pages = {75-84},\n title = {Design and Development of Diagnostic Chabot for supporting Primary Health Care Systems},\n volume = {167},\n year = {2020}\n}\n'}",,"{'volume': '167', 'pages': '75-84', 'name': 'Procedia Computer Science'}",11.0,Design and Development of Diagnostic Chabot for supporting Primary Health Care Systems,2020.0
2540,dd178759c55036b982d71a5013381fc4b3d8f492,"In this paper, a theoretical model is proposed to account for the incorporation of critical social influence effects appropriate to the design and implementation of collaborative virtual environments. The proposed social influence threshold model provides a framework for both substantive basic and applied research, as well as a way of determining functional specifications for implementing behavioral realism within collaborative virtual environments and testing the social interactive functionality of them.","[{'authorId': '2307657', 'name': 'J. Blascovich'}]",101.0,"{'bibtex': '@Inproceedings{Blascovich2002ATM,\n author = {J. Blascovich},\n pages = {25-30},\n title = {A theoretical model of social influence for increasing the utility of collaborative virtual environments},\n year = {2002}\n}\n'}",,{'pages': '25-30'},12.0,A theoretical model of social influence for increasing the utility of collaborative virtual environments,2002.0
2541,dd22e6f27c71c1226e8c6b2d3379d140afe3ebc7,"Contents: R. Clifton, P. Berman, Preface. P.J. Long, R.F. Simons, M.T.Balaban, Part I:Current Investigations of the Classical Theory of Orienting and Defense. E. Sokolov, J. Cacioppo, Orienting and Defense Reflexes: Vector Coding the Cardiac Response. D. Siddle, O. Lipp, Orienting, Habituation and Information Processing: The Effects of Omission, the Role of Expectancy, and the Problem of Dishabituation. Part II:Biological and Evolutionary Foundations of Orienting, Startle, and Defense: Motivational and Emotional Factors That Modulate Attention. B. Campbell et al., Origins of Orienting and Defensive Responses: An Evolutionary Perspective. M. Davis, The Neurophysiological Basis of Acoustic Startle Modulation: Research on Fear Motivation and Sensory Gating. P. Lang, M. Bradley, B. Cutbert, Motivated Attention: Affect, Activation, and Action. E. Cook, G. Turpin, Differentiating Orienting, Startle, and Defense Responses: The Role of Affect and Its Implications for Psychopathology. A. Ohman, As Fast as the Blink of an Eye: Evolutionary Preparedness for Preattentive Processing of Threat. Part III:Startle Reflex and Electro-Cortical Studies of Attention and Stimulus Gating. H. Hoffman, Attentional Factors in the Elicitation and Modification of the Startle Reaction. S. Hackley, A.J.W. Boelhouwer, The More or Less Startling Effects of Weak Prestimulation -- Revisited: Prepulse Modulation of Multicomponent Blink Reflexes. R.F. Simons, W.M. Perlstein, A Tale of Two Reflexes: An ERP Analysis of Prepulse Inhibition and Orienting. M.E. Dawson, A.M. Schell, N.R. Swerdlow, D.L. Filion, Cognitive, Clinical and Neurophysiological Implications of Startle Modification. C.H.M. Brunia, Gating in Readiness. R. Naatanen, R. Ilmoniemi, K. Alho, Magnetoencephalography in Studies of Attention. Part IV:Studies of Attention, Affect, and Action in Child Development. M. Posner, M.K. Rothbart, G. Gerardi, L. Thomas-Thrapp, Functions of Orienting in Early Infancy. K. Berg, J. Richards, Attention Across Time in Infant Development. M.T. Balaban, N. Snidman, J. Kagan, Attention, Emotion, and Reactivity in Infancy and Early Childhood. J. Campos, R. Kermoian, D. Witherington, H. Chen, Q. Dong, Activity, Attention, and Developmental Transitions in Infancy. F.K. Graham, Afterword: Preattentive Processing and Passive and Active Attention.","[{'authorId': '143853826', 'name': 'P. Lang'}, {'authorId': '2567266', 'name': 'R. Simons'}, {'authorId': '39779943', 'name': 'M. Balaban'}]",1027.0,"{'bibtex': '@Inproceedings{Lang1997AttentionAO,\n author = {P. Lang and R. Simons and M. Balaban},\n title = {Attention and Orienting : Sensory and Motivational Processes},\n year = {1997}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Attention and Orienting : Sensory and Motivational Processes,1997.0
2542,dd287421a471025304bcbba1e92a5b55884f211f,"Part 1. Overview of Attachment Theory. Cassidy, The Nature of the Child's Ties. Kobak, Madsen, Disruptions in Attachment Bonds: Implications for Theory, Research, and Clinical Intervention. Shaver, Fraley, Attachment, Loss, and Grief: Bowlby's Views and Current Controversies. Weinfield, Sroufe, Egeland, Carlson, Individual Differences in Infant-caregiver Attachment: Conceptual and Empirical Aspects of Security. Bretherton, Munholland, Internal Working Models in Attachment Relationships: Elaborating a Central Construct in Attachment Theory. Part 2. Biological Perspectives. Simpson, Belsky, Attachment Theory within a Modern Evolutionary Framework. Polan, Hofer, Psychobiological Origins of Infant Attachment and Its Role in Development. Suomi, Attachment in Rhesus Monkeys. Vaughn, Bost, van IJzendoorn, Attachment and Temperament: Additive and Interactive Influences on Behavior, Affect, and Cognition During Infancy and Childhood. Fox, Hane, Studying the Biology of Human Attachment. Coan, Toward a Neuroscience of Attachment. Part 3. Attachment in Infancy and Childhood. Marvin, Britner, Normative Development: The Ontogeny of Attachment. Belsky, Fearon, Precursors of Attachment Security. Howes, Spieker, Attachment Relationships in the Context of Multiple Caregivers. Berlin, Cassidy, Appleyard, The Influence of Early Attachments on Other Relationships. Thompson, Early Attachment and Later Development: Familiar Questions, New Answers. Kerns, Attachment in Middle Childhood. Solomon, George, The Measurement of Attachment Security in Infancy and Early Childhood. Part 4. Attachment in Adolescence and Adulthood. Allen, The Attachment System in Adolescence. Zeifman, Hazan, Pair Bonds as Attachments: Reevaluating the Evidence. Feeney, Adult Romantic Attachment: Developments in the Study of Couple Relationships. Mohr, Same-sex Romantic Attachment. Mikulincer, Shaver, Adult Attachment and Affect Regulation. Magai, Attachment in Middle and Later Life. Hesse, The Adult Attachment Interview: Protocol, Method of Analysis, and Empirical Studies. Crowell, Fraley, Shaver, Measurement of Individual Differences in Adolescent and Adult Attachment. Part 5. Psychopathology and Clinical Applications of Attachment Theory and Research. DeKlyen, Greenberg, Attachment and Psychopathology in Childhood. Lyons-Ruth, Jacobvitz, Attachment Disorganization: Genetic Factors, Parenting Contexts, and Developmental Transformation from Infancy to Adulthood. Dozier, Rutter, Challenges to the Development of Attachment Relationships Faced by Young Children in Foster and Adoptive Care. Dozier, Stovall-McClough, Albus, Attachment and Psychopathology in Adulthood. Berlin, Zeanah, Lieberman, Prevention and Intervention Programs for Supporting Early Attachment Security. Slade, The Implications of Attachment Theory and Research for Adult Psychotherapy: Research and Clinical Perspectives. Fonagy, Gergely, Target, Psychoanalytic Theory from the Viewpoint of Attachment Theory and Research. Johnson, Couple and Family Therapy: An Attachment Perspective. Part 6. Systems, Culture, and Context. George, Solomon, The Caregiving System: A Behavioral Systems Approach to Parenting. K. Grossmann, K. E. Grossmann, Kindler, Zimmermann, A Wider View of Attachment and Exploration: The Influence of Mothers and Fathers on the Development of Psychological Security from Infancy to Young Adulthood. van IJzendoorn, Sagi-Schwartz, Cross-Cultural Patterns of Attachment: Universal and Contextual Dimensions. Granqvist, Kirkpatrick, Attachment and Religious Representations and Behavior. Feeney, Monin, An Attachment-Theoretical Perspective on Divorce. Rutter, Implications of Attachment Theory and Research for Child Care Policy.","[{'authorId': '35669149', 'name': 'J. Cassidy'}, {'authorId': '4509891', 'name': 'P. Shaver'}]",2775.0,"{'bibtex': '@Inproceedings{Cassidy1999HandbookOA,\n author = {J. Cassidy and P. Shaver},\n title = {Handbook of attachment : theory, research, and clinical applications},\n year = {1999}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Handbook of attachment : theory, research, and clinical applications",1999.0
2543,dd41f4bcfe6c37140deb8f56c27fd8daa647614e,,"[{'authorId': '34654263', 'name': 'W. C. Ho'}, {'authorId': '2055786828', 'name': 'Scott Watson'}]",29.0,"{'bibtex': '@Inproceedings{Ho2006AutobiographicKF,\n author = {W. C. Ho and Scott Watson},\n pages = {383-394},\n title = {Autobiographic Knowledge for Believable Virtual Characters},\n year = {2006}\n}\n'}",,{'pages': '383-394'},42.0,Autobiographic Knowledge for Believable Virtual Characters,2006.0
2544,dd512c4c26272b14287c9a696a0c988673314261,,"[{'authorId': '144426526', 'name': 'Maha Salem'}, {'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '2296579', 'name': 'K. Rohlfing'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1759554', 'name': 'F. Joublin'}]",321.0,"{'bibtex': '@Article{Salem2013ToEI,\n author = {Maha Salem and F. Eyssel and K. Rohlfing and S. Kopp and F. Joublin},\n journal = {International Journal of Social Robotics},\n pages = {313-323},\n title = {To Err is Human(-like): Effects of Robot Gesture on Perceived Anthropomorphism and Likability},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'pages': '313-323', 'name': 'International Journal of Social Robotics'}",34.0,To Err is Human(-like): Effects of Robot Gesture on Perceived Anthropomorphism and Likability,2013.0
2545,ddbdde502c1f8260ed9758bf8462513143a7d8ba,"Background Web-based cognitive-behavioral therapeutic (CBT) apps have demonstrated efficacy but are characterized by poor adherence. Conversational agents may offer a convenient, engaging way of getting support at any time. Objective The objective of the study was to determine the feasibility, acceptability, and preliminary efficacy of a fully automated conversational agent to deliver a self-help program for college students who self-identify as having symptoms of anxiety and depression. Methods In an unblinded trial, 70 individuals age 18-28 years were recruited online from a university community social media site and were randomized to receive either 2 weeks (up to 20 sessions) of self-help content derived from CBT principles in a conversational format with a text-based conversational agent (Woebot) (n=34) or were directed to the National Institute of Mental Health ebook, “Depression in College Students,” as an information-only control group (n=36). All participants completed Web-based versions of the 9-item Patient Health Questionnaire (PHQ-9), the 7-item Generalized Anxiety Disorder scale (GAD-7), and the Positive and Negative Affect Scale at baseline and 2-3 weeks later (T2). Results Participants were on average 22.2 years old (SD 2.33), 67% female (47/70), mostly non-Hispanic (93%, 54/58), and Caucasian (79%, 46/58). Participants in the Woebot group engaged with the conversational agent an average of 12.14 (SD 2.23) times over the study period. No significant differences existed between the groups at baseline, and 83% (58/70) of participants provided data at T2 (17% attrition). Intent-to-treat univariate analysis of covariance revealed a significant group difference on depression such that those in the Woebot group significantly reduced their symptoms of depression over the study period as measured by the PHQ-9 (F=6.47; P=.01) while those in the information control group did not. In an analysis of completers, participants in both groups significantly reduced anxiety as measured by the GAD-7 (F1,54= 9.24; P=.004). Participants’ comments suggest that process factors were more influential on their acceptability of the program than content factors mirroring traditional therapy. Conclusions Conversational agents appear to be a feasible, engaging, and effective way to deliver CBT.","[{'authorId': '2194363', 'name': 'K. Fitzpatrick'}, {'authorId': '6120000', 'name': 'Alison M Darcy'}, {'authorId': '14061540', 'name': 'Molly Vierhile'}]",1075.0,"{'bibtex': '@Article{Fitzpatrick2017DeliveringCB,\n author = {K. Fitzpatrick and Alison M Darcy and Molly Vierhile},\n journal = {JMIR Mental Health},\n title = {Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial},\n volume = {4},\n year = {2017}\n}\n'}",,"{'volume': '4', 'name': 'JMIR Mental Health'}",35.0,Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial,2017.0
2547,dddb97ae469f9f7319644cfae34f6b06a1006295,,"[{'authorId': '2250960427', 'name': 'Edmund C. Sanford'}, {'authorId': '8300603', 'name': 'E. Titchener'}]",129.0,"{'bibtex': '@Article{Sanford1901ExperimentalPA,\n author = {Edmund C. Sanford and E. Titchener},\n journal = {The Philosophical Review},\n pages = {424},\n title = {Experimental Psychology: A Manual of Laboratory Practice.},\n volume = {10},\n year = {1901}\n}\n'}",,"{'volume': '10', 'pages': '424', 'name': 'The Philosophical Review'}",0.0,Experimental Psychology: A Manual of Laboratory Practice.,1901.0
2548,dddd9dc2a0830267090c829bf2aca9486b845417,"Despite widely differing methodologies, previous studies on affect (emotion as represented in language) have often obtained what appear to be the same basic dimensions. Whether these similarly named dimensions from different methodologies are actually equivalent was tested here by intercorrelating dimensions obtained from multidimensional scaling, successive-intervals scaling, semantic differential scaling, and factor analysis of verbal self-report data. Results strongly supported the convergent validity of pleasure-displeasure and degree of arousal, but were equivocal on additional dimensions. Separate multidimensional scalings of pleasant, intermediate, and unpleasant affect terms (a) confirmed the presence of an arousal dimension at each level of pleasure and (b) obtained three additional dimensions: control/potency/dominance, depth of experience, and locus of causation. These three dimensions were interpreted as describing not the emotion per se but rather beliefs about the antecedents or consequences of the emotion. In a final study, internal versus external locus of causation was shown to be reliably decoded from emotion-denoting words.","[{'authorId': '46367714', 'name': 'J. Russell'}]",498.0,"{'bibtex': '@Article{Russell1978EvidenceOC,\n author = {J. Russell},\n journal = {Journal of Personality and Social Psychology},\n pages = {1152-1168},\n title = {Evidence of Convergent Validity on the Dimensions of Affect},\n volume = {36},\n year = {1978}\n}\n'}",,"{'volume': '36', 'pages': '1152-1168', 'name': 'Journal of Personality and Social Psychology'}",48.0,Evidence of Convergent Validity on the Dimensions of Affect,1978.0
2549,de0ecb1302996ae70d2f6e3ef47b2a7282d6a0b0,"Virtual Reality (VR) has emerged as a promising tool in many domains of therapy and rehabilitation, and has recently attracted the attention of researchers and clinicians working with elderly people with MCI, Alzheimer’s disease and related disorders. Here we present a study testing the feasibility of using highly realistic image-based rendered VR with patients with MCI and dementia. We designed an attentional task to train selective and sustained attention, and we tested a VR and a paper version of this task in a single-session within-subjects design. Results showed that participants with MCI and dementia reported to be highly satisfied and interested in the task, and they reported high feelings of security, low discomfort, anxiety and fatigue. In addition, participants reported a preference for the VR condition compared to the paper condition, even if the task was more difficult. Interestingly, apathetic participants showed a preference for the VR condition stronger than that of non-apathetic participants. These findings suggest that VR-based training can be considered as an interesting tool to improve adherence to cognitive training in elderly people with cognitive impairment.","[{'authorId': '4192721', 'name': 'V. Manera'}, {'authorId': '2650457', 'name': 'E. Chapoulie'}, {'authorId': '50075632', 'name': 'J. Bourgeois'}, {'authorId': '1830025', 'name': 'R. Guerchouche'}, {'authorId': '145768419', 'name': 'R. David'}, {'authorId': '2050262', 'name': 'Jan Ondřej'}, {'authorId': '1721779', 'name': 'G. Drettakis'}, {'authorId': '2053591824', 'name': 'P. Robert'}]",154.0,"{'bibtex': '@Article{Manera2016AFS,\n author = {V. Manera and E. Chapoulie and J. Bourgeois and R. Guerchouche and R. David and Jan Ondřej and G. Drettakis and P. Robert},\n journal = {PLoS ONE},\n title = {A Feasibility Study with Image-Based Rendered Virtual Reality in Patients with Mild Cognitive Impairment and Dementia},\n volume = {11},\n year = {2016}\n}\n'}",,"{'volume': '11', 'name': 'PLoS ONE'}",54.0,A Feasibility Study with Image-Based Rendered Virtual Reality in Patients with Mild Cognitive Impairment and Dementia,2016.0
2550,de16f421b49e5110a1feffcddec2c9ac2f2106a6,"Being imitated has a wide range of pro-social effects, but it is not clear how these effects are mediated. Naturalistic studies of the effects of being imitated have not established whether pro-social outcomes are due to the similarity and/or the contingency between the movements performed by the actor and those of the imitator. Similarity is often assumed to be the active ingredient, but we hypothesized that contingency might also be important, as it produces positive affect in infants and can be detected by phylogenetically ancient mechanisms of associative learning. We manipulated similarity and contingency between performed and observed actions in a computerized task. Similarity had no positive effects; however, contingency resulted in greater enjoyment of the task, reported closeness to others, and helping behavior. These results suggest that the pro-social effects of being imitated may rely on associative mechanisms.","[{'authorId': '2441222', 'name': 'C. Catmur'}, {'authorId': '31433567', 'name': 'C. Heyes'}]",52.0,"{'bibtex': '@Article{Catmur2013IsIW,\n author = {C. Catmur and C. Heyes},\n journal = {Cognitive science},\n pages = {\n          1541-52\n        },\n title = {Is It What You Do, or When You Do It? The Roles of Contingency and Similarity in Pro-Social Effects of Imitation},\n volume = {37 8},\n year = {2013}\n}\n'}",,"{'volume': '37 8', 'pages': '\n          1541-52\n        ', 'name': 'Cognitive science'}",32.0,"Is It What You Do, or When You Do It? The Roles of Contingency and Similarity in Pro-Social Effects of Imitation",2013.0
2551,de2997a2521a9564e55a42fd8352ab0a648adf5a,,"[{'authorId': '7427016', 'name': 'Émilie Ouellet'}, {'authorId': '40589979', 'name': 'Benjamin Boller'}, {'authorId': '1404469831', 'name': 'N. Corriveau-Lecavalier'}, {'authorId': '20817800', 'name': 'Simon Cloutier'}, {'authorId': '145580293', 'name': 'S. Belleville'}]",72.0,"{'bibtex': '@Article{Ouellet2018TheVS,\n author = {Émilie Ouellet and Benjamin Boller and N. Corriveau-Lecavalier and Simon Cloutier and S. Belleville},\n journal = {Journal of Neuroscience Methods},\n pages = {126-135},\n title = {The Virtual Shop: A new immersive virtual reality environment and scenario for the assessment of everyday memory},\n volume = {303},\n year = {2018}\n}\n'}",,"{'volume': '303', 'pages': '126-135', 'name': 'Journal of Neuroscience Methods'}",92.0,The Virtual Shop: A new immersive virtual reality environment and scenario for the assessment of everyday memory,2018.0
2552,de597d639ef2b887fa2a7002be5b6013b31f5a7b,"The past 15 years have witnessed a rapid growth in computational modeling of emotion and cognitiveaffective architectures. Architectures are being built both to elucidate mechanisms of emotions, and to enhance believability and effectiveness of synthetic agents and robots. Yet in spite of the many emotion models developed to date, there is a lack of consistency, and clarity, regarding what exactly it means to ‘model emotions’. The purpose of this paper is to attempt to deconstruct the vague term ‘emotion modeling’ by (1) suggesting that we view emotion models in terms of two fundamental categories of processes: emotion generation and emotion effects; and (2) identifying some of the fundamental computational tasks necessary to implement these processes. These ‘model building blocks’ can then provide a basis for the development of more systematic guidelines for the theoretical and data requirements, and the representational and reasoning alternatives, in emotion modeling. Identification of a set of generic computational tasks is also a good starting point for a systematic comparison of alternative approaches.","[{'authorId': '2348728', 'name': 'E. Hudlicka'}]",43.0,"{'bibtex': '@Inproceedings{Hudlicka2008WhatAW,\n author = {E. Hudlicka},\n pages = {52-59},\n title = {What Are We Modeling When We Model Emotion?},\n year = {2008}\n}\n'}",,{'pages': '52-59'},50.0,What Are We Modeling When We Model Emotion?,2008.0
2553,de8c2dbcaddc299d4a7e7ad8482e964cea05f4b1,"The idea that emotions regulate social interaction is increasingly popular. But exactly how do emotions do this? To address this question, I draw on research on the interpersonal effects of emotions on behavior in personal relationships, parent–child interactions, conflict, negotiation, and leadership, and propose a new framework that can account for existing findings and guide future research: the emotions as social information (EASI) model. I demonstrate that emotional expressions affect observers' behavior by triggering inferential processes and/or affective reactions in them. The predictive strength of these two processes—which may inspire different behaviors—depends on the observer's information processing and on social-relational factors. Examples of moderators that determine the relative predictive strength of inferences and affective reactions include power, need for cognitive closure, time pressure, display rules, and the appropriateness and target of the emotional expression, which are all discussed.","[{'authorId': '5980688', 'name': 'Gerben A. van Kleef'}]",995.0,"{'bibtex': '@Article{Kleef2009HowER,\n author = {Gerben A. van Kleef},\n journal = {Current Directions in Psychological Science},\n pages = {184 - 188},\n title = {How Emotions Regulate Social Life},\n volume = {18},\n year = {2009}\n}\n'}",,"{'volume': '18', 'pages': '184 - 188', 'name': 'Current Directions in Psychological Science'}",26.0,How Emotions Regulate Social Life,2009.0
2554,dec4e18ea8e8a45c7d880ef5cca498b3312fc351,"A recent review on facial mimicry concludes that emotional mimicry is less ubiquitous than has been suggested, and only occurs in interactions that are potentially affiliative (see Hess and Fischer, in revision). We hypothesize that individuals do not mimic facial expressions that can be perceived as offensive, such as disgust, and mimic positive emotion displays, but only when the context is affiliative (i.e., with intimates). Second, we expect that in spontaneous interactions not mimicry, but empathic feelings with the other predict the accurateness of emotion recognition. Data were collected in a pseudo-experimental setting, during an event organized for subscribers of a large Dutch women’s magazine. One woman (expresser) was exposed to two emotional stimuli (i.e., a vile smell, a compliment) in order to evoke disgust and pride respectively. Another woman (observer: intimate or stranger) was sitting opposite of her. We collected self-report measures on emotions and empathy, and coded facial expressions of disgust and smiling on the basis of FACS. The results show that participants do not mimic disgust. In contrast, smiles displayed after the vile smell and the compliment were mimicked, but only among intimates. We also found that self-reported empathy and not mimicry is related to the recognition of disgust. These findings are discussed in the light of a Social Contextual view on emotional mimicry.","[{'authorId': '7444483', 'name': 'A. Fischer'}, {'authorId': '36863432', 'name': 'D. Becker'}, {'authorId': '3814992', 'name': 'Lotte Veenstra'}]",66.0,"{'bibtex': '@Article{Fischer2012EmotionalMI,\n author = {A. Fischer and D. Becker and Lotte Veenstra},\n journal = {Frontiers in Psychology},\n title = {Emotional Mimicry in Social Context: The Case of Disgust and Pride},\n volume = {3},\n year = {2012}\n}\n'}",,"{'volume': '3', 'name': 'Frontiers in Psychology'}",70.0,Emotional Mimicry in Social Context: The Case of Disgust and Pride,2012.0
2555,df1ec1bfb66ffa141bca936e8dbf9226378c77d1,"Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type-specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing-dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.","[{'authorId': '2711974', 'name': 'G. Bi'}, {'authorId': '1823913', 'name': 'M. Poo'}]",1442.0,"{'bibtex': ""@Article{Bi2001SynapticMB,\n author = {G. Bi and M. Poo},\n journal = {Annual review of neuroscience},\n pages = {\n          139-66\n        },\n title = {Synaptic modification by correlated activity: Hebb's postulate revisited.},\n volume = {24},\n year = {2001}\n}\n""}",,"{'volume': '24', 'pages': '\n          139-66\n        ', 'name': 'Annual review of neuroscience'}",183.0,Synaptic modification by correlated activity: Hebb's postulate revisited.,2001.0
2556,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","[{'authorId': '39172707', 'name': 'Jacob Devlin'}, {'authorId': '1744179', 'name': 'Ming-Wei Chang'}, {'authorId': '2544107', 'name': 'Kenton Lee'}, {'authorId': '3259253', 'name': 'Kristina Toutanova'}]",64497.0,"{'bibtex': '@Inproceedings{Devlin2019BERTPO,\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n pages = {4171-4186},\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n year = {2019}\n}\n'}",,{'pages': '4171-4186'},63.0,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2019.0
2557,df3d2f514d41c0c37293d88d4a594e5cfc6c3bea,"There is evidence that specific regions of the face such as the eyes are particularly relevant for the decoding of emotional expressions, but it has not been examined whether scan paths of observers vary for facial expressions with different emotional content. In this study, eye-tracking was used to monitor scanning behavior of healthy participants while looking at different facial expressions. Locations of fixations and their durations were recorded, and a dominance ratio (i.e., eyes and mouth relative to the rest of the face) was calculated. Across all emotional expressions, initial fixations were most frequently directed to either the eyes or the mouth. Especially in sad facial expressions, participants more frequently issued the initial fixation to the eyes compared with all other expressions. In happy facial expressions, participants fixated the mouth region for a longer time across all trials. For fearful and neutral facial expressions, the dominance ratio indicated that both the eyes and mouth are equally important. However, in sad and angry facial expressions, the eyes received more attention than the mouth. These results confirm the relevance of the eyes and mouth in emotional decoding, but they also demonstrate that not all facial expressions with different emotional content are decoded equally. Our data suggest that people look at regions that are most characteristic for each emotion.","[{'authorId': '4754441', 'name': 'H. Eisenbarth'}, {'authorId': '2711307', 'name': 'G. Alpers'}]",355.0,"{'bibtex': '@Article{Eisenbarth2011HappyMA,\n author = {H. Eisenbarth and G. Alpers},\n journal = {Emotion},\n pages = {\n          860-865\n        },\n title = {Happy mouth and sad eyes: scanning emotional facial expressions.},\n volume = {11 4},\n year = {2011}\n}\n'}",,"{'volume': '11 4', 'pages': '\n          860-865\n        ', 'name': 'Emotion'}",55.0,Happy mouth and sad eyes: scanning emotional facial expressions.,2011.0
2558,df50c6e1903b1e2d657f78c28ab041756baca86a,"1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition.","[{'authorId': '1712517', 'name': 'L. Rabiner'}, {'authorId': '143604406', 'name': 'B. Juang'}]",9111.0,"{'bibtex': '@Inproceedings{Rabiner1993FundamentalsOS,\n author = {L. Rabiner and B. Juang},\n pages = {I-XXXV, 1-507},\n title = {Fundamentals of speech recognition},\n year = {1993}\n}\n'}",,"{'pages': 'I-XXXV, 1-507'}",0.0,Fundamentals of speech recognition,1993.0
2559,df56e3bfd5cec28463bb0ddd3660f4a98ddf17a7,"3D facial animation synthesis from audio has been a focus in recent years. However, most existing literature works are designed to map audio and visual content, providing limited knowledge regarding the relationship between emotion in audio and expressive facial animation. This work generates audio‐matching facial animations with the specified emotion label. In such a task, we argue that separating the content from audio is indispensable—the proposed model must learn to generate facial content from audio content while expressions from the specified emotion. We achieve it by an adaptive instance normalization module that isolates the content in the audio and combines the emotion embedding from the specified label. The joint content‐emotion embedding is then used to generate 3D facial vertices and texture maps. We compare our method with state‐of‐the‐art baselines, including the facial segmentation‐based and voice conversion‐based disentanglement approaches. We also conduct a user study to evaluate the performance of emotion conditioning. The results indicate that our proposed method outperforms the baselines in animation quality and expression categorization accuracy.","[{'authorId': '2152342254', 'name': 'Che-Jui Chang'}, {'authorId': '48096253', 'name': 'Long Zhao'}, {'authorId': '2175553614', 'name': 'Sen Zhang'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",5.0,"{'bibtex': '@Article{Chang2022DisentanglingAC,\n author = {Che-Jui Chang and Long Zhao and Sen Zhang and Mubbasir Kapadia},\n journal = {Computer Animation and Virtual Worlds},\n title = {Disentangling audio content and emotion with adaptive instance normalization for expressive facial animation synthesis},\n volume = {33},\n year = {2022}\n}\n'}",,"{'volume': '33', 'name': 'Computer Animation and Virtual Worlds'}",30.0,Disentangling audio content and emotion with adaptive instance normalization for expressive facial animation synthesis,2022.0
2560,df74aa5e672002e0d053870bbc42fd9ce737fc33,"To investigate whether subliminally priming for competition influences facial reactions to facial emotional displays, 49 participants were either subliminally competition primed or neutrally primed. Thereafter, they viewed computer generated avatar faces with happy, neutral, and sad expressions while Corrugator supercilii and Zygomaticus major reactions were recorded. Results revealed facial mimicry to happy and sad faces in the neutrally primed group but not the competition primed group. Furthermore, subliminal competition priming enhanced Corrugator supercilii activity after an initial relaxation while viewing happy faces. An impression formation task revealed counter empathic effects confirming successful competition priming. Overall, results indicate that nonconscious processes influence a presumably nonconscious behavior.","[{'authorId': '152592651', 'name': 'P. Weyers'}, {'authorId': '1684604', 'name': 'A. Mühlberger'}, {'authorId': '80633381', 'name': 'A. Kund'}, {'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '145825010', 'name': 'P. Pauli'}]",120.0,"{'bibtex': '@Article{Weyers2009ModulationOF,\n author = {P. Weyers and A. Mühlberger and A. Kund and U. Hess and P. Pauli},\n journal = {Psychophysiology},\n pages = {\n          328-35\n        },\n title = {Modulation of facial reactions to avatar emotional faces by nonconscious competition priming.},\n volume = {46 2},\n year = {2009}\n}\n'}",,"{'volume': '46 2', 'pages': '\n          328-35\n        ', 'name': 'Psychophysiology'}",58.0,Modulation of facial reactions to avatar emotional faces by nonconscious competition priming.,2009.0
2561,df7715f0f07b876b7c33160b7485d89645f0f293,,"[{'authorId': '145966408', 'name': 'R. Sun'}]",103.0,"{'bibtex': '@Article{Sun2009MotivationalRW,\n author = {R. Sun},\n journal = {Cognitive Computation},\n pages = {91-103},\n title = {Motivational Representations within a Computational Cognitive Architecture},\n volume = {1},\n year = {2009}\n}\n'}",,"{'volume': '1', 'pages': '91-103', 'name': 'Cognitive Computation'}",60.0,Motivational Representations within a Computational Cognitive Architecture,2009.0
2562,df7f0093291a74d1f6a8cbe6bababa93ca636fb0,,"[{'authorId': '4026257', 'name': 'Komal Batool'}, {'authorId': '1795560', 'name': 'M. Niazi'}]",54.0,"{'bibtex': '@Article{Batool2017ModelingTI,\n author = {Komal Batool and M. Niazi},\n journal = {Complex Adaptive Systems Modeling},\n pages = {1-19},\n title = {Modeling the internet of things: a hybrid modeling approach using complex networks and agent-based models},\n volume = {5},\n year = {2017}\n}\n'}",,"{'volume': '5', 'pages': '1-19', 'name': 'Complex Adaptive Systems Modeling'}",40.0,Modeling the internet of things: a hybrid modeling approach using complex networks and agent-based models,2017.0
2563,df9128ffe2e246916060fb3a13d0f5796949b322,"A novel inter-vehicle communication system based on emotion enabled cognitive (EEC) agent has been anticipated as an intelligent solution to evade the road catastrophe due to hasty decisions by drivers. An input stimulus is processed in human brain using a short route and a long route during any emergency situation. The proposed EEC agent, mounted inside a vehicle, acts like a human brain and is stirred by short route information processing mechanism of human brain in fear condition. The results are acquired for the decisions made by the proposed approach through the EEC agent using short route and human drivers using long route during urgent situations. A pre crash sensing and avoidance algorithm has been proposed as well to mitigate the lateral or side by side collision using EEC agent.  Experimental findings reveal that by commencing emotions with cognition, and using formulated L-PCSA algorithm, lateral collisions chances can be sensed and avoided to secure the valued lives of passengers. It has been pragmatic that the new approach is very effectual and useful in shunning the lateral road collisions.","[{'authorId': '40611071', 'name': 'F. Riaz'}, {'authorId': '1810492', 'name': 'Syed Ismail Shah'}, {'authorId': '145263477', 'name': 'Muhammad Raees'}, {'authorId': '1831811', 'name': 'I. Shafi'}, {'authorId': '2055756798', 'name': 'Arslan Iqbal'}]",12.0,"{'bibtex': '@Article{Riaz2013LateralPS,\n author = {F. Riaz and Syed Ismail Shah and Muhammad Raees and I. Shafi and Arslan Iqbal},\n journal = {Int. J. Commun. Networks Inf. Secur.},\n title = {Lateral Pre-crash Sensing and Avoidance in Emotion Enabled Cognitive Agent based Vehicle-2-Vehicle Communication System},\n volume = {5},\n year = {2013}\n}\n'}",,"{'volume': '5', 'name': 'Int. J. Commun. Networks Inf. Secur.'}",36.0,Lateral Pre-crash Sensing and Avoidance in Emotion Enabled Cognitive Agent based Vehicle-2-Vehicle Communication System,2013.0
2564,dfa5139ddf3c367e3a8ee8fccff15737f182a1e1,"Embodied social agents, through their ability to afford embodied interaction using nonverbal human communicative cues, hold great promise in application areas such as education, training, rehabilitation, and collaborative work. Gaze cues are particularly important for achieving significant social and communicative goals. In this research, I explore how agents - both virtual agents and humanlike robots - might achieve such goals through the use of various gaze mechanisms. To this end, I am developing computational control models of gaze behavior that treat gaze as the output of a system with a number of multimodal inputs. These inputs can be characterized at different levels of interaction, from non-interactive (e.g., physical characteristics of the agent itself) to fully interactive (e.g., speech and gaze behavior of a human interlocutor). This research will result in a number of control models that each focus on a different gaze mechanism, combined into an open-source library of gaze behaviors that will be usable by both human-robot and human-virtual agent interaction designers. System-level evaluations in naturalistic settings will validate this gaze library for its ability to evoke positive social and cognitive responses in human users.","[{'authorId': '2211183', 'name': 'Sean Andrist'}]",1.0,"{'bibtex': '@Inproceedings{Andrist2013ControllableMO,\n author = {Sean Andrist},\n pages = {333-336},\n title = {Controllable models of gaze behavior for virtual agents and humanlike robots},\n year = {2013}\n}\n'}",,{'pages': '333-336'},14.0,Controllable models of gaze behavior for virtual agents and humanlike robots,2013.0
2565,e0061f9010ca2b933067b58ce421870fee8afa76,,"[{'authorId': '2068695177', 'name': 'Christian Becker'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",137.0,"{'bibtex': '@Inproceedings{Becker2004SimulatingTE,\n author = {Christian Becker and S. Kopp and I. Wachsmuth},\n pages = {154-165},\n title = {Simulating the Emotion Dynamics of a Multimodal Conversational Agent},\n year = {2004}\n}\n'}",,{'pages': '154-165'},20.0,Simulating the Emotion Dynamics of a Multimodal Conversational Agent,2004.0
2566,e006b12fd623900ea0db593c638c30e02178e30a,"ABSTRACT Virtual reality (VR) allows for the creation of ecological environments that could be used for cognitive assessment and intervention. This study comprises two parts that describe and assess an immersive VR task, the Virtual Shop, which can be used to measure episodic memory. Part 1 addresses its applicability in healthy older adults by measuring presence, motivation, and cybersickness symptoms. Part 2 addresses its construct validity by investigating correlations between performance in the VR task and on a traditional experimental memory task, and by measuring whether the VR task is sensitive to age-related memory differences. Fifty-seven older and 20 younger adults were assessed in the Virtual Shop, in which they memorised and fetched 12 familiar items. Part 1 showed high levels of presence, higher levels of motivation for the VR than for the traditional task, and negligible cybersickness symptoms. Part 2 indicates that memory performance in the VR task is positively correlated with performance on a traditional memory task for both age groups, and age-related differences were found on the VR and traditional memory tasks. Thus, the use of VR is feasible in older adults and the Virtual Shop is a valid task to assess and train episodic memory in this population.","[{'authorId': '48889585', 'name': 'Nick Corriveau Lecavalier'}, {'authorId': '7427016', 'name': 'Émilie Ouellet'}, {'authorId': '40589979', 'name': 'Benjamin Boller'}, {'authorId': '145580293', 'name': 'S. Belleville'}]",66.0,"{'bibtex': '@Article{Lecavalier2018UseOI,\n author = {Nick Corriveau Lecavalier and Émilie Ouellet and Benjamin Boller and S. Belleville},\n journal = {Neuropsychological Rehabilitation},\n pages = {462 - 480},\n title = {Use of immersive virtual reality to assess episodic memory: A validation study in older adults},\n volume = {30},\n year = {2018}\n}\n'}",,"{'volume': '30', 'pages': '462 - 480', 'name': 'Neuropsychological Rehabilitation'}",53.0,Use of immersive virtual reality to assess episodic memory: A validation study in older adults,2018.0
2567,e009942441ac200f7950077d099de47d41d81e2b,,"[{'authorId': '1768825', 'name': 'D. Helbing'}, {'authorId': '34014051', 'name': 'I. Farkas'}, {'authorId': '1981267', 'name': 'T. Vicsek'}]",4374.0,"{'bibtex': '@Article{Helbing2000SimulatingDF,\n author = {D. Helbing and I. Farkas and T. Vicsek},\n journal = {Nature},\n pages = {487-490},\n title = {Simulating dynamical features of escape panic},\n volume = {407},\n year = {2000}\n}\n'}",,"{'volume': '407', 'pages': '487-490', 'name': 'Nature'}",36.0,Simulating dynamical features of escape panic,2000.0
2569,e029c5d0e0600917a5a98b0694d7645c011d77bd,,"[{'authorId': '1732377', 'name': 'R. Aylett'}]",17.0,"{'bibtex': '@Inproceedings{Aylett2004AgentsAA,\n author = {R. Aylett},\n pages = {496-504},\n title = {Agents and Affect: Why Embodied Agents Need Affective Systems},\n year = {2004}\n}\n'}",,{'pages': '496-504'},23.0,Agents and Affect: Why Embodied Agents Need Affective Systems,2004.0
2570,e042f9a2a3612cb4445995ee9376398d7a73f7aa,,"[{'authorId': '1759118', 'name': 'Radoslaw Niewiadomski'}, {'authorId': '1724289', 'name': 'M. Ochs'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",46.0,"{'bibtex': '@Inproceedings{Niewiadomski2008ExpressionsOE,\n author = {Radoslaw Niewiadomski and M. Ochs and C. Pelachaud},\n pages = {37-44},\n title = {Expressions of Empathy in ECAs},\n year = {2008}\n}\n'}",,{'pages': '37-44'},18.0,Expressions of Empathy in ECAs,2008.0
2572,e04ced4b298965293fba1f22b74bd3301aeb6540,"We discuss issues in designing virtual humans for applications that require long-term voluntary use and the problem of maintaining engagement with users over time. Concepts and theories related to engagement from a variety of disciplines are reviewed. We describe a platform for conducting studies into long-term interactions between humans and virtual agents and present the results of two longitudinal, randomized, controlled experiments in which the effect of manipulations of agent behavior on user engagement was assessed.","[{'authorId': '1690448', 'name': 'T. Bickmore'}, {'authorId': '50247170', 'name': 'Daniel Schulman'}, {'authorId': '2721397', 'name': 'Langxuan Yin'}]",205.0,"{'bibtex': '@Article{Bickmore2010MAINTAININGEI,\n author = {T. Bickmore and Daniel Schulman and Langxuan Yin},\n journal = {Applied Artificial Intelligence},\n pages = {648 - 666},\n title = {MAINTAINING ENGAGEMENT IN LONG-TERM INTERVENTIONS WITH RELATIONAL AGENTS},\n volume = {24},\n year = {2010}\n}\n'}",,"{'volume': '24', 'pages': '648 - 666', 'name': 'Applied Artificial Intelligence'}",29.0,MAINTAINING ENGAGEMENT IN LONG-TERM INTERVENTIONS WITH RELATIONAL AGENTS,2010.0
2573,e06a4a5955d1a581a92a17ce5e00edf824e19786,"Intent detection is one of the crucial Natural Language Understanding(NLU) tasks studied extensively. Misclassification of intents impacts the overall performance of the conversational systems as natural language understanding is the first mean of interaction between a user and a virtual agent. The traditional approach of intent detection is limited only to textual features of user utterances and overlooks other semantic features. The sentiment and emotional state of the speaker are two such semantic features that have essential impacts on intent detection, as they implicitly express user intention conveyed through the user’s message. Depending on the context, these features can help the dialogue agent respond to the same intent with varying degrees of sentiment and emotion. Thus, investigating the role of sentiment and emotion on intent detection is a matter of great interest. The current work investigates the impact of utilizing sentiment and emotion information on intent detection tasks and proposes emotion and sentiment aided intent detection models. We also investigate the impact of sentiment and emotion using three different multitasking frameworks and present a joint model that utilizes the co-relation information across these tasks to correctly identify all these NLU aspects (intent, sentiment, and emotion). The obtained experimental results by the proposed models outperform several baselines and state-of-the-art intent detection models on multiple datasets by a significant margin of 1.8% - 3%, demonstrating the significant role of sentiment and emotion features in intent detection1.","[{'authorId': '2192841735', 'name': 'Ashutosh Kumar Trivedi'}, {'authorId': '2057810863', 'name': 'A. Tiwari'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '40585053', 'name': 'Anutosh Maitra'}, {'authorId': '3040439', 'name': 'Roshni Ramnani'}, {'authorId': '2062808558', 'name': 'Shubhashis Sengupta'}]",0.0,"{'bibtex': '@Article{Trivedi2022TowardsSA,\n author = {Ashutosh Kumar Trivedi and A. Tiwari and S. Saha and Anutosh Maitra and Roshni Ramnani and Shubhashis Sengupta},\n booktitle = {International Conference on Pattern Recognition},\n journal = {2022 26th International Conference on Pattern Recognition (ICPR)},\n pages = {2510-2516},\n title = {Towards Sentiment and Emotion aided Intent Detection},\n year = {2022}\n}\n'}",[],"{'name': '2022 26th International Conference on Pattern Recognition (ICPR)', 'pages': '2510-2516'}",24.0,Towards Sentiment and Emotion aided Intent Detection,2022.0
2574,e07abf28587adb06e7044254b24196ce6cf0204e,"This volume, Grice's first hook, includes the long-delayed publication of his enormously influential 1967 William James Lectures. But there is much, much more in this work. Paul Grice himself has carefully arranged and framed the sequence of essays to emphasize not a certain set of ideas but a habit of mind, a style of philosophizing. Grice has, to be sure, provided philosophy with crucial ideas. His account of speaker-meaning is the standard that others use to define their own minor divergences or future elaborations. His discussion of conversational implicatures has given philosophers an important tool for the investigation of all sorts of problems; it has also laid the foundation for a great deal of work by other philosophers and linguists about presupposition. His metaphysical defense of absolute values is starting to be considered the beginning of a new phase in philosophy. This is a vital book for all who are interested in Anglo-American philosophy.","[{'authorId': '144377810', 'name': 'T. Burge'}, {'authorId': '113023839', 'name': 'P. Grice'}]",3172.0,"{'bibtex': '@Article{Burge1989StudiesIT,\n author = {T. Burge and P. Grice},\n journal = {The Philosophical Quarterly},\n pages = {393},\n title = {Studies in the Way of Words.},\n volume = {40},\n year = {1989}\n}\n'}",,"{'volume': '40', 'pages': '393', 'name': 'The Philosophical Quarterly'}",0.0,Studies in the Way of Words.,1989.0
2575,e07da3f8bed9911fd5d2a6909e41abfa42dbd77a,"In recent years, many studies have shown that perceiving other individuals’ direct gaze has robust effects on various attentional and cognitive processes. However, considerably less attention has been devoted to investigating the affective effects triggered by eye contact. This article reviews research concerning the effects of others’ gaze direction on observers’ affective responses. The review focuses on studies in which affective reactions have been investigated in well-controlled laboratory experiments, and in which contextual factors possibly influencing perceivers’ affects have been controlled. Two important themes emerged from this review. First, explicit affective evaluations of seeing another’s direct versus averted gaze have resulted in rather inconsistent findings; some studies report more positive subjective feelings to direct compared to averted gaze, whereas others report the opposite pattern. These contradictory findings may be related, for example, to differences between studies in terms of the capability of direct-gaze stimuli to elicit feelings of self-involvement. Second, studies relying on various implicit measures have reported more consistent results; they indicate that direct gaze increases affective arousal, and more importantly, that eye contact automatically evokes a positively valenced affective reaction. Based on the review, possible psychological mechanisms for the positive affective reactions elicited by eye contact are described.","[{'authorId': '34601273', 'name': 'J. Hietanen'}]",89.0,"{'bibtex': '@Article{Hietanen2018AffectiveEC,\n author = {J. Hietanen},\n journal = {Frontiers in Psychology},\n title = {Affective Eye Contact: An Integrative Review},\n volume = {9},\n year = {2018}\n}\n'}",,"{'volume': '9', 'name': 'Frontiers in Psychology'}",157.0,Affective Eye Contact: An Integrative Review,2018.0
2576,e0867d523f610b32267fa7ec35a510936b8b595f,"Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.","[{'authorId': '3161007', 'name': 'S. Mavadati'}, {'authorId': '145531712', 'name': 'M. Mahoor'}, {'authorId': '2067587629', 'name': 'Kevin Bartlett'}, {'authorId': '2096029932', 'name': 'Philip Trinh'}, {'authorId': '1737918', 'name': 'J. Cohn'}]",601.0,"{'bibtex': '@Article{Mavadati2013DISFAAS,\n author = {S. Mavadati and M. Mahoor and Kevin Bartlett and Philip Trinh and J. Cohn},\n journal = {IEEE Transactions on Affective Computing},\n pages = {151-160},\n title = {DISFA: A Spontaneous Facial Action Intensity Database},\n volume = {4},\n year = {2013}\n}\n'}",,"{'volume': '4', 'pages': '151-160', 'name': 'IEEE Transactions on Affective Computing'}",76.0,DISFA: A Spontaneous Facial Action Intensity Database,2013.0
2577,e0a2df1cc4e06c6c337b24eba491dceafad8efe8,"Most socially interactive virtual agents that generate facial expressions lack critical visual features such as expressive wrinkles, which could reduce their realistic appearance. Here, we examined the impact of dynamic facial texture on perceptions of realism of facial expressions of emotion and identified the emotion-specific features that enhance this perception. In a human perceptual judgment task, participants (20 white Westerners, 10 female) viewed pairs of facial expressions of the six classic emotions - happy, surprise, fear, disgust, anger and sad - with and without dynamic textures and selected the most realistic one from the pair. Analysis of participant choices showed that facial expressions with dynamic texture are perceived significantly more often as more realistic for all emotions except sad. Further analysis of the facial expression signals showed that emotion-specific features, such as darker forehead furrows in surprise, unilateral nose wrinkling in disgust, and shade variations around the cheeks in happy, enhanced perceptions of realism. Together, our results highlight the importance of equipping virtual agents with dynamic face movement texture to produce realistic facial expressions of emotion.","[{'authorId': '6416348', 'name': 'Chaona Chen'}, {'authorId': '48522841', 'name': 'Oliver G. B. Garrod'}, {'authorId': '2287417', 'name': 'P. Schyns'}, {'authorId': '2143019', 'name': 'Rachael E. Jack'}]",4.0,"{'bibtex': '@Article{Chen2020DynamicFM,\n author = {Chaona Chen and Oliver G. B. Garrod and P. Schyns and Rachael E. Jack},\n journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},\n title = {Dynamic Face Movement Texture Enhances the Perceived Realism of Facial Expressions of Emotion},\n year = {2020}\n}\n'}",,{'name': 'Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents'},18.0,Dynamic Face Movement Texture Enhances the Perceived Realism of Facial Expressions of Emotion,2020.0
2578,e0b162440d1e166d7da93b276ba29866a24e1a1f,"Intent classification (IC) and Named Entity Recognition (NER) are arguably the two main components needed to build a Natural Language Understanding (NLU) engine, which is a main component of conversational agents. The IC and NER components are closely intertwined and the entities are often connected to the underlying intent. Current research has primarily focused to model IC and NER as two separate units, which results in error propagation, and thus, sub-optimal performance. In this paper, we propose a simple yet effective novel framework for NLU where the parameters of the IC and the NER models are jointly trained in a consolidated parameter space. Text semantic representations are obtained from popular pre-trained contextual language models, which are fine-tuned for our task, and these parameters are propagated to other deep neural layers in our framework leading to a faithful unified modelling of the IC and NER parameters. The overall framework results in a faithful parameter sharing when the training is underway, leading to a more coherent learning. Experiments on two public datasets, ATIS and SNIPS, show that our model outperforms other methods by a noticeable margin. On the SNIPS dataset, we obtain a 1.42% improvement in NER in terms of the F1 score, and 1% improvement in intent accuracy score. On ATIS, we achieve 1.54% improvement in intent accuracy score. We also present qualitative results to showcase the effectiveness of our model.","[{'authorId': '146002215', 'name': 'Alberto Jose Benayas Alamos'}, {'authorId': '1738696028', 'name': 'Reyhaneh Hashempour'}, {'authorId': '102822559', 'name': 'D. Rumble'}, {'authorId': '38797620', 'name': 'Shoaib Jameel'}, {'authorId': '2028807', 'name': 'Renato Cordeiro de Amorim'}]",3.0,"{'bibtex': '@Article{Alamos2021UnifiedTM,\n author = {Alberto Jose Benayas Alamos and Reyhaneh Hashempour and D. Rumble and Shoaib Jameel and Renato Cordeiro de Amorim},\n journal = {IEEE Access},\n pages = {1-1},\n title = {Unified Transformer Multi-task Learning for Intent Classification with Entity Recognition},\n volume = {PP},\n year = {2021}\n}\n'}",,"{'volume': 'PP', 'pages': '1-1', 'name': 'IEEE Access'}",45.0,Unified Transformer Multi-task Learning for Intent Classification with Entity Recognition,2021.0
2579,e0c4f40917cc632fba6a9e90165cc83f6a735f47,"One of the main challenges when developing Embodied Conversational Agents is to give them the ability to autonomously produce meaningful and coordinated verbal and nonverbal behaviors. The relation between these means of communication is more complex than a direct mapping that has often been applied in previous models. In this paper, we propose an intermediate mapping approach we apply on metaphoric gestures first but that could be extended to other representational gestures. Leveraging from previous work in text analysis, embodied cognition and co-verbal behavior production, we introduce a framework articulating speech and metaphoric gesture invariants around a common mental representation: Image Schemas. We establish the components of our framework, detailing the different steps leading to the production of the metaphoric gestures, and we present some preliminary results and demonstrations. We end the paper by laying down the perspectives to integrate, evaluate and improve our model.","[{'authorId': '1682486', 'name': 'Brian Ravenet'}, {'authorId': '2049106', 'name': 'C. Clavel'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",17.0,"{'bibtex': '@Inproceedings{Ravenet2018AutomaticNB,\n author = {Brian Ravenet and C. Clavel and C. Pelachaud},\n pages = {1667-1674},\n title = {Automatic Nonverbal Behavior Generation from Image Schemas},\n year = {2018}\n}\n'}",,{'pages': '1667-1674'},35.0,Automatic Nonverbal Behavior Generation from Image Schemas,2018.0
2580,e0dcb204155a6111cde35cc1aedb129efab2ef35,,"[{'authorId': '7859950', 'name': 'J. D. Greeff'}, {'authorId': '3015062', 'name': 'S. Nolfi'}]",29.0,"{'bibtex': '@Inproceedings{Greeff2010EvolutionOI,\n author = {J. D. Greeff and S. Nolfi},\n pages = {179-214},\n title = {Evolution of Implicit and Explicit Communication in Mobile Robots},\n year = {2010}\n}\n'}",,{'pages': '179-214'},33.0,Evolution of Implicit and Explicit Communication in Mobile Robots,2010.0
2581,e0ea20d239c8ae9c4af2ea295baa6a2442ac465d,"Virtual Human and its harmonious interaction technology is one of hot spots in the current information science and life science.Emotional Information Processing is an important field which has been always concerned by the artificial intelligence and cognitive science research.This paper introduces the modeling of the virtual human emotion based on HowNet.This model is a six-dimensional space which is composed of six basic emotions: calm,angry,joy,sorrow,relaxation,and anxiety.It is proved that the model can be good for human emotion simulation.","[{'authorId': '2073432549', 'name': 'Chen Yi-xiang'}]",3.0,"{'bibtex': '@Article{Yi-xiang2008TheMO,\n author = {Chen Yi-xiang},\n journal = {Techniques of Automation and Applications},\n title = {The Modeling of Virtual Human Emotions Based on HowNet},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': 'Techniques of Automation and Applications'}",0.0,The Modeling of Virtual Human Emotions Based on HowNet,2008.0
2582,e0f990cb647b8da845dd025e286fc8d1126179b0,,"[{'authorId': '48549092', 'name': 'Yao Qian'}, {'authorId': '1705574', 'name': 'F. Soong'}, {'authorId': '2108965426', 'name': 'Yining Chen'}, {'authorId': '145821584', 'name': 'Min Chu'}]",51.0,"{'bibtex': '@Inproceedings{Qian2006AnHM,\n author = {Yao Qian and F. Soong and Yining Chen and Min Chu},\n pages = {223-232},\n title = {An HMM-Based Mandarin Chinese Text-To-Speech System},\n year = {2006}\n}\n'}",,{'pages': '223-232'},18.0,An HMM-Based Mandarin Chinese Text-To-Speech System,2006.0
2583,e12ca95b83a30e3494f8892aa3766e599c3af203,"We present an interactive robotic framework that delivers emotional and social behaviors for multi-sensory therapy for children with autism spectrum disorders. Our framework includes emotion-based robotic gestures and facial expressions, as well as vision and audio-based monitoring system for quantitative measurement of the interaction. We also discuss the special aspects of interacting with children with autism with multi-sensory stimuli and the potentials of our approach for personalized therapies for social and behavioral learning.","[{'authorId': '48312223', 'name': 'Rachael Bevill'}, {'authorId': '1695172', 'name': 'C. Park'}, {'authorId': '2109608503', 'name': 'Hyung Jung Kim'}, {'authorId': '2108391549', 'name': 'Jongwon Lee'}, {'authorId': '3389828', 'name': 'A. Rennie'}, {'authorId': '2572836', 'name': 'M. Jeon'}, {'authorId': '145065293', 'name': 'A. Howard'}]",10.0,"{'bibtex': '@Article{Bevill2016InteractiveRF,\n author = {Rachael Bevill and C. Park and Hyung Jung Kim and Jongwon Lee and A. Rennie and M. Jeon and A. Howard},\n journal = {2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {421-422},\n title = {Interactive robotic framework for multi-sensory therapy for children with autism spectrum disorder},\n year = {2016}\n}\n'}",,"{'pages': '421-422', 'name': '2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",8.0,Interactive robotic framework for multi-sensory therapy for children with autism spectrum disorder,2016.0
2584,e15cf50aa89fee8535703b9f9512fca5bfc43327,"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.","[{'authorId': '2574060', 'name': 'Christian Szegedy'}, {'authorId': '2157222093', 'name': 'Wei Liu'}, {'authorId': '39978391', 'name': 'Yangqing Jia'}, {'authorId': '3142556', 'name': 'P. Sermanet'}, {'authorId': '144828948', 'name': 'Scott E. Reed'}, {'authorId': '1838674', 'name': 'Dragomir Anguelov'}, {'authorId': '1761978', 'name': 'D. Erhan'}, {'authorId': '2657155', 'name': 'Vincent Vanhoucke'}, {'authorId': '39863668', 'name': 'Andrew Rabinovich'}]",37972.0,"{'bibtex': '@Article{Szegedy2014GoingDW,\n author = {Christian Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and Scott E. Reed and Dragomir Anguelov and D. Erhan and Vincent Vanhoucke and Andrew Rabinovich},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1-9},\n title = {Going deeper with convolutions},\n year = {2014}\n}\n'}",,"{'pages': '1-9', 'name': '2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'}",264.0,Going deeper with convolutions,2014.0
2585,e17d775c07431ffe44502c02d9dec4005385e1e4,"Rewarding Bursts of Dopamine Dopaminergic neurons are thought to be involved in the cognitive and hedonic underpinnings of motivated behaviors. However, it is still unclear whether dopaminergic neuron activation is sufficient to elicit reward-related behavior and which type of neuronal activity pattern serves this purpose. Tsai et al. (p. 1080; published online 23 April) directly compared tonic versus phasic firing of dopaminergic cells in the ventral tegmental area, and the effects on both behavior and dopamine release. Using a transgenic system and virus injection in mice, they targeted the dopaminergic cells with rhodopsin. Light stimulation was then used to drive dopaminergic cells either with a tonic low level of pulses or bursts of high-frequency pulses, with the number of pulses being equal across conditions. Only the high-frequency phasic firing induced a conditioned place preference and dopamine release. High-frequency pulses in deep brain cells release dopamine and cause reward-related behavior in mice. Natural rewards and drugs of abuse can alter dopamine signaling, and ventral tegmental area (VTA) dopaminergic neurons are known to fire action potentials tonically or phasically under different behavioral conditions. However, without technology to control specific neurons with appropriate temporal precision in freely behaving mammals, the causal role of these action potential patterns in driving behavioral changes has been unclear. We used optogenetic tools to selectively stimulate VTA dopaminergic neuron action potential firing in freely behaving mammals. We found that phasic activation of these neurons was sufficient to drive behavioral conditioning and elicited dopamine transients with magnitudes not achieved by longer, lower-frequency spiking. These results demonstrate that phasic dopaminergic activity is sufficient to mediate mammalian behavioral conditioning.","[{'authorId': '2879186', 'name': 'T. Thesen'}, {'authorId': '2128761249', 'name': 'Chunmao Wang'}, {'authorId': '145966661', 'name': 'O. Devinsky'}, {'authorId': '2305833', 'name': 'R. Kuzniecky'}, {'authorId': '1850025', 'name': 'W. Doyle'}, {'authorId': '35584263', 'name': 'J. Madsen'}, {'authorId': '2322053', 'name': 'E. Bromfield'}, {'authorId': '3453473', 'name': 'L. Erőss'}, {'authorId': '119686591', 'name': 'P. Halász'}, {'authorId': '5161907', 'name': 'G. Karmos'}, {'authorId': '4182314', 'name': 'Richárd Csercsa'}, {'authorId': '5609930', 'name': 'L. Wittner'}, {'authorId': '2870547', 'name': 'I. Ulbert'}]",1158.0,"{'bibtex': '@Article{Thesen2009PhasicFI,\n author = {T. Thesen and Chunmao Wang and O. Devinsky and R. Kuzniecky and W. Doyle and J. Madsen and E. Bromfield and L. Erőss and P. Halász and G. Karmos and Richárd Csercsa and L. Wittner and I. Ulbert},\n journal = {Science},\n pages = {1080 - 1084},\n title = {Phasic Firing in Dopaminergic Neurons Is Sufficient for Behavioral Conditioning},\n volume = {324},\n year = {2009}\n}\n'}",,"{'volume': '324', 'pages': '1080 - 1084', 'name': 'Science'}",46.0,Phasic Firing in Dopaminergic Neurons Is Sufficient for Behavioral Conditioning,2009.0
2586,e191ec44860ee25c8512cd3d678a2ba78a1205b6,,"[{'authorId': '102649579', 'name': 'T. S. Cohn'}]",4.0,"{'bibtex': '@Inproceedings{Cohn1955TheHO,\n author = {T. S. Cohn},\n title = {The Handbook of Social Psychology. Gardner Lindzey},\n year = {1955}\n}\n'}",,,0.0,The Handbook of Social Psychology. Gardner Lindzey,1955.0
2587,e205a5a6613421ee818e6b1b98aba4271dc5435d,"Research conducted within the general paradigm of cognitive bias modification (CBM) reveals that emotional biases in attention, interpretation, and memory are not merely associated with emotional disorders but contribute to them. After briefly describing research on both emotional biases and their modification, the authors examine similarities between CBM paradigms and older experimental paradigms used in research on learning and memory. The techniques and goals of CBM research are compared with other approaches to understanding cognition–emotion interactions. From a functional perspective, the CBM tradition reminds us to use experimental tools to evaluate assumptions about clinical phenomena and, more generally, about causal relationships between cognitive processing and emotion.","[{'authorId': '37228832', 'name': 'P. Hertel'}, {'authorId': '1808296', 'name': 'A. Mathews'}]",262.0,"{'bibtex': '@Article{Hertel2011CognitiveBM,\n author = {P. Hertel and A. Mathews},\n journal = {Perspectives on Psychological Science},\n pages = {521 - 536},\n title = {Cognitive Bias Modification},\n volume = {6},\n year = {2011}\n}\n'}",,"{'volume': '6', 'pages': '521 - 536', 'name': 'Perspectives on Psychological Science'}",119.0,Cognitive Bias Modification,2011.0
2588,e229fdc16fecc79e61d4b79b819a568ad989198e,,"[{'authorId': '40614066', 'name': 'S. Muncer'}, {'authorId': '50602179', 'name': 'J. Ling'}]",250.0,"{'bibtex': '@Article{Muncer2006PsychometricAO,\n author = {S. Muncer and J. Ling},\n journal = {Personality and Individual Differences},\n pages = {1111-1119},\n title = {Psychometric analysis of the empathy quotient (EQ) scale.},\n volume = {40},\n year = {2006}\n}\n'}",,"{'volume': '40', 'pages': '1111-1119', 'name': 'Personality and Individual Differences'}",24.0,Psychometric analysis of the empathy quotient (EQ) scale.,2006.0
2589,e256bf5a474fa93a759c0778849bbb3daff847b9,,"[{'authorId': '21112145', 'name': 'Joshua D. Greene'}]",109.0,"{'bibtex': '@Article{Greene2015TheRO,\n author = {Joshua D. Greene},\n journal = {Cognition},\n pages = {39-42},\n title = {The rise of moral cognition},\n volume = {135},\n year = {2015}\n}\n'}",,"{'volume': '135', 'pages': '39-42', 'name': 'Cognition'}",74.0,The rise of moral cognition,2015.0
2590,e2586c490280a932e91cb67e589f97b69c610064,"This paper discusses driver models as components of driver assistance systems and driving simulations. The focus is on generating believable and consistent behavior of simulated drivers by using driver models which imitate emotional influence on the human driverpsilas decisions. The designed and implemented simulation intends to build a platform for learning algorithms which will later be used in adaptive driver assistance systems. This work adapts the cognitive appraisal model as described by Orthony, Clore and Collins ill for the imitation of human emotional reactions by integrating it with a model of risk. A system design is presented and discussed. Moreover, a simulation environment is presented and the integration of emotional drivers is shown. The application of the model to a specific driver assistance system is presented.","[{'authorId': '145130350', 'name': 'D. Reichardt'}]",12.0,"{'bibtex': '@Article{Reichardt2008ApproachingDM,\n author = {D. Reichardt},\n journal = {2008 IEEE Intelligent Vehicles Symposium},\n pages = {234-239},\n title = {Approaching driver models which integrate models of emotion and risk},\n year = {2008}\n}\n'}",,"{'pages': '234-239', 'name': '2008 IEEE Intelligent Vehicles Symposium'}",20.0,Approaching driver models which integrate models of emotion and risk,2008.0
2591,e279361ec12587d0a6f3d419671545f45d5fe09a,,"[{'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '3175881', 'name': 'N. Degens'}, {'authorId': '145136631', 'name': 'Ana Paiva'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '2584600', 'name': 'G. Hofstede'}, {'authorId': '153680364', 'name': 'A. Beulens'}, {'authorId': '1732377', 'name': 'R. Aylett'}]",28.0,"{'bibtex': '@Article{Mascarenhas2016ModelingCI,\n author = {S. Mascarenhas and N. Degens and Ana Paiva and R. Prada and G. Hofstede and A. Beulens and R. Aylett},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {931-962},\n title = {Modeling culture in intelligent virtual agents},\n volume = {30},\n year = {2016}\n}\n'}",,"{'volume': '30', 'pages': '931-962', 'name': 'Autonomous Agents and Multi-Agent Systems'}",59.0,Modeling culture in intelligent virtual agents,2016.0
2592,e27e6f6d53c77b9dff0a986cd57d08558fad2e1f,,"[{'authorId': '49239080', 'name': 'E. Prochazkova'}, {'authorId': '2512383', 'name': 'M. Kret'}]",229.0,"{'bibtex': '@Article{Prochazkova2017ConnectingMA,\n author = {E. Prochazkova and M. Kret},\n journal = {Neuroscience & Biobehavioral Reviews},\n pages = {99-114},\n title = {Connecting minds and sharing emotions through mimicry: A neurocognitive model of emotional contagion},\n volume = {80},\n year = {2017}\n}\n'}",,"{'volume': '80', 'pages': '99-114', 'name': 'Neuroscience & Biobehavioral Reviews'}",197.0,Connecting minds and sharing emotions through mimicry: A neurocognitive model of emotional contagion,2017.0
2593,e28be1d444d90c5fea1307a023bee1665d953296,,"[{'authorId': '15358519', 'name': 'John E. Perez'}, {'authorId': '39641777', 'name': 'R. Riggio'}]",45.0,"{'bibtex': '@Inproceedings{Perez2003NonverbalSS,\n author = {John E. Perez and R. Riggio},\n pages = {17-44},\n title = {Nonverbal Social Skills and Psychopathology},\n year = {2003}\n}\n'}",,"{'volume': '', 'pages': '17-44', 'name': ''}",0.0,Nonverbal Social Skills and Psychopathology,2003.0
2594,e2a7c9b4c6540fbca3cf5bdb8470f48ca3721227,"This study examined the social effects of emotions related to supplication and appeasement in conflict and negotiation. In a computer-simulated negotiation, participants in Experiment 1 were confronted with a disappointed or worried opponent (supplication), with a guilty or regretful opponent (appeasement), or with a nonemotional opponent (control). Compared with controls, participants conceded more when the other experienced supplication emotions and conceded less when the other experienced appeasement emotions (especially guilt). Experiment 2 replicated the effects of disappointment and guilt and showed that they are moderated by the perceiver's dispositional trust: Negotiators high in trust conceded more to a disappointed counterpart than to a happy one, but those with low trust were unaffected. In Experiment 3, trust was manipulated through information about the other's personality (cooperative vs. competitive), and a similar moderation was obtained.","[{'authorId': '5980688', 'name': 'Gerben A. van Kleef'}, {'authorId': '8494133', 'name': 'C. D. De Dreu'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",252.0,"{'bibtex': '@Article{Kleef2006SupplicationAA,\n author = {Gerben A. van Kleef and C. D. De Dreu and A. Manstead},\n journal = {Journal of personality and social psychology},\n pages = {\n          124-42\n        },\n title = {Supplication and appeasement in conflict and negotiation: The interpersonal effects of disappointment, worry, guilt, and regret.},\n volume = {91 1},\n year = {2006}\n}\n'}",,"{'volume': '91 1', 'pages': '\n          124-42\n        ', 'name': 'Journal of personality and social psychology'}",140.0,"Supplication and appeasement in conflict and negotiation: The interpersonal effects of disappointment, worry, guilt, and regret.",2006.0
2595,e2ef4bc659fd0d2badbc26b1fb543aab445955db,,"[{'authorId': '2148761004', 'name': 'Longbing Cao'}, {'authorId': '3059035', 'name': 'D. Ru-wei'}]",7.0,"{'bibtex': '@Inproceedings{Cao2008OpenCI,\n author = {Longbing Cao and D. Ru-wei},\n title = {Open Complex Intelligent Systems: Fundamentals, Concepts, Analysis, Design and Implementation},\n year = {2008}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Open Complex Intelligent Systems: Fundamentals, Concepts, Analysis, Design and Implementation",2008.0
2597,e2ffe2196f74fa83ca2ce586e97a9c2d0ec4ff3d,"In this article, we analyze the performance of an agent developed according to a well-accepted appraisal theory of human emotion with respect to how it modulates play in the context of a social dilemma. We ask if the agent will be capable of generating interactions that are considered to be more human-like than machine-like. We conducted an experiment with 117 participants and show how participants rated our agent on dimensions of human-uniqueness (separating humans from animals) and human-nature (separating humans from machines). We show that our appraisal theoretic agent is perceived to be more human-like than the baseline models, by significantly improving both human-nature and human-uniqueness aspects of the intelligent agent. We also show that perception of humanness positively affects enjoyment and cooperation in the social dilemma, and discuss consequences for the task duration recall.","[{'authorId': '3372144', 'name': 'M. Ghafurian'}, {'authorId': '81350809', 'name': 'Neil Budnarain'}, {'authorId': '145803385', 'name': 'J. Hoey'}]",5.0,"{'bibtex': '@Article{Ghafurian2019ImprovingHO,\n author = {M. Ghafurian and Neil Budnarain and J. Hoey},\n journal = {IEEE Transactions on Affective Computing},\n pages = {1461-1471},\n title = {Improving Humanness of Virtual Agents and Users’ Cooperation Through Emotions},\n volume = {14},\n year = {2019}\n}\n'}",,"{'volume': '14', 'pages': '1461-1471', 'name': 'IEEE Transactions on Affective Computing'}",74.0,Improving Humanness of Virtual Agents and Users’ Cooperation Through Emotions,2019.0
2598,e30662126c78d1fb7d70f576e04c7bec448f65b3,"Participants in a conversation are normally receptive to their surroundings and their interlocutors, even while they are speaking and can, if necessary, adapt their ongoing utterance. Typical dialogue systems are not receptive and cannot adapt while uttering. We present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the flexibility they can achieve with an example system that adapts to a listener's acoustic understanding problems by pausing, repeating and possibly rephrasing problematic parts of an utterance. In an evaluation, this system was rated as significantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause; it also has a lower response time.","[{'authorId': '2849488', 'name': 'Hendrik Buschmeier'}, {'authorId': '8800150', 'name': 'Timo Baumann'}, {'authorId': '3167440', 'name': 'Benjamin Dosch'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1817455', 'name': 'David Schlangen'}]",56.0,"{'bibtex': '@Inproceedings{Buschmeier2012CombiningIL,\n author = {Hendrik Buschmeier and Timo Baumann and Benjamin Dosch and S. Kopp and David Schlangen},\n pages = {295-303},\n title = {Combining Incremental Language Generation and Incremental Speech Synthesis for Adaptive Information Presentation},\n year = {2012}\n}\n'}",,{'pages': '295-303'},24.0,Combining Incremental Language Generation and Incremental Speech Synthesis for Adaptive Information Presentation,2012.0
2599,e336a7b10b5bc89b66bf7115bd90e30d2045bed9,"Improving the expressiveness of virtual humans is essential for qualitative interactions and development of an emotional bond. It is certainly indicated for all applications using the user's cognitive processes, such as applications dedicated to training or health. Our study aims to contribute to the design of an expressive virtual human, by identifying and adapting visual factors promoting transcription of emotions. In this paper, we investigate the effect of expressive wrinkles and variation of pupil size. We propose to compare the recognition of basic emotions on a real human and on an expressive virtual human. The virtual human was subject to two different factors: expressive wrinkles and/or pupil size. Our results indicate that emotion recognition rates on the virtual agent are high. Moreover, expressive wrinkles affect emotion recognition. The effect of pupillary size is less significant. However, both are recommended to design an expressive virtual human.","[{'authorId': '2079143895', 'name': 'Anne-Sophie Milcent'}, {'authorId': '34861774', 'name': 'Erik Geslin'}, {'authorId': '39792357', 'name': 'Abdelmajid Kadri'}, {'authorId': '2646589', 'name': 'S. Richir'}]",10.0,"{'bibtex': '@Book{Milcent2019ExpressiveVH,\n author = {Anne-Sophie Milcent and Erik Geslin and Abdelmajid Kadri and S. Richir},\n booktitle = {International Conference on Intelligent Virtual Agents},\n journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},\n title = {Expressive Virtual Human: Impact of expressive wrinkles and pupillary size on emotion recognition},\n year = {2019}\n}\n'}","[{'paperId': 'a279965f6f10e1105f88a6b8d3a4c392cf0aaca1', 'title': 'Towards affective computing that works for everyone'}, {'paperId': 'a909ee2b3d3e67ab38116469a4252580e0e1e1fc', 'title': 'How do people respond to computer-generated versus human faces? A systematic review and meta-analyses'}, {'paperId': '36af84dedf5145e285a6f66566f2c640c2e1201c', 'title': 'Effect of Social Actors Perceived Agency on Social Presence in Computer-Mediated Communication'}, {'paperId': '8c2130ccfb7eb3559dfe516a7472ea2cae7d7e55', 'title': 'Small Pupils Lead to Lower Judgements of a Person’s Characteristics for Exaggerated, but Not for Realistic Pupils'}, {'paperId': '2d220c09cf160f73c182bc0cbf96bd3e4260360d', 'title': 'Impact of avatar facial anthropomorphism on body ownership, attractiveness and social presence in collaborative tasks in immersive virtual environments'}, {'paperId': '00def85f6effac0ef13ad87dac695a45a0ff3b4c', 'title': 'Using Facial Expressiveness of a Virtual Agent to Induce Empathy in Users'}, {'paperId': '9ace103db6bccfde198c27ed25900c1de4cc2a3c', 'title': ""Real-time simulation of virtual humans' emotional facial expressions, harnessing autonomic physiological and musculoskeletal control""}, {'paperId': '67f31df6cfec64a42c2161b0641b23d7a8405e17', 'title': 'Emotional Valence Recognition on Virtual, Robotic, and Human Faces: a Comparative Study'}, {'paperId': '0935fe0484bc369028fc67cadb207a3e0238accd', 'title': 'Emotional Valence Recognition on Virtual, Robotic, and Human Faces: a Comparative Study'}, {'paperId': '31f8aa4adbe9bdd567330e0f05777b94b5db3948', 'title': 'Science Arts & Métiers (SAM)'}]",{'name': 'Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents'},24.0,Expressive Virtual Human: Impact of expressive wrinkles and pupillary size on emotion recognition,2019.0
2600,e35277e5dcb61ac3b4077f3a08b02fba80ccf085,,"[{'authorId': '32565728', 'name': 'A. E. Basak'}, {'authorId': '1746035', 'name': 'U. Güdükbay'}, {'authorId': '2643744', 'name': 'Funda Durupinar'}]",26.0,"{'bibtex': '@Article{Basak2018UsingRL,\n author = {A. E. Basak and U. Güdükbay and Funda Durupinar},\n journal = {Comput. Graph.},\n pages = {70-81},\n title = {Using real life incidents for creating realistic virtual crowds with data-driven emotion contagion},\n volume = {72},\n year = {2018}\n}\n'}",,"{'volume': '72', 'pages': '70-81', 'name': 'Comput. Graph.'}",41.0,Using real life incidents for creating realistic virtual crowds with data-driven emotion contagion,2018.0
2601,e3569567eb3b2a0ed3b578e9481a140c9223759f,"ABSTRACT In this paper, we report on our efforts in developing affective character-based interfaces, i.e., interfaces that recognize and measure affective information of the user and address user affect by employing embodied characters. In particular, we describe the Empathic Companion, an animated interface agent that accompanies the user in the setting of a virtual job interview. This interface application takes physiological data (skin conductance and electromyography) of a user in realtime, interprets them as emotions, and addresses the user's affective states in the form of empathic feedback. The Empathic Companion is conceived as an educational agent that supports job seekers preparing for a job interview. We also present results from an exploratory study that aims to evaluate the impact of the Empathic Companion by measuring users' skin conductance and heart rate. While an overall positive effect of the Empathic Companion could not be shown, the outcome of the experiment suggests that empathic feedback has a positive effect on the interviewee's stress level while hearing the interviewer question.","[{'authorId': '2356111', 'name': 'H. Prendinger'}, {'authorId': '144666118', 'name': 'M. Ishizuka'}]",311.0,"{'bibtex': ""@Article{Prendinger2005THEEC,\n author = {H. Prendinger and M. Ishizuka},\n journal = {Applied Artificial Intelligence},\n pages = {267 - 285},\n title = {THE EMPATHIC COMPANION: A CHARACTER-BASED INTERFACE THAT ADDRESSES USERS' AFFECTIVE STATES},\n volume = {19},\n year = {2005}\n}\n""}",,"{'volume': '19', 'pages': '267 - 285', 'name': 'Applied Artificial Intelligence'}",30.0,THE EMPATHIC COMPANION: A CHARACTER-BASED INTERFACE THAT ADDRESSES USERS' AFFECTIVE STATES,2005.0
2603,e37d2293f3cf33aa8abf424cc9b9e8ddc94340b0,,"[{'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '144411873', 'name': 'R. Damiano'}]",1.0,"{'bibtex': '@Inproceedings{Battaglino2014NarrativeSA,\n author = {C. Battaglino and R. Damiano},\n pages = {36-39},\n title = {Narrative Scenarios as a Testbed for Moral Agents},\n year = {2014}\n}\n'}",,{'pages': '36-39'},12.0,Narrative Scenarios as a Testbed for Moral Agents,2014.0
2604,e3a472699ae4d2d53b8ff5115c59fb64f428eb93,,"[{'authorId': '145979199', 'name': 'J. Watson'}]",1.0,"{'bibtex': '@Inproceedings{Watson2011JohnW,\n author = {J. Watson},\n title = {John Watson - Little Albert},\n year = {2011}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,John Watson - Little Albert,2011.0
2605,e3a62233800cd8ed46ce82a856d2fd2df197e705,"In this article we describe results froman experiment of user interaction with autonomous , human - like ( humanoid ) conversational agents . We hypothesize that for embodied conversational agents , nonverbal behaviors related to the process of conversation , what we call envelope feedback, is much more important than other feedback , such as emotional expression . We test this hypothesis by having subjects interact with three autonomous agents , all capable of full - duplex multimodal interaction: able to generate and recognize speech , intonation , facial displays , and gesture . Each agent , however , gave a different kind of feedback: ( 1 ) content - related only , ( 2 ) content + envelope feedback , and ( 3 ) content + emotional . Content-related feedback includes answering questions and executing commands; envelope feedback includes behaviors such as gaze , manual beat gesture , and head movements; emotional feedback includes smiles and looks of puzzlement . Subjects' evaluations of the systemwere c...","[{'authorId': '145431806', 'name': 'Justine Cassell'}, {'authorId': '1727838', 'name': 'K. Thórisson'}]",451.0,"{'bibtex': '@Article{Cassell1999ThePO,\n author = {Justine Cassell and K. Thórisson},\n journal = {Appl. Artif. Intell.},\n pages = {519-538},\n title = {The Power of a Nod and a Glance: Envelope Vs. Emotional Feedback in Animated Conversational Agents},\n volume = {13},\n year = {1999}\n}\n'}",,"{'volume': '13', 'pages': '519-538', 'name': 'Appl. Artif. Intell.'}",0.0,The Power of a Nod and a Glance: Envelope Vs. Emotional Feedback in Animated Conversational Agents,1999.0
2607,e3f5e1473400d02936b6f52cddbce9fb0eb49cd9,,"[{'authorId': '71799216', 'name': 'J. Kilner'}, {'authorId': '1874712', 'name': 'Y. Paulignan'}, {'authorId': '2797046', 'name': 'S. Blakemore'}]",759.0,"{'bibtex': '@Article{Kilner2003AnIE,\n author = {J. Kilner and Y. Paulignan and S. Blakemore},\n journal = {Current Biology},\n pages = {522-525},\n title = {An Interference Effect of Observed Biological Movement on Action},\n volume = {13},\n year = {2003}\n}\n'}",,"{'volume': '13', 'pages': '522-525', 'name': 'Current Biology'}",23.0,An Interference Effect of Observed Biological Movement on Action,2003.0
2608,e4003dbeaf40c872b2f2346c22a66696c7bd6370,"In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.","[{'authorId': '40122420', 'name': 'C. Toprak'}, {'authorId': '1853561', 'name': 'Niklas Jakob'}, {'authorId': '1730400', 'name': 'Iryna Gurevych'}]",145.0,"{'bibtex': '@Inproceedings{Toprak2010SentenceAE,\n author = {C. Toprak and Niklas Jakob and Iryna Gurevych},\n pages = {575-584},\n title = {Sentence and Expression Level Annotation of Opinions in User-Generated Discourse},\n year = {2010}\n}\n'}",,{'pages': '575-584'},22.0,Sentence and Expression Level Annotation of Opinions in User-Generated Discourse,2010.0
2609,e40c4a0c8e0b888b6669db4a26d53e115e547901,,"[{'authorId': '28940714', 'name': 'Korrina A. Duffy'}, {'authorId': '6026289', 'name': 'T. Chartrand'}]",94.0,"{'bibtex': '@Article{Duffy2015MimicryCA,\n author = {Korrina A. Duffy and T. Chartrand},\n journal = {Current Opinion in Behavioral Sciences},\n pages = {112-116},\n title = {Mimicry: causes and consequences},\n volume = {3},\n year = {2015}\n}\n'}",,"{'volume': '3', 'pages': '112-116', 'name': 'Current Opinion in Behavioral Sciences'}",65.0,Mimicry: causes and consequences,2015.0
2610,e4196fc64eb6b9e5b1cfc4e457de6fc112df9a1a,,"[{'authorId': '2584600', 'name': 'G. Hofstede'}, {'authorId': '2584600', 'name': 'G. Hofstede'}, {'authorId': '39708729', 'name': 'M. Minkov'}, {'authorId': '114807699', 'name': 'McGraw-Hill New'}]",2815.0,"{'bibtex': '@Inproceedings{Hofstede2010CulturesAO,\n author = {G. Hofstede and G. Hofstede and M. Minkov and McGraw-Hill New},\n title = {Cultures and Organizations: Software of the Mind, 3rd ed.},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Cultures and Organizations: Software of the Mind, 3rd ed.",2010.0
2611,e423265bda7cc1260fe37813facb9b904429aa81,"Current methods of assessing psychopathology depend almost entirely on verbal report (clinical interview or questionnaire) of patients, their family, or caregivers. They lack systematic and efficient ways of incorporating behavioral observations that are strong indicators of psychological disorder, much of which may occur outside the awareness of either individual. We compared clinical diagnosis of major depression with automatically measured facial actions and vocal prosody in patients undergoing treatment for depression. Manual FACS coding, active appearance modeling (AAM) and pitch extraction were used to measure facial and vocal expression. Classifiers using leave-one-out validation were SVM for FACS and for AAM and logistic regression for voice. Both face and voice demonstrated moderate concurrent validity with depression. Accuracy in detecting depression was 88% for manual FACS and 79% for AAM. Accuracy for vocal prosody was 79%. These findings suggest the feasibility of automatic detection of depression, raise new issues in automated facial image analysis and machine learning, and have exciting implications for clinical theory and practice.","[{'authorId': '1737918', 'name': 'J. Cohn'}, {'authorId': '3306260', 'name': 'T. S. Kruez'}, {'authorId': '1711695', 'name': 'I. Matthews'}, {'authorId': '2118771372', 'name': 'Ying Yang'}, {'authorId': '1698158', 'name': 'Minh Hoai Nguyen'}, {'authorId': '31173734', 'name': 'M. T. Padilla'}, {'authorId': '2114903588', 'name': 'Feng Zhou'}, {'authorId': '143867160', 'name': 'F. D. L. Torre'}]",415.0,"{'bibtex': '@Article{Cohn2009DetectingDF,\n author = {J. Cohn and T. S. Kruez and I. Matthews and Ying Yang and Minh Hoai Nguyen and M. T. Padilla and Feng Zhou and F. D. L. Torre},\n journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},\n pages = {1-7},\n title = {Detecting depression from facial actions and vocal prosody},\n year = {2009}\n}\n'}",,"{'pages': '1-7', 'name': '2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops'}",50.0,Detecting depression from facial actions and vocal prosody,2009.0
2612,e445d42bc7958ffb97f478182cde4c2ea85a550e,"L'appreciation formative est un processus essentiel en apprentissage, il sert a juger et a evaluer le travail et les performances pour ensuite aider a enrichir les competences. Cet article donne un apercu du role de la retroaction en classe, de son role dans l'enseignement et dans l'apprentissage, et de sa contribution aux discussions sur les objectifs de rendement","[{'authorId': '12163592', 'name': 'P. Tunstall'}, {'authorId': '117404403', 'name': 'Caroline Gsipps'}]",353.0,"{'bibtex': '@Article{Tunstall1996TeacherFT,\n author = {P. Tunstall and Caroline Gsipps},\n journal = {British Educational Research Journal},\n pages = {389-404},\n title = {Teacher Feedback to Young Children in Formative Assessment: a typology},\n volume = {22},\n year = {1996}\n}\n'}",,"{'volume': '22', 'pages': '389-404', 'name': 'British Educational Research Journal'}",20.0,Teacher Feedback to Young Children in Formative Assessment: a typology,1996.0
2613,e44d334f46465300dbf365e3efc8680481644919,"Traditionally, experimental economics uses controlled and incentivized field and lab experiments to analyze economic behavior. However, investigating peer effects in the classic settings is challenging due to the reflection problem: Who is influencing whom? To overcome this, we enlarge the methodological toolbox of these experiments by means of Virtual Reality. After introducing and validating a real-effort sorting task, we embed a virtual agent as peer of a human subject, who independently performs an identical sorting task. We conducted two experiments investigating (a) the subject's productivity adjustment due to peer effects and (b) the incentive effects on competition. Our results indicate a great potential for Virtual-Reality-based economic experiments.","[{'authorId': '3249697', 'name': 'A. Bönsch'}, {'authorId': '39812907', 'name': 'J. Wendt'}, {'authorId': '47973447', 'name': 'H. Overath'}, {'authorId': '66809640', 'name': 'Ö. Gürerk'}, {'authorId': '2536104', 'name': 'C. Harbring'}, {'authorId': '8470293', 'name': 'C. Grund'}, {'authorId': '2856539', 'name': 'T. Kittsteiner'}, {'authorId': '144483066', 'name': 'T. Kuhlen'}]",12.0,"{'bibtex': '@Article{Bönsch2017PeersAW,\n author = {A. Bönsch and J. Wendt and H. Overath and Ö. Gürerk and C. Harbring and C. Grund and T. Kittsteiner and T. Kuhlen},\n journal = {2017 IEEE Virtual Reality (VR)},\n pages = {301-302},\n title = {Peers at work: Economic real-effort experiments in the presence of virtual co-workers},\n year = {2017}\n}\n'}",,"{'pages': '301-302', 'name': '2017 IEEE Virtual Reality (VR)'}",3.0,Peers at work: Economic real-effort experiments in the presence of virtual co-workers,2017.0
2615,e469f4ea694e7f1873886c6059bda116ff92b68d,"A between-group experiment was carried out to assess whether two different presence questionnaires can distinguish between real and virtual experiences. One group of ten subjects searched for a box in a real office environment. A second group of ten subjects carried out the same task in a virtual environment that simulated the same office. Immediately after their experience, subjects were given two different presence questionnaires in randomized order: the Witmer and Singer Presence (WS), and the questionnaire developed by Slater, Usoh, and Steed (SUS). The paper argues that questionnaires should be able to pass a reality test whereby under current conditions the presence scores should be higher for real experiences than for virtual ones. Nevertheless, only the SUS had a marginally higher mean score for the real compared to the virtual, and there was no significant difference at all between the WS mean scores. It is concluded that, although such questionnaires may be useful when all subjects experience the same type of environment, their utility is doubtful for the comparison of experiences across environments, such as immersive virtual compared to real, or desktop compared to immersive virtual.","[{'authorId': '2919479', 'name': 'M. Usoh'}, {'authorId': '40651681', 'name': 'E. Catena'}, {'authorId': '2126982905', 'name': 'S. Arman'}, {'authorId': '144931212', 'name': 'M. Slater'}]",795.0,"{'bibtex': '@Article{Usoh2000UsingPQ,\n author = {M. Usoh and E. Catena and S. Arman and M. Slater},\n journal = {Presence: Teleoperators & Virtual Environments},\n pages = {497-503},\n title = {Using Presence Questionnaires in Reality},\n volume = {9},\n year = {2000}\n}\n'}",,"{'volume': '9', 'pages': '497-503', 'name': 'Presence: Teleoperators & Virtual Environments'}",20.0,Using Presence Questionnaires in Reality,2000.0
2616,e472904815fbd41453edfaa7aeb51f837987f6c7,,"[{'authorId': '1690354', 'name': 'K. Macdorman'}, {'authorId': '2072877440', 'name': 'R. Green'}, {'authorId': '2472620', 'name': 'Chin-Chang Ho'}, {'authorId': '152439878', 'name': 'C. T. Koch'}]",446.0,"{'bibtex': '@Article{Macdorman2009TooRF,\n author = {K. Macdorman and R. Green and Chin-Chang Ho and C. T. Koch},\n journal = {Computers in human behavior},\n pages = {\n          695-710\n        },\n title = {Too real for comfort? Uncanny responses to computer generated faces},\n volume = {25 3},\n year = {2009}\n}\n'}",,"{'volume': '25 3', 'pages': '\n          695-710\n        ', 'name': 'Computers in human behavior'}",96.0,Too real for comfort? Uncanny responses to computer generated faces,2009.0
2617,e4901faa28061fd52ac4e7c64d21dc0085aa5fd3,"Studies indicate that making learners feel good is important only minor to clear knowledge transformation. Many studies have tried to use virtual humans as a part of interface in learning systems to increase the effect of instructions. Based on social interaction and pedagogical theories, many e-learning systems use animated films or virtual reality to boost human-computer engagement and ease their negative emotions. However, affective learning systems still need much research to improve their functionalities and usability. This study proposed a convenient approach to develop an emotionally interactive learning system; learners can express their emotions by mouse-clicking while learning. A virtual human was created to empathically react to learners in proactive and reactive ways to encourage and persuade them into persistent learning and help achieve their goals. Experimental results show that, averagely, subjects can tell virtual human's emotions and agree to its empathic reactions. Persuasion conducted by virtual human could not increase subjects' learning time, but could significantly increase their completion rate of exercises.","[{'authorId': '2388316', 'name': 'Chin-Yeh Wang'}, {'authorId': '2108973704', 'name': 'Gwo-Dong Chen'}, {'authorId': '98694949', 'name': 'Chen-Chung Liu'}, {'authorId': '1797298', 'name': 'Baw-Jhiune Liu'}]",29.0,"{'bibtex': '@Inproceedings{Wang2009DesignAE,\n author = {Chin-Yeh Wang and Gwo-Dong Chen and Chen-Chung Liu and Baw-Jhiune Liu},\n pages = {27-32},\n title = {Design an empathic virtual human to encourage and persuade learners in e-learning systems},\n year = {2009}\n}\n'}",,{'pages': '27-32'},23.0,Design an empathic virtual human to encourage and persuade learners in e-learning systems,2009.0
2618,e4beeff8cf47dcc0faf6efc8f4c1b3fefc052afb,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).","[{'authorId': '1763455', 'name': 'F. Burkhardt'}, {'authorId': '2840105', 'name': 'A. Paeschke'}, {'authorId': '104089060', 'name': 'M. Rolfes'}, {'authorId': '2711349', 'name': 'W. Sendlmeier'}, {'authorId': '35216383', 'name': 'Benjamin Weiss'}]",1970.0,"{'bibtex': '@Inproceedings{Burkhardt2005ADO,\n author = {F. Burkhardt and A. Paeschke and M. Rolfes and W. Sendlmeier and Benjamin Weiss},\n pages = {1517-1520},\n title = {A database of German emotional speech},\n year = {2005}\n}\n'}",,{'pages': '1517-1520'},24.0,A database of German emotional speech,2005.0
2619,e4d739ffa79a607008f3a251912083a23ce37607,,"[{'authorId': '1854783', 'name': 'J. Pennebaker'}, {'authorId': '48182912', 'name': 'R. Booth'}, {'authorId': '144896996', 'name': 'Martha E. Francis'}]",2367.0,"{'bibtex': '@Inproceedings{Pennebaker2007LinguisticIA,\n author = {J. Pennebaker and R. Booth and Martha E. Francis},\n title = {Linguistic Inquiry and Word Count (LIWC2007)},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",11.0,Linguistic Inquiry and Word Count (LIWC2007),2007.0
2620,e4dbf9fb160e286bea83b5b8f04063c867f6b186,,"[{'authorId': '1714059', 'name': 'Bas R. Steunebrink'}, {'authorId': '1707738', 'name': 'M. Dastani'}, {'authorId': '1691228', 'name': 'J. Meyer'}]",30.0,"{'bibtex': '@Inproceedings{Steunebrink2009AFM,\n author = {Bas R. Steunebrink and M. Dastani and J. Meyer},\n pages = {174-186},\n title = {A Formal Model of Emotion-Based Action Tendency for Intelligent Agents},\n year = {2009}\n}\n'}",,{'pages': '174-186'},25.0,A Formal Model of Emotion-Based Action Tendency for Intelligent Agents,2009.0
2621,e504d9e6195c781b735aca55e7fb6dce3bf13647,"This paper presents the post‐mortem report upon completion of the Long Lamai e‐commerce development project. Some weaknesses with regards to the current software modelling approach are identified and an alternative role‐based approach is proposed. We argue that the existing software modelling technique is not suitable for modelling, making it difficult to establish a good contract between stakeholders causing delays in the project delivery. The role‐based approach is able to explicitly highlight the responsibilities among stakeholders, while also forming the contract agreement among them leading towards sustainable ICT4D.","[{'authorId': '3078761', 'name': 'W. Cheah'}, {'authorId': '30632706', 'name': 'Alfian Abdul Halin'}, {'authorId': '2236265', 'name': 'Marlene Lu'}, {'authorId': '73734171', 'name': 'Gary CheeWhye'}]",16.0,"{'bibtex': '@Article{Cheah2016LongLC,\n author = {W. Cheah and Alfian Abdul Halin and Marlene Lu and Gary CheeWhye},\n journal = {The Electronic Journal of Information Systems in Developing Countries},\n title = {Long Lamai Community ICT4D E‐Commerce System Modelling: An Agent Oriented Role‐Based Approach},\n volume = {75},\n year = {2016}\n}\n'}",,"{'volume': '75', 'name': 'The Electronic Journal of Information Systems in Developing Countries'}",18.0,Long Lamai Community ICT4D E‐Commerce System Modelling: An Agent Oriented Role‐Based Approach,2016.0
2622,e53336fbf78e5e1d6da2a0ab867cb0412200535c,,"[{'authorId': '121050155', 'name': 'C. Allen'}]",467.0,"{'bibtex': '@Article{Allen1947DimensionsOP,\n author = {C. Allen},\n journal = {Nature},\n pages = {887-887},\n title = {Dimensions of Personality},\n volume = {160},\n year = {1947}\n}\n'}",,"{'volume': '160', 'pages': '887-887', 'name': 'Nature'}",0.0,Dimensions of Personality,1947.0
2623,e5477563a138fa4dcceda35eab4f539408765276,"This paper presents a realistic visual speech synthesis based on the hybrid concatenation method. Unlike previous methods based on phoneme level unit selection or hidden Markov model (HMM), etc., the hybrid concatenation method uses a frame level-based unit selection method combined with a fused HMM, and is able to generate more expressive and stable facial animations. The fused HMM can be used to explicitly model the loose synchronization of tightly coupled streams, with much better results than a normal HMM for audiovisual mapping. After fused HMM is created, facial animation is generated via the unit selection method at the frame level by using the fused HMM output probabilities. To accelerate the computing efficiency of the unit selection on a large corpus, this paper also proposes a two-layer Viterbi search method in which only the subsets that have been selected in the first layer are further checked in the second layer. Using this idea, the system has been successfully integrated into real-time applications. Furthermore, the paper also proposes a mapping method to generate emotional facial expressions from neutral facial expressions based on Gaussian mixture models (GMMs). Final experiments prove that the method described can output synthesized facial parameters with high quality. Compared with other audiovisual mapping methods, our method has better performance with respect to expressiveness, stability, and system running speed.","[{'authorId': '37670752', 'name': 'J. Tao'}, {'authorId': '2070509641', 'name': 'Le Xin'}, {'authorId': '2526255', 'name': 'Panrong Yin'}]",24.0,"{'bibtex': '@Article{Tao2009RealisticVS,\n author = {J. Tao and Le Xin and Panrong Yin},\n journal = {IEEE Transactions on Audio, Speech, and Language Processing},\n pages = {469-477},\n title = {Realistic Visual Speech Synthesis Based on Hybrid Concatenation Method},\n volume = {17},\n year = {2009}\n}\n'}",,"{'volume': '17', 'pages': '469-477', 'name': 'IEEE Transactions on Audio, Speech, and Language Processing'}",34.0,Realistic Visual Speech Synthesis Based on Hybrid Concatenation Method,2009.0
2624,e5529db6978e4afc46bd03e85e06092f07253624,,"[{'authorId': '22686353', 'name': 'J. A. Greene'}, {'authorId': '145394858', 'name': 'R. Azevedo'}]",308.0,"{'bibtex': '@Article{Greene2009AMA,\n author = {J. A. Greene and R. Azevedo},\n journal = {Contemporary Educational Psychology},\n pages = {18-29},\n title = {A macro-level analysis of SRL processes and their relations to the acquisition of a sophisticated mental model of a complex system☆☆☆},\n volume = {34},\n year = {2009}\n}\n'}",,"{'volume': '34', 'pages': '18-29', 'name': 'Contemporary Educational Psychology'}",80.0,A macro-level analysis of SRL processes and their relations to the acquisition of a sophisticated mental model of a complex system☆☆☆,2009.0
2625,e587a613ebe6484483a7a0ea79078ebdb6f46639,"Social dilemmas are situations in which collective interests are at odds with private interests: pollution, depletion of natural resources, and intergroup conflicts, are at their core social dilemmas. Because of their multidisciplinarity and their importance, social dilemmas have been studied by economists, biologists, psychologists, sociologists, and political scientists. These studies typically explain tendency to cooperation by dividing people in proself and prosocial types, or appealing to forms of external control or, in iterated social dilemmas, to long-term strategies. But recent experiments have shown that cooperation is possible even in one-shot social dilemmas without forms of external control and the rate of cooperation typically depends on the payoffs. This makes impossible a predictive division between proself and prosocial people and proves that people have attitude to cooperation by nature. The key innovation of this article is in fact to postulate that humans have attitude to cooperation by nature and consequently they do not act a priori as single agents, as assumed by standard economic models, but they forecast how a social dilemma would evolve if they formed coalitions and then they act according to their most optimistic forecast. Formalizing this idea we propose the first predictive model of human cooperation able to organize a number of different experimental findings that are not explained by the standard model. We show also that the model makes satisfactorily accurate quantitative predictions of population average behavior in one-shot social dilemmas.","[{'authorId': '31558629', 'name': 'V. Capraro'}]",136.0,"{'bibtex': '@Article{Capraro2013AMO,\n author = {V. Capraro},\n journal = {PLoS ONE},\n title = {A Model of Human Cooperation in Social Dilemmas},\n volume = {8},\n year = {2013}\n}\n'}",,"{'volume': '8', 'name': 'PLoS ONE'}",43.0,A Model of Human Cooperation in Social Dilemmas,2013.0
2626,e5991b24a0375041855b2b4f493a7d6a0094f0ae,"The majority of current systems for end-to-end dialog generation focus on response quality without an explicit control over the affective content of the responses. In this paper, we present an affect-driven dialog system, which generates emotional responses in a controlled manner using a continuous representation of emotions. The system achieves this by modeling emotions at a word and sequence level using: (1) a vector representation of the desired emotion, (2) an affect regularizer, which penalizes neutral words, and (3) an affect sampling method, which forces the neural network to generate diverse words that are emotionally relevant. During inference, we use a re-ranking procedure that aims to extract the most emotionally relevant responses using a human-in-the-loop optimization process. We study the performance of our system in terms of both quantitative (BLEU score and response diversity), and qualitative (emotional appropriateness) measures.","[{'authorId': '46985469', 'name': 'Pierre Colombo'}, {'authorId': '37297179', 'name': 'Wojciech Witon'}, {'authorId': '2477939', 'name': 'Ashutosh Modi'}, {'authorId': '2113614351', 'name': 'J. Kennedy'}, {'authorId': '143980996', 'name': 'Mubbasir Kapadia'}]",86.0,"{'bibtex': '@Article{Colombo2019AffectDrivenDG,\n author = {Pierre Colombo and Wojciech Witon and Ashutosh Modi and J. Kennedy and Mubbasir Kapadia},\n journal = {ArXiv},\n title = {Affect-Driven Dialog Generation},\n volume = {abs/1904.02793},\n year = {2019}\n}\n'}",,"{'volume': 'abs/1904.02793', 'name': 'ArXiv'}",31.0,Affect-Driven Dialog Generation,2019.0
2627,e5bec5f61839a5e489a5b2cf897d699b61fa9254,"
 
 The presence of interesting and compelling characters is an essential component of effective narrative. Well-developed characters have features that enable them to significantly enhance the believability and quality of a story. In this paper, we describe the results of an experiment to evaluate a planning-based narrative generation system that focuses on the generation of stories that express character. The system is designed to automatically produce narratives that show character personality traits through the choices characters make when selecting the means by which they achieve their goals. Results from our study support the hypothesis that an audience presented with stories generated by Mask will attribute personality traits to the story characters that have significant correlation with the computational model of personality used to drive the characters' choices.
 
","[{'authorId': '2842986', 'name': 'Julio César Bahamón'}, {'authorId': '145513579', 'name': 'R. Young'}]",16.0,"{'bibtex': '@Inproceedings{Bahamón2021AnEE,\n author = {Julio César Bahamón and R. Young},\n pages = {144-150},\n title = {An Empirical Evaluation of a Generative Method for the Expression of Personality Traits through Action Choice},\n year = {2021}\n}\n'}",,{'pages': '144-150'},29.0,An Empirical Evaluation of a Generative Method for the Expression of Personality Traits through Action Choice,2021.0
2628,e5fce53c30a9a9f8a28e96b43b9ed74595bc874c,,"[{'authorId': '9569946', 'name': 'Leonid Berov'}]",9.0,"{'bibtex': '@Inproceedings{Berov2017SteeringPT,\n author = {Leonid Berov},\n pages = {293-299},\n title = {Steering Plot Through Personality and Affect: An Extended BDI Model of Fictional Characters},\n year = {2017}\n}\n'}",,{'pages': '293-299'},13.0,Steering Plot Through Personality and Affect: An Extended BDI Model of Fictional Characters,2017.0
2629,e609684188c842092ae7bcae81429471e422df4d,"Human speech is often accompanied by hand and arm gestures. We present a method for cross-modal translation from ""in-the-wild"" monologue speech of a single speaker to their conversational gesture motion. We train on unlabeled videos for which we only have noisy pseudo ground truth from an automatic pose detection system. Our proposed model significantly outperforms baseline methods in a quantitative comparison. To support research toward obtaining a computational understanding of the relationship between gesture and speech, we release a large video dataset of person-specific gestures.","[{'authorId': '2361255', 'name': 'Shiry Ginosar'}, {'authorId': '2063958674', 'name': 'Amir Bar'}, {'authorId': '51169014', 'name': 'Gefen Kohavi'}, {'authorId': '50993063', 'name': 'Caroline Chan'}, {'authorId': '144956994', 'name': 'Andrew Owens'}, {'authorId': '143751119', 'name': 'Jitendra Malik'}]",232.0,"{'bibtex': '@Article{Ginosar2019LearningIS,\n author = {Shiry Ginosar and Amir Bar and Gefen Kohavi and Caroline Chan and Andrew Owens and Jitendra Malik},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3492-3501},\n title = {Learning Individual Styles of Conversational Gesture},\n year = {2019}\n}\n'}",,"{'pages': '3492-3501', 'name': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'}",49.0,Learning Individual Styles of Conversational Gesture,2019.0
2630,e653f5024773a5bc079e72a862de5a5af784ff33,"We present the second Audio-Visual Emotion recognition Challenge and workshop (AVEC 2012), which aims to bring together researchers from the audio and video analysis communities around the topic of emotion recognition. The goal of the challenge is to recognise four continuously valued affective dimensions: arousal, expectancy, power, and valence. There are two sub-challenges: in the Fully Continuous Sub-Challenge participants have to predict the values of the four dimensions at every moment during the recordings, while for the Word-Level Sub-Challenge a single prediction has to be given per word uttered by the user. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks.","[{'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '1751126', 'name': 'F. Eyben'}, {'authorId': '145635430', 'name': 'R. Cowie'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",294.0,"{'bibtex': '@Inproceedings{Schuller2012AVEC2T,\n author = {Björn Schuller and M. Valstar and F. Eyben and R. Cowie and M. Pantic},\n pages = {449-456},\n title = {AVEC 2012: the continuous audio/visual emotion challenge},\n year = {2012}\n}\n'}",,{'pages': '449-456'},20.0,AVEC 2012: the continuous audio/visual emotion challenge,2012.0
2631,e65594a323f979d3366b108fa30bddde5a9f8d90,"We propose a novel dialogue modeling framework, the first-ever nonparametric kernel functions based approach for dialogue modeling, which learns kernelized hashcodes as compressed text representations; unlike traditional deep learning models, it handles well relatively small datasets, while also scaling to large ones. We also derive a novel lower bound on mutual information, used as a model-selection criterion favoring representations with better alignment between the utterances of participants in a collaborative dialogue setting, as well as higher predictability of the generated responses. As demonstrated on three real-life datasets, including prominently psychotherapy sessions, the proposed approach significantly outperforms several state-of-art neural network based dialogue systems, both in terms of computational efficiency, reducing training time from days or weeks to hours, and the response quality, achieving an order of magnitude improvement over competitors in frequency of being chosen as the best model by human evaluators.","[{'authorId': '144529280', 'name': 'S. Garg'}, {'authorId': '2109771', 'name': 'I. Rish'}, {'authorId': '40193335', 'name': 'G. Cecchi'}, {'authorId': '3436466', 'name': 'Palash Goyal'}, {'authorId': '3022427', 'name': 'Sarik Ghazarian'}, {'authorId': '3092863', 'name': 'Shuyang Gao'}, {'authorId': '1719898', 'name': 'G. V. Steeg'}, {'authorId': '143728483', 'name': 'A. Galstyan'}]",2.0,"{'bibtex': '@Article{Garg2018ModelingPD,\n author = {S. Garg and I. Rish and G. Cecchi and Palash Goyal and Sarik Ghazarian and Shuyang Gao and G. V. Steeg and A. Galstyan},\n journal = {arXiv: Learning},\n title = {Modeling Psychotherapy Dialogues with Kernelized Hashcode Representations: A Nonparametric Information-Theoretic Approach.},\n year = {2018}\n}\n'}",,"{'volume': '', 'name': 'arXiv: Learning'}",76.0,Modeling Psychotherapy Dialogues with Kernelized Hashcode Representations: A Nonparametric Information-Theoretic Approach.,2018.0
2632,e6834f74b95702987b6c51d3bd88fa394ef6bc51,"User experience is an emerging research area with a range of issues to be resolved. Among them, the measurability of UX remains contentious. The key argument hinges on the meaningfulness, validity and usefulness of reducing fuzzy experiential qualities such as fun, challenge and trust to numbers. UX people seem ambivalent towards UX measures. In UX empirical studies, qualitative approaches are predominant, though the popular use of questionnaires in these studies suggests that some form of numeric measures is deemed useful or even necessary. The tension between the two camps (i.e. qualitative design-based and quantitative model-based) stimulates scientific discussions to bring the field forward. As measures may enable us to predict, the concomitant issue of UX predictability is explored. Besides, we look into theoretical frameworks that potentially contribute to a deeper understanding of UX. Of particular interest is theory of memory.","[{'authorId': '1732194', 'name': 'E. Law'}]",127.0,"{'bibtex': '@Inproceedings{Law2011TheMA,\n author = {E. Law},\n pages = {1-10},\n title = {The measurability and predictability of user experience},\n year = {2011}\n}\n'}",,{'pages': '1-10'},52.0,The measurability and predictability of user experience,2011.0
2633,e6bd15f3483bd2e54533802439864d86f7a307dc,"This article presents an algorithm for learning hatching styles from line drawings. An artist draws a single hatching illustration of a 3D object. Her strokes are analyzed to extract the following per-pixel properties: hatching level (hatching, cross-hatching, or no strokes), stroke orientation, spacing, intensity, length, and thickness. A mapping is learned from input geometric, contextual, and shading features of the 3D object to these hatching properties, using classification, regression, and clustering techniques. Then, a new illustration can be generated in the artist's style, as follows. First, given a new view of a 3D object, the learned mapping is applied to synthesize target stroke properties for each pixel. A new illustration is then generated by synthesizing hatching strokes according to the target properties.","[{'authorId': '2808670', 'name': 'E. Kalogerakis'}, {'authorId': '1795014', 'name': 'D. Nowrouzezahrai'}, {'authorId': '1984863', 'name': 'Simon Breslav'}, {'authorId': '1747779', 'name': 'Aaron Hertzmann'}]",572.0,"{'bibtex': '@Article{Kalogerakis2012LearningHF,\n author = {E. Kalogerakis and D. Nowrouzezahrai and Simon Breslav and Aaron Hertzmann},\n journal = {ACM Trans. Graph.},\n pages = {1:1-1:17},\n title = {Learning hatching for pen-and-ink illustration of surfaces},\n volume = {31},\n year = {2012}\n}\n'}",,"{'volume': '31', 'pages': '1:1-1:17', 'name': 'ACM Trans. Graph.'}",52.0,Learning hatching for pen-and-ink illustration of surfaces,2012.0
2634,e6d09d04fc8737094c193da471e2a50a809f77d4,"The STAI serves as an indicator of two types of anxiety, the state and trait anxiety, and measure the severity of the overall anxiety level.The STAI, which is appropriate for those who have at least a sixth grade reading level, contains four-point Likert items. The instrument is divided into two sections, each having twenty questions. Approximately 15 minutes are required for adults to complete the both STAI. The number on the scale is positively correlated to the anxiety related to in the question.","[{'authorId': '118670580', 'name': 'C. Spielberger'}, {'authorId': '5144541', 'name': 'R. Gorsuch'}, {'authorId': '50660087', 'name': 'R. Lushene'}]",24928.0,"{'bibtex': '@Inproceedings{Spielberger1970ManualFT,\n author = {C. Spielberger and R. Gorsuch and R. Lushene},\n title = {Manual for the State-Trait Anxiety Inventory},\n year = {1970}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Manual for the State-Trait Anxiety Inventory,1970.0
2635,e6d92b5fe5eca7ccfa6c9bd36081a05df9414a05,,"[{'authorId': '50604395', 'name': 'Matt Jones'}, {'authorId': '145705027', 'name': 'G. Buchanan'}, {'authorId': '1402145527', 'name': 'N. Mohd-Nasir'}]",306.0,"{'bibtex': '@Inproceedings{Jones1999AnEO,\n author = {Matt Jones and G. Buchanan and N. Mohd-Nasir},\n pages = {343-345},\n title = {An Evaluation of WebTwig - A Site Outliner for Handheld Web Access},\n year = {1999}\n}\n'}",,{'pages': '343-345'},2.0,An Evaluation of WebTwig - A Site Outliner for Handheld Web Access,1999.0
2636,e6e7d40173d0b1ac06ae3b5a804297d4e199f531,,"[{'authorId': '27516716', 'name': 'Daniel P. Kennedy'}, {'authorId': '2151203', 'name': 'J. Gläscher'}, {'authorId': '2987533', 'name': 'J. Tyszka'}, {'authorId': '46306086', 'name': 'R. Adolphs'}]",360.0,"{'bibtex': '@Article{Kennedy2009PersonalSR,\n author = {Daniel P. Kennedy and J. Gläscher and J. Tyszka and R. Adolphs},\n journal = {Nature neuroscience},\n pages = {1226 - 1227},\n title = {Personal Space Regulation by the Human Amygdala},\n volume = {12},\n year = {2009}\n}\n'}",,"{'volume': '12', 'pages': '1226 - 1227', 'name': 'Nature neuroscience'}",24.0,Personal Space Regulation by the Human Amygdala,2009.0
2637,e709201c10a53e55c0a79edef10b866e92cd4453,"Facial expressions convey important information on emotional states of our interaction partners. However, in interactions between younger and older adults, there is evidence for a reduced ability to accurately decode emotional facial expressions. Previous studies have often followed up this phenomenon by examining the effect of the observers' age. However, decoding emotional faces is also likely to be influenced by stimulus features, and age-related changes in the face such as wrinkles and folds may render facial expressions of older adults harder to decode. In this paper, we review theoretical frameworks and empirical findings on age effects on decoding emotional expressions, with an emphasis on age-of-face effects. We conclude that the age of the face plays an important role for facial expression decoding. Lower expressivity, age-related changes in the face, less elaborated emotion schemas for older faces, negative attitudes toward older adults, and different visual scan patterns and neural processing of older than younger faces may lower decoding accuracy for older faces. Furthermore, age-related stereotypes and age-related changes in the face may bias the attribution of specific emotions such as sadness to older faces.","[{'authorId': '3510001', 'name': 'Mara Fölster'}, {'authorId': '3067657', 'name': 'U. Hess'}, {'authorId': '5487174', 'name': 'K. Werheid'}]",107.0,"{'bibtex': '@Article{Fölster2014FacialAA,\n author = {Mara Fölster and U. Hess and K. Werheid},\n journal = {Frontiers in Psychology},\n title = {Facial age affects emotional expression decoding},\n volume = {5},\n year = {2014}\n}\n'}",,"{'volume': '5', 'name': 'Frontiers in Psychology'}",104.0,Facial age affects emotional expression decoding,2014.0
2638,e721eec2db17866294bcb55111031a814048a9b0,"Using results in the psycholinguistics literature on the speed and timing of American Sign Language (ASL), we built algorithms to calculate the time-duration of signs and the location/length of pauses during an ASL animation. We conducted a study in which native ASL signers evaluated the ASL animations processed by our algorithms, and we found that: (1) adding linguistically motivated pauses and variations in sign-durations improved signers' performance on a comprehension task and (2) these animations were rated as more understandable by ASL signers.","[{'authorId': '1747703', 'name': 'Matt Huenerfauth'}]",38.0,"{'bibtex': '@Inproceedings{Huenerfauth2008EvaluationOA,\n author = {Matt Huenerfauth},\n pages = {129-136},\n title = {Evaluation of a psycholinguistically motivated timing model for animations of american sign language},\n year = {2008}\n}\n'}",,{'pages': '129-136'},21.0,Evaluation of a psycholinguistically motivated timing model for animations of american sign language,2008.0
2639,e7659c1d3ab3051b7f4c78d878a7dde5c104537a,"This study conceptualized perceived trustworthiness of the individual, self-disclosure to the individual, perceived trustworthiness of people in general, and disclosive tendencies to other people in general to be indicants of a broader construct of trust. Self-disclosure and perceived trustworthiness of the individual were found to be related constructs assessing differential aspects of the trust construct. Likewise, self-disclosure and perceived trustworthiness of the individual were found to be criterial attributes of interpersonal solidarity. These communication-related phenomena indicated the solidarity of interpersonal relationships. In the progress of the research, a 20-item measure of perceived interpersonal solidarity was developed as a criterion for assessing the impact of communication-related variables on interpersonal relationships. Other exploratory research issues were investigated.","[{'authorId': '118894644', 'name': 'Lawrence R. Wheeless'}]",420.0,"{'bibtex': '@Article{Wheeless1978AFS,\n author = {Lawrence R. Wheeless},\n journal = {Human Communication Research},\n pages = {143-157},\n title = {A Follow-up Study of the Relationships among Trust, Disclosure, and Interpersonal Solidarity},\n volume = {4},\n year = {1978}\n}\n'}",,"{'volume': '4', 'pages': '143-157', 'name': 'Human Communication Research'}",24.0,"A Follow-up Study of the Relationships among Trust, Disclosure, and Interpersonal Solidarity",1978.0
2640,e7820ccccd9e7b8f6a84bdcef5cc7f8d6301fbd5,"An encounter with a virtual person can be one of the most compelling experiences in immersive virtual reality, as Mel Slater and his group have shown in many experiments on social interaction in VR. Much of this is due to virtual reality's ability to accurately represent body language, since participants can share a 3D space with a character. However, creating virtual characters capable of body language is a challenging task. It is a tacit, embodied skill that cannot be well represented in code. This paper surveys a series of experiments performed by Mel Slater and colleagues that show the power of Virtual Characters in VR and summarizes details of the technical infrastructure used, and Slater's theories of why virtual characters are effective. It they discusses the issues involved in creating virtual characters and the type of tool required. It concludes by proposing that Interactive Machine Learning can provide this type of tool.","[{'authorId': '2589934', 'name': 'M. Gillies'}]",2.0,"{'bibtex': '@Article{Gillies2018CreatingVC,\n author = {M. Gillies},\n journal = {Proceedings of the 5th International Conference on Movement and Computing},\n title = {Creating Virtual Characters},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 5th International Conference on Movement and Computing'},18.0,Creating Virtual Characters,2018.0
2641,e78b1370eb38c842b1c280f78e08da04f2702a24,,"[{'authorId': '2660575', 'name': 'P. Dodds'}, {'authorId': '1783914', 'name': 'D. Watts'}]",381.0,"{'bibtex': '@Article{Dodds2005AGM,\n author = {P. Dodds and D. Watts},\n journal = {Journal of theoretical biology},\n pages = {\n          587-604\n        },\n title = {A generalized model of social and biological contagion.},\n volume = {232 4},\n year = {2005}\n}\n'}",,"{'volume': '232 4', 'pages': '\n          587-604\n        ', 'name': 'Journal of theoretical biology'}",37.0,A generalized model of social and biological contagion.,2005.0
2642,e7dbd9ba29c59c68a9dae9f40dfc4040476c4624,"The authors report results from 5 experiments that describe the influence of emotional states on trust. They found that incidental emotions significantly influence trust in unrelated settings. Happiness and gratitude—emotions with positive valence—increase trust, and anger—an emotion with negative va-lence—decreases trust. Specifically, they found that emotions characterized by other-person control (anger and gratitude) and weak control appraisals (happiness) influence trust significantly more than emotions characterized by personal control (pride and guilt) or situational control (sadness). These findings suggest that emotions are more likely to be misattributed when the appraisals of the emotion are consistent with the judgment task than when the appraisals of the emotion are inconsistent with the judgment task. Emotions do not influence trust when individuals are aware of the source of their emotions or when individuals are very familiar with the trustee.","[{'authorId': '49395952', 'name': 'Jennifer R Dunn'}, {'authorId': '1889202', 'name': 'M. Schweitzer'}, {'authorId': '1838029', 'name': 'Sigal G. Barsade'}, {'authorId': '2237804723', 'name': 'Rachel Croson'}, {'authorId': '2252561465', 'name': 'Jack Hershey'}, {'authorId': '2241415470', 'name': 'Howard C. Kunreuther'}, {'authorId': '2252085673', 'name': 'Robert Meyer'}, {'authorId': '2252855176', 'name': 'Nancy Rothbard'}, {'authorId': '2251689914', 'name': 'Deborah'}, {'authorId': '2252182640', 'name': 'Bryan Chao'}, {'authorId': '2252048107', 'name': 'Catherine Marks'}, {'authorId': '2252027618', 'name': 'Samantha Rudolph'}, {'authorId': '2252866335', 'name': 'Lauren Sokolowski'}, {'authorId': '74750104', 'name': 'Liya'}, {'authorId': '2252199027', 'name': 'R. Dunn'}]",1128.0,"{'bibtex': '@Inproceedings{Dunn2005FeelingAB,\n author = {Jennifer R Dunn and M. Schweitzer and Sigal G. Barsade and Rachel Croson and Jack Hershey and Howard C. Kunreuther and Robert Meyer and Nancy Rothbard and Deborah and Bryan Chao and Catherine Marks and Samantha Rudolph and Lauren Sokolowski and Liya and R. Dunn},\n title = {Feeling and Believing: The Influence of Emotion on Trust},\n year = {2005}\n}\n'}",,,62.0,Feeling and Believing: The Influence of Emotion on Trust,2005.0
2644,e7ddfcc5c04c641b1f1e319e6bdee71ae8cd63e7,,"[{'authorId': '6895470', 'name': 'A. Gallup'}, {'authorId': '2058487293', 'name': 'Andrew Chong'}, {'authorId': '5097081', 'name': 'A. Kacelnik'}, {'authorId': '31533290', 'name': 'J. Krebs'}, {'authorId': '3191663', 'name': 'I. Couzin'}]",26.0,"{'bibtex': '@Article{Gallup2014TheIO,\n author = {A. Gallup and Andrew Chong and A. Kacelnik and J. Krebs and I. Couzin},\n journal = {Scientific Reports},\n title = {The influence of emotional facial expressions on gaze-following in grouped and solitary pedestrians},\n volume = {4},\n year = {2014}\n}\n'}",,"{'volume': '4', 'name': 'Scientific Reports'}",27.0,The influence of emotional facial expressions on gaze-following in grouped and solitary pedestrians,2014.0
2645,e85d34f6af375d09b5563bb588aae2f9b06e7ff2,,"[{'authorId': '2685682', 'name': 'Rachel F. Adler'}, {'authorId': '1878195', 'name': 'Francisco Iacobelli'}, {'authorId': '3387186', 'name': 'Yehuda Gutstein'}]",22.0,"{'bibtex': '@Article{Adler2016AreYC,\n author = {Rachel F. Adler and Francisco Iacobelli and Yehuda Gutstein},\n journal = {Comput. Hum. Behav.},\n pages = {75-81},\n title = {Are you convinced? A Wizard of Oz study to test emotional vs. rational persuasion strategies in dialogues},\n volume = {57},\n year = {2016}\n}\n'}",,"{'volume': '57', 'pages': '75-81', 'name': 'Comput. Hum. Behav.'}",31.0,Are you convinced? A Wizard of Oz study to test emotional vs. rational persuasion strategies in dialogues,2016.0
2646,e87031959451b2d9518a490bfabe308b1e265a62,,"[{'authorId': '144447024', 'name': 'N. Fridman'}, {'authorId': '1725049', 'name': 'G. Kaminka'}]",40.0,"{'bibtex': '@Article{Fridman2010ModelingPC,\n author = {N. Fridman and G. Kaminka},\n journal = {Computational and Mathematical Organization Theory},\n pages = {348-372},\n title = {Modeling pedestrian crowd behavior based on\xa0a\xa0cognitive model of social comparison theory},\n volume = {16},\n year = {2010}\n}\n'}",,"{'volume': '16', 'pages': '348-372', 'name': 'Computational and Mathematical Organization Theory'}",42.0,Modeling pedestrian crowd behavior based on a cognitive model of social comparison theory,2010.0
2647,e8a9fc1a52522acb6882bf7112f0c99eafc7638e,"We present a novel interactive approach, PedVR, to generate plausible behaviors for a large number of virtual humans, and to enable natural interaction between the real user and virtual agents. Our formulation is based on a coupled approach that combines a 2D multi-agent navigation algorithm with 3D human motion synthesis. The coupling can result in plausible movement of virtual agents and can generate gazing behaviors, which can considerably increase the believability. We have integrated our formulation with the DK-2 HMD and demonstrate the benefits of our crowd simulation algorithm over prior decoupled approaches. Our user evaluation suggests that the combination of coupled methods and gazing behavior can considerably increase the behavioral plausibility.","[{'authorId': '3294674', 'name': 'S. Narang'}, {'authorId': '10817944', 'name': 'A. Best'}, {'authorId': '3352747', 'name': 'Tanmay Randhavane'}, {'authorId': '145109163', 'name': 'Ari Shapiro'}, {'authorId': '1699159', 'name': 'Dinesh Manocha'}]",54.0,"{'bibtex': '@Article{Narang2016PedVRSG,\n author = {S. Narang and A. Best and Tanmay Randhavane and Ari Shapiro and Dinesh Manocha},\n journal = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},\n title = {PedVR: simulating gaze-based interactions between a real user and virtual crowds},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology'},46.0,PedVR: simulating gaze-based interactions between a real user and virtual crowds,2016.0
2649,e8b12467bdc20bde976750b8a28decdb33246d1d,"We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.","[{'authorId': '48950628', 'name': 'Navneet Dalal'}, {'authorId': '1756114', 'name': 'B. Triggs'}]",32546.0,"{'bibtex': ""@Article{Dalal2005HistogramsOO,\n author = {Navneet Dalal and B. Triggs},\n journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},\n pages = {886-893 vol. 1},\n title = {Histograms of oriented gradients for human detection},\n volume = {1},\n year = {2005}\n}\n""}",,"{'volume': '1', 'pages': '886-893 vol. 1', 'name': ""2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)""}",26.0,Histograms of oriented gradients for human detection,2005.0
2650,e8fa407b73585881e8af675b23009183ad9ffaac,"Emotion regulation has been proposed to be a transdiagnostic factor in the development and maintenance of psychopathology in the general population, yet the nature of the relationships between emotion regulation strategy use and psychological well-being has not been comprehensively explored in individuals with autism spectrum disorder (ASD). The aim of this study was to assess how the individual differences in self-reported emotion regulation strategy use relate to levels of both positive and negative psychological well-being. In total, 56 individuals with ASD aged 14–24 years (Mage = 18.15; SDage = 2.30) completed Emotion Regulation Questionnaire, Diagnostic and Statistical Manual of Mental Disorders-5 Generalized Anxiety Disorder Dimensional Scale, Patient Health Questionnaire-9, Warwick-Edinburgh Mental Well-being Scale and Autism-Spectrum Quotient – Short. Individuals were grouped into four clusters based on their Emotion Regulation Questionnaire subscale scores. Individuals in the high suppression and low reappraisal group expressed higher depressive symptoms and lower positive well-being when compared with the low suppression and high reappraisal group. Interestingly, individuals who self-reported using both high suppression and reappraisal expressed relatively high positive well-being and low depression symptoms. We suggest that the maladaptive effect of habitual suppression usage may be buffered by the habitual use of reappraisal, and this interaction between adaptive and maladaptive emotion regulation strategy use has clinical implications.","[{'authorId': '11450626', 'name': 'R. Cai'}, {'authorId': '6848517', 'name': 'A. Richdale'}, {'authorId': '4697606', 'name': 'C. Dissanayake'}, {'authorId': '143764000', 'name': 'J. Trollor'}, {'authorId': '5416756', 'name': 'M. Uljarević'}]",37.0,"{'bibtex': '@Article{Cai2019EmotionRI,\n author = {R. Cai and A. Richdale and C. Dissanayake and J. Trollor and M. Uljarević},\n journal = {Autism},\n pages = {737 - 749},\n title = {Emotion regulation in autism: Reappraisal and suppression interactions},\n volume = {23},\n year = {2019}\n}\n'}",,"{'volume': '23', 'pages': '737 - 749', 'name': 'Autism'}",84.0,Emotion regulation in autism: Reappraisal and suppression interactions,2019.0
2651,e90b55616b8415876bcd8f3454bdf5608beab6ca,"To endow virtual agents with more human-like behaviour, the current project proposes a new model for decision making based on emotions and trust. The project focuses first on the conceptual design of the model, then on the implementation of this model. To try to reproduce human decision making, 
the model has, mostly, been based on psychologically grounded findings. This model is based on the BDI structure, and has been extended with submodels for emotions and trust. To further prove the implementability of the model, it was implemented first - at a conceptual level - in LeadsTo, and thereafter in the RoboCup soccer environment. The implementation in LeadsTo shows exactly how the connections and information flows of the model work. The results demonstrate that agents using emotions to bias their decisions act differently than the agents that use rational thinking. To investigate these results further, and in a more dynamic environment, the model was then successfully implemented in the RoboCup soccer environment, 2D simulation. Some of the results were significant differences in actions between players with different goals, and differences in some of the performed actions between a positive versus a negative emotional team.","[{'authorId': '118480239', 'name': 'D. Höhle'}]",1.0,"{'bibtex': '@Inproceedings{Höhle2010AGA,\n author = {D. Höhle},\n title = {A General Agent Model of Emotion and Trust using the BDI Structure},\n year = {2010}\n}\n'}","[{'paperId': '3eb2208c741a294b22af3c4a7be92c5babe2b1b4', 'title': 'Enhancing Believability of Virtual Soccer Players: Application of a BDI-Model with Emotions and Trust'}]","{'name': '', 'volume': ''}",29.0,A General Agent Model of Emotion and Trust using the BDI Structure,2010.0
2652,e90d4a805b9ce0c46211bfa971ec757d1148d5f9,"Based on the notion that approach-avoidance underlies impression formation processes and that approach-avoidance is more directly based on appraisals of others' morality (M) than competence (C), we hypothesized that M-related information played a more important role at various phases of global impression formation than C-related information on target persons. In four studies (N = 342 university students), we predicted and found that (a) M traits showed a higher chronic accessibility than C traits; (b) when gathering information to formulate a global impression, perceivers were more interested in M traits than C traits; (c) global impressions of real persons were better predicted from M trait ascriptions than C trait ascriptions, and (d) positivity-negativity of impressions of fictitious persons was decided mainly by the M content of their behavior, whereas C information served as a weak modifier of impression intensity. The dominance of M traits over C traits was more pronounced for female perceivers than for male perceivers.","[{'authorId': '2585858', 'name': 'B. Wojciszke'}, {'authorId': '13512301', 'name': 'Róża Bazińska'}, {'authorId': '20673990', 'name': 'Marcin Jaworski'}]",607.0,"{'bibtex': '@Article{Wojciszke1998OnTD,\n author = {B. Wojciszke and Róża Bazińska and Marcin Jaworski},\n journal = {Personality and Social Psychology Bulletin},\n pages = {1251 - 1263},\n title = {On the Dominance of Moral Categories in Impression Formation},\n volume = {24},\n year = {1998}\n}\n'}",,"{'volume': '24', 'pages': '1251 - 1263', 'name': 'Personality and Social Psychology Bulletin'}",33.0,On the Dominance of Moral Categories in Impression Formation,1998.0
2653,e93c7d71074c0d4890915263c7c34711d41b6940,"Content-based music information retrieval tasks have traditionally been solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort and the need for prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e.g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We train convolutional neural networks using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations.","[{'authorId': '48373216', 'name': 'S. Dieleman'}, {'authorId': '2621946', 'name': 'B. Schrauwen'}]",357.0,"{'bibtex': '@Article{Dieleman2014EndtoendLF,\n author = {S. Dieleman and B. Schrauwen},\n journal = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {6964-6968},\n title = {End-to-end learning for music audio},\n year = {2014}\n}\n'}",,"{'pages': '6964-6968', 'name': '2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}",29.0,End-to-end learning for music audio,2014.0
2654,e93c87956c86124be936cfa483e1cb960985db61,"We investigate the capabilities of automatic nonverbal behavior descriptors to identify indicators of psychological disorders such as depression, anxiety, and post-traumatic stress disorder. We seek to confirm and enrich present state of the art, predominantly based on qualitative manual annotations, with automatic quantitative behavior descriptors. In this paper, we propose four nonverbal behavior descriptors that can be automatically estimated from visual signals. We introduce a new dataset called the Distress Assessment Interview Corpus (DAIC) which includes 167 dyadic interactions between a confederate interviewer and a paid participant. Our evaluation on this dataset shows correlation of our automatic behavior descriptors with specific psychological disorders as well as a generic distress measure. Our analysis also includes a deeper study of self-adaptor and fidgeting behaviors based on detailed annotations of where these behaviors occur.","[{'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '97930679', 'name': 'M. Mahmoud'}, {'authorId': '6349590', 'name': 'Jill Boberg'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",135.0,"{'bibtex': '@Article{Scherer2013AutomaticBD,\n author = {Stefan Scherer and Giota Stratou and M. Mahmoud and Jill Boberg and J. Gratch and A. Rizzo and Louis-Philippe Morency},\n journal = {2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},\n pages = {1-8},\n title = {Automatic behavior descriptors for psychological disorder analysis},\n year = {2013}\n}\n'}",,"{'pages': '1-8', 'name': '2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)'}",35.0,Automatic behavior descriptors for psychological disorder analysis,2013.0
2655,e96afe23bd651075504f73bb622aabd9cdc43925,"Emotional expressions of virtual agents are widely believed to enhance the interaction with the user by utilizing more natural means of communication. However, as a result of the current technology virtual agents are often only able to produce facial expressions to convey emotional meaning. The presented research investigates the effects of unimodal vs. multimodal expressions of emotions on the users' recognition of the respective emotional state. We found that multimodal expressions of emotions yield the highest recognition rates. Additionally, emotionally neutral cues in one modality, when presented together with emotionally relevant cues in the other modality, impair the recognition of the correct emotion category as well as intense emotional states.","[{'authorId': '2561845', 'name': 'Benny Liebold'}, {'authorId': '2268622', 'name': 'P. Ohler'}]",15.0,"{'bibtex': '@Article{Liebold2013MultimodalEE,\n author = {Benny Liebold and P. Ohler},\n journal = {2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},\n pages = {405-410},\n title = {Multimodal Emotion Expressions of Virtual Agents, Mimic and Vocal Emotion Expressions and Their Effects on Emotion Recognition},\n year = {2013}\n}\n'}",,"{'pages': '405-410', 'name': '2013 Humaine Association Conference on Affective Computing and Intelligent Interaction'}",25.0,"Multimodal Emotion Expressions of Virtual Agents, Mimic and Vocal Emotion Expressions and Their Effects on Emotion Recognition",2013.0
2657,e9750c1fba2133c7d8ac35003ba29e84ad78f700,,"[{'authorId': '37837552', 'name': 'M. Mortillaro'}, {'authorId': '2462740', 'name': 'K. Scherer'}]",517.0,"{'bibtex': '@Inproceedings{Mortillaro2009BodilyEO,\n author = {M. Mortillaro and K. Scherer},\n title = {Bodily expression of emotion},\n year = {2009}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Bodily expression of emotion,2009.0
2659,e9a35442fae761a84c417b5d02e6d6b83a17822b,,"[{'authorId': '2152170419', 'name': 'Ning Wang'}, {'authorId': '145834590', 'name': 'W. Johnson'}, {'authorId': '1819200', 'name': 'R. Mayer'}, {'authorId': '1773424', 'name': 'Paola Rizzo'}, {'authorId': '143878462', 'name': 'Erin Shaw'}, {'authorId': '145011207', 'name': 'Heather Collins'}]",273.0,"{'bibtex': '@Article{Wang2008ThePE,\n author = {Ning Wang and W. Johnson and R. Mayer and Paola Rizzo and Erin Shaw and Heather Collins},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {98-112},\n title = {The politeness effect: Pedagogical agents and learning outcomes},\n volume = {66},\n year = {2008}\n}\n'}",,"{'volume': '66', 'pages': '98-112', 'name': 'Int. J. Hum. Comput. Stud.'}",110.0,The politeness effect: Pedagogical agents and learning outcomes,2008.0
2660,e9b43cd29c4761223273cfec6ee9d475a167aa69,"A meta-analysis examined emotion recognition within and across cultures. Emotions were universally recognized at better-than-chance levels. Accuracy was higher when emotions were both expressed and recognized by members of the same national, ethnic, or regional group, suggesting an in-group advantage. This advantage was smaller for cultural groups with greater exposure to one another, measured in terms of living in the same nation, physical proximity, and telephone communication. Majority group members were poorer at judging minority group members than the reverse. Cross-cultural accuracy was lower in studies that used a balanced research design, and higher in studies that used imitation rather than posed or spontaneous emotional expressions. Attributes of study design appeared not to moderate the size of the in-group advantage.","[{'authorId': '2924324', 'name': 'Hillary Anger Elfenbein'}, {'authorId': '2781492', 'name': 'N. Ambady'}]",1782.0,"{'bibtex': '@Article{Elfenbein2002OnTU,\n author = {Hillary Anger Elfenbein and N. Ambady},\n journal = {Psychological bulletin},\n pages = {\n          203-35\n        },\n title = {On the universality and cultural specificity of emotion recognition: a meta-analysis.},\n volume = {128 2},\n year = {2002}\n}\n'}",,"{'volume': '128 2', 'pages': '\n          203-35\n        ', 'name': 'Psychological bulletin'}",153.0,On the universality and cultural specificity of emotion recognition: a meta-analysis.,2002.0
2661,e9bec8451b0a3e4a736c9902272a88d3e16dbb54,,"[{'authorId': '2069073454', 'name': 'Takashi Numata'}, {'authorId': None, 'name': 'Hiroki Sato'}, {'authorId': '2661328', 'name': 'Yasuhiro Asa'}, {'authorId': '2816538', 'name': 'T. Koike'}, {'authorId': '1611005390', 'name': 'Kohei Miyata'}, {'authorId': '38381724', 'name': 'E. Nakagawa'}, {'authorId': '38719428', 'name': 'M. Sumiya'}, {'authorId': '1843699', 'name': 'N. Sadato'}]",14.0,"{'bibtex': '@Article{Numata2020AchievingAH,\n author = {Takashi Numata and Hiroki Sato and Yasuhiro Asa and T. Koike and Kohei Miyata and E. Nakagawa and M. Sumiya and N. Sadato},\n booktitle = {Scientific Reports},\n journal = {Scientific Reports},\n title = {Achieving affective human–virtual agent communication by enabling virtual agents to imitate positive expressions},\n volume = {10},\n year = {2020}\n}\n'}","[{'paperId': 'ba2890f012966c69e1399c8a685ee10296a3663f', 'title': 'May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability'}, {'paperId': '3ec48aed2f07bb2d5e8d1d7cae1f0fb72db550d6', 'title': 'Designing Virtual Agent Human–Machine Interfaces Depending on the Communication and Anthropomorphism Levels in Augmented Reality'}, {'paperId': '793f81613a2a0beb3db98eef285de601b36a40fe', 'title': 'Unravelling the relation between altruistic cooperativeness trait, smiles, and cooperation: a mediation analysis'}, {'paperId': '6120852c7a11a92bda046f996fb64020a7ac22d7', 'title': ""My doctor is an avatar! The effect of anthropomorphism and emotional receptivity on individuals' intention to use digital-based healthcare services""}, {'paperId': 'dfff521c01494781fa4eaf1c238ed31fd587a16f', 'title': 'Effects of virtual agent interactivity on pro-environmental behavior promotion'}, {'paperId': 'b4227655b5a6a9080613a0cbed7666948423a597', 'title': '""Help Me Help the AI"": Understanding How Explainability Can Support Human-AI Interaction'}, {'paperId': '15e4491004211facd607c3c431c0c160d667bdf4', 'title': 'Impact of adaptive multimodal empathic behavior on the user interaction'}, {'paperId': '3f7262ac7d942e279394745796751c580e3bff53', 'title': 'How pedagogical agents communicate with students: A two-phase systematic review'}, {'paperId': '27f76e859e3902c07fd3447c304cf338ae7bf71f', 'title': 'Could artificial intelligence have consciousness? Some perspectives from neurology and parapsychology'}, {'paperId': '776d008bf5f1fb8e54c6f854234c34333f9ad6aa', 'title': 'From Affect Theoretical Foundations to Computational Models of Intelligent Affective Agents'}, {'paperId': '392452d21c7bdcf3f213e60cab1b452ff0aee86e', 'title': 'Bridging the gap between emotion and joint action'}, {'paperId': 'f216ea6fc4a77ffdfb40c4d9aabd095aabcc7dfb', 'title': 'Does the Goal Matter? Emotion Recognition Tasks Can Change the Social Value of Facial Mimicry Towards Artificial Agents'}, {'paperId': '2e2bd401df19f1c8a8e0d43d0c182e6190a260e4', 'title': ""The neural network underpinning social feedback contingent upon one's action: An fMRI study""}, {'paperId': '57618c7fa6fed91c362e32c93604a187df7ec1b7', 'title': 'Integration of Advanced Health Technology Within the Healthcare System to Fight the Global Pandemic: Current Challenges and Future Opportunities.'}]","{'name': 'Scientific Reports', 'volume': '10'}",49.0,Achieving affective human–virtual agent communication by enabling virtual agents to imitate positive expressions,2020.0
2662,e9e1f2185f352051cba334b7a919edcee703bb66,"Data reviewed suggest that previous theories of emotion experience are too narrow in scope and that lack of consensus is due to the fact that emotion experience takes various forms and is heterogenous. The authors treat separately the content of emotion experience, the underlying nonconscious correspondences, and processes producing emotion experience. They classify the nature and content of emotion experience and propose that it depends on 3 aspects of attention: mode (analytic-synthetic; detached-immersed), direction (self-world), and focus (evaluation-action). The account is informed by a 2-level view of consciousness in which phenomenology (1st order) is distinguished from awareness (2nd order). These distinctions enable the authors to differentiate and account for cases of ""unconscious"" emotion, in which there is an apparent lack of phenomenology or awareness.","[{'authorId': '47049825', 'name': 'J. Lambie'}, {'authorId': '7012360', 'name': 'A. Marcel'}]",586.0,"{'bibtex': '@Article{Lambie2002ConsciousnessAT,\n author = {J. Lambie and A. Marcel},\n journal = {Psychological review},\n pages = {\n          219-59\n        },\n title = {Consciousness and the varieties of emotion experience: a theoretical framework.},\n volume = {109 2},\n year = {2002}\n}\n'}",,"{'volume': '109 2', 'pages': '\n          219-59\n        ', 'name': 'Psychological review'}",170.0,Consciousness and the varieties of emotion experience: a theoretical framework.,2002.0
2663,ea0a40e9070e85cefd5e468db645018bb2f573e7,"Life expectancy is constantly increasing in developed countries. Unfortunately, a longer life does not always correspond to a healthier life, as even normal aging is associated with cognitive decline and increased risk factors for neurodegenerative diseases. Episodic memory (EM) is one of the most vulnerable cognitive functions in aging, and its decline is the hallmark of typical Alzheimer’s disease. This memory system is defined as the ability to acquire and recollect personally experienced episodes associated with a specific affective, spatial, and temporal context. However, most of the neuropsychological and experimental tasks currently employed to assess EM consist in learning simple material (e.g., list of words) in highly stereotyped contexts. In the same vein, classical paper-and-pencil or numeric remediation tools have shown their limitations in the transfer of acquired skills to daily life. Virtual reality (VR), thanks to its immersive properties, and the possibility of delivering realistic and complex scenarios, seems a promising tool to address the limitations of the assessment and remediation of EM. Here, we review existing studies employing VR in normal and pathological aging to assess and reeducate EM. Overall, we show that VR has been mainly used via non-immersive systems. Further studies should, therefore, test the impact of different degrees of immersion. Moreover, there is a lack of VR remediation tools specifically targeting EM. We propose that future studies should fill this gap, addressing in particular the adaptivity of VR remediation protocols.","[{'authorId': '88741266', 'name': 'V. La Corte'}, {'authorId': '2876177', 'name': 'Marco Sperduti'}, {'authorId': '5259482', 'name': 'Kouloud Abichou'}, {'authorId': '2395089', 'name': 'P. Piolino'}]",35.0,"{'bibtex': '@Article{Corte2019EpisodicMA,\n author = {V. La Corte and Marco Sperduti and Kouloud Abichou and P. Piolino},\n journal = {Frontiers in Psychology},\n title = {Episodic Memory Assessment and Remediation in Normal and Pathological Aging Using Virtual Reality: A Mini Review},\n volume = {10},\n year = {2019}\n}\n'}",,"{'volume': '10', 'name': 'Frontiers in Psychology'}",43.0,Episodic Memory Assessment and Remediation in Normal and Pathological Aging Using Virtual Reality: A Mini Review,2019.0
2664,ea1f99b89687b040b1753e09b9945e08c2fb144a,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '1976147', 'name': 'M. Pontier'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",67.0,"{'bibtex': '@Article{Bosse2010ACM,\n author = {T. Bosse and M. Pontier and Jan Treur},\n journal = {Cognitive Systems Research},\n pages = {211-230},\n title = {A computational model based on Gross’ emotion regulation theory},\n volume = {11},\n year = {2010}\n}\n'}",,"{'volume': '11', 'pages': '211-230', 'name': 'Cognitive Systems Research'}",71.0,A computational model based on Gross’ emotion regulation theory,2010.0
2665,ea65e67fdb5152afd234d0f300be1770144e62b9,"The realistic depiction of lifelike virtual humans has been the goal of many movie makers in the last decade. Recently, films such as Tron: Legacy and The Curious Case of Benjamin Button have produced highly realistic characters. In the real-time domain, there is also a need to deliver realistic virtual characters, with the increase in popularity of interactive drama video games (such as L.A. Noire™ or Heavy Rain™). There have been mixed reactions from audiences to lifelike characters used in movies and games, with some saying that the increased realism highlights subtle imperfections, which can be disturbing. Some developers opt for a stylized rendering (such as cartoon-shading) to avoid a negative reaction [Thompson 2004]. In this paper, we investigate some of the consequences of choosing realistic or stylized rendering in order to provide guidelines for developers for creating appealing virtual characters. We conducted a series of psychophysical experiments to determine whether render style affects how virtual humans are perceived. Motion capture with synchronized eye-tracked data was used throughout to animate custom-made virtual model replicas of the captured actors.","[{'authorId': '145795454', 'name': 'R. Mcdonnell'}, {'authorId': '2016068', 'name': 'M. Breidt'}, {'authorId': '1747836', 'name': 'H. Bülthoff'}]",154.0,"{'bibtex': '@Article{Mcdonnell2012RenderMR,\n author = {R. Mcdonnell and M. Breidt and H. Bülthoff},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 11},\n title = {Render me real?},\n volume = {31},\n year = {2012}\n}\n'}",,"{'volume': '31', 'pages': '1 - 11', 'name': 'ACM Transactions on Graphics (TOG)'}",26.0,Render me real?,2012.0
2668,ea8467e1d26f472a5fc8579d0fc8372149283acc,"Social emotion regulation, which can be understood as the intentional efforts by one person to regulate emotions of another person, is something we encounter and benefit from every day, and becomes especially important when a person is unable to handle an emotion or an emotional event by themselves. A paradigm that examines whether someone can perceive and benefit from regulatory efforts by another person, represented here by a virtual agent, would be highly relevant for experimental studies investigating social emotion regulation, as well as for interventions in the clinical and sub-clinical context. Virtual reality (VR) provides perhaps the ideal opportunity to test social interactions and difficulties with them, as it counters typical methodological problems of behavioral experiments, such as the trade-off between ecological validity and experimental control, as well as the difficulty of replicating social situations. The goal of the present methods paper is twofold: to provide a detailed description of the development of a novel paradigm consisting of two scenarios in VR designed to test the efficacy of social emotion regulation, and to present the anticipated results for the target populations of typically developing and autistic youth. Participants are presented with a virtual school environment and take part in two activities with a class of students and a teacher, all of whom are virtual agents. In both scenarios, participants experience a potentially stressful situation and are subsequently offered emotional support by a friendly student. Throughout the experiment, self-reports in the form of virtual smiley scales and psychophysiological measurements are collected as markers of the participants’ emotional states. Pilot results will be discussed in line with anticipated outcomes, to indicate that the experiment will be able to show the efficacy of social support by a virtual agent and provide insight into social emotion regulation for different populations. The school environment and the character of the friendly student also have the potential to be adapted for follow-up experiments on additional aspects of social emotion regulation for a variety of contexts.","[{'authorId': '2124332447', 'name': 'Lina Stallmann'}, {'authorId': '39779139', 'name': 'Daniel Dukes'}, {'authorId': '2054039824', 'name': 'Michel Tran'}, {'authorId': '2158785974', 'name': 'Valentin Durand de Gevigney'}, {'authorId': '3090887', 'name': 'D. Rudrauf'}, {'authorId': '38707445', 'name': 'Andrea C. Samson'}]",0.0,"{'bibtex': '@Article{Stallmann2022SociallySB,\n author = {Lina Stallmann and Daniel Dukes and Michel Tran and Valentin Durand de Gevigney and D. Rudrauf and Andrea C. Samson},\n booktitle = {Frontiers in Virtual Reality},\n title = {Socially Supported by an Embodied Agent: The Development of a Virtual-Reality Paradigm to Study Social Emotion Regulation},\n volume = {3},\n year = {2022}\n}\n'}",[],{'volume': '3'},53.0,Socially Supported by an Embodied Agent: The Development of a Virtual-Reality Paradigm to Study Social Emotion Regulation,2022.0
2669,eaf4ea5510e56f7cd849ccf5ac59e7e5bd0b44ed,"This article introduces a novel, ecological, obstructed walking paradigm. Gait adaptations to circumvent obstacles undergoing uncertain displacements, and the effect of revealing the obstacle's action beforehand, were investigated in young adults. The personal space (PS) maintained during walking was quantified for the first time under different environmental factors including auditory distractions. Obstacle movement and its uncertainty resulted in gait adjustments aimed at gaining time to assess the situation. Early gait adaptations and constant clearances around the obstacle suggest that anticipation and preplanning are involved in such navigational tasks. Participants systematically maintained an elliptical PS during circumvention, but they adjusted its size according to different environmental factors. There was a relationship between the size of PS and level of attention, which suggests that the regulation of PS is used to control locomotion. This novel paradigm has important implications for the assessment and training of locomotor ability within real world environments.","[{'authorId': '1422621837', 'name': 'Martin Gérin-Lajoie'}, {'authorId': '1852074', 'name': 'C. Richards'}, {'authorId': '1862105', 'name': 'B. McFadyen'}]",194.0,"{'bibtex': '@Article{Gérin-Lajoie2005TheNO,\n author = {Martin Gérin-Lajoie and C. Richards and B. McFadyen},\n journal = {Motor control},\n pages = {\n          242-69\n        },\n title = {The negotiation of stationary and moving obstructions during walking: anticipatory locomotor adaptations and preservation of personal space.},\n volume = {9 3},\n year = {2005}\n}\n'}",,"{'volume': '9 3', 'pages': '\n          242-69\n        ', 'name': 'Motor control'}",31.0,The negotiation of stationary and moving obstructions during walking: anticipatory locomotor adaptations and preservation of personal space.,2005.0
2670,eafa0467039ebf2aff4409e3627b4dc7875fc76c,"We found that children sometimes abused a social robot placed in a shopping mall hallway. They verbally abused the robot, repeatedly obstructed its path, and sometimes even kicked and punched the robot. To investigate the reasons for the abuse, we conducted a field study in which we interviewed visiting children who exhibited serious abusive behaviors, including physical contact. We analyzed interview contents to determine whether the children perceived the robot as human-like, why they abused it, and whether they thought that the robot would suffer from their abusive behavior. We obtained valid interviews from 23 children (age range, 5–9 years old) over 13 days of observations. We found that 1) the majority of the children engaged in abuse because they were curious about the robot’s reactions or enjoyed abusing it and considered it human-like, and 2) about half of them believed the robot was capable of perceiving their abusive behaviors.","[{'authorId': '1768404', 'name': 'T. Nomura'}, {'authorId': '48309591', 'name': 'T. Kanda'}, {'authorId': '119331208', 'name': 'Hiroyoshi Kidokoro'}, {'authorId': '1814875', 'name': 'Yoshitaka Suehiro'}, {'authorId': '1895054', 'name': 'Sachie Yamada'}]",44.0,"{'bibtex': '@Article{Nomura2016WhyDC,\n author = {T. Nomura and T. Kanda and Hiroyoshi Kidokoro and Yoshitaka Suehiro and Sachie Yamada},\n journal = {Interaction Studies},\n pages = {347-369},\n title = {Why do children abuse robots},\n volume = {17},\n year = {2016}\n}\n'}",,"{'volume': '17', 'pages': '347-369', 'name': 'Interaction Studies'}",25.0,Why do children abuse robots,2016.0
2671,eafda8a94e410f1ad53b3e193ec124e80d57d095,"to name a few. Because of its importance to the study of emotion, a number of observer-based systems of facial expression measurement have been developed Using FACS and viewing video-recorded facial behavior at frame rate and slow motion, coders can manually code nearly all possible facial expressions, which are decomposed into action units (AUs). Action units, with some qualifications , are the smallest visually discriminable facial movements. By comparison, other systems are less thorough (Malatesta et al., 1989), fail to differentiate between some anatomically distinct movements (Oster, Hegley, & Nagel, 1992), consider movements that are not anatomically distinct as separable (Oster et al., 1992), and often assume a one-to-one mapping between facial expression and emotion (for a review of these systems, see Cohn & Ekman, in press). Unlike systems that use emotion labels to describe expression , FACS explicitly distinguishes between facial actions and inferences about what they mean. FACS itself is descriptive and includes no emotion-specified descriptors. Hypotheses and inferences about the emotional meaning of facial actions are extrinsic to FACS. If one wishes to make emotion based inferences from FACS codes, a variety of related resources exist. These include the FACS Investigators' Guide These resources suggest combination rules for defining emotion-specified expressions from FACS action units, but this inferential step remains extrinsic to FACS. Because of its descriptive power, FACS is regarded by many as the standard measure for facial behavior and is used widely in diverse fields. Beyond emotion science, these include facial neuromuscular disorders","[{'authorId': '1737918', 'name': 'J. Cohn'}, {'authorId': '2059653', 'name': 'Z. Ambadar'}, {'authorId': '21451088', 'name': 'P. Ekman'}]",336.0,"{'bibtex': '@Inproceedings{Cohn2007ObserverbasedMO,\n author = {J. Cohn and Z. Ambadar and P. Ekman},\n title = {Observer-based measurement of facial expression with the Facial Action Coding System.},\n year = {2007}\n}\n'}",,"{'volume': '', 'name': ''}",93.0,Observer-based measurement of facial expression with the Facial Action Coding System.,2007.0
2672,eb08f02249c4cdbc858202fa0d2e38c43fc084c9,"R. W. White (1959) proposed that certain motives, such as curiosity, autonomy, and play (called intrinsic motives, or IMs), have common characteristics that distinguish them from drives. The evidence that mastery is common to IMs is anecdotal, not scientific. The assertion that “intrinsic enjoyment” is common to IMs exaggerates the significance of pleasure in human motivation and expresses the hedonistic fallacy of confusing consequence for cause. Nothing has been shown scientifically to be common to IMs that differentiates them from drives. An empirically testable theory of 16 basic desires is put forth based on psychometric research and subsequent behavior validation. The desires are largely unrelated to each other and may have different evolutionary histories.","[{'authorId': '116568668', 'name': 'S. Reiss'}]",529.0,"{'bibtex': '@Article{Reiss2004MultifacetedNO,\n author = {S. Reiss},\n journal = {Review of General Psychology},\n pages = {179 - 193},\n title = {Multifaceted Nature of Intrinsic Motivation: The Theory of 16 Basic Desires},\n volume = {8},\n year = {2004}\n}\n'}",,"{'volume': '8', 'pages': '179 - 193', 'name': 'Review of General Psychology'}",52.0,Multifaceted Nature of Intrinsic Motivation: The Theory of 16 Basic Desires,2004.0
2673,eb08f6c1929ad98b6dd95247d17780e62d0ad6e5,...................................................................................................................... ix CHAPTER,"[{'authorId': '122571342', 'name': 'K. Cissell'}]",6.0,"{'bibtex': '@Inproceedings{Cissell2013ASO,\n author = {K. Cissell},\n title = {A Study Of The Effects Of Computer Animated Character Body Style On Perception Of Facial Expression},\n year = {2013}\n}\n'}",,"{'volume': '', 'name': ''}",23.0,A Study Of The Effects Of Computer Animated Character Body Style On Perception Of Facial Expression,2013.0
2674,eb27f32ea9b872eae50c16fef9ea35e30384a0d3,"According to appeasement hypotheses, embarrassment should have a distinct nonverbal display that is more readily perceived when displayed by individuals from lower status groups. The evidence from 5 studies supported these two claims. The nonverbal behavior of embarrassment was distinct from a related emotion (amusement), resembled the temporal pattern of facial expressions of emotion, was uniquely related to self-reports of embarrassment, and was accurately identified by observers who judged the spontaneous displays of various emotions. Across the judgment studies, observers were more accurate and attributed more emotion to the embarrassment displays of female and AfricanAmerican targets than those of male and Caucasian targets. Discussion focused on the universality and appeasement function of the embarrassment display. Since universal facial expressions of a limited set of emotions were first documented (Ekman & Friesen, 1971; Ekman, Sorenson, & Friesen, 1969; Izard, 1971), sparse attention has been given to facial expressions of other emotions. The resulting lacuna in the field—that the emotions with identified displays are fewer (7 to 10) than the states that lay people (Fehr & Russell, 1984) and emotion theorists (Ekman, 1992; Izard, 1977; Tomkins, 1963, 1984) label as emotions—presents intriguing possibilities. Displays of other emotions may be blends of other emotional displays, unidentifiable, or may await discovery.","[{'authorId': '3990536', 'name': 'D. Keltner'}]",757.0,"{'bibtex': '@Article{Keltner1995SignsOA,\n author = {D. Keltner},\n journal = {Journal of Personality and Social Psychology},\n pages = {441-454},\n title = {Signs of appeasement: evidence for the distinct displays of embarrassment, amusement, and shame},\n volume = {68},\n year = {1995}\n}\n'}",,"{'volume': '68', 'pages': '441-454', 'name': 'Journal of Personality and Social Psychology'}",63.0,"Signs of appeasement: evidence for the distinct displays of embarrassment, amusement, and shame",1995.0
2675,eb2f3fa0017c42c7171659f870373b21d17c7889,"We present the SimSensei system, a fully automatic virtual agent that conducts interviews to assess indicators of psychological distress. With this demo, we focus our attention on the perception part of the system, a multimodal framework which captures and analyzes user state behavior for both behavioral understanding and interactional purposes. We will demonstrate real-time user state sensing as a part of the SimSensei architecture and discuss how this technology enabled automatic analysis of behaviors related to psychological distress.","[{'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}, {'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '1705118', 'name': 'Arno Hartholt'}, {'authorId': '2432742', 'name': 'Edward Fast'}, {'authorId': '1930380', 'name': 'Margot Lhommet'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '2223582', 'name': 'Fabrizio Morbini'}, {'authorId': '3194430', 'name': 'Kallirroi Georgila'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '29861580', 'name': 'A. Rizzo'}]",18.0,"{'bibtex': '@Article{Stratou2015ADO,\n author = {Giota Stratou and Louis-Philippe Morency and D. DeVault and Arno Hartholt and Edward Fast and Margot Lhommet and Gale M. Lucas and Fabrizio Morbini and Kallirroi Georgila and Stefan Scherer and J. Gratch and S. Marsella and D. Traum and A. Rizzo},\n journal = {2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},\n pages = {787-789},\n title = {A demonstration of the perception system in SimSensei, a virtual human application for healthcare interviews},\n year = {2015}\n}\n'}",,"{'pages': '787-789', 'name': '2015 International Conference on Affective Computing and Intelligent Interaction (ACII)'}",15.0,"A demonstration of the perception system in SimSensei, a virtual human application for healthcare interviews",2015.0
2676,eb3e9a585515c2b627c24dfb780e878bcbfcfa32,,"[{'authorId': '51137959', 'name': 'Mohammed Slim Ben Mimoun'}, {'authorId': '18163606', 'name': 'Ingrid Poncin'}, {'authorId': '2080261', 'name': 'Marion Garnier'}]",3.0,"{'bibtex': '@Article{Mimoun2012WITHDRAWNCS,\n author = {Mohammed Slim Ben Mimoun and Ingrid Poncin and Marion Garnier},\n journal = {Journal of Retailing and Consumer Services},\n title = {WITHDRAWN: Case study: Embodied virtual agents: An analysis on reasons for failure},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': 'Journal of Retailing and Consumer Services'}",50.0,WITHDRAWN: Case study: Embodied virtual agents: An analysis on reasons for failure,2012.0
2677,eb5583763621788b6a1c8dfe25091dc20a21e4b1,"Background A World Health Organization 2017 report stated that major depression affects almost 5% of the human population. Major depression is associated with impaired psychosocial functioning and reduced quality of life. Challenges such as shortage of mental health personnel, long waiting times, perceived stigma, and lower government spends pose barriers to the alleviation of mental health problems. Face-to-face psychotherapy alone provides only point-in-time support and cannot scale quickly enough to address this growing global public health challenge. Artificial intelligence (AI)-enabled, empathetic, and evidence-driven conversational mobile app technologies could play an active role in filling this gap by increasing adoption and enabling reach. Although such a technology can help manage these barriers, they should never replace time with a health care professional for more severe mental health problems. However, app technologies could act as a supplementary or intermediate support system. Mobile mental well-being apps need to uphold privacy and foster both short- and long-term positive outcomes. Objective This study aimed to present a preliminary real-world data evaluation of the effectiveness and engagement levels of an AI-enabled, empathetic, text-based conversational mobile mental well-being app, Wysa, on users with self-reported symptoms of depression. Methods In the study, a group of anonymous global users were observed who voluntarily installed the Wysa app, engaged in text-based messaging, and self-reported symptoms of depression using the Patient Health Questionnaire-9. On the basis of the extent of app usage on and between 2 consecutive screening time points, 2 distinct groups of users (high users and low users) emerged. The study used mixed-methods approach to evaluate the impact and engagement levels among these users. The quantitative analysis measured the app impact by comparing the average improvement in symptoms of depression between high and low users. The qualitative analysis measured the app engagement and experience by analyzing in-app user feedback and evaluated the performance of a machine learning classifier to detect user objections during conversations. Results The average mood improvement (ie, difference in pre- and post-self-reported depression scores) between the groups (ie, high vs low users; n=108 and n=21, respectively) revealed that the high users group had significantly higher average improvement (mean 5.84 [SD 6.66]) compared with the low users group (mean 3.52 [SD 6.15]); Mann-Whitney P=.03 and with a moderate effect size of 0.63. Moreover, 67.7% of user-provided feedback responses found the app experience helpful and encouraging. Conclusions The real-world data evaluation findings on the effectiveness and engagement levels of Wysa app on users with self-reported symptoms of depression show promise. However, further work is required to validate these initial findings in much larger samples and across longer periods.","[{'authorId': '46691130', 'name': 'B. Inkster'}, {'authorId': '51961236', 'name': 'Shubhankar Sarda'}, {'authorId': '20764756', 'name': 'V. Subramanian'}]",341.0,"{'bibtex': '@Article{Inkster2018AnEC,\n author = {B. Inkster and Shubhankar Sarda and V. Subramanian},\n journal = {JMIR mHealth and uHealth},\n title = {An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being: Real-World Data Evaluation Mixed-Methods Study},\n volume = {6},\n year = {2018}\n}\n'}",,"{'volume': '6', 'name': 'JMIR mHealth and uHealth'}",55.0,"An Empathy-Driven, Conversational Artificial Intelligence Agent (Wysa) for Digital Mental Well-Being: Real-World Data Evaluation Mixed-Methods Study",2018.0
2678,eb67f1f49ff5b24886075b1ccf77f423db8bfaf2,"We aim at building a new human-computer interface for Information Delivering applications: the conversational agent that we have developed is a multimodal believable agent able to converse with the User by exhibiting a synchronized and coherent verbal and nonverbal behavior. The agent is provided with a personality and a social role, that allows her to show her emotion or to refrain from showing it, depending on the context in which the conversation takes place. The agent is provided with a face and a mind. The mind is designed according to a BDI structure that depends on the agent's personality; it evolves dynamically during the conversation, according to the User's dialog moves and to emotions triggered as a consequence of the Interlocutor's move; such cognitive features are then translated into facial behaviors. In this paper, we describe the overall architecture of our system and its various components; in particular, we present our dynamic model of emotions. We illustrate our results with an example of dialog all along the paper. We pay particular attention to the generation of verbal and nonverbal behaviors and to the way they are synchronized and combined with each other. We also discuss how these acts are translated into facial expressions.","[{'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1694255', 'name': 'V. Carofiglio'}, {'authorId': '1739256', 'name': 'B. D. Carolis'}, {'authorId': '1807752', 'name': 'F. D. Rosis'}, {'authorId': '1802126', 'name': 'I. Poggi'}]",139.0,"{'bibtex': '@Inproceedings{Pelachaud2002EmbodiedCA,\n author = {C. Pelachaud and V. Carofiglio and B. D. Carolis and F. D. Rosis and I. Poggi},\n pages = {758-765},\n title = {Embodied contextual agent in information delivering application},\n year = {2002}\n}\n'}",,{'pages': '758-765'},33.0,Embodied contextual agent in information delivering application,2002.0
2679,eba0c2c1c27e63a9bd8b99af38fa7d4ce418917f,,"[{'authorId': '1742939', 'name': 'S. Buisine'}, {'authorId': '2110130919', 'name': 'Jean-Claude Martin'}]",55.0,"{'bibtex': ""@Article{Buisine2007TheEO,\n author = {S. Buisine and Jean-Claude Martin},\n journal = {Interact. Comput.},\n pages = {484-493},\n title = {The effects of speech-gesture cooperation in animated agents' behavior in multimedia presentations},\n volume = {19},\n year = {2007}\n}\n""}",,"{'volume': '19', 'pages': '484-493', 'name': 'Interact. Comput.'}",36.0,The effects of speech-gesture cooperation in animated agents' behavior in multimedia presentations,2007.0
2680,ebb4c2e0cb5c5aa0ccdf8882f3607c79f3b00fe5,"The Distress Analysis Interview Corpus (DAIC) contains clinical interviews designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and post traumatic stress disorder. The interviews are conducted by humans, human controlled agents and autonomous agents, and the participants include both distressed and non-distressed individuals. Data collected include audio and video recordings and extensive questionnaire responses; parts of the corpus have been transcribed and annotated for a variety of verbal and non-verbal features. The corpus has been used to support the creation of an automated interviewer agent, and for research on the automatic identification of psychological distress.","[{'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2038490', 'name': 'Ron Artstein'}, {'authorId': '2419453', 'name': 'Gale M. Lucas'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '2551269', 'name': 'Angela Nazarian'}, {'authorId': '2072346682', 'name': 'Rachel Wood'}, {'authorId': '6349590', 'name': 'Jill Boberg'}, {'authorId': '144662324', 'name': 'D. DeVault'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '144518646', 'name': 'D. Traum'}, {'authorId': '29861580', 'name': 'A. Rizzo'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",352.0,"{'bibtex': '@Inproceedings{Gratch2014TheDA,\n author = {J. Gratch and Ron Artstein and Gale M. Lucas and Giota Stratou and Stefan Scherer and Angela Nazarian and Rachel Wood and Jill Boberg and D. DeVault and S. Marsella and D. Traum and A. Rizzo and Louis-Philippe Morency},\n pages = {3123-3128},\n title = {The Distress Analysis Interview Corpus of human and computer interviews},\n year = {2014}\n}\n'}",,{'pages': '3123-3128'},40.0,The Distress Analysis Interview Corpus of human and computer interviews,2014.0
2681,ebbbe30fd6536227c401d12b13a602f75af2569e,,"[{'authorId': '2019292', 'name': 'J. Rickel'}, {'authorId': '145834590', 'name': 'W. Johnson'}]",218.0,"{'bibtex': '@Inproceedings{Rickel2001TaskorientedCW,\n author = {J. Rickel and W. Johnson},\n pages = {95-122},\n title = {Task-oriented collaboration with embodied agents in virtual worlds},\n year = {2001}\n}\n'}",,"{'volume': '', 'pages': '95-122', 'name': ''}",46.0,Task-oriented collaboration with embodied agents in virtual worlds,2001.0
2682,ebcace991c46f5d63202734889e1c3634308bc39,"Eye gaze of is an informative social signal in interactions with other humans and also with virtual agents (VA). But for a successful communication, users have to accurately perceive the VA's point of gaze (POG). In our study, participants sitting opposite to a VA at a table indicated its POG by positioning a token on the table surface. We measured the perceptual accuracy within and between participants as well as the participants' response times and eye movements for five horizontally aligned gaze targets. We demonstrated that perceiving the VA met perceptual benchmarks from human lookers: a) variances within and between participants were only slightly larger, b) the VA's visual angle was linearly overestimated, and c) variances increased with the visual angle. Finally, participants showed large individual differences but were consistent in their own gaze behaviour and response times across trials and gaze targets.","[{'authorId': '48677925', 'name': 'Sebastian Loth'}, {'authorId': '1935524', 'name': 'G. Horstmann'}, {'authorId': '51924580', 'name': 'Corinnna Osterbrink'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",2.0,"{'bibtex': '@Article{Loth2018AccuracyOP,\n author = {Sebastian Loth and G. Horstmann and Corinnna Osterbrink and S. Kopp},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {Accuracy of Perceiving Precisely Gazing Virtual Agents},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},24.0,Accuracy of Perceiving Precisely Gazing Virtual Agents,2018.0
2683,ebf0615fc4d98cf1dbe527c79146ce1e50dce9af,"We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL","[{'authorId': '2841331', 'name': 'A. Dosovitskiy'}, {'authorId': '51007566', 'name': 'G. Ros'}, {'authorId': '2962623', 'name': 'Felipe Codevilla'}, {'authorId': '2110294430', 'name': 'Antonio M. López'}, {'authorId': '145231047', 'name': 'V. Koltun'}]",3328.0,"{'bibtex': '@Inproceedings{Dosovitskiy2017CARLAAO,\n author = {A. Dosovitskiy and G. Ros and Felipe Codevilla and Antonio M. López and V. Koltun},\n pages = {1-16},\n title = {CARLA: An Open Urban Driving Simulator},\n year = {2017}\n}\n'}",,{'pages': '1-16'},30.0,CARLA: An Open Urban Driving Simulator,2017.0
2684,ec273d231800ea3a074438e2195d5b35aa5b03b9,"We report two experiments that compare the performance of young and older adults on perceptual-motor tasks involving division of attention. Previous studies have shown older people to be especially penalized by divided attention situations, but the generality of this finding was recently challenged by Somberg and Salthouse (1982). The present study was conducted to investigate the possibility that age differences in dual-task performance are amplified by an increase in the difficulty of the constituent tasks, where difficulty was manipulated by varying the central, cognitive nature of the tasks (Experiment 1) or the degree of choice involved (Experiment 2). With the present tasks, strong evidence was found for an age-related decrement in divided attention performance. Contrary to our original expectations, however, it does not seem that division of attention presents some especial difficulty to older people. Rather, division of attention is one of several equivalent ways to increase overall task complexity. In turn, age differences are exaggerated as tasks are made more complex.","[{'authorId': '4759624', 'name': 'J. Mcdowd'}, {'authorId': '2435816', 'name': 'F. Craik'}]",419.0,"{'bibtex': '@Article{Mcdowd1988EffectsOA,\n author = {J. Mcdowd and F. Craik},\n journal = {Journal of experimental psychology. Human perception and performance},\n pages = {\n          267-280\n        },\n title = {Effects of aging and task difficulty on divided attention performance.},\n volume = {14 2},\n year = {1988}\n}\n'}",,"{'volume': '14 2', 'pages': '\n          267-280\n        ', 'name': 'Journal of experimental psychology. Human perception and performance'}",31.0,Effects of aging and task difficulty on divided attention performance.,1988.0
2685,ec341a8d8840f910447922ad124532383dbcb9ac,,"[{'authorId': '71463834', 'name': 'S. Brennan'}, {'authorId': '2145229740', 'name': 'Xin Chen'}, {'authorId': '145688063', 'name': 'Christopher A. Dickinson'}, {'authorId': '2184661', 'name': 'M. Neider'}, {'authorId': '1696991', 'name': 'G. Zelinsky'}]",317.0,"{'bibtex': '@Article{Brennan2008CoordinatingCT,\n author = {S. Brennan and Xin Chen and Christopher A. Dickinson and M. Neider and G. Zelinsky},\n journal = {Cognition},\n pages = {1465-1477},\n title = {Coordinating cognition: The costs and benefits of shared gaze during collaborative search},\n volume = {106},\n year = {2008}\n}\n'}",,"{'volume': '106', 'pages': '1465-1477', 'name': 'Cognition'}",41.0,Coordinating cognition: The costs and benefits of shared gaze during collaborative search,2008.0
2686,ec3d052b29c4f0600cff870c160672112ee93bf4,"Background Virtual reality (VR) delivered through immersive headsets creates an opportunity to deliver interventions to improve physical, mental, and psychosocial health outcomes. VR app studies with older adults have primarily focused on rehabilitation and physical function including gait, balance, fall prevention, pain management, and cognition. Several systematic reviews have previously been conducted, but much of the extant literature is focused on rehabilitation or other institutional settings, and little is known about the effectiveness of VR apps using immersive headsets to target health outcomes among community-dwelling older adults. Objective The objective of this review was to evaluate the effectiveness of VR apps delivered using commercially available immersive headsets to improve physical, mental, or psychosocial health outcomes in community-dwelling older adults. Methods Peer-reviewed publications that included community-dwelling older adults aged ≥60 years residing in residential aged care settings and nursing homes were included. This systematic review was conducted in accordance with the Joanna Briggs Institute (JBI) methodology for systematic reviews of effectiveness evidence. The title of this review was registered with JBI, and the systematic review protocol was registered with the International Prospective Register of Systematic Reviews. Results In total, 7 studies that specifically included community-dwelling older adults were included in this review. VR apps using a head-mounted display led to improvements in a number of health outcomes, including pain management, posture, cognitive functioning specifically related to Alzheimer disease, and a decreased risk of falls. A total of 6 studies reported a statistically significant difference post VR intervention, and 1 study reported an improvement in cognitive function to reduce navigational errors. Only one study reported on the usability and acceptability of the interventions delivered through VR. While one study used a distraction mechanism for pain management, none of the studies used gaming technology to promote enjoyment. Conclusions Interventions to improve health outcomes through VR have demonstrated potential; however, the ability to synthesize findings by primary outcome for the older adult population is not possible. A number of factors, especially related to frailty, usability, and acceptability, also need to be explored before more substantial recommendations on the effectiveness of VR interventions for older adults can be made. Trial Registration PROSPERO CRD42019143504; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=143504","[{'authorId': '13922334', 'name': 'Gordana Dermody'}, {'authorId': '3763941', 'name': 'L. Whitehead'}, {'authorId': '2070349378', 'name': 'Graham Wilson'}, {'authorId': '1491679969', 'name': 'Courtney Glass'}]",76.0,"{'bibtex': '@Article{Dermody2020TheRO,\n author = {Gordana Dermody and L. Whitehead and Graham Wilson and Courtney Glass},\n journal = {Journal of Medical Internet Research},\n title = {The Role of Virtual Reality in Improving Health Outcomes for Community-Dwelling Older Adults: Systematic Review},\n volume = {22},\n year = {2020}\n}\n'}",,"{'volume': '22', 'name': 'Journal of Medical Internet Research'}",85.0,The Role of Virtual Reality in Improving Health Outcomes for Community-Dwelling Older Adults: Systematic Review,2020.0
2687,ec540ca6b6933fdb485d6cf640ddf81990529b09,"It is still an open question whether software agents should be personified in the interface. In order to study the effects of faces and facial expressions in the interface a series of experiments was conducted to compare subjects' responses to and evaluation of different faces and facial expressions. The experimental results obtained demonstrate that: (1) personified interfaces help users engage in a task, and are well suited for an entertainment domain; (2) people's impressions of a face in a task are different from ones of the face in isolation. Perceived intelligence of a face is determined not by the agent's appearance but by its competence; (3) there is a dichotomy between user groups which have opposite opinions about personification. Thus, agent-based interfaces should be flexible to support the diversity of users' preferences and the nature of tasks.","[{'authorId': '2060600', 'name': 'Tomoko Koda'}, {'authorId': '1701876', 'name': 'P. Maes'}]",226.0,"{'bibtex': ""@Article{Koda1996AgentsWF,\n author = {Tomoko Koda and P. Maes},\n journal = {Proceedings 5th IEEE International Workshop on Robot and Human Communication. RO-MAN'96 TSUKUBA},\n pages = {189-194},\n title = {Agents with faces: the effect of personification},\n year = {1996}\n}\n""}",,"{'pages': '189-194', 'name': ""Proceedings 5th IEEE International Workshop on Robot and Human Communication. RO-MAN'96 TSUKUBA""}",9.0,Agents with faces: the effect of personification,1996.0
2688,ec5e4a406740a5077eb7c3fd3832d02d829e021f,"This paper examines the reflection problem that arises when a researcher observing the distribution of behaviour in a population tries to infer whether the average behaviour in some group influences the behaviour of the individuals that comprise the group. It is found that inference is not possible unless the researcher has prior information specifying the compisition of reference groups. If this information is available, the prospects for inference depend critically on the population relationship between the variables defining reference groups and those directly affecting outcomes. Inference is difficult to implossible if these variables are functionally dependent or are statistically independent. The prospects are better if the variables defining reference groups and those directly affecting outcomes are moderately related in the population.","[{'authorId': '3210428', 'name': 'C. Manski'}]",6242.0,"{'bibtex': '@Article{Manski1993IdentificationOE,\n author = {C. Manski},\n journal = {The Review of Economic Studies},\n pages = {531-542},\n title = {Identification of Endogenous Social Effects: The Reflection Problem},\n volume = {60},\n year = {1993}\n}\n'}",,"{'volume': '60', 'pages': '531-542', 'name': 'The Review of Economic Studies'}",34.0,Identification of Endogenous Social Effects: The Reflection Problem,1993.0
2690,ec85eeb27f71f389416ebbf6e13c725ddef78bea,"Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as -greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.","[{'authorId': '9165544', 'name': 'Kristof Van Moffaert'}, {'authorId': '144336828', 'name': 'A. Nowé'}]",205.0,"{'bibtex': '@Article{Moffaert2014MultiobjectiveRL,\n author = {Kristof Van Moffaert and A. Nowé},\n journal = {J. Mach. Learn. Res.},\n pages = {3483-3512},\n title = {Multi-objective reinforcement learning using sets of pareto dominating policies},\n volume = {15},\n year = {2014}\n}\n'}",,"{'volume': '15', 'pages': '3483-3512', 'name': 'J. Mach. Learn. Res.'}",33.0,Multi-objective reinforcement learning using sets of pareto dominating policies,2014.0
2691,ec914ed9bd50fc89026ec1c4bde73df8c725d995,"In this paper, we present IrisTK - a toolkit for rapid development of real-time systems for multi-party face-to-face interaction. The toolkit consists of a message passing system, a set of modules for multi-modal input and output, and a dialog authoring language based on the notion of statecharts. The toolkit has been applied to a large scale study in a public museum setting, where the back-projected robot head Furhat interacted with the visitors in multi-party dialog.","[{'authorId': '1711959', 'name': 'Gabriel Skantze'}, {'authorId': '32201536', 'name': 'S. Moubayed'}]",116.0,"{'bibtex': '@Inproceedings{Skantze2012IrisTKAS,\n author = {Gabriel Skantze and S. Moubayed},\n pages = {69-76},\n title = {IrisTK: a statechart-based toolkit for multi-party face-to-face interaction},\n year = {2012}\n}\n'}",,{'pages': '69-76'},23.0,IrisTK: a statechart-based toolkit for multi-party face-to-face interaction,2012.0
2692,eca6f0d0c1c58a703ac19488fae17c94b438b30e,,"[{'authorId': '3177708', 'name': 'A. Gulsrud'}, {'authorId': '4879861', 'name': 'Laudan B. Jahromi'}, {'authorId': '2592071', 'name': 'C. Kasari'}]",135.0,"{'bibtex': '@Article{Gulsrud2009TheCO,\n author = {A. Gulsrud and Laudan B. Jahromi and C. Kasari},\n journal = {Journal of Autism and Developmental Disorders},\n pages = {227 - 237},\n title = {The Co-Regulation of Emotions Between Mothers and their Children with Autism},\n volume = {40},\n year = {2009}\n}\n'}",,"{'volume': '40', 'pages': '227 - 237', 'name': 'Journal of Autism and Developmental Disorders'}",48.0,The Co-Regulation of Emotions Between Mothers and their Children with Autism,2009.0
2693,ecb5336bf7b54a62109f325e7152bb74c4c7f527,"Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification. 1","[{'authorId': '39483833', 'name': 'Duyu Tang'}, {'authorId': '152277111', 'name': 'Bing Qin'}, {'authorId': '40282288', 'name': 'Ting Liu'}]",1349.0,"{'bibtex': '@Inproceedings{Tang2015DocumentMW,\n author = {Duyu Tang and Bing Qin and Ting Liu},\n pages = {1422-1432},\n title = {Document Modeling with Gated Recurrent Neural Network for Sentiment Classification},\n year = {2015}\n}\n'}",,{'pages': '1422-1432'},61.0,Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,2015.0
2694,ecc0c983b75ab02fe1160c39c27cefdcaea57f0b,"The Noisy-OR function is extensively used in probabilistic reasoning, and usually justified with heuristic arguments. This paper investigates sets of conditions that imply the Noisy-OR function.","[{'authorId': '7668712', 'name': 'Fabio Gagliardi Cozman'}]",34.0,"{'bibtex': '@Inproceedings{Cozman2004AxiomatizingN,\n author = {Fabio Gagliardi Cozman},\n pages = {979-980},\n title = {Axiomatizing Noisy-OR},\n year = {2004}\n}\n'}",,{'pages': '979-980'},27.0,Axiomatizing Noisy-OR,2004.0
2695,ecdd4731e197f4afda804602f533565c19ffc271,,"[{'authorId': '23226369', 'name': 'D. Simmons'}, {'authorId': '40371378', 'name': 'Ashley E. Robertson'}, {'authorId': '3043139', 'name': 'Lawrie S. McKay'}, {'authorId': '12658236', 'name': 'E. Toal'}, {'authorId': '144728689', 'name': 'P. McAleer'}, {'authorId': '2819854', 'name': 'F. Pollick'}]",746.0,"{'bibtex': '@Article{Simmons2009VisionIA,\n author = {D. Simmons and Ashley E. Robertson and Lawrie S. McKay and E. Toal and P. McAleer and F. Pollick},\n journal = {Vision Research},\n pages = {2705-2739},\n title = {Vision in autism spectrum disorders},\n volume = {49},\n year = {2009}\n}\n'}",,"{'volume': '49', 'pages': '2705-2739', 'name': 'Vision Research'}",450.0,Vision in autism spectrum disorders,2009.0
2696,ed0d30eba5ba1d4c2920d29b78f7979968ae1604,"In greeting encounters, first impressions of personality and attitude are quickly formed and might determine important relational decisions, such as the likelihood and frequency of subsequent encounters. An anthropomorphic user interface is not immune to these judgments, specifically when exhibiting social interaction skills in public spaces. A favorable impression may help engaging users in interaction and attaining acceptance for long-term interactions. We present three studies implementing a model of first impressions for initiating user interactions with an anthropomorphic museum guide agent with socio-relational skills. We focus on nonverbal behavior exhibiting personality and interpersonal attitude. In two laboratory studies, we demonstrate that impressions of an agent's personality are quickly formed based on proximity, whereas interpersonal attitude is conveyed through smile and gaze. We also found that interpersonal attitude has greater impact than personality on the user's decision to spend time with the agent. These findings are then applied to a museum guide agent exhibited at the Boston Museum of Science. In this field study, we show that employing our model increases the number of visitors engaging in interaction.","[{'authorId': '40123315', 'name': 'Angelo Cafaro'}, {'authorId': '2451989', 'name': 'H. Vilhjálmsson'}, {'authorId': '1690448', 'name': 'T. Bickmore'}]",87.0,"{'bibtex': '@Article{Cafaro2016FirstII,\n author = {Angelo Cafaro and H. Vilhjálmsson and T. Bickmore},\n journal = {ACM Transactions on Computer-Human Interaction (TOCHI)},\n pages = {1 - 40},\n title = {First Impressions in Human--Agent Virtual Encounters},\n volume = {23},\n year = {2016}\n}\n'}",,"{'volume': '23', 'pages': '1 - 40', 'name': 'ACM Transactions on Computer-Human Interaction (TOCHI)'}",138.0,First Impressions in Human--Agent Virtual Encounters,2016.0
2698,ed32cd837c0824bf444d3cf03be9ec510d868913,"e-learning is one of the emerging needs of the information age. Access to education is going to become crucial for the success of our information society, and therefore a lot of potential is seen in distance learning and distributed virtual environments. The communicative character of the distributed virtual environments would allow students and staff to meet in social shared spaces and engage in on-line real-time seminars and tutorials. Such technologies may mitigate some of the problems of isolation that distance learning brings. This paper presents our work in multi-user distributed virtual environments which are designed and implemented for educational uses in the bounds of the VES project. Furthermore, it presents our proposal for the extensions and reconstruction of the current system in order to create a more efficient system, which can be characterized as a learning virtual environment.","[{'authorId': '1696462', 'name': 'C. Bouras'}, {'authorId': '2577582', 'name': 'A. Philopoulos'}, {'authorId': '1802967', 'name': 'T. Tsiatsos'}]",85.0,"{'bibtex': '@Article{Bouras2001eLearningTD,\n author = {C. Bouras and A. Philopoulos and T. Tsiatsos},\n journal = {J. Netw. Comput. Appl.},\n pages = {175-199},\n title = {e-Learning through distributed virtual environments},\n volume = {24},\n year = {2001}\n}\n'}",,"{'volume': '24', 'pages': '175-199', 'name': 'J. Netw. Comput. Appl.'}",26.0,e-Learning through distributed virtual environments,2001.0
2699,ed492092ac35aad6e35257ec324a72dcc768b9ce,"Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN that makes it a useful tool in multimodality research.","[{'authorId': '1681130', 'name': 'P. Wittenburg'}, {'authorId': '1778610', 'name': 'H. Brugman'}, {'authorId': '143990706', 'name': 'A. Russel'}, {'authorId': '1776920', 'name': 'A. Klassmann'}, {'authorId': '1711769', 'name': 'H. Sloetjes'}]",1130.0,"{'bibtex': '@Inproceedings{Wittenburg2006ELANAP,\n author = {P. Wittenburg and H. Brugman and A. Russel and A. Klassmann and H. Sloetjes},\n pages = {1556-1559},\n title = {ELAN: a Professional Framework for Multimodality Research},\n year = {2006}\n}\n'}",,{'pages': '1556-1559'},3.0,ELAN: a Professional Framework for Multimodality Research,2006.0
2700,ed6c281ed506945dcdcaf30c78b40a842439bf54,,"[{'authorId': '2032259060', 'name': 'L. Thompson'}]",418.0,"{'bibtex': '@Article{Thompson1991InformationEI,\n author = {L. Thompson},\n journal = {Journal of Experimental Social Psychology},\n pages = {161-179},\n title = {Information exchange in negotiation},\n volume = {27},\n year = {1991}\n}\n'}",,"{'volume': '27', 'pages': '161-179', 'name': 'Journal of Experimental Social Psychology'}",15.0,Information exchange in negotiation,1991.0
2701,edcbb50b93f1e6050550141db950954be19af8dd,,"[{'authorId': '144445231', 'name': 'H. Lane'}, {'authorId': '49324966', 'name': 'Clara Cahill'}, {'authorId': '2649117', 'name': 'Susan Foutz'}, {'authorId': '143961465', 'name': 'Daniel Auerbach'}, {'authorId': '31989292', 'name': 'Dan Noren'}, {'authorId': '2912014', 'name': 'Catherine Lussenhop'}, {'authorId': '1684040', 'name': 'W. Swartout'}]",54.0,"{'bibtex': '@Inproceedings{Lane2013TheEO,\n author = {H. Lane and Clara Cahill and Susan Foutz and Daniel Auerbach and Dan Noren and Catherine Lussenhop and W. Swartout},\n pages = {309-318},\n title = {The Effects of a Pedagogical Agent for Informal Science Education on Learner Behaviors and Self-efficacy},\n year = {2013}\n}\n'}",,{'pages': '309-318'},26.0,The Effects of a Pedagogical Agent for Informal Science Education on Learner Behaviors and Self-efficacy,2013.0
2702,ee421d5e7df939466dbea787a37f33644772f1e3,,"[{'authorId': '2006502', 'name': 'R. Rhodes'}, {'authorId': '5191122', 'name': 'H. Hausenblas'}]",7.0,"{'bibtex': '@Inproceedings{Rhodes2016ExercisePP,\n author = {R. Rhodes and H. Hausenblas},\n title = {Exercise Psychology: Physical Activity and Sedentary Behavior},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Exercise Psychology: Physical Activity and Sedentary Behavior,2016.0
2703,ee43989b0aeebe8b9e6d9311178d5e4b81eb56a7,"The aim of this paper is to summarise how pronunciation feedback on the phoneme level should be given in computer-assisted pronunciation training (CAPT) in order to be effective. The study contains a literature survey of feedback in the language classroom, interviews with language teachers and their students about their attitudes towards pronunciation feedback, and observations of how feedback is given in their classrooms. The study was carried out using focus group meetings, individual semi-structured interviews and classroom observations. The feedback strategies that were advocated and observed in the study on pronunciation feedback from human teachers were implemented in a computer-animated language tutor giving articulation feedback. The virtual tutor was subsequently tested in a user trial and evaluated with a questionnaire. The article proposes several feedback strategies that would improve the pedagogical soundness of CAPT systems.","[{'authorId': '1898479', 'name': 'Olov Engwall'}, {'authorId': '2557365', 'name': 'Olle Bälter'}]",94.0,"{'bibtex': '@Article{Engwall2007PronunciationFF,\n author = {Olov Engwall and Olle Bälter},\n journal = {Computer Assisted Language Learning},\n pages = {235 - 262},\n title = {Pronunciation feedback from real and virtual language teachers},\n volume = {20},\n year = {2007}\n}\n'}",,"{'volume': '20', 'pages': '235 - 262', 'name': 'Computer Assisted Language Learning'}",37.0,Pronunciation feedback from real and virtual language teachers,2007.0
2704,ee5230f00d35d3bcee8848981ff6d0eaa373e31c,Mindblindness and mindreading evolutionary psychology and social chess mindreading - nature's choice developing mindreading - the four steps autism and mindblindness how brains read minds the language of the eyes mindreading - back to the future.,"[{'authorId': '1390019127', 'name': 'S. Baron-Cohen'}]",4191.0,"{'bibtex': '@Inproceedings{Baron-Cohen1997MindblindnessAE,\n author = {S. Baron-Cohen},\n title = {Mindblindness: An Essay on Autism and Theory of Mind},\n year = {1997}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Mindblindness: An Essay on Autism and Theory of Mind,1997.0
2705,ee5516c94fed6510ef340159280bce97c0a33106,"Non-verbal behavior, particularly eye movement, plays a fundamental role in nonverbal communication among people. In order to realize natural and intuitive human-agent interaction, the virtual agents need to employ this communicative channel effectively. Against this background, our research addresses the problem of emotionally expressive eye movement manner by describing a preliminary approach based on the parameters picked from real-time eye movement data (pupil size, blink rate and saccade).","[{'authorId': '2146248147', 'name': 'Zheng Li'}, {'authorId': '1724834', 'name': 'Xia Mao'}, {'authorId': '2150801616', 'name': 'Lei Liu'}]",11.0,"{'bibtex': '@Inproceedings{Li2009ProvidingEE,\n author = {Zheng Li and Xia Mao and Lei Liu},\n pages = {241-244},\n title = {Providing expressive eye movement to virtual agents},\n year = {2009}\n}\n'}",,{'pages': '241-244'},12.0,Providing expressive eye movement to virtual agents,2009.0
2706,ee652f0f63ed5b0cfe0af4cb4ea76b2ecf790c8d,"Preschool and elementary school children were asked to study a set of items until they were sure they could recall them perfectly (Flavell, Friedrichs, & Hoyt, 1970). The older subjects studied for a while, said they were ready, and usually were, that is, they showed perfect recall. The younger children studied for a while, said they were ready, and usually were not. In another study, elementary school children were asked to help the experimenter evaluate the communicative adequacy of verbal instructions, indicating any omissions and obscurities (Markman, 1977). Although the instructions were riddled with blatant omissions and obscurities, the younger subjects were surprisingly poor at detecting them. They incorrectly thought they had understood and could follow the instructions, much as their counterparts in the study by Flavell et al. (1970) incorrectly thought they had memorized and could recall the items. Results such as these have suggested that young children are quite limited in their knowledge and cognition about cognitive phenomena, or in their metacognition, and do relatively little monitoring of their own memory, comprehension, and other cognitive enterprises (see, e.g., Brown, 1978; Flavell, 1978; Flavell & Wellman, 1977; Kreutzer, Leonard, & Flavell, 1975; Flavell, Note 1, Note 2, Note 3; Markman, Note 4). Investigators have recently concluded that metacognition plays an important role in oral communication of information, oral persuasion, oral comprehension, reading comprehension, writing, language acquisition, attention, memory, problem solving, social cognition, and, various types of self-control and self-instruction; there are also clear indications that ideas about metacognition are beginning to make contact with similar ideas in the areas of social learning theory, cognitive behavior modification, personalty development, and education (Flavell, Note 1, Note 2, Note 3). Thus, the nature and de-","[{'authorId': '153137871', 'name': 'J. Flavell'}]",8275.0,"{'bibtex': '@Article{Flavell1979MetacognitionAC,\n author = {J. Flavell},\n journal = {American Psychologist},\n pages = {906-911},\n title = {Metacognition and Cognitive Monitoring: A New Area of Cognitive-Developmental Inquiry.},\n volume = {34},\n year = {1979}\n}\n'}",,"{'volume': '34', 'pages': '906-911', 'name': 'American Psychologist'}",13.0,Metacognition and Cognitive Monitoring: A New Area of Cognitive-Developmental Inquiry.,1979.0
2707,ee96f570ffacc8501348220f5972bc66532bd43a,"Our goal is to develop a believable embodied agent able to dialogue with a user. In particular, we aim at making an agent that can also combine facial expressions in a complex and subtle way, just like a human agent does. We first review a taxonomy of communicative functions that our agent is able to express non-verbally; but we point out that, due to the complexity of communication, in some cases different information can be provided at once by different parts and actions of an agent's face. In this paper we are interested in assessing and treating what happens, at the meaning and signal levels of behaviour, when different communicative functions have to be displayed at the same time and necessarily have to make use of the same expressive resources. In some of these cases the complexity of the agent's communication can give rise to conflicts between the parts or movements of the face. In this paper, we propose a way to manage the possible conflicts between different modalities of communication through the tool of belief networks, and we show how this tool allows us to combine facial expressions of different communicative functions and to display complex and subtle expressions. Copyright © 2002 John Wiley & Sons, Ltd.","[{'authorId': '1703084', 'name': 'C. Pelachaud'}, {'authorId': '1802126', 'name': 'I. Poggi'}]",130.0,"{'bibtex': '@Article{Pelachaud2002SubtletiesOF,\n author = {C. Pelachaud and I. Poggi},\n journal = {Comput. Animat. Virtual Worlds},\n pages = {301-312},\n title = {Subtleties of facial expressions in embodied agents},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '301-312', 'name': 'Comput. Animat. Virtual Worlds'}",35.0,Subtleties of facial expressions in embodied agents,2002.0
2709,eef41ae597a20ea377461d522fd5100da6a7a9b7,,"[{'authorId': '2054241', 'name': 'H. McGurk'}, {'authorId': '144239757', 'name': 'J. MacDonald'}]",5846.0,"{'bibtex': '@Article{McGurk1976HearingLA,\n author = {H. McGurk and J. MacDonald},\n journal = {Nature},\n pages = {746-748},\n title = {Hearing lips and seeing voices},\n volume = {264},\n year = {1976}\n}\n'}",,"{'volume': '264', 'pages': '746-748', 'name': 'Nature'}",2.0,Hearing lips and seeing voices,1976.0
2710,ef09ce85d82dc465fe9275fb13aef287166fd397,,"[{'authorId': '123512743', 'name': 'Giovanna Magnani'}]",1.0,"{'bibtex': '@Inproceedings{Magnani2017ModelingIT,\n author = {Giovanna Magnani},\n pages = {1065-1101},\n title = {Modeling in the Macroeconomics of Financial Markets},\n year = {2017}\n}\n'}",,"{'volume': '', 'pages': '1065-1101', 'name': ''}",96.0,Modeling in the Macroeconomics of Financial Markets,2017.0
2711,ef3673d113b91d6dfc9efb45036bd73c6c3f46ae,"This paper presents a model of agent's behavior that takes into account emotions and moral values. In our proposal, when the description of the current situation reveals that a moral value is 'at stake', the moral goal of re-establishing the threatened value is included among the active goals. The compliance with values generates positive emotions like pride and admiration, while the opposite brings to shame and self-reproach. 
 
During the deliberation phase, the agent appraises her plans in terms of the emotional reward they are expected to yield, given the trade off between moral and individual goals. In this phase, the emotional reward affects the agent's choices about her behavior. After the execution phase, one's and others' actions are appraised again in terms of the agent's values, giving rise to moral emotions. 
 
The paper shows how emotional appraisal can be coupled with the choice among possible lines of action, presenting a mapping between plans and emotions that integrates and extends preceding proposals.","[{'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '144411873', 'name': 'R. Damiano'}, {'authorId': '2338933', 'name': 'L. Lesmo'}]",32.0,"{'bibtex': '@Inproceedings{Battaglino2013EmotionalRI,\n author = {C. Battaglino and R. Damiano and L. Lesmo},\n pages = {769-776},\n title = {Emotional range in value-sensitive deliberation},\n year = {2013}\n}\n'}",,{'pages': '769-776'},35.0,Emotional range in value-sensitive deliberation,2013.0
2712,ef3a96d8f42e8caa1994caba2e53ca98121b4d1f,"There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.","[{'authorId': '3064807', 'name': 'Andrea Madotto'}, {'authorId': '38524906', 'name': 'Etsuko Ishii'}, {'authorId': '100466830', 'name': 'Zhaojiang Lin'}, {'authorId': '3491117', 'name': 'Sumanth Dathathri'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]",46.0,"{'bibtex': '@Article{Madotto2020PlugandPlayCM,\n author = {Andrea Madotto and Etsuko Ishii and Zhaojiang Lin and Sumanth Dathathri and Pascale Fung},\n journal = {ArXiv},\n title = {Plug-and-Play Conversational Models},\n volume = {abs/2010.04344},\n year = {2020}\n}\n'}",,"{'volume': 'abs/2010.04344', 'name': 'ArXiv'}",66.0,Plug-and-Play Conversational Models,2020.0
2713,ef57b243750b9fcb5183638b1db1206a8fdda357,"In the past decade there has been a rapid advance in the use of virtual reality (VR) technologies for leisure, training and education. VR is argued to offer particular benefits for children on the autism spectrum, chiefly because it can offer simulations of authentic real-world situations in a carefully controlled and safe environment. Given the real world social difficulties experienced by children on the spectrum, this technology has therefore been argued to offer distinct advantages and benefits for social and life skills training compared to other approaches. Whilst there has been some progress in testing the relevance and applicability of VR for children on the autism spectrum in educational contexts, there remains a significant challenge in developing robust and usable technologies that can really make a difference in real world classrooms. This article considers the evidence that has been published over the past 10 years to assess how the potential of VR has been explored in practice and reflect on the current state-of-the-art in this field.","[{'authorId': '144674468', 'name': 'S. Parsons'}, {'authorId': '20955942', 'name': 'S. Cobb'}]",302.0,"{'bibtex': '@Article{Parsons2011StateoftheartOV,\n author = {S. Parsons and S. Cobb},\n journal = {European Journal of Special Needs Education},\n pages = {355 - 366},\n title = {State-of-the-art of virtual reality technologies for children on the autism spectrum},\n volume = {26},\n year = {2011}\n}\n'}",,"{'volume': '26', 'pages': '355 - 366', 'name': 'European Journal of Special Needs Education'}",50.0,State-of-the-art of virtual reality technologies for children on the autism spectrum,2011.0
2714,ef61d5c346ae770b80e69593ecef499f8dcd5bd6,"Negotiation skills are essential in everyday life, whether in a professional or personal context. Negotiation enables two parties to address misunderstandings and avoid conflicts through an exchange that depends as much on the interpersonal skills of the negotiators as the tactics employed. Acquiring these skills requires not only sound conceptual knowledge but also practice and mentoring. This paper describes the BiLAT game-based simulation and tutoring system developed to provide students, initially United States Army soldiers, with an environment to practice preparing for and conducting bilateral negotiations. We describe the models that were created to implement BiLAT, with a particular focus on the challenge of designing for and tutoring in the ill-defined domain of negotiation. An initial assessment of the training effectiveness of the system indicates significant situation-judgment gains by novices.","[{'authorId': '47964935', 'name': 'Julia M. Kim'}, {'authorId': '1812270', 'name': 'R. Hill'}, {'authorId': '1724385', 'name': 'P. Durlach'}, {'authorId': '144445231', 'name': 'H. Lane'}, {'authorId': '2097330', 'name': 'Eric Forbell'}, {'authorId': '3122851', 'name': 'Mark G. Core'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '1748597', 'name': 'D. Pynadath'}, {'authorId': '2055460171', 'name': 'John Hart'}]",168.0,"{'bibtex': '@Article{Kim2009BiLATAG,\n author = {Julia M. Kim and R. Hill and P. Durlach and H. Lane and Eric Forbell and Mark G. Core and S. Marsella and D. Pynadath and John Hart},\n journal = {Int. J. Artif. Intell. Educ.},\n pages = {289-308},\n title = {BiLAT: A Game-Based Environment for Practicing Negotiation in a Cultural Context},\n volume = {19},\n year = {2009}\n}\n'}",,"{'volume': '19', 'pages': '289-308', 'name': 'Int. J. Artif. Intell. Educ.'}",29.0,BiLAT: A Game-Based Environment for Practicing Negotiation in a Cultural Context,2009.0
2715,ef92e15f6704f3dfe8ee7c336990fdb7d59e8a6d,"The way people feel about a technology can determine whether the technology is embraced or rejected by its intended users. People’s feelings and emotions are particularly important for uptake of socio-technical systems involving social behaviour. For example, a photo sharing web application will not be used if people do not feel engaged or in-touch with their friends and family during use. Considering the feelings of the intended users of a system can uncover new requirements, leading to an improved and more accepted system. We contend that requirements engineering, in developing systems for the new social age, needs modified lightweight practices. In this paper, we propose adapting a requirements engineering methodology to include emotion modelling for socio-technical systems. We extend a set of existing agent oriented lightweight models to consider emotions, describe the process for modelling, and demonstrate both models and process in an example of personal alarm systems for older adults.","[{'authorId': '1401292565', 'name': 'A. Lopez-Lorca'}, {'authorId': '144658641', 'name': 'Tim Miller'}, {'authorId': '3199529', 'name': 'S. Pedell'}, {'authorId': '145977411', 'name': 'L. Sterling'}, {'authorId': '3292250', 'name': 'M. Curumsing'}]",8.0,"{'bibtex': '@Inproceedings{Lopez-Lorca2014ModellingER,\n author = {A. Lopez-Lorca and Tim Miller and S. Pedell and L. Sterling and M. Curumsing},\n title = {Modelling Emotional Requirements},\n year = {2014}\n}\n'}",,,13.0,Modelling Emotional Requirements,2014.0
2716,ef9366070fc54155c8540ad6b1e411dcef1ccd44,,"[{'authorId': '2386187', 'name': 'J. Loomis'}, {'authorId': '2307657', 'name': 'J. Blascovich'}, {'authorId': '40458739', 'name': 'A. Beall'}]",661.0,"{'bibtex': '@Article{Loomis1999ImmersiveVE,\n author = {J. Loomis and J. Blascovich and A. Beall},\n journal = {Behavior Research Methods, Instruments, & Computers},\n pages = {557-564},\n title = {Immersive virtual environment technology as a basic research tool in psychology},\n volume = {31},\n year = {1999}\n}\n'}",,"{'volume': '31', 'pages': '557-564', 'name': 'Behavior Research Methods, Instruments, & Computers'}",67.0,Immersive virtual environment technology as a basic research tool in psychology,1999.0
2717,efa5e81448845fbe34b275647f91107312439846,,"[{'authorId': '3350062', 'name': 'Emmanuel Ayedoun'}, {'authorId': '1980261', 'name': 'Masataka Tokumaru'}]",1.0,"{'bibtex': '@Inproceedings{Ayedoun2022TowardsEE,\n author = {Emmanuel Ayedoun and Masataka Tokumaru},\n pages = {12-18},\n title = {Towards Emotionally Expressive Virtual Agent to Foster Independent Speaking Tasks: A Preliminary Study},\n year = {2022}\n}\n'}",,{'pages': '12-18'},0.0,Towards Emotionally Expressive Virtual Agent to Foster Independent Speaking Tasks: A Preliminary Study,2022.0
2718,f03da64a082a677bd3281865098f0c5113a49aa8,"We seek to investigate voice quality characteristics, in particular on a breathy to tense dimension, as an indicator for psychological distress, i.e. depression and post-traumatic stress disorder (PTSD), within semi-structured virtual human interviews. Our evaluation identifies significant differences between the voice quality of psychologically distressed participants and not-distressed participants within this limited corpus. We investigate the capability of automatic algorithms to classify psychologically distressed speech in speaker-independent experiments. Additionally, we examine the impact of the posed questions’ affective polarity, as motivated by findings in the literature on positive stimulus attenuation and negative stimulus potentiation in emotional reactivity of psychologically distressed participants. The experiments yield promising results using standard machine learning algorithms and solely four distinct features capturing the tenseness of the speaker’s voice.","[{'authorId': '1770312', 'name': 'Stefan Scherer'}, {'authorId': '2624478', 'name': 'Giota Stratou'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",109.0,"{'bibtex': '@Inproceedings{Scherer2013InvestigatingVQ,\n author = {Stefan Scherer and Giota Stratou and J. Gratch and Louis-Philippe Morency},\n pages = {847-851},\n title = {Investigating voice quality as a speaker-independent indicator of depression and PTSD},\n year = {2013}\n}\n'}",,{'pages': '847-851'},27.0,Investigating voice quality as a speaker-independent indicator of depression and PTSD,2013.0
2719,f04fb68ec6c202c6601c493a4303ea532370e338,"Synthetic images of facial expression were used to assess whether judges can correctly recognize emotions exclusively on the basis of configurations of facial muscle movements. A first study showed that static, synthetic images modeled after a series of photographs that are widely used in facial expression research yielded recognition rates and confusion patterns comparable to posed photos. In a second study, animated synthetic images were used to examine whether schematic facial expressions consisting entirely of theoretically postulated facial muscle configurations can be correctly recognized. Recognition rates for the synthetic expressions were far above chance, and the confusion patterns were comparable to those obtained with posed photos. In addition, the effect of static versus dynamic presentation of the expressions was studied. Dynamic presentation increased overall recognition accuracy and reduced confusions between unrelated emotions.","[{'authorId': '1719131', 'name': 'T. Wehrle'}, {'authorId': '10539662', 'name': 'S. Kaiser'}, {'authorId': '2238656881', 'name': 'Susanne Schmidt'}, {'authorId': '2238696921', 'name': 'Klaus R Scherer'}]",341.0,"{'bibtex': '@Article{Wehrle2000StudyingTD,\n author = {T. Wehrle and S. Kaiser and Susanne Schmidt and Klaus R Scherer},\n journal = {Journal of personality and social psychology},\n pages = {\n          105-19\n        },\n title = {Studying the dynamics of emotional expression using synthesized facial muscle movements.},\n volume = {78 1},\n year = {2000}\n}\n'}",,"{'volume': '78 1', 'pages': '\n          105-19\n        ', 'name': 'Journal of personality and social psychology'}",61.0,Studying the dynamics of emotional expression using synthesized facial muscle movements.,2000.0
2721,f07ee564a9460a87dccf76f6b5c601efc9bdd330,"There is good reason to believe that gaze direction and facial displays of emotion share an information value as signals of approach or avoidance. The combination of these cues in the analysis of social communication, however, has been a virtually neglected area of inquiry. Two studies were conducted to test the prediction that direct gaze would facilitate the processing of facially communicated approach-oriented emotions (e.g., anger and joy), whereas averted gaze would facilitate the processing of facially communicated avoidance-oriented emotions (e.g., fear and sadness). The results of both studies confirmed the central hypothesis and suggest that gaze direction and facial expression are combined in the processing of emotionally relevant facial information.","[{'authorId': '2075454382', 'name': 'R. B. Adams'}, {'authorId': '4170904', 'name': 'R. Kleck'}]",554.0,"{'bibtex': '@Article{Adams2003PerceivedGD,\n author = {R. B. Adams and R. Kleck},\n journal = {Psychological Science},\n pages = {644 - 647},\n title = {Perceived Gaze Direction and the Processing of Facial Displays of Emotion},\n volume = {14},\n year = {2003}\n}\n'}",,"{'volume': '14', 'pages': '644 - 647', 'name': 'Psychological Science'}",24.0,Perceived Gaze Direction and the Processing of Facial Displays of Emotion,2003.0
2723,f09f7b464c89cee948027d84e619ae36568a0ee0,,"[{'authorId': '47985333', 'name': 'A. Kendon'}]",487.0,"{'bibtex': '@Article{Kendon1970MovementCI,\n author = {A. Kendon},\n journal = {Acta psychologica},\n pages = {\n          100-25\n        },\n title = {Movement coordination in social interaction: some examples described.},\n volume = {32 2},\n year = {1970}\n}\n'}",,"{'volume': '32 2', 'pages': '\n          100-25\n        ', 'name': 'Acta psychologica'}",11.0,Movement coordination in social interaction: some examples described.,1970.0
2725,f0a54b5bb0e19a98f3556d5073f32046c1d397ce,,"[{'authorId': '2001300', 'name': 'G. Caridakis'}, {'authorId': '3346592', 'name': 'A. Raouzaiou'}, {'authorId': '1772136', 'name': 'Elisabetta Bevacqua'}, {'authorId': '2169958', 'name': 'M. Mancini'}, {'authorId': '1715144', 'name': 'K. Karpouzis'}, {'authorId': '2185181', 'name': 'Lori Malatesta'}, {'authorId': '1703084', 'name': 'C. Pelachaud'}]",59.0,"{'bibtex': '@Article{Caridakis2007VirtualAM,\n author = {G. Caridakis and A. Raouzaiou and Elisabetta Bevacqua and M. Mancini and K. Karpouzis and Lori Malatesta and C. Pelachaud},\n journal = {Language Resources and Evaluation},\n pages = {367-388},\n title = {Virtual agent multimodal mimicry of humans},\n volume = {41},\n year = {2007}\n}\n'}",,"{'volume': '41', 'pages': '367-388', 'name': 'Language Resources and Evaluation'}",37.0,Virtual agent multimodal mimicry of humans,2007.0
2726,f0b5c554108196c22eaca95d6c9d33bfbd197824,"Background Observing incongruent actions interferes with ongoing action execution. This ‘interference effect’ is larger for observed biological actions than for non-biological actions. The current study used virtual reality to investigate the biological specificity of interference effects of action observation in autism spectrum conditions (ASC). Method High-functioning adults with ASC and age- and IQ-matched healthy controls performed horizontal sinusoidal arm movements whilst observing arm movements conducted by a virtual reality agent with either human or robot form, which moved with either biological motion or at a constant velocity. In another condition, participants made the same arm movements while observing a real human. Observed arm movements were either congruent or incongruent with executed arm movements. An interference effect was calculated as the average variance in the incongruent action dimension during observation of incongruent compared with congruent movements. Results Control participants exhibited an interference effect when observing real human and virtual human agent incongruent movements but not when observing virtual robot agent movements. Individuals with ASC differed from controls in that they showed no interference effects for real human, virtual human or virtual robot movements. Conclusions The current study demonstrates atypical interference effects in ASC.","[{'authorId': '2259095458', 'name': 'Jennifer Cook'}, {'authorId': '2259443901', 'name': 'David Swapp'}, {'authorId': '2260334935', 'name': 'Xueni Pan'}, {'authorId': '2259442234', 'name': 'Nadia Bianchi-Berthouze'}, {'authorId': '2255585142', 'name': 'Sarah-Jayne Blakemore'}]",47.0,"{'bibtex': '@Article{Cook2013AtypicalIE,\n author = {Jennifer Cook and David Swapp and Xueni Pan and Nadia Bianchi-Berthouze and Sarah-Jayne Blakemore},\n journal = {Psychological Medicine},\n pages = {731 - 740},\n title = {Atypical interference effect of action observation in autism spectrum conditions},\n volume = {44},\n year = {2013}\n}\n'}",,"{'volume': '44', 'pages': '731 - 740', 'name': 'Psychological Medicine'}",53.0,Atypical interference effect of action observation in autism spectrum conditions,2013.0
2727,f0d929a65ec49eea9e86810c896a401083346ba3,"The last few years have seen great maturation in understanding how to use computer graphics technology to portray 3D embodied characters or virtual humans. Unlike the off-line, animator-intensive methods used in the special effects industry, real-time embodied agents are expected to exist and interact with us ""live."" They can be represent other people or function as autonomous helpers, teammates, or tutors enabling novel interactive educational and training applications. We should be able to interact and communicate with them through modalities we already use, such as language, facial expressions, and gesture. Various aspects and issues in real-time virtual humans will be discussed, including consistent parameterizations for gesture and facial actions using movement observation principles, and the representational basis for character believability, personality, and affect. We also describe a Parameterized Action Representation (PAR) that allows an agent to act, plan, and reason about its actions or actions of others. Besides embodying the semantics of human action, the PAR is designed for building future behaviors into autonomous agents and controlling the animation parameters that portray personality, mood, and affect in an embodied agent.","[{'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '2427954', 'name': 'Liwei Zhao'}, {'authorId': '1994564', 'name': 'M. Byun'}]",122.0,"{'bibtex': '@Article{Badler2002RepresentingAP,\n author = {N. Badler and J. Allbeck and Liwei Zhao and M. Byun},\n journal = {Proceedings of Computer Animation 2002 (CA 2002)},\n pages = {133-143},\n title = {Representing and parameterizing agent behaviors},\n year = {2002}\n}\n'}",,"{'pages': '133-143', 'name': 'Proceedings of Computer Animation 2002 (CA 2002)'}",57.0,Representing and parameterizing agent behaviors,2002.0
2728,f0dbe7e0ae00fea02c7ab6de432b8970b9f3ef43,,"[{'authorId': '32964910', 'name': 'Yanghee Kim'}, {'authorId': '25550816', 'name': 'A. L. Baylor'}]",297.0,"{'bibtex': '@Article{Kim2006ASF,\n author = {Yanghee Kim and A. L. Baylor},\n journal = {Educational Technology Research and Development},\n pages = {569-596},\n title = {A Social-Cognitive Framework for Pedagogical Agents as Learning Companions},\n volume = {54},\n year = {2006}\n}\n'}",,"{'volume': '54', 'pages': '569-596', 'name': 'Educational Technology Research and Development'}",120.0,A Social-Cognitive Framework for Pedagogical Agents as Learning Companions,2006.0
2729,f106e3afbdd01022962d7a73a847869d0534a036,In this contribution we introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared throughout the paper. Within the first method a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines and results achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English language is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86% recognition rate. As a basis of comparison the similar judgment of human deciders classifying the same corpus at 79.8% recognition rate was analyzed.,"[{'authorId': '145411696', 'name': 'Björn Schuller'}, {'authorId': '145512909', 'name': 'G. Rigoll'}, {'authorId': '3130518', 'name': 'M. Lang'}]",618.0,"{'bibtex': ""@Article{Schuller2003HiddenMM,\n author = {Björn Schuller and G. Rigoll and M. Lang},\n journal = {2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)},\n pages = {I-401},\n title = {Hidden Markov model-based speech emotion recognition},\n volume = {1},\n year = {2003}\n}\n""}",,"{'volume': '1', 'pages': 'I-401', 'name': ""2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)""}",6.0,Hidden Markov model-based speech emotion recognition,2003.0
2730,f11be60b2a822e1e20d3d2224245c4222f5ce176,"American Sign Language (ASL) generation software can improve the accessibility of information and services for deaf individuals with low English literacy. The understand-ability of current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, calibration, and recording protocol, we conducted an evaluation study with native ASL signers.","[{'authorId': '39227955', 'name': 'Pengfei Lu'}, {'authorId': '1747703', 'name': 'Matt Huenerfauth'}]",48.0,"{'bibtex': '@Inproceedings{Lu2010CollectingAM,\n author = {Pengfei Lu and Matt Huenerfauth},\n pages = {89-97},\n title = {Collecting a Motion-Capture Corpus of American Sign Language for Data-Driven Generation Research},\n year = {2010}\n}\n'}",,{'pages': '89-97'},34.0,Collecting a Motion-Capture Corpus of American Sign Language for Data-Driven Generation Research,2010.0
2731,f129124da1d10b1a4b33e2dc7e01d6a4349886e7,"It is suggested that the motion of pedestrians can be described as if they would be subject to ``social forces.'' These ``forces'' are not directly exerted by the pedestrians' personal environment, but they are a measure for the internal motivations of the individuals to perform certain actions (movements). The corresponding force concept is discussed in more detail and can also be applied to the description of other behaviors. In the presented model of pedestrian behavior several force terms are essential: first, a term describing the acceleration towards the desired velocity of motion; second, terms reflecting that a pedestrian keeps a certain distance from other pedestrians and borders; and third, a term modeling attractive effects. The resulting equations of motion of nonlinearly coupled Langevin equations. Computer simulations of crowds of interacting pedestrians show that the social force model is capable of describing the self-organization of several observed collective effects of pedestrian behavior very realistically.","[{'authorId': '1768825', 'name': 'D. Helbing'}, {'authorId': '2068050852', 'name': 'P. Molnár'}]",5586.0,"{'bibtex': '@Article{Helbing1995SocialFM,\n author = {D. Helbing and P. Molnár},\n journal = {Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics},\n pages = {\n          4282-4286\n        },\n title = {Social force model for pedestrian dynamics.},\n volume = {51 5},\n year = {1995}\n}\n'}",,"{'volume': '51 5', 'pages': '\n          4282-4286\n        ', 'name': 'Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics'}",32.0,Social force model for pedestrian dynamics.,1995.0
2732,f1967d2d7187f6b5a10c2b09a37ec31ead4496ba,"The Nash equilibrium, the main solution concept in analytical game theory, cannot make precise predictions about the outcome of repeated mixed-motive games. Nor can it tell us much about the dynamics by which a population of players moves from one equilibrium to another. These limitations, along with concerns about the cognitive demands of forward-looking rationality, have motivated efforts to explore backward-looking alternatives to analytical game theory. Most of the effort has been invested in evolutionary models of population dynamics. We shift attention to a learning-theoretic alternative. Computational experiments with adaptive agents identify a fundamental solution concept for social dilemmas–−stochastic collusion–−based on a random walk from a self-limiting noncooperative equilibrium into a self-reinforcing cooperative equilibrium. However, we show that this solution is viable only within a narrow range of aspiration levels. Below the lower threshold, agents are pulled into a deficient equilibrium that is a stronger attractor than mutual cooperation. Above the upper threshold, agents are dissatisfied with mutual cooperation. Aspirations that adapt with experience (producing habituation to stimuli) do not gravitate into the window of viability; rather, they are the worst of both worlds. Habituation destabilizes cooperation and stabilizes defection. Results from the two-person problem suggest that applications to multiplex and embedded relationships will yield unexpected insights into the global dynamics of cooperation in social dilemmas.","[{'authorId': '1863307', 'name': 'M. Macy'}, {'authorId': '2201619', 'name': 'A. Flache'}]",542.0,"{'bibtex': '@Article{Macy2002LearningDI,\n author = {M. Macy and A. Flache},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n pages = {7229 - 7236},\n title = {Learning dynamics in social dilemmas},\n volume = {99},\n year = {2002}\n}\n'}",,"{'volume': '99', 'pages': '7229 - 7236', 'name': 'Proceedings of the National Academy of Sciences of the United States of America'}",21.0,Learning dynamics in social dilemmas,2002.0
2733,f1af714b92372c8e606485a3982eab2f16772ad8,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,"[{'authorId': '1795764', 'name': 'Niki Aifanti'}, {'authorId': '2075670585', 'name': 'Christos Papachristou'}, {'authorId': '143685457', 'name': 'A. Delopoulos'}]",319.0,"{'bibtex': '@Article{Aifanti2010TheMF,\n author = {Niki Aifanti and Christos Papachristou and A. Delopoulos},\n journal = {11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10},\n pages = {1-4},\n title = {The MUG facial expression database},\n year = {2010}\n}\n'}",,"{'pages': '1-4', 'name': '11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10'}",10.0,The MUG facial expression database,2010.0
2734,f1d790d9e67f682c38d3574e2642a9d682d72f79,"The Expressive Gaze Model is a hierarchical framework for composing simple behaviors into emotionally expressive gaze shifts for virtual characters. Its primary components are the Gaze Warping Transformation, which generates emotionally expressive head and torso movement in a gaze shift, and an eye movement model.","[{'authorId': '145417478', 'name': 'Brent Lance'}, {'authorId': '1788771', 'name': 'S. Marsella'}]",30.0,"{'bibtex': '@Article{Lance2010TheEG,\n author = {Brent Lance and S. Marsella},\n journal = {IEEE Computer Graphics and Applications},\n pages = {62-73},\n title = {The Expressive Gaze Model: Using Gaze to Express Emotion},\n volume = {30},\n year = {2010}\n}\n'}",,"{'volume': '30', 'pages': '62-73', 'name': 'IEEE Computer Graphics and Applications'}",16.0,The Expressive Gaze Model: Using Gaze to Express Emotion,2010.0
2735,f1e518b21ee9db316ebc919c8efd33cdfe4542cd,,"[{'authorId': '1689001', 'name': 'C. Jonker'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",358.0,"{'bibtex': '@Inproceedings{Jonker1999FormalAO,\n author = {C. Jonker and Jan Treur},\n pages = {221-231},\n title = {Formal Analysis of Models for the Dynamics of Trust Based on Experiences},\n year = {1999}\n}\n'}",,{'pages': '221-231'},11.0,Formal Analysis of Models for the Dynamics of Trust Based on Experiences,1999.0
2736,f211a64d0cda39172d21d80cbde0565ff3a80adf,,"[{'authorId': '34966245', 'name': 'P. Wisniewski'}, {'authorId': '8121434', 'name': 'M. Prietula'}]",25.0,"{'bibtex': '@Article{Wisniewski2010CASAWA,\n author = {P. Wisniewski and M. Prietula},\n journal = {Comput. Hum. Behav.},\n pages = {1761-1771},\n title = {CASA, WASA, and the dimensions of us},\n volume = {26},\n year = {2010}\n}\n'}",,"{'volume': '26', 'pages': '1761-1771', 'name': 'Comput. Hum. Behav.'}",80.0,"CASA, WASA, and the dimensions of us",2010.0
2737,f216ea6fc4a77ffdfb40c4d9aabd095aabcc7dfb,"In this paper, we present a study aimed at understanding whether the embodiment and humanlikeness of an artificial agent can affect people’s spontaneous and instructed mimicry of its facial expressions. The study followed a mixed experimental design and revolved around an emotion recognition task. Participants were randomly assigned to one level of humanlikeness (between-subject variable: humanlike, characterlike, or morph facial texture of the artificial agents) and observed the facial expressions displayed by three artificial agents differing in embodiment (within-subject variable: video-recorded robot, physical robot, and virtual agent) and a human (control). To study both spontaneous and instructed facial mimicry, we divided the experimental sessions into two phases. In the first phase, we asked participants to observe and recognize the emotions displayed by the agents. In the second phase, we asked them to look at the agents’ facial expressions, replicate their dynamics as closely as possible, and then identify the observed emotions. In both cases, we assessed participants’ facial expressions with an automated Action Unit (AU) intensity detector. Contrary to our hypotheses, our results disclose that the agent that was perceived as the least uncanny, and most anthropomorphic, likable, and co-present, was the one spontaneously mimicked the least. Moreover, they show that instructed facial mimicry negatively predicts spontaneous facial mimicry. Further exploratory analyses revealed that spontaneous facial mimicry appeared when participants were less certain of the emotion they recognized. Hence, we postulate that an emotion recognition goal can flip the social value of facial mimicry as it transforms a likable artificial agent into a distractor. Further work is needed to corroborate this hypothesis. Nevertheless, our findings shed light on the functioning of human-agent and human-robot mimicry in emotion recognition tasks and help us to unravel the relationship between facial mimicry, liking, and rapport.","[{'authorId': '1556566185', 'name': 'G. Perugia'}, {'authorId': '2047241817', 'name': 'Maike Paetzel-Prüsmann'}, {'authorId': '2321433', 'name': 'I. Hupont'}, {'authorId': '1958033', 'name': 'G. Varni'}, {'authorId': '1680828', 'name': 'M. Chetouani'}, {'authorId': '144687810', 'name': 'Christopher E. Peters'}, {'authorId': '39540970', 'name': 'Ginevra Castellano'}]",0.0,"{'bibtex': '@Article{Perugia2021DoesTG,\n author = {G. Perugia and Maike Paetzel-Prüsmann and I. Hupont and G. Varni and M. Chetouani and Christopher E. Peters and Ginevra Castellano},\n booktitle = {Frontiers in Robotics and AI},\n journal = {Frontiers in Robotics and AI},\n title = {Does the Goal Matter? Emotion Recognition Tasks Can Change the Social Value of Facial Mimicry Towards Artificial Agents},\n volume = {8},\n year = {2021}\n}\n'}",[],"{'name': 'Frontiers in Robotics and AI', 'volume': '8'}",88.0,Does the Goal Matter? Emotion Recognition Tasks Can Change the Social Value of Facial Mimicry Towards Artificial Agents,2021.0
2738,f22a8ab1dd90eea10f797d4e9f4fd46f5d6f4876,"This article describes a procedure for the study of destructive obedience in the laboratory. It consists of ordering a naive S to administer increasingly more severe punishment to a victim in the context of a learning experiment. Punishment is administered by means of a shock generator with 30 graded switches ranging from Slight Shock to Danger: Severe Shock. The victim is a confederate of the E. The primary dependent variable is the maximum shock the S is willing to administer before he refuses to continue further. 26 Ss obeyed the experimental commands fully, and administered the highest shock on the generator. 14 Ss broke off the experiment at some point after the victim protested and refused to provide further answers. The procedure created extreme levels of nervous tension in some Ss. Profuse sweating, trembling, and stuttering were typical expressions of this emotional disturbance. One unexpected sign of tension — yet to be explained — was the regular occurrence of nervous laughter, which in some Ss developed into uncontrollable seizures. The variety of interesting behavioral dynamics observed in the experiment, the reality of the situation for the S, and the possibility of parametric variation within the framework of the procedure, point to the fruitfulness of further study.1 OBEDIENCE is as basic an element in the structure of social life as one can point to. Some system of authority is a requirement of all communal living, and it is only the man dwelling in isolation who is not forced to respond, through defiance or submission, to the commands of others. Obedience, as a determinant of behavior, is of particular relevance to our time. It has been reliably established that from 1933–45 millions of innocent persons were systematically slaughtered on command. Gas chambers were built, death camps were guarded; daily quotas of corpses were produced with the same efficiency as the manufacture of appliances. These inhumane policies may have originated in the mind of a single person, but they could only be carried out on a massive scale if a very large number of persons obeyed orders. Obedience is the psychological mechanism that links individual action to political purpose. It is the dispositional cement that binds men to systems of authority. Facts of recent history and observation in daily life suggest that for many persons obedience may be a deeply ingrained behavior tendency, indeed a prepotent impulse overriding training in ethics, sympathy, and moral conduct. C. P. Snow (1961) points to its importance when he writes: When you think of the long and gloomy history of man, you will find more hideous crimes have been committed in the name of obedience than have ever been committed in the name of rebellion. If you doubt that, read William Shirer’s Rise and Fall of the Third Reich. The German Officer Corps were brought up in the most rigorous code of obedience . . . in the name of obedience they were party to, and assisted in, the most wicked large scale actions in the history of the world [p. 24].","[{'authorId': '40457603', 'name': 'S. Milgram'}]",3871.0,"{'bibtex': '@Article{Milgram1963BEHAVIORALSO,\n author = {S. Milgram},\n journal = {Journal of abnormal psychology},\n pages = {\n          371-8\n        },\n title = {BEHAVIORAL STUDY OF OBEDIENCE.},\n volume = {67},\n year = {1963}\n}\n'}",,"{'volume': '67', 'pages': '\n          371-8\n        ', 'name': 'Journal of abnormal psychology'}",17.0,BEHAVIORAL STUDY OF OBEDIENCE.,1963.0
2739,f22d304e0c22bd29206600d9fc4ec8f234a57641,"This article describes My Science Tutor (MyST), an intelligent tutoring system designed to improve science learning by students in 3rd, 4th, and 5th grades (7 to 11 years old) through conversational dialogs with a virtual science tutor. In our study, individual students engage in spoken dialogs with the virtual tutor Marni during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts embedded in the investigations. The spoken dialogs in MyST are designed to scaffold learning by presenting open-ended questions accompanied by illustrations or animations related to the classroom investigations and the science concepts being learned. The focus of the interactions is to elicit self-expression from students. To this end, Marni applies some of the principles of Questioning the Author, a proven approach to classroom conversations, to challenge students to think about and integrate new concepts with prior knowledge to construct enriched mental models that can be used to explain and predict scientific phenomena. In this article, we describe how spoken dialogs using Automatic Speech Recognition (ASR) and natural language processing were developed to stimulate students' thinking, reasoning and self explanations. We describe the MyST system architecture and Wizard of Oz procedure that was used to collect data from tutorial sessions with elementary school students. Using data collected with the procedure, we present evaluations of the ASR and semantic parsing components. A formal evaluation of learning gains resulting from system use is currently being conducted. This paper presents survey results of teachers' and children's impressions of MyST.","[{'authorId': '1866226', 'name': 'Wayne H. Ward'}, {'authorId': '8089863', 'name': 'R. Cole'}, {'authorId': '144667246', 'name': 'Daniel Bolaños'}, {'authorId': '1403911433', 'name': 'Cindy Buchenroth-Martin'}, {'authorId': '2083251271', 'name': 'Edward Svirsky'}, {'authorId': '145037594', 'name': 'Sarel van Vuuren'}, {'authorId': '3035147', 'name': 'T. Weston'}, {'authorId': '2145106857', 'name': 'Jing Zheng'}, {'authorId': '2065398554', 'name': 'Lee Becker'}]",98.0,"{'bibtex': '@Article{Ward2011MyST,\n author = {Wayne H. Ward and R. Cole and Daniel Bolaños and Cindy Buchenroth-Martin and Edward Svirsky and Sarel van Vuuren and T. Weston and Jing Zheng and Lee Becker},\n journal = {ACM Trans. Speech Lang. Process.},\n pages = {18:1-18:29},\n title = {My science tutor: A conversational multimedia virtual tutor for elementary school science},\n volume = {7},\n year = {2011}\n}\n'}",,"{'volume': '7', 'pages': '18:1-18:29', 'name': 'ACM Trans. Speech Lang. Process.'}",91.0,My science tutor: A conversational multimedia virtual tutor for elementary school science,2011.0
2740,f234fc9706195a9fa884c3a49e14d7931be749ea,"Over the past few years we have been developing an expressive embodied conversational agent system. In particular, we have developed a model of multimodal behaviours that includes dynamism and complex facial expressions. The first feature refers to the qualitative execution of behaviours. Our model is based on perceptual studies and encompasses several parameters that modulate multimodal behaviours. The second feature, the model of complex expressions, follows a componential approach where a new expression is obtained by combining facial areas of other expressions. Lately we have been working on adding temporal dynamism to expressions. So far they have been designed statically, typically at their apex. Only full-blown expressions could be modelled. To overcome this limitation, we have defined a representation scheme that describes the temporal evolution of the expression of an emotion. It is no longer represented by a static definition but by a temporally ordered sequence of multimodal signals.","[{'authorId': '2250548269', 'name': 'Catherine Pelachaud'}]",137.0,"{'bibtex': '@Article{Pelachaud2009ModellingME,\n author = {Catherine Pelachaud},\n journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},\n pages = {3539 - 3548},\n title = {Modelling multimodal expression of emotion in a virtual agent},\n volume = {364},\n year = {2009}\n}\n'}",,"{'volume': '364', 'pages': '3539 - 3548', 'name': 'Philosophical Transactions of the Royal Society B: Biological Sciences'}",77.0,Modelling multimodal expression of emotion in a virtual agent,2009.0
2743,f25baf281c13207c2459b0264aa3fa30212ab5e8,"The automatic analysis of emotions conveyed in social media content, e.g., tweets, has many beneficial applications. In the Philippines, one of the most disaster-prone countries in the world, such methods could potentially enable first responders to make timely decisions despite the risk of data deluge. However, recognising emotions expressed in Philippine-generated tweets, which are mostly written in Filipino, English or a mix of both, is a non-trivial task. In order to facilitate the development of natural language processing (NLP) methods that will automate such type of analysis, we have built a corpus of tweets whose predominant emotions have been manually annotated by means of crowdsourcing. Defining measures ensuring that only high-quality annotations were retained, we have produced a gold standard corpus of 1,146 emotion-labelled Filipino and English tweets. We validate the value of this manually produced resource by demonstrating that an automatic emotion-prediction method based on the use of a publicly available word-emotion association lexicon was unable to reproduce the labels assigned via crowdsourcing. While we are planning to make a few extensions to the corpus in the near future, its current version has been made publicly available in order to foster the development of emotion analysis methods based on advanced Filipino and English NLP.","[{'authorId': '35645750', 'name': 'F. Lapitan'}, {'authorId': '1400900759', 'name': 'R. Batista-Navarro'}, {'authorId': '2674576', 'name': 'E. Albacea'}]",10.0,"{'bibtex': '@Inproceedings{Lapitan2016CrowdsourcingbasedAO,\n author = {F. Lapitan and R. Batista-Navarro and E. Albacea},\n pages = {74-82},\n title = {Crowdsourcing-based Annotation of Emotions in Filipino and English Tweets},\n year = {2016}\n}\n'}",,{'pages': '74-82'},19.0,Crowdsourcing-based Annotation of Emotions in Filipino and English Tweets,2016.0
2744,f2743cb3cd26bad5a36fabaeb82fd86166e93b53,,"[{'authorId': '8745904', 'name': 'B. Fasel'}, {'authorId': '1678373', 'name': 'J. Luettin'}]",1991.0,"{'bibtex': '@Article{Fasel2003AutomaticFE,\n author = {B. Fasel and J. Luettin},\n journal = {Pattern Recognit.},\n pages = {259-275},\n title = {Automatic facial expression analysis: a survey},\n volume = {36},\n year = {2003}\n}\n'}",,"{'volume': '36', 'pages': '259-275', 'name': 'Pattern Recognit.'}",86.0,Automatic facial expression analysis: a survey,2003.0
2745,f2831b3142925e73175686f1226c9388669463ca,Part I. Introduction: 1. The media equation Part II. Media and Manners: 2. Politeness 3. Interpersonal distance 4. Flattery 5. Judging others and ourselves Part III. Media and Personality: 6. Personality of characters 7. Personality of interfaces 8. Imitating a personality Part IV. Media and emotion: 9. Good versus bad 10. Negativity 11. Arousal Part V. Media and Social Roles: 12. Specialists 13. Teammates 14. Gender 15. Voices 16. Source orientation Part VI. Media and Form: 17. Image size 18. Fidelity 19. Synchrony 20. Motion 21. Scene changes 22. Subliminal images Part VII. Final Words: 23. Conclusions about the media equation References.,"[{'authorId': '143923082', 'name': 'Byron Reeves'}, {'authorId': '2029850', 'name': 'C. Nass'}]",5059.0,"{'bibtex': '@Inproceedings{Reeves1996TheME,\n author = {Byron Reeves and C. Nass},\n pages = {I-XIII, 1-305},\n title = {The media equation - how people treat computers, television, and new media like real people and places},\n year = {1996}\n}\n'}",,"{'pages': 'I-XIII, 1-305'}",0.0,"The media equation - how people treat computers, television, and new media like real people and places",1996.0
2749,f2847dfe542ba81485161fe727a89ffc7cd850fd,"Human make behavior by perception, modeling believable perception for a virtual agent is a meaningful topic in computer games. A model of adaptive perception of a virtual agent with emotion is proposed based on Q-learning method; the formulas of emotion value are presented. A demo system is realized on PC; an agent with the model can dynamically control the range of the perception according to virtual environment, and express believable emotions.","[{'authorId': '2115377494', 'name': 'Yuan Hong'}, {'authorId': '2109341502', 'name': 'Zhen Liu'}]",0.0,"{'bibtex': '@Conference{Hong2010ModelingAP,\n author = {Yuan Hong and Zhen Liu},\n booktitle = {International Conferences on Audio, Language and Image Processing},\n journal = {2010 International Conference on Audio, Language and Image Processing},\n pages = {941-944},\n title = {Modeling adaptive perception system of virtual agent with emotion based on Q-learning},\n year = {2010}\n}\n'}",[],"{'name': '2010 International Conference on Audio, Language and Image Processing', 'pages': '941-944'}",7.0,Modeling adaptive perception system of virtual agent with emotion based on Q-learning,2010.0
2750,f288805630dbd6637d23e6dde1fba1344297f42e,,"[{'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '2487649', 'name': 'G. Bente'}]",120.0,"{'bibtex': '@Article{Krämer2010PersonalizingET,\n author = {N. Krämer and G. Bente},\n journal = {Educational Psychology Review},\n pages = {71-87},\n title = {Personalizing e-Learning. The Social Effects of Pedagogical Agents},\n volume = {22},\n year = {2010}\n}\n'}",,"{'volume': '22', 'pages': '71-87', 'name': 'Educational Psychology Review'}",112.0,Personalizing e-Learning. The Social Effects of Pedagogical Agents,2010.0
2751,f2c10a0ca0e10f9e68fad85ea77ad75636abde18,,"[{'authorId': '2699703', 'name': 'S. Hofmann'}, {'authorId': '32200813', 'name': 'Joseph K. Carpenter'}, {'authorId': '46627773', 'name': 'Joshua E. Curtiss'}]",132.0,"{'bibtex': '@Article{Hofmann2016InterpersonalER,\n author = {S. Hofmann and Joseph K. Carpenter and Joshua E. Curtiss},\n journal = {Cognitive Therapy and Research},\n pages = {341-356},\n title = {Interpersonal Emotion Regulation Questionnaire (IERQ): Scale Development and Psychometric Characteristics},\n volume = {40},\n year = {2016}\n}\n'}",,"{'volume': '40', 'pages': '341-356', 'name': 'Cognitive Therapy and Research'}",59.0,Interpersonal Emotion Regulation Questionnaire (IERQ): Scale Development and Psychometric Characteristics,2016.0
2752,f2cd6c1b04035b249aedeb61f4e5d4b9f98097b5,,"[{'authorId': '115855742', 'name': 'M. D. Meijer'}]",417.0,"{'bibtex': '@Article{Meijer1989TheCO,\n author = {M. D. Meijer},\n journal = {Journal of Nonverbal Behavior},\n pages = {247-268},\n title = {The contribution of general features of body movement to the attribution of emotions},\n volume = {13},\n year = {1989}\n}\n'}",,"{'volume': '13', 'pages': '247-268', 'name': 'Journal of Nonverbal Behavior'}",32.0,The contribution of general features of body movement to the attribution of emotions,1989.0
2753,f2e08db0b87a1034586a3dcb4bf967fb9a0e019d,"Most of the research on multiagent systems has focused on the development of rational utility-maximizing agents. However, research shows that emotions have a strong effect on peoples' physical states, motivations, beliefs, and desires. By introducing primary and secondary emotion into BDI architecture, we present a generic architecture for an emotional agent, EBDI, which can merge various emotion theories with an agent's reasoning process. It implements practical reasoning techniques separately from the specific emotion mechanism. The separation allows us to plug in emotional models as needed or upgrade the agent's reasoning engine independently.","[{'authorId': '2158159744', 'name': 'Hong Jiang'}, {'authorId': '104832894', 'name': 'J. Vidal'}, {'authorId': '1744170', 'name': 'M. Huhns'}]",128.0,"{'bibtex': '@Inproceedings{Jiang2007EBDIAA,\n author = {Hong Jiang and J. Vidal and M. Huhns},\n pages = {11},\n title = {EBDI: an architecture for emotional agents},\n year = {2007}\n}\n'}",,{'pages': '11'},32.0,EBDI: an architecture for emotional agents,2007.0
2754,f318c892091974adb06a4d70ec2b5c5558acde4a,"The facial feedback hypothesis, that skeletal muscle feedback from facial expressions plays a causal role in regulating emotional experience and behavior, is an important part of several contemporary theories of emotion. A review of relevant research indicates that studies reporting support for this hypothesis have, without exception, used within-subjects designs and that therefore only a restricted version of the hypothesis has been tested. Also, the results of some of these studies must be questioned due to demand characteristics and other problems. It is suggested that visceral feedback may make a more direct contribution to emotional processes than facial feedback does and that the ""readout"" functions of facial expressions are more important than any feedback functions.","[{'authorId': '33400830', 'name': 'R. Buck'}]",329.0,"{'bibtex': '@Article{Buck1980NonverbalBA,\n author = {R. Buck},\n journal = {Journal of personality and social psychology},\n pages = {\n          811-24\n        },\n title = {Nonverbal behavior and the theory of emotion: the facial feedback hypothesis.},\n volume = {38 5},\n year = {1980}\n}\n'}",,"{'volume': '38 5', 'pages': '\n          811-24\n        ', 'name': 'Journal of personality and social psychology'}",58.0,Nonverbal behavior and the theory of emotion: the facial feedback hypothesis.,1980.0
2755,f31df4527959a6e522efb2c2115dc8d0e3d29d49,,"[{'authorId': '2276310', 'name': 'A. Öhman'}]",378.0,"{'bibtex': '@Article{Öhman2005TheRO,\n author = {A. Öhman},\n journal = {Psychoneuroendocrinology},\n pages = {953-958},\n title = {The role of the amygdala in human fear: Automatic detection of threat},\n volume = {30},\n year = {2005}\n}\n'}",,"{'volume': '30', 'pages': '953-958', 'name': 'Psychoneuroendocrinology'}",39.0,The role of the amygdala in human fear: Automatic detection of threat,2005.0
2756,f357cffffca307e290d113ef450db47a31115172,"Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction. Previous works involve unimodal analysis of emotion, while recent efforts focus on multimodal emotion recognition from vision and speech. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset.","[{'authorId': '2120818109', 'name': 'C. Lee'}, {'authorId': '35287507', 'name': 'Kyu Ye Song'}, {'authorId': '2115679080', 'name': 'Jihoon Jeong'}, {'authorId': '2115123897', 'name': 'W. Choi'}]",66.0,"{'bibtex': '@Article{Lee2018ConvolutionalAN,\n author = {C. Lee and Kyu Ye Song and Jihoon Jeong and W. Choi},\n journal = {ArXiv},\n title = {Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data},\n volume = {abs/1805.06606},\n year = {2018}\n}\n'}",,"{'volume': 'abs/1805.06606', 'name': 'ArXiv'}",20.0,Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data,2018.0
2757,f35bed4a04d3d0cef445574bbb7303afde8a47e4,,"[{'authorId': '39900740', 'name': 'P. Tan'}, {'authorId': '1707756', 'name': 'M. Steinbach'}, {'authorId': '2107978997', 'name': 'Vipin Kumar'}]",1609.0,"{'bibtex': '@Inproceedings{Tan2005IntroductionTD,\n author = {P. Tan and M. Steinbach and Vipin Kumar},\n title = {Introduction to Data Mining, (First Edition)},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Introduction to Data Mining, (First Edition)",2005.0
2758,f37017e1bc1e3b6ce9408ab2835fd67b206b4763,,"[{'authorId': '117048675', 'name': 'Magnani Lorenzo'}, {'authorId': '150048421', 'name': 'Bertolotti Tommaso Wayne'}]",22.0,"{'bibtex': '@Inproceedings{Lorenzo2017SpringerHO,\n author = {Magnani Lorenzo and Bertolotti Tommaso Wayne},\n title = {Springer Handbook of Model-Based Science},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Springer Handbook of Model-Based Science,2017.0
2759,f38ac11046190d4c09e74fe4a1e05fe4a374e56a,"Computer Science offers a lot of different approaches for solving the problem of automatic facial expression recognition. Of course these algorithms do not have a recognition rate of 100%. This paper aims at answering the question, how reliable humans can specify the six universal facial expression investigated by the psychologists Ekman and Friesen. We conduct a survey to determine the capability of humans concerning this task. The results of that survey are interpreted and evaluated regarding the confusion of facial expressions by humans. Recent algorithms in this field are briefly presented and compared to the recognition rate of humans.","[{'authorId': '1989987', 'name': 'Ursula Zucker'}]",1.0,"{'bibtex': '@Inproceedings{Zucker2007FacialER,\n author = {Ursula Zucker},\n title = {Facial Expression Recognition-A Comparison Between Humans and Algorithms},\n year = {2007}\n}\n'}",,,19.0,Facial Expression Recognition-A Comparison Between Humans and Algorithms,2007.0
2760,f39d53555872d83f42ac2385e2ae8e4f5e2d0282,"In this work we report on the progress of building a system that enables fully automated fast and robust facial expression recognition from face video. We analyse subtle changes in facial expression by recognizing facial muscle action units (AUs) and analysing their temporal behavior. By detecting AUs from face video we enable the analysis of various facial communicative signals including facial expressions of emotion, attitude and mood. For an input video picturing a facial expression we detect per frame whether any of 15 different AUs is activated, whether that facial action is in the onset, apex, or offset phase, and what the total duration of the activation in question is. We base this process upon a set of spatio-temporal features calculated from tracking data for 20 facial fiducial points. To detect these 20 points of interest in the first frame of an input face video, we utilize a fully automatic, facial point localization method that uses individual feature GentleBoost templates built from Gabor wavelet features. Then, we exploit a particle filtering scheme that uses factorized likelihoods and a novel observation model that combines a rigid and a morphological model to track the facial points. The AUs displayed in the input video and their temporal segments are recognized finally by Support Vector Machines trained on a subset of most informative spatio-temporal features selected by AdaBoost. For Cohn-Kanade andMMI databases, the proposed system classifies 15 AUs occurring alone or in combination with other AUs with a mean agreement rate of 90.2% with human FACS coders.","[{'authorId': '1795528', 'name': 'M. Valstar'}, {'authorId': '145387780', 'name': 'M. Pantic'}]",374.0,"{'bibtex': ""@Article{Valstar2006FullyAF,\n author = {M. Valstar and M. Pantic},\n journal = {2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)},\n pages = {149-149},\n title = {Fully Automatic Facial Action Unit Detection and Temporal Analysis},\n year = {2006}\n}\n""}",,"{'pages': '149-149', 'name': ""2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)""}",21.0,Fully Automatic Facial Action Unit Detection and Temporal Analysis,2006.0
2761,f43fa6f5a00246958e19d3ab8fa9cd5dbb511e33,"Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like ""bleed"" and ""punch"" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.","[{'authorId': '2660071', 'name': 'Ethan Fast'}, {'authorId': '1592064987', 'name': 'Binbin Chen'}, {'authorId': '145879842', 'name': 'Michael S. Bernstein'}]",336.0,"{'bibtex': '@Article{Fast2016EmpathUT,\n author = {Ethan Fast and Binbin Chen and Michael S. Bernstein},\n journal = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},\n title = {Empath: Understanding Topic Signals in Large-Scale Text},\n year = {2016}\n}\n'}",,{'name': 'Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems'},49.0,Empath: Understanding Topic Signals in Large-Scale Text,2016.0
2762,f458561ff791beee73f9dcba1d287c4ed2eec21e,,"[{'authorId': '2059583', 'name': 'H. Maaref'}, {'authorId': '69466176', 'name': 'C. Barret'}]",66.0,"{'bibtex': '@Article{Maaref2000SensorbasedFN,\n author = {H. Maaref and C. Barret},\n journal = {Control Engineering Practice},\n pages = {757-768},\n title = {Sensor-based fuzzy navigation of an autonomous mobile robot in an indoor environment},\n volume = {8},\n year = {2000}\n}\n'}",,"{'volume': '8', 'pages': '757-768', 'name': 'Control Engineering Practice'}",20.0,Sensor-based fuzzy navigation of an autonomous mobile robot in an indoor environment,2000.0
2763,f46cdf30800c5715394030515ee224d3db74fa16,"Human communication is mainly based on speech, gestures, facial and bodily expressions. Nonverbal expressions, as body posture and small movements, regulate social interactions. These results lead to an ever-growing effort from behalf of researchers in the human-computer interaction field to include emotions and emotional expressivity in their projects. Evaluation of emotion expressions recognition becomes, thus, an important step in the development of project involving interaction between humans and avatars. In the present paper we describe the results of an evaluation experiment made in two settings: discrimination between three videos looking for a specific emotion expression and identification task that asks for the selection of a list of a seen expression. The obtained results are similar to previous studies from the field and underline again the necessity of careful design of some particular emotions, especially those one that can be considered from the same family.","[{'authorId': '2700753', 'name': 'Ildikó Pelczer'}, {'authorId': '152954909', 'name': 'F. C. Contreras'}, {'authorId': '2068289483', 'name': 'F. G. Rodríguez'}]",6.0,"{'bibtex': '@Article{Pelczer2007ExpressionsOE,\n author = {Ildikó Pelczer and F. C. Contreras and F. G. Rodríguez},\n journal = {2007 IEEE Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems},\n pages = {31-35},\n title = {Expressions of Emotions in Virtual Agents: Empirical Evaluation},\n year = {2007}\n}\n'}",,"{'pages': '31-35', 'name': '2007 IEEE Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems'}",7.0,Expressions of Emotions in Virtual Agents: Empirical Evaluation,2007.0
2764,f47d1a2fa6303de9af4636b7eddf7ed585cab567,,"[{'authorId': '69866536', 'name': 'M. Boekaerts'}, {'authorId': '100780648', 'name': 'S. Maes'}]",159.0,"{'bibtex': '@Article{Boekaerts1987EmotionAC,\n author = {M. Boekaerts and S. Maes},\n journal = {Communication and Cognition. Monographies},\n pages = {127-205},\n title = {Emotion and Cognition.},\n volume = {20},\n year = {1987}\n}\n'}",,"{'volume': '20', 'pages': '127-205', 'name': 'Communication and Cognition. Monographies'}",0.0,Emotion and Cognition.,1987.0
2765,f481dee2dd82b73c49f2db4230b35fa1e081a3f8,"Effective leadership can increase team performance, however the underlying micro-level behaviors that support team performance are still unclear. At the same time, traditional behavioral observation methods rely on manual video annotation which is a time consuming and costly process. In this work, we employ wearable motion sensors to automatically extract nonverbal cues from body motion. We utilize activity recognition methods to detect relevant nonverbal cues such as head nodding, gesticulating and posture changes. Further, we combine the detected individual cues to quantify behavioral mimicry between interaction partners. We evaluate our methods on data that was acquired during a psychological experiment in which 55 groups of three persons worked on a decision-making task. Group leaders were instructed to either lead with individual consideration orin an authoritarian way. We demonstrate that nonverbal cues can be detected with a F1-measure between 56% and 100%. Moreover, we show how our methods can highlight nonverbal behavioral differences of the two leadership styles. Our findings suggest that individually considerate leaders mimic head nods of their followers twice as often and that their face touches are mimicked three times as often by their followers when compared with authoritarian leaders.","[{'authorId': '3043165', 'name': 'S. Feese'}, {'authorId': '2798136', 'name': 'B. Arnrich'}, {'authorId': '144119654', 'name': 'G. Tröster'}, {'authorId': '46738324', 'name': 'Bertolt Meyer'}, {'authorId': '3220899', 'name': 'K. Jonas'}]",35.0,"{'bibtex': '@Article{Feese2012QuantifyingBM,\n author = {S. Feese and B. Arnrich and G. Tröster and Bertolt Meyer and K. Jonas},\n journal = {2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing},\n pages = {520-525},\n title = {Quantifying Behavioral Mimicry by Automatic Detection of Nonverbal Cues from Body Motion},\n year = {2012}\n}\n'}",,"{'pages': '520-525', 'name': '2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing'}",17.0,Quantifying Behavioral Mimicry by Automatic Detection of Nonverbal Cues from Body Motion,2012.0
2766,f48fecb84242c1bbc6a059b061692ce7e002fba0,"This study reports on the development of two Messenger bots, designed to facilitate the learning of introductory and intermediate accounting. The Messenger bots were developed using a visual development environment that requires no coding knowledge. A thick description of the development of the Messenger bots is provided to encourage replication. It is submitted that instructors, rather than programmers, should take ownership of developing Messenger bots for teaching and learning. Preliminary exploration of the students' satisfaction yielded positive results. Suggestions are made for specific applications of Messenger bots in teaching and learning and for further research exploring the use of Messenger bots in teaching and learning. Practitioner NotesWhat is already known about this topicMobile instant messaging (MIM) applications (apps) have potential to facilitate effective social constructivist‐based collaborative learning.Students extensively use MIM apps.There is reluctance from instructors to engage in after hours MIM student consultation.Messenger bots can deliver content on demand, using inter alia text, images and video, and the effective use thereof in teaching and learning has not yet been explored.What this paper addsInstructors, without coding knowledge, are enabled to develop Messenger bots for teaching and learning.Social constructivist‐based suggestions for specific teaching and learning applications of Messenger bots are provided.Preliminary evidence suggests students positively experienced the use of Messenger bots in their learning.Implications for practice and/or policyMessenger bots may support instruction in large classes.Messenger bots may be suited to supplemental instruction rather than replacing face‐to‐face instruction.Specific teaching and learning applications of Messenger bots should be implemented and the effectiveness thereof explored. [ABSTRACT FROM AUTHOR] uracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","[{'authorId': '75163006', 'name': 'Astrid Schmulian'}, {'authorId': '80762598', 'name': 'S. Coetzee'}]",23.0,"{'bibtex': ""@Article{Schmulian2019TheDO,\n author = {Astrid Schmulian and S. Coetzee},\n journal = {Br. J. Educ. Technol.},\n pages = {2751-2777},\n title = {The development of Messenger bots for teaching and learning and accounting students' experience of the use thereof},\n volume = {50},\n year = {2019}\n}\n""}",,"{'volume': '50', 'pages': '2751-2777', 'name': 'Br. J. Educ. Technol.'}",41.0,The development of Messenger bots for teaching and learning and accounting students' experience of the use thereof,2019.0
2767,f492e48796bcdd9a37763a563f2230a9654603cc,"We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at this https URL","[{'authorId': '8719864', 'name': 'Tom Bocklisch'}, {'authorId': '147062331', 'name': 'Joe Faulkner'}, {'authorId': '39034356', 'name': 'Nick Pawlowski'}, {'authorId': '2058816675', 'name': 'Alan Nichol'}]",318.0,"{'bibtex': '@Article{Bocklisch2017RasaOS,\n author = {Tom Bocklisch and Joe Faulkner and Nick Pawlowski and Alan Nichol},\n journal = {ArXiv},\n title = {Rasa: Open Source Language Understanding and Dialogue Management},\n volume = {abs/1712.05181},\n year = {2017}\n}\n'}",,"{'volume': 'abs/1712.05181', 'name': 'ArXiv'}",15.0,Rasa: Open Source Language Understanding and Dialogue Management,2017.0
2768,f4b364973a6447e00300b0a5c97482890fb94eb6,"We use reinforcement learning (RL) to learn a multi-issue negotiation dialogue policy. For training and evaluation, we build a hand-crafted agenda-based policy, which serves as the negotiation partner of the RL policy. Both the agendabased and the RL policies are designed to work for a large variety of negotiation settings, and perform well against negotiation partners whose behavior has not been observed before. We evaluate the two models by having them negotiate against each other under various settings. The learned model consistently outperforms the agenda-based model. We also ask human raters to rate negotiation transcripts between the RL policy and the agenda-based policy, regarding the rationality of the two negotiators. The RL policy is perceived as more rational than the agenda-based policy.","[{'authorId': '1710287', 'name': 'A. Papangelis'}, {'authorId': '3194430', 'name': 'Kallirroi Georgila'}]",17.0,"{'bibtex': '@Inproceedings{Papangelis2013ReinforcementLO,\n author = {A. Papangelis and Kallirroi Georgila},\n pages = {154-158},\n title = {Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies},\n year = {2013}\n}\n'}",,{'pages': '154-158'},28.0,Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies,2013.0
2769,f4b518c73de023803c9c610d4cac8e9b3516434a,"Previous research indicates that photorealistic virtual representations (i.e., agents and avatars) of the self can influence attitude and behavior change. this study was designed to test participants’ physiological reactions to exercising or still agents that resembled the self or a stranger. a within-subjects experiment tested participants’ (n = 10) skin conductance in response to running and loitering virtual selves (created from participants’ photographs) and virtual others. Participants entered a fully immersive virtual environment and observed the agents as their physiological response was measured. arousal was greatest when exposed to a running virtual self or a loitering virtual other. the finding that the virtual self causes physiological arousal may explain why a running virtual self has been shown in previous research to increase exercise behavior after exposure. implications for the development of Virtual Reality exercise treatments and other virtual therapies are discussed.","[{'authorId': '143619505', 'name': 'Jesse Fox'}, {'authorId': '1737161', 'name': 'J. Bailenson'}, {'authorId': '95101383', 'name': 'T. Ricciardi'}]",29.0,"{'bibtex': '@Inproceedings{Fox2012PhysiologicalRT,\n author = {Jesse Fox and J. Bailenson and T. Ricciardi},\n title = {Physiological ResPonses to ViRtual selVes and ViRtual otheRs},\n year = {2012}\n}\n'}",,,20.0,Physiological ResPonses to ViRtual selVes and ViRtual otheRs,2012.0
2770,f4fc65dcd1789d6300bccd7fa390bfeeb8853b47,,"[{'authorId': '145966408', 'name': 'R. Sun'}]",204.0,"{'bibtex': '@Inproceedings{Sun2005CognitionAM,\n author = {R. Sun},\n title = {Cognition and Multi-Agent Interaction: The CLARION Cognitive Architecture: Extending Cognitive Modeling to Social Simulation},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Cognition and Multi-Agent Interaction: The CLARION Cognitive Architecture: Extending Cognitive Modeling to Social Simulation,2005.0
2772,f59c8ad5a1527df51ad51d74bd02fdacf5910df4,"Conversational agents are supposed to combine speech with non‐verbal modalities for intelligible multimodal utterances. In this paper, we focus on the generation of gesture and speech from XML‐based descriptions of their overt form. An incremental production model is presented that combines the synthesis of synchronized gestural, verbal, and facial behaviors with mechanisms for linking them in fluent utterances with natural co‐articulation and transition effects. In particular, an efficient kinematic approach for animating hand gestures from shape specifications is presented, which provides fine adaptation to temporal constraints that are imposed by cross‐modal synchrony. Copyright © 2004 John Wiley & Sons, Ltd.","[{'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",283.0,"{'bibtex': '@Article{Kopp2004SynthesizingMU,\n author = {S. Kopp and I. Wachsmuth},\n journal = {Computer Animation and Virtual Worlds},\n title = {Synthesizing multimodal utterances for conversational agents},\n volume = {15},\n year = {2004}\n}\n'}",,"{'volume': '15', 'name': 'Computer Animation and Virtual Worlds'}",24.0,Synthesizing multimodal utterances for conversational agents,2004.0
2773,f5af862bfabeb4fbafffc29d8d202bce0272086f,"Adding gesture to spoken instructions makes those instructions more effective. The question we ask here is why. A group of 49 third and fourth grade children were given instruction in mathematical equivalence with gesture or without it. Children given instruction that included a correct problem-solving strategy in gesture were significantly more likely to produce that strategy in their own gestures during the same instruction period than children not exposed to the strategy in gesture. Those children were then significantly more likely to succeed on a posttest than children who did not produce the strategy in gesture. Gesture during instruction encourages children to produce gestures of their own, which, in turn, leads to learning. Children may be able to use their hands to change their minds.","[{'authorId': '2995580', 'name': 'S. Cook'}, {'authorId': '115377287', 'name': 'S. Goldin‐Meadow'}]",282.0,"{'bibtex': '@Article{Cook2006TheRO,\n author = {S. Cook and S. Goldin‐Meadow},\n journal = {Journal of Cognition and Development},\n pages = {211 - 232},\n title = {The Role of Gesture in Learning: Do Children Use Their Hands to Change Their Minds?},\n volume = {7},\n year = {2006}\n}\n'}",,"{'volume': '7', 'pages': '211 - 232', 'name': 'Journal of Cognition and Development'}",51.0,The Role of Gesture in Learning: Do Children Use Their Hands to Change Their Minds?,2006.0
2774,f5c33f3ea625ff36ad653030000d6e814904a8ea,"With android robots becoming increasingly sophisticated in their technical as well as artistic design, their non-verbal expressiveness is getting closer to that of real humans. Accordingly, this paper presents results of two online surveys designed to evaluate a female android's facial display of five basic emotions. We prepared both surveys in English, German, and Japanese language allowing us to analyze for inter-cultural differences. Accordingly, we not only found that our design of the emotional expressions “fearful” and “surprised” were often confused, but also that many Japanese participants seemed to confuse “angry” with “sad” in contrast to the German and English participants. Although similar facial displays portrayed by the model person of Geminoid F achieved higher recognition rates overall, portraying fearful has been similarly difficult for the model person. We conclude that improving the android's expressiveness especially around the eyes would be a useful next step in android design. In general, these results could be complemented by an evaluation of dynamic facial expressions of Geminoid F in future research.","[{'authorId': '1403827243', 'name': 'C. Becker-Asano'}, {'authorId': '1687808', 'name': 'H. Ishiguro'}]",115.0,"{'bibtex': '@Article{Becker-Asano2011EvaluatingFD,\n author = {C. Becker-Asano and H. Ishiguro},\n journal = {2011 IEEE Workshop on Affective Computational Intelligence (WACI)},\n pages = {1-8},\n title = {Evaluating facial displays of emotion for the android robot Geminoid F},\n year = {2011}\n}\n'}",,"{'pages': '1-8', 'name': '2011 IEEE Workshop on Affective Computational Intelligence (WACI)'}",23.0,Evaluating facial displays of emotion for the android robot Geminoid F,2011.0
2775,f5c903a49bf6c97db58e8fe5ee010fc92ef07ef9,"We propose a data-driven approach for measuring, validating and optimizing crowd simulation systems by learning parameters from real-life videos. We discuss the common traits of incidents and their video footage suitable for the learning step. We then demonstrate the learning process in a real-life incident that happened in 2015, Ankara, Turkey. We reanimate the incident with an existing emotion contagion and crowd simulation framework and optimize the parameters that take role in defining agent behavior with respect to the data extracted from the video footage of the incident.","[{'authorId': '32565728', 'name': 'A. E. Basak'}]",1.0,"{'bibtex': '@Inproceedings{Basak2017LearningFR,\n author = {A. E. Basak},\n title = {Learning From Real-Life Experiences : A Data-Driven Emotion Contagion Approach Towards More Realistic Virtual},\n year = {2017}\n}\n'}","[{'paperId': '4250b7eec31114ec43d828a52ecff264048af63f', 'title': 'Emotional Contagion Driven of Parent-Child’s Agents in Crowd during Panic Situation'}]",,25.0,Learning From Real-Life Experiences : A Data-Driven Emotion Contagion Approach Towards More Realistic Virtual,2017.0
2776,f5db81872b51e00c1e74e8bf5da08309c6434d02,,"[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '144074133', 'name': 'M. Hoogendoorn'}, {'authorId': '144038398', 'name': 'M. Klein'}, {'authorId': '1726343', 'name': 'Jan Treur'}, {'authorId': '1881843', 'name': 'C. N. V. D. Wal'}, {'authorId': '1809908', 'name': 'A. V. Wissen'}]",139.0,"{'bibtex': '@Article{Bosse2013ModellingCD,\n author = {T. Bosse and M. Hoogendoorn and M. Klein and Jan Treur and C. N. V. D. Wal and A. V. Wissen},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {52-84},\n title = {Modelling collective decision making in groups and crowds: Integrating social contagion and interacting emotions, beliefs and intentions},\n volume = {27},\n year = {2013}\n}\n'}",,"{'volume': '27', 'pages': '52-84', 'name': 'Autonomous Agents and Multi-Agent Systems'}",59.0,"Modelling collective decision making in groups and crowds: Integrating social contagion and interacting emotions, beliefs and intentions",2013.0
2779,f5e61ef588ad90ec1d6349d23c8af8d00eaf457a,"When individuals experience empathy, they often seek to bolster others' well-being. But what do empathizers want others to feel? Though psychologists have studied empathy and prosociality for decades, this question has yet to be clearly addressed. This is because virtually all existing research focuses on a model under which improving others' well-being also comprises heightening their positive affect or decreasing their negative affect and helping them reach their own emotional goals. In this review, I argue that real-life empathic goals encompass a broader range-including sometimes worsening targets' affect or contravening their wishes in order to improve their well-being-that can be productively integrated into the framework of interpersonal emotion regulation (IER). I review the empathic IER spectrum in a number of contexts, including close relationships, professional caregiving, and group-based emotions. Integrating empathy and IER provides a synthetic and generative way to ask new questions about how social emotions produce prosocial actions. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.","[{'authorId': '2268731', 'name': 'Jamil Zaki'}]",87.0,"{'bibtex': '@Article{Zaki2020IntegratingEA,\n author = {Jamil Zaki},\n journal = {Annual review of psychology},\n title = {Integrating Empathy and Interpersonal Emotion Regulation.},\n year = {2020}\n}\n'}",,{'name': 'Annual review of psychology'},154.0,Integrating Empathy and Interpersonal Emotion Regulation.,2020.0
2780,f5fa683bfaf0fbb41030e10326dce01b74d3b609,"To understand the epistemological meaning of simulation, it does not suffice to interpret simulation practice and theory in the framework of philosophy of science alone. Theory, experiment, measurement and observation are important activities of the scientific method. But what regards an epistemological interpretation of simulation, philosophical truth theories allow gaining additional insights. This paper discusses philosophical truth theories â€“ e.g. the correspondence, coherence and consensus theory â€“ and relates them to simulation practice and methodology, focussing on validation.","[{'authorId': '71304100', 'name': 'A. Schmid'}]",50.0,"{'bibtex': '@Article{Schmid2005WhatIT,\n author = {A. Schmid},\n journal = {J. Artif. Soc. Soc. Simul.},\n title = {What is the Truth of Simulation?},\n volume = {8},\n year = {2005}\n}\n'}",,"{'volume': '8', 'name': 'J. Artif. Soc. Soc. Simul.'}",0.0,What is the Truth of Simulation?,2005.0
2781,f61b5b5b66670b1ae468743ec56b7f9e7b165320,"Background Mental health problems are common among individuals with autism spectrum disorder (ASD), and difficulties with emotion regulation processes may underlie these issues. Cognitive behavior therapy (CBT) is considered an efficacious treatment for anxiety in children with ASD. Additional research is needed to examine the efficacy of a transdiagnostic treatment approach, whereby the same treatment can be applied to multiple emotional problems, beyond solely anxiety. The purpose of the present study was to examine the efficacy of a manualized and individually delivered 10‐session, transdiagnostic CBT intervention, aimed at improving emotion regulation and mental health difficulties in children with ASD. Methods Sixty‐eight children (M age = 9.75, SD = 1.27) and their parents participated in the study, randomly allocated to either a treatment immediate (n = 35) or waitlist control condition (n = 33) (ISRCTN #67079741). Parent‐, child‐, and clinician‐reported measures of emotion regulation and mental health were administered at baseline, postintervention/postwaitlist, and at 10‐week follow‐up. Results Children in the treatment immediate condition demonstrated significant improvements on measures of emotion regulation (i.e., emotionality, emotion regulation abilities with social skills) and aspects of psychopathology (i.e., a composite measure of internalizing and externalizing symptoms, adaptive behaviors) compared to those in the waitlist control condition. Treatment gains were maintained at follow‐up. Conclusions This study is the first transdiagnostic CBT efficacy trial for children with ASD. Additional investigations are needed to further establish its relative efficacy compared to more traditional models of CBT for children with ASD and other neurodevelopmental conditions.","[{'authorId': '2150018495', 'name': 'J. Weiss'}, {'authorId': '34628583', 'name': 'K. Thomson'}, {'authorId': '4079867', 'name': 'Priscilla Burnham Riosa'}, {'authorId': '40974129', 'name': 'C. Albaum'}, {'authorId': '2057710352', 'name': 'Victoria Chan'}, {'authorId': '31412592', 'name': 'Andrea L. Maughan'}, {'authorId': '5550702', 'name': 'P. Tablon'}, {'authorId': '36172799', 'name': 'Karen R. Black'}]",95.0,"{'bibtex': '@Article{Weiss2018ARW,\n author = {J. Weiss and K. Thomson and Priscilla Burnham Riosa and C. Albaum and Victoria Chan and Andrea L. Maughan and P. Tablon and Karen R. Black},\n journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},\n pages = {1180 - 1191},\n title = {A randomized waitlist‐controlled trial of cognitive behavior therapy to improve emotion regulation in children with autism},\n volume = {59},\n year = {2018}\n}\n'}",,"{'volume': '59', 'pages': '1180 - 1191', 'name': 'Journal of Child Psychology and Psychiatry, and Allied Disciplines'}",80.0,A randomized waitlist‐controlled trial of cognitive behavior therapy to improve emotion regulation in children with autism,2018.0
2782,f637fe5fd1aeb081c3514c3ef1fcba1686693c96,"This paper presents an experimental study investigating the impact of dancing IVAs on human emotions. The results showed that watching a dancing IVA depicting different emotions significantly influenced human emotions. Additionally, the participant's anger, sadness and happiness were significantly dependent on which dancing character's emotion they watched. Moreover, the results of the study showed that while most of the participants were able to recognize the emotions depicted by the dancing IVA, correct recognition of the dancing IVAs' emotions was not necessary to have the intended influence on human emotions. These results suggest that some of the benefits associated with dancing, such as dance therapy, could be achieved just by watching an IVA.","[{'authorId': '144037536', 'name': 'Deborah Richards'}, {'authorId': '2768530', 'name': 'Jon Cedric Roxas'}, {'authorId': '36367022', 'name': 'A. Bilgin'}, {'authorId': '50561368', 'name': 'Nader Hanna'}]",1.0,"{'bibtex': '@Article{Richards2015ADV,\n author = {Deborah Richards and Jon Cedric Roxas and A. Bilgin and Nader Hanna},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1701-1702},\n title = {A Dancing Virtual Agent to Evoke Human Emotions},\n year = {2015}\n}\n'}","[{'paperId': 'bafba8def014ce5aadacf466ac68832c1e87a29d', 'title': 'An overview of self-adaptive technologies within virtual reality training'}]",{'pages': '1701-1702'},9.0,A Dancing Virtual Agent to Evoke Human Emotions,2015.0
2783,f656c2ab2b8f0a280daf439c1b6ce1cb1f545ac8,,"[{'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1757117', 'name': 'Timo Sowa'}, {'authorId': '1721018', 'name': 'I. Wachsmuth'}]",48.0,"{'bibtex': '@Inproceedings{Kopp2003ImitationGW,\n author = {S. Kopp and Timo Sowa and I. Wachsmuth},\n pages = {436-447},\n title = {Imitation Games with an Artificial Agent: From Mimicking to Understanding Shape-Related Iconic Gestures},\n year = {2003}\n}\n'}",,{'pages': '436-447'},23.0,Imitation Games with an Artificial Agent: From Mimicking to Understanding Shape-Related Iconic Gestures,2003.0
2784,f663f5eb76d4e1860d1ef6be9fbae1121ec67423,"Speech act classification determining the communicative intent of an utterance has been studied widely over the years as an independent task. This holds true for discussion in any for a, including social media platforms such as Twitter. However, the tweeter’s emotional state has a huge impact on its pragmatic content because communication is fundamentally characterized and mediated by direct emotions. Sentiment as a human behavior often has a strong relation to emotion, and one helps to understand the other better. We hypothesize that the association between emotion and sentiment will provide a clearer understanding of the tweeter’s state of mind, aiding the identification of tweet acts (speech acts in Twitter, TAs). As the first step, we create a new multimodal, emotion-TA, EmoTA dataset collected from the open-source Twitter dataset. To incorporate these multiple aspects, we propose a multitask ensemble adversarial learning framework for multimodal TA classification (TAC). In addition, we also incorporate a joint embedding network, with bidirectional constraints to capture and efficiently integrate the shared semantic relationships across modalities and learn generalized features across multiple tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, TAC, by benefiting from the two secondary tasks, i.e., sentiment and emotion analyses compared to its unimodal and single-task TAC variants.","[{'authorId': '52219377', 'name': 'Tulika Saha'}, {'authorId': '2000036680', 'name': 'Apoorva Upadhyaya'}, {'authorId': '145470045', 'name': 'S. Saha'}, {'authorId': '145532184', 'name': 'P. Bhattacharyya'}]",18.0,"{'bibtex': '@Article{Saha2022AMM,\n author = {Tulika Saha and Apoorva Upadhyaya and S. Saha and P. Bhattacharyya},\n journal = {IEEE Transactions on Computational Social Systems},\n pages = {508-517},\n title = {A Multitask Multimodal Ensemble Model for Sentiment- and Emotion-Aided Tweet Act Classification},\n volume = {9},\n year = {2022}\n}\n'}",,"{'volume': '9', 'pages': '508-517', 'name': 'IEEE Transactions on Computational Social Systems'}",0.0,A Multitask Multimodal Ensemble Model for Sentiment- and Emotion-Aided Tweet Act Classification,2022.0
2785,f669a3fe78d0405899dc572e7622fc020b71b317,"This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part of a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML focuses on an extensible set of performatives, which defines the permissible “speech acts” agents may use and comprise a substrate on which to develop higher-level models of interagent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitors which coordinate the interactions of other agents. The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling.","[{'authorId': '144121212', 'name': 'Timothy W. Finin'}, {'authorId': '1910723', 'name': 'R. Fritzson'}, {'authorId': '39552279', 'name': 'D. McKay'}, {'authorId': '144568707', 'name': 'R. McEntire'}]",2599.0,"{'bibtex': '@Inproceedings{Finin1994KQMLAA,\n author = {Timothy W. Finin and R. Fritzson and D. McKay and R. McEntire},\n pages = {456-463},\n title = {KQML as an agent communication language},\n year = {1994}\n}\n'}",,{'pages': '456-463'},37.0,KQML as an agent communication language,1994.0
2786,f66ca043c9fffccd3e1ea82687d04aa868ae51f1,,"[{'authorId': '3822686', 'name': 'Lauren M. Bylsma'}, {'authorId': '6953013', 'name': 'B. Morris'}, {'authorId': '6141636', 'name': 'J. Rottenberg'}]",768.0,"{'bibtex': '@Article{Bylsma2008AMO,\n author = {Lauren M. Bylsma and B. Morris and J. Rottenberg},\n journal = {Clinical psychology review},\n pages = {\n          676-91\n        },\n title = {A meta-analysis of emotional reactivity in major depressive disorder.},\n volume = {28 4},\n year = {2008}\n}\n'}",,"{'volume': '28 4', 'pages': '\n          676-91\n        ', 'name': 'Clinical psychology review'}",122.0,A meta-analysis of emotional reactivity in major depressive disorder.,2008.0
2787,f6957dfccf90254e970fa6da834649457fd1cd79,"Figure 1: The affect model used by the agent Through simulations, we tested these models for internal consistency and were successful in establishing the relationships among the factors as suggested by the earlier user studies [3]. We confronted our agent system with real users to check whether users recognize that our agents function in similar ways as humans do. Through a structured questionnaire, users informed us that they recognized that our agents evaluated the user's aesthetics and moral stance while building up a level of involvement with the user and a degree of willingness to interact with the user again [4]. In future research, we plan to focus on more factors, such as affordances and realism, compare the performance of several emotion models with each other and a Wizard of Oz condition of human-human interaction, which allows for making stronger claims to the behavioral fidelity of an agent’s affective response mechanisms.","[{'authorId': '118596324', 'name': 'Pontier'}, {'authorId': '20751226', 'name': 'M. Otte'}, {'authorId': '71825175', 'name': 'J. Hoorn'}, {'authorId': '2060285239', 'name': 'Vũ'}, {'authorId': '51967394', 'name': 'F. Wetenschappen'}]",0.0,"{'bibtex': '@Inproceedings{Pontier2010AnAV,\n author = {Pontier and M. Otte and J. Hoorn and Vũ and F. Wetenschappen},\n title = {An Affective Virtual Agent for Natural Human-Agent Interaction},\n year = {2010}\n}\n'}",[],"{'name': '', 'volume': ''}",4.0,An Affective Virtual Agent for Natural Human-Agent Interaction,2010.0
2788,f69ed437c21c30ba7992b0f95380bddace1d4e4a,"Providing social robots an internal model of emotions can help them guide their behaviour in a more humane manner by simulating the ability to feel empathy towards others. Furthermore, the growing interest in creating robots that are capable of collaborating with other humans in team settings provides an opportunity to explore another side of human emotion, namely, group-based emotions. This paper contributes with the first model on group-based emotions in social robotic partners. We defined a model of group-based emotions for social robots that allowed us to create two distinct robotic characters that express either individual or group-based emotions. This paper also contributes with a user study where two autonomous robots embedded the previous characters, and formed two human-robot teams to play a competitive game. Our results showed that participants perceived the robot that expresses group-based emotions as more likeable and attributed higher levels of group identification and group trust towards their teams, when compared to the robotic partner that expresses individual-based emotions.","[{'authorId': '144106225', 'name': 'Filipa Correia'}, {'authorId': '145689493', 'name': 'S. Mascarenhas'}, {'authorId': '143825592', 'name': 'R. Prada'}, {'authorId': '145125979', 'name': 'Francisco S. Melo'}, {'authorId': '145136631', 'name': 'Ana Paiva'}]",65.0,"{'bibtex': '@Article{Correia2018GroupbasedEI,\n author = {Filipa Correia and S. Mascarenhas and R. Prada and Francisco S. Melo and Ana Paiva},\n journal = {2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {261-269},\n title = {Group-based Emotions in Teams of Humans and Robots},\n year = {2018}\n}\n'}",,"{'pages': '261-269', 'name': '2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",50.0,Group-based Emotions in Teams of Humans and Robots,2018.0
2789,f6c6ae791bf0696a4053e171fd27918e8fc17ab3,"We present a real-time crowd model based on continuum dynamics. In our model, a dynamic potential field simultaneously integrates global navigation with moving obstacles such as other people, efficiently solving for the motion of large crowds without the need for explicit collision avoidance. Simulations created with our system run at interactive rates, demonstrate smooth flow under a variety of conditions, and naturally exhibit emergent phenomena that have been observed in real crowds.","[{'authorId': '3064395', 'name': 'Adrien Treuille'}, {'authorId': '145442343', 'name': 'Seth Cooper'}, {'authorId': '1986848', 'name': 'Zoran Popovic'}]",947.0,"{'bibtex': '@Article{Treuille2006ContinuumC,\n author = {Adrien Treuille and Seth Cooper and Zoran Popovic},\n journal = {ACM SIGGRAPH 2006 Papers},\n title = {Continuum crowds},\n year = {2006}\n}\n'}",,{'name': 'ACM SIGGRAPH 2006 Papers'},40.0,Continuum crowds,2006.0
2791,f6cfe53eb436df75fa2883c9cf9130621fbb28e0,"Computational models of emotion are useful in a variety of domains, including games, virtual realty training and HCI to name a few. Many of these models are inspired by appraisal theory. Most appraisal theories share with virtual agents the assumption that beliefs, desires and intentions are the basis of reasoning and thus of the emotional evaluation of the agent's situation. Consequently most computational models of emotion are deeply embedded into the agent model. In this paper we address the problem of how to emotionally instrument a system in a modular and extensible way, so that emotional sophistication can be added incrementally to a system. We propose a solution based on a modular, signalbased approach to computational emotions that allows us to develop scalable appraisal models that are easily added to non-emotional systems. Our approach allows runtime tradeoff between emotional quality and performance, which makes it particularly useful in domains in which available computation time is unknown, like the gaming domain. We present experimental results that back-up our approach.","[{'authorId': '1735303', 'name': 'J. Broekens'}]",35.0,"{'bibtex': '@Inproceedings{Broekens2005SCALABLEAF,\n author = {J. Broekens},\n title = {SCALABLE AND FLEXIBLE APPRAISAL MODELS FOR VIRTUAL AGENTS},\n year = {2005}\n}\n'}",,"{'volume': '', 'name': ''}",13.0,SCALABLE AND FLEXIBLE APPRAISAL MODELS FOR VIRTUAL AGENTS,2005.0
2792,f6ed2e59d7dbd451e12e609344a1f0a35879eeb9,"We describe a Parameterized Action Representation (PAR) designed to bridge the gap between natural language instructions and the virtual agents who are to carry them out. The PAR is therefore constructed based jointly on implemented motion capabilities of virtual human figures and linguistic requirements for instruction interpretation. We will illustrate PAR and a real-time execution architecture controlling 3D animated virtual human avatars. Comments Postprint version. Published in American Association for Artificial Intelligence, Spring Synposium, 2000. Author(s) Norman I. Badler, Ramamani Bindiganavale, Juliet C. Bourne, Martha Palmer, Jianping Shi, and William Schuler This journal article is available at ScholarlyCommons: http://repository.upenn.edu/hms/26 A Parameterized Action Representation for Virtual Human Agents Norman Badler, Rama Bindiganavale, Juliet Bourne Martha Palmer, Jianping Shi, William Schuler Center for Human Modeling and Simulation Computer and Information Science Department University of Pennsylvania Philadelphia, PA 19104-6389","[{'authorId': '1699200', 'name': 'N. Badler'}, {'authorId': '1936110', 'name': 'R. Bindiganavale'}, {'authorId': '1855748', 'name': 'J. Allbeck'}, {'authorId': '1747648', 'name': 'William Schuler'}, {'authorId': '2427954', 'name': 'Liwei Zhao'}, {'authorId': '145755155', 'name': 'Martha Palmer'}]",187.0,"{'bibtex': '@Inproceedings{Badler2001APA,\n author = {N. Badler and R. Bindiganavale and J. Allbeck and William Schuler and Liwei Zhao and Martha Palmer},\n title = {A Parameterized Action Representation for Virtual Human Agents},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",16.0,A Parameterized Action Representation for Virtual Human Agents,2001.0
2793,f7034bcd3f6addd7b2801f289cd17c6acd3e1835,"Descartes' Error offers the scientific basis for ending the division between mind and body. Antonio Damasio contends that rational decisions are not the product of logic alone - they require the support of emotion and feeling. Drawing on his experience with neurological patients affected with brain damage, Dr Damasio shows how absence of emotions and feelings can break down rationality. He also offers a new perspective on what emotions and feelings actually are: a direct view of our own body states; a link between the body and its survival-oriented regulation on the one hand, and consciousness on the other. Written as a conversation between the author and an imaginary listener, Descartes' Error leads us to conclude that human organisms are endowed from their very beginning with a spirited passion for making choices, which the social mind can then use to build rational behaviour.","[{'authorId': '2656777', 'name': 'A. Damasio'}]",5530.0,"{'bibtex': ""@Inproceedings{Damasio1994DescartesEE,\n author = {A. Damasio},\n title = {Descartes' error: emotion, reason, and the human brain. avon books},\n year = {1994}\n}\n""}",,"{'volume': '', 'name': ''}",0.0,"Descartes' error: emotion, reason, and the human brain. avon books",1994.0
2795,f7113e72260f51d29a2d4269a97bdb8ffcc63fc6,"Harald T. Schupp,1 Jessica Stockburger,1 Maurizio Codispoti,2 Markus Junghöfer,3 Almut I. Weike,4 and Alfons O. Hamm4 1Department of Psychology, University of Konstanz, 78457 Konstanz, Germany, 2Department of Psychology, University of Bologna, 40127 Bologna, Italy, 3Institute for Biomagnetism and Biosignalanalysis, University of Münster, 48149 Münster, Germany, and 4Department of Psychology, University of Greifswald, 17487 Greifswald, Germany","[{'authorId': '2436479', 'name': 'H. Schupp'}, {'authorId': '2656869', 'name': 'Jessica Stockburger'}, {'authorId': '3355595', 'name': 'M. Codispoti'}, {'authorId': '3258242', 'name': 'M. Junghöfer'}, {'authorId': '4377282', 'name': 'A. Weike'}, {'authorId': '2337585', 'name': 'A. Hamm'}]",497.0,"{'bibtex': '@Inproceedings{Schupp2007BehavioralS,\n author = {H. Schupp and Jessica Stockburger and M. Codispoti and M. Junghöfer and A. Weike and A. Hamm},\n title = {Behavioral / Systems / Cognitive Selective Visual Attention to Emotion},\n year = {2007}\n}\n'}",,,50.0,Behavioral / Systems / Cognitive Selective Visual Attention to Emotion,2007.0
2796,f731b6745d829241941307c3ebf163e90e200318,"!, Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply modelbased methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images. Q 199s A&& prrss, IN.","[{'authorId': '7205190', 'name': 'Tim Cootes'}, {'authorId': '144482985', 'name': 'C. Taylor'}, {'authorId': '32250556', 'name': 'D. Cooper'}, {'authorId': '47581828', 'name': 'J. Graham'}]",8148.0,"{'bibtex': '@Article{Cootes1995ActiveSM,\n author = {Tim Cootes and C. Taylor and D. Cooper and J. Graham},\n journal = {Comput. Vis. Image Underst.},\n pages = {38-59},\n title = {Active Shape Models-Their Training and Application},\n volume = {61},\n year = {1995}\n}\n'}",,"{'volume': '61', 'pages': '38-59', 'name': 'Comput. Vis. Image Underst.'}",19.0,Active Shape Models-Their Training and Application,1995.0
2797,f74ecf9935e26007daaacf971fa95273701df42b,,"[{'authorId': '2054281', 'name': 'J. Grolleman'}, {'authorId': '1727902', 'name': 'B. V. Dijk'}, {'authorId': '144483472', 'name': 'A. Nijholt'}, {'authorId': '118628265', 'name': 'A. V. Emst'}]",74.0,"{'bibtex': '@Inproceedings{Grolleman2006BreakTH,\n author = {J. Grolleman and B. V. Dijk and A. Nijholt and A. V. Emst},\n pages = {133-141},\n title = {Break the Habit! Designing an e-Therapy Intervention Using a Virtual Coach in Aid of Smoking Cessation},\n year = {2006}\n}\n'}",,{'pages': '133-141'},20.0,Break the Habit! Designing an e-Therapy Intervention Using a Virtual Coach in Aid of Smoking Cessation,2006.0
2798,f77ec14eb931a4629bd3bc671d58ed4d5c4759fb,"Previous work suggests that a range of mental states can be read from facial expressions, beyond the “basic emotions”. Experiment 1 tested this in more detail, by using a standardized method, and by testing the role of face parts (eyes vs. mouth vs. the whole face). Adult subjects were shown photographs of an actress posing 10 basic emotions (happy, sad, angry, afraid, etc.) and 10 complex mental states (scheme, admire, interest, thoughtfulness, etc.). For each mental state, each subject was shown the whole face, the eyes alone, or the mouth alone, and were given a forced choice of two mental state terms. Results indicated that: (1) Subjects show remarkable agreement in ascribing a wide range of mental states to facial expressions, (2) for the basic emotions, the whole face is more informative than either the eyes or the mouth, (3) for the complex mental states, seeing the eyes alone produced significantly better performance than seeing the mouth alone, and was as informative as the whole face. In Experim...","[{'authorId': '1390019127', 'name': 'S. Baron-Cohen'}, {'authorId': '3159706', 'name': 'S. Wheelwright'}, {'authorId': '5646383', 'name': 'T. Jolliffe'}]",954.0,"{'bibtex': '@Article{Baron-Cohen1997IsTA,\n author = {S. Baron-Cohen and S. Wheelwright and T. Jolliffe},\n journal = {Visual Cognition},\n pages = {311-331},\n title = {Is There a ""Language of the Eyes""? Evidence from Normal Adults, and Adults with Autism or Asperger Syndrome},\n volume = {4},\n year = {1997}\n}\n'}",,"{'volume': '4', 'pages': '311-331', 'name': 'Visual Cognition'}",55.0,"Is There a ""Language of the Eyes""? Evidence from Normal Adults, and Adults with Autism or Asperger Syndrome",1997.0
2799,f79e849a6cd26f338400ea9501f57d96c972c3b2,"yet it is a compassionate and sensitive work that abounds in current useful information. It projects a positive and realistic approach to issues confronting disabled people and their families. The lives and aspirations of the disabled are discussed in an easily readable style, which includes such topics as: labeling, accessibility, institutionalization, relationships with the medical field, and representation in the media and technology. Special People is not written for the professional who requires extensive information on a specific topic or exceptionality. Rather, it is a short book (172 pages) that brings together knowledge and thought from several disciplines to deal with the everyday problems of coping. Doctors, clergymen, parents, older students, relatives, and the general reader will find Special People useful and informative. One of the strongest aspects of this book is the author's ability to relate her personal experiences and feelings. She is sensitive to the adjustment problems of the nonhandicapped and helps them to understand their own reactions by sharing hers. For example, after realizing that a particular child had Downs' Syndrome, Ms. Cohen relates, ""The shock stayed with me all afternoon. Even though I had known that this was a program for retarded infants, I hadn't been prepared. I was still plagued by my stereotypes"" (p. 117). There are pertinent references throughout the text to professional articles, books for adults, and some for children that expound upon the topics discussed. Regular classroom teachers may be interested in reading, ""Don't Feel Sorry for Paul,"" which describes the life of a physically handicapped child. Parents may chose to read, ""About Handicaps,"" to children who are afraid of ""catching handicap germs."" There are also several accounts of innovative programs, such as Sweden's consideration of blind, physically, and mentally handicapped persons when traffic patterns were changed. This was done because ""they are passengers ... they are pedestrians ... they are part of the world around the driver .... "" There are several excerpts from autobiographies of disabled people, transcripts of conversations with parents, and reactions of children. Helen Jones describes inconsistencies in the airlines' willingness to accept disabled passengers: her statement is especially piognant for those who organize service companies for the public. The parent of a physically handicapped child expresses appreciation for a sequence on the ""Mr. Rogers"" television show, which included disabled children. Shirley Cohen does not leave her reader uninvolved. One chapter, entitled ""And Now You,"" presents situations for selfinquiry as well as for discussion. While the general reader and the disabled themselves are asked to think about their own behaviors and feelings, the professional team member can re-examine his own beliefs regarding the equality and worthiness of the people he serves. This chapter is quite short and could be expanded. One possible criticism is Ms. Cohen's introduction as it later relates to her text on public education. Although the ""Right to Education"" is discussed in accordance","[{'authorId': '2124146851', 'name': 'Shelley Masion Rosenberg'}]",566.0,"{'bibtex': '@Article{Rosenberg1978BodilyC,\n author = {Shelley Masion Rosenberg},\n journal = {Journal of Visual Impairment & Blindness},\n pages = {111 - 112},\n title = {Bodily Communication},\n volume = {72},\n year = {1978}\n}\n'}",,"{'volume': '72', 'pages': '111 - 112', 'name': 'Journal of Visual Impairment & Blindness'}",0.0,Bodily Communication,1978.0
2801,f7fa89c90d8539ab4e5b3ed8b6fd9df58af123b2,,"[{'authorId': '1750852', 'name': 'N. Krämer'}, {'authorId': '5864138', 'name': 'S. Kopp'}, {'authorId': '1403827243', 'name': 'C. Becker-Asano'}, {'authorId': '2071068301', 'name': 'Nicole Sommer'}]",86.0,"{'bibtex': ""@Article{Krämer2013SmileAT,\n author = {N. Krämer and S. Kopp and C. Becker-Asano and Nicole Sommer},\n journal = {Int. J. Hum. Comput. Stud.},\n pages = {335-349},\n title = {Smile and the world will smile with you - The effects of a virtual agent's smile on users' evaluation and behavior},\n volume = {71},\n year = {2013}\n}\n""}",,"{'volume': '71', 'pages': '335-349', 'name': 'Int. J. Hum. Comput. Stud.'}",100.0,Smile and the world will smile with you - The effects of a virtual agent's smile on users' evaluation and behavior,2013.0
2802,f7fdc55ff89bf9173bd55676fdbf7a3dab5537e9,,"[{'authorId': '2104907270', 'name': 'Rie Tamagawa'}, {'authorId': '3087841', 'name': 'C. Watson'}, {'authorId': '144797761', 'name': 'I. Kuo'}, {'authorId': '145285619', 'name': 'B. MacDonald'}, {'authorId': '145095677', 'name': 'E. Broadbent'}]",83.0,"{'bibtex': '@Article{Tamagawa2011TheEO,\n author = {Rie Tamagawa and C. Watson and I. Kuo and B. MacDonald and E. Broadbent},\n journal = {International Journal of Social Robotics},\n pages = {253-262},\n title = {The Effects of Synthesized Voice Accents on User Perceptions of Robots},\n volume = {3},\n year = {2011}\n}\n'}",,"{'volume': '3', 'pages': '253-262', 'name': 'International Journal of Social Robotics'}",37.0,The Effects of Synthesized Voice Accents on User Perceptions of Robots,2011.0
2803,f8027359391974074c9a4c00d486733766abcb53,"Depression is considered a serious medical condition and a large number of people around the world are suffering from it. Within this context, a lot of studies have been proposed to estimate the degree of depression based on different features and modalities, specific to depression. Supported by medical studies that show how depression is a disorder of impaired emotion regulation, we propose a different approach, which relies on the rationale that the estimation of depression level can benefit from the concurrent learning of emotion intensity. To test this hypothesis, we design different attention-based multi-task architectures that concurrently regress/classify both depression level and emotion intensity using text data. Experiments based on two benchmark datasets, namely, the Distress Analysis Interview Corpus - a Wizard of Oz (DAIC-WOZ), and the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) show that substantial performance improvements can be achieved when compared to emotion-unaware single-task and multitask approaches.","[{'authorId': '102701933', 'name': 'Syed Arbaaz Qureshi'}, {'authorId': '143673279', 'name': 'G. Dias'}, {'authorId': '144231505', 'name': 'Mohammed Hasanuzzaman'}, {'authorId': '145470045', 'name': 'S. Saha'}]",23.0,"{'bibtex': '@Article{Qureshi2020ImprovingDL,\n author = {Syed Arbaaz Qureshi and G. Dias and Mohammed Hasanuzzaman and S. Saha},\n journal = {IEEE Computational Intelligence Magazine},\n pages = {47-59},\n title = {Improving Depression Level Estimation by Concurrently Learning Emotion Intensity},\n volume = {15},\n year = {2020}\n}\n'}",,"{'volume': '15', 'pages': '47-59', 'name': 'IEEE Computational Intelligence Magazine'}",41.0,Improving Depression Level Estimation by Concurrently Learning Emotion Intensity,2020.0
2804,f82f152244b1cb861db0f290d55302011aee28dc,"In recent studies of the structure of affect, positive and negative affect have consistently emerged as two dominant and relatively independent dimensions. A number of mood scales have been created to measure these factors; however, many existing measures are inadequate, showing low reliability or poor convergent or discriminant validity. To fill the need for reliable and valid Positive Affect and Negative Affect scales that are also brief and easy to administer, we developed two 10-item mood scales that comprise the Positive and Negative Affect Schedule (PANAS). The scales are shown to be highly internally consistent, largely uncorrelated, and stable at appropriate levels over a 2-month time period. Normative data and factorial and external evidence of convergent and discriminant validity for the scales are also presented.","[{'authorId': '145213999', 'name': 'D. Watson'}, {'authorId': '10034636', 'name': 'L. Clark'}, {'authorId': '116114697', 'name': 'A. Tellegen'}]",35827.0,"{'bibtex': '@Article{Watson1988DevelopmentAV,\n author = {D. Watson and L. Clark and A. Tellegen},\n journal = {Journal of personality and social psychology},\n pages = {\n          1063-70\n        },\n title = {Development and validation of brief measures of positive and negative affect: the PANAS scales.},\n volume = {54 6},\n year = {1988}\n}\n'}",,"{'volume': '54 6', 'pages': '\n          1063-70\n        ', 'name': 'Journal of personality and social psychology'}",50.0,Development and validation of brief measures of positive and negative affect: the PANAS scales.,1988.0
2807,f8631d98f7b4bfc2ce5763bf57a4ddc2e73cfbd1,,"[{'authorId': '1761178', 'name': 'I. Pandzic'}, {'authorId': '1767736', 'name': 'R. Forchheimer'}]",155.0,"{'bibtex': '@Inproceedings{Pandzic2002MPEG4FA,\n author = {I. Pandzic and R. Forchheimer},\n title = {MPEG-4 Facial Animation},\n year = {2002}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,MPEG-4 Facial Animation,2002.0
2808,f874e953aadcaa109e646f82acdf6157745daa78,,"[{'authorId': '2049038716', 'name': 'Yanjun Yin'}, {'authorId': '3224101', 'name': 'Weiqing Tang'}, {'authorId': '2108716395', 'name': 'Weiqing Li'}]",5.0,"{'bibtex': '@Inproceedings{Yin2012ModelingGE,\n author = {Yanjun Yin and Weiqing Tang and Weiqing Li},\n pages = {240-247},\n title = {Modeling Group Emotion Based on Emotional Contagion},\n year = {2012}\n}\n'}",,{'pages': '240-247'},19.0,Modeling Group Emotion Based on Emotional Contagion,2012.0
2809,f8badb03517d812176a7ba982fb81a0aa05edd54,,"[{'authorId': '2095517457', 'name': 'J. Velásquez'}, {'authorId': '1701876', 'name': 'P. Maes'}]",85.0,"{'bibtex': '@Inproceedings{Velásquez1997CathexisAC,\n author = {J. Velásquez and P. Maes},\n pages = {518-519},\n title = {Cathexis: a computational model of emotions},\n year = {1997}\n}\n'}",,{'pages': '518-519'},9.0,Cathexis: a computational model of emotions,1997.0
2811,f8c0897b5fc071d8ad2cdbf5469f47e5edc604eb,"First, third, and fifth grade children's recall of emotional behaviours, emotional labels, and nonemotional behaviours in text was examined in two experiments. Across experiments, all children recalled more emotional behaviours (e.g., “That night Maria dropped a carton of eggs in the kitchen and her parents got mad at her”) than nonemotional behaviours (e.g., “After dinner, Maria and her brothers did their homework together”). In fact, with short stories and few items to-be-remembered (Experiment 1), no significant differences were found in first, third, and fifth grade children's recall of emotional behaviours. In contrast, older children recalled more nonemotional behaviours than younger children. With longer and more complex stories (Experiment 2), older children recalled more emotional behaviours than younger children. Nevertheless, all children recalled more emotional behaviours than nonemotional behaviours across experiments. The effects of varying the valence of the emotion, the labelling of emotion, and the length of retention interval on memory were also examined. The results are discussed in terms of a recent model of emotion and in terms of the implications for understanding the development of memory.","[{'authorId': '144251558', 'name': 'D. Davidson'}, {'authorId': '7865188', 'name': 'Zupei Luo'}, {'authorId': '4028811', 'name': 'M. J. Burden'}]",59.0,"{'bibtex': ""@Article{Davidson2001ChildrensRO,\n author = {D. Davidson and Zupei Luo and M. J. Burden},\n journal = {Cognition and Emotion},\n pages = {1 - 26},\n title = {Children's recall of emotional behaviours, emotional labels, and nonemotional behaviours: Does emotion enhance memory?},\n volume = {15},\n year = {2001}\n}\n""}",,"{'volume': '15', 'pages': '1 - 26', 'name': 'Cognition and Emotion'}",43.0,"Children's recall of emotional behaviours, emotional labels, and nonemotional behaviours: Does emotion enhance memory?",2001.0
2812,f8c61e9308eec8125cf57735d36b1c8a0133ba3f,The bachelor thesis’ aim was to develop a framework allowing to design and conduct virtual-reality-based user studies gaining insight into the concept of personal space.,"[{'authorId': '66298470', 'name': 'Jan Schnathmeier'}, {'authorId': '2782974', 'name': 'U. Habel'}, {'authorId': '144483066', 'name': 'T. Kuhlen'}, {'authorId': '3249697', 'name': 'A. Bönsch'}, {'authorId': '145880611', 'name': 'Sina Radke'}, {'authorId': '47973447', 'name': 'H. Overath'}]",1.0,"{'bibtex': '@Inproceedings{Schnathmeier2017DoNI,\n author = {Jan Schnathmeier and U. Habel and T. Kuhlen and A. Bönsch and Sina Radke and H. Overath},\n title = {Do Not Invade: A Virtual-Reality-Framework to Study Personal Space},\n year = {2017}\n}\n'}",,"{'volume': '', 'name': ''}",4.0,Do Not Invade: A Virtual-Reality-Framework to Study Personal Space,2017.0
2813,f8e1fc3f05345bf8eb0e2b7bc88bfaa3d725b912,"In this paper we explore a new direction for pedagogical computer characters, which we believe will maximize students' learning gains and enjoyment. To the traditional scenario where students interact primarily with a single coach or tutor character on-screen, we introduce the addition of both a social, animate colearner, and the student's own avatar character. Variations of the colearner's attributes, informed by research literature on human partners, are explored through an online testbed application of English language idioms. Results from an experimental study with 76 Japanese college students reveal that cooperative colearners have a positive impact on students' performance and experience, as well as increasing perceptions of the character's intelligence and credibility. Findings provide grounding for a fruitful new direction for pedagogical characters, where students learn alongside emotional companions.","[{'authorId': '145904305', 'name': 'Heidy Maldonado'}, {'authorId': '2108367283', 'name': 'Jong-Eun Roselyn Lee'}, {'authorId': '2739604', 'name': 'Scott Brave'}, {'authorId': '2029850', 'name': 'C. Nass'}, {'authorId': '16845205', 'name': 'H. Nakajima'}, {'authorId': '2071316348', 'name': 'Ryota Yamada'}, {'authorId': '10605736', 'name': 'Kimihiko Iwamura'}, {'authorId': '2402842', 'name': 'Yasunori Morishima'}]",98.0,"{'bibtex': '@Inproceedings{Maldonado2005WeLB,\n author = {Heidy Maldonado and Jong-Eun Roselyn Lee and Scott Brave and C. Nass and H. Nakajima and Ryota Yamada and Kimihiko Iwamura and Yasunori Morishima},\n pages = {408-417},\n title = {We learn better together: enhancing eLearning with emotional characters},\n year = {2005}\n}\n'}",,{'pages': '408-417'},38.0,We learn better together: enhancing eLearning with emotional characters,2005.0
2814,f8e8b3fd2b8eb91eec9a5507671a5dffe295bf59,,"[{'authorId': '34987625', 'name': 'Samantha Chan'}, {'authorId': '3834933', 'name': 'J. Rawana'}]",9.0,"{'bibtex': '@Article{Chan2021ExaminingTA,\n author = {Samantha Chan and J. Rawana},\n journal = {Cognitive Therapy and Research},\n pages = {652 - 662},\n title = {Examining the associations between interpersonal emotion regulation and psychosocial adjustment in emerging adulthood},\n volume = {45},\n year = {2021}\n}\n'}",,"{'volume': '45', 'pages': '652 - 662', 'name': 'Cognitive Therapy and Research'}",67.0,Examining the associations between interpersonal emotion regulation and psychosocial adjustment in emerging adulthood,2021.0
2815,f8f9e47deaf537fa8532664906049fc7afe8d5ad,"Mood contagion is an automatic mechanism that induces a congruent mood state by means of the observation of another person's emotional expression. In this paper, we address the question whether robot mood displayed during an imitation game can (a) be recognized by participants and (b) produce contagion effects. Robot mood was displayed by applying a generic framework for mood expression using body language. By modulating the set of available behavior parameters in this framework for controlling pose and motion dynamics, the gestures performed by the humanoid robot NAO were adjusted to display either a positive or negative mood. In the study performed, we varied both mood as well as task difficulty. Our results show that participants are able to differentiate between positive and negative robot mood. Moreover, self-reported mood matches the mood of the robot in the easy task condition. Additional evidence for mood contagion is provided by the fact that we were able to replicate an expected effect of negative mood on task performance: in the negative mood condition participants performed better on difficult tasks than in the positive mood condition, even though participants' self-reported mood did not match that of the robot.","[{'authorId': '34859885', 'name': 'Junchao Xu'}, {'authorId': '1735303', 'name': 'J. Broekens'}, {'authorId': '1751831', 'name': 'K. Hindriks'}, {'authorId': '1784286', 'name': 'Mark Antonius Neerincx'}]",48.0,"{'bibtex': '@Inproceedings{Xu2014RobotMI,\n author = {Junchao Xu and J. Broekens and K. Hindriks and Mark Antonius Neerincx},\n pages = {973-980},\n title = {Robot mood is contagious: effects of robot body language in the imitation game},\n year = {2014}\n}\n'}",,{'pages': '973-980'},41.0,Robot mood is contagious: effects of robot body language in the imitation game,2014.0
2817,f913f2b4856289360a1e4a15911dae69afea5549,"The nascent field of neuroeconomics seeks to ground economic decisionmaking in the biological substrate of the brain. We used functional magnetic resonance imaging of Ultimatum Game players to investigate neural substrates of cognitive and emotional processes involved in economic decision-making. In this game, two players split a sum of money;one player proposes a division and the other can accept or reject this. We scanned players as they responded to fair and unfair proposals. Unfair offers elicited activity in brain areas related to both emotion (anterior insula) and cognition (dorsolateral prefrontal cortex). Further, significantly heightened activity in anterior insula for rejected unfair offers suggests an important role for emotions in decision-making.","[{'authorId': '2080922', 'name': 'A. Sanfey'}, {'authorId': '1830680', 'name': 'J. Rilling'}, {'authorId': '27458355', 'name': 'J. Aronson'}, {'authorId': '2100648', 'name': 'L. Nystrom'}, {'authorId': '153564781', 'name': 'J. Cohen'}]",3182.0,"{'bibtex': '@Article{Sanfey2003TheNB,\n author = {A. Sanfey and J. Rilling and J. Aronson and L. Nystrom and J. Cohen},\n journal = {Science},\n pages = {1755 - 1758},\n title = {The Neural Basis of Economic Decision-Making in the Ultimatum Game},\n volume = {300},\n year = {2003}\n}\n'}",,"{'volume': '300', 'pages': '1755 - 1758', 'name': 'Science'}",58.0,The Neural Basis of Economic Decision-Making in the Ultimatum Game,2003.0
2818,f9182ba1d9046aea8abb4571eef3eb89e5702fd5,"After returning from an extended combat deployment to Iraq, 348 National Guard soldiers were administered the PTSD Checklist (PCL-M), and the Beck Depression Inventory II (BDI-II) followed, on average, 3 months later by structured diagnostic interviews including the Clinician-Administered PTSD Scale (CAPS) for the Diagnostic and Statistical Manual of Mental Disorders (4th ed.). There were 6.5% of the soldiers who met diagnostic criteria for posttraumatic stress disorder (PTSD) based on structured interview. The predictive validity of the PCL was examined and contrasted with the predictive validity of the BDI-II in identifying soldiers meeting CAPS diagnosis for PTSD. The best identified PCL cut scores produced between 65% and 76% false positive errors when used as the sole source for identification of enduring PTSD. Comparison of prediction between the PCL and the BDI-II in identifying PTSD suggested that both instruments may be operating through tapping generalized distress rather than specific aspects of the disorder.","[{'authorId': '3979253', 'name': 'P. Arbisi'}, {'authorId': '5187410', 'name': 'M. Kaler'}, {'authorId': '1398898634', 'name': 'S. Kehle-Forbes'}, {'authorId': '5092239', 'name': 'Christopher R. Erbes'}, {'authorId': '5159096', 'name': 'Melissa A. Polusny'}, {'authorId': '3725454', 'name': 'P. Thuras'}]",63.0,"{'bibtex': '@Article{Arbisi2012ThePV,\n author = {P. Arbisi and M. Kaler and S. Kehle-Forbes and Christopher R. Erbes and Melissa A. Polusny and P. Thuras},\n journal = {Psychological assessment},\n pages = {\n          1034-40\n        },\n title = {The predictive validity of the PTSD Checklist in a nonclinical sample of combat-exposed National Guard troops.},\n volume = {24 4},\n year = {2012}\n}\n'}",,"{'volume': '24 4', 'pages': '\n          1034-40\n        ', 'name': 'Psychological assessment'}",0.0,The predictive validity of the PTSD Checklist in a nonclinical sample of combat-exposed National Guard troops.,2012.0
2819,f9878350aec52f5d25b43889d7696b226c28cd98,,"[{'authorId': '1902889', 'name': 'T. Iachini'}, {'authorId': '3200187', 'name': 'Y. Coello'}, {'authorId': '2903600', 'name': 'F. Frassinetti'}, {'authorId': '2593233', 'name': 'V. P. Senese'}, {'authorId': '2072423', 'name': 'F. Galante'}, {'authorId': '3352149', 'name': 'G. Ruggiero'}]",176.0,"{'bibtex': '@Article{Iachini2016PeripersonalAI,\n author = {T. Iachini and Y. Coello and F. Frassinetti and V. P. Senese and F. Galante and G. Ruggiero},\n journal = {Journal of Environmental Psychology},\n pages = {154-164},\n title = {Peripersonal and interpersonal space in virtual and real environments: Effects of gender and age},\n volume = {45},\n year = {2016}\n}\n'}",,"{'volume': '45', 'pages': '154-164', 'name': 'Journal of Environmental Psychology'}",64.0,Peripersonal and interpersonal space in virtual and real environments: Effects of gender and age,2016.0
2821,f988476d2ebfbd68f1baccb009e14dd748d9d7f7,"AimsTo determine the effectiveness of 2 interventions for different aspects of emotion-processing deficits in adults with acquired brain injury (ABI). ParticipantsNineteen participants with ABI (minimum 1 year postinjury) from Western New York and Southern Ontario, Canada. Interventions(1) Emotion processing from faces (“facial affect recognition” or FAR) and (2) emotion processing from written context by using “stories of emotional inference” (SEI). Ten randomly assigned participants received the FAR intervention, and 9 received the SEI protocol. Both interventions were administered 1 hour per day, 3 times per week, and completed in 6 to 9 sessions, and both incorporated participants' personal emotional experiences into training. Outcome Measures(1) Facial affect, (2) vocal affect, (3) affect from videos, (4) emotional inference from context, and (5) emotional behavior. There were 2 pretests, a posttest, and a 2-week follow-up. ResultsFAR participants showed significantly improved emotion recognition from faces, ability to infer emotions from context, and socioemotional behavior, while the SEI group members exhibited significantly improved ability to infer how they would feel in a given context. ConclusionTraining can improve emotion perception in persons with ABI. Although further research is needed, the interventions are clinically practical and show promise for the population with ABI.","[{'authorId': '1399151230', 'name': 'Dawn Radice-Neumann'}, {'authorId': '5705546', 'name': 'Barbra Zupan'}, {'authorId': '31749194', 'name': 'M. Tomita'}, {'authorId': '8982797', 'name': 'B. Willer'}]",93.0,"{'bibtex': '@Article{Radice-Neumann2009TrainingEP,\n author = {Dawn Radice-Neumann and Barbra Zupan and M. Tomita and B. Willer},\n journal = {Journal of Head Trauma Rehabilitation},\n pages = {313–323},\n title = {Training Emotional Processing in Persons With Brain Injury},\n volume = {24},\n year = {2009}\n}\n'}",,"{'volume': '24', 'pages': '313–323', 'name': 'Journal of Head Trauma Rehabilitation'}",49.0,Training Emotional Processing in Persons With Brain Injury,2009.0
2822,f98deb71c6cb959929d51e1cf383ce1924022d8f,"Introduction to Cultural Psychology and Emotion Research, Shinobu Kitayama and Hazel Rose Markus Sense, Culture and Sensibility, Phoebe C. Ellsworth The Social Roles and Functions of Emotion, Nico H. Frijda and Batja Mesquita The Cultural Construction of Self and Emotion - Implications for Social Behaviour, Hazel Rose Markus and Shinobu Kitayama Emotion, Language and Cultural Scripts, Anna Wierzbicka Cognitive Sciences Contributions to Culture and Emotion, Michael I. Posner et al Affecting Culture - Emotion and Morality in Everyday Life, Geoffrey M. White Kali's Tongue - Cultural Psychology and the Power of Shame in Orissa, India, Usha Menon and Richard A. Shweder Major Cultural Syndromes and Emotion, Harry C. Triandis Culture, Emotion and Psychopathology, Janis H. Jenkins The Cultural Shaping of Emotion - a Conceptual Framework.","[{'authorId': '34648282', 'name': 'S. Kitayama'}, {'authorId': '6007602', 'name': 'H. Markus'}]",986.0,"{'bibtex': '@Inproceedings{Kitayama1994EmotionAC,\n author = {S. Kitayama and H. Markus},\n title = {Emotion and culture: Empirical studies of mutual influence.},\n year = {1994}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Emotion and culture: Empirical studies of mutual influence.,1994.0
2823,f99f8c9923c668c205126d4066838a42b5ce064a,,"[{'authorId': '2055572295', 'name': 'M. G. Gallego'}, {'authorId': '2118739294', 'name': 'J. García'}]",103.0,"{'bibtex': ""@Article{Gallego2017MusicTA,\n author = {M. G. Gallego and J. García},\n journal = {Neurologia},\n pages = {300-308},\n title = {Music therapy and Alzheimer's disease: Cognitive, psychological, and behavioural effects},\n volume = {32},\n year = {2017}\n}\n""}",,"{'volume': '32', 'pages': '300-308', 'name': 'Neurologia'}",40.0,"Music therapy and Alzheimer's disease: Cognitive, psychological, and behavioural effects",2017.0
2824,f9e8d5c52de83cadbe422ae46f6dea3b4baf2ba3,,"[{'authorId': '1869023', 'name': 'Pejman Sajjadi'}, {'authorId': '3689632', 'name': 'Laura Hoffmann'}, {'authorId': '1748977', 'name': 'P. Cimiano'}, {'authorId': '5864138', 'name': 'S. Kopp'}]",30.0,"{'bibtex': '@Article{Sajjadi2019APE,\n author = {Pejman Sajjadi and Laura Hoffmann and P. Cimiano and S. Kopp},\n journal = {Entertain. Comput.},\n title = {A personality-based emotional model for embodied conversational agents: Effects on perceived social presence and game experience of users},\n volume = {32},\n year = {2019}\n}\n'}",,"{'volume': '32', 'name': 'Entertain. Comput.'}",44.0,A personality-based emotional model for embodied conversational agents: Effects on perceived social presence and game experience of users,2019.0
2825,fa0dcaae23392fe54c534a85c1e7c430146580af,"Individuals with Autism spectrum disorder (ASD) are known to have significantly impaired social interaction and communication abilities. These impairments are characterized by their difficulties in using and perceiving non-verbal cues, such as facial expressions. The difficulty in processing communicators facial expressions is often attributed to the atypical gaze patterns in individuals with ASD. We present a computational study of gaze behavior in adolescents with ASD during their interaction with virtual agents (avatars) in a virtual reality based social communication platform. We study the implications on the subjects pupil response (pupil diameter changes) and looking pattern (fixation coordinates and duration) when exposed to the avatars demonstrating context-relevant emotional expressions. The data related to fixation and pupil response is collected using a commercial eye-tracker for subjects with and without ASD during their interactions with the avatars. This data is analyzed to investigate how the pupil response dynamics and fixation patterns of the ASD group differ from their typically developing peers. Our results indicate that communicators facial expressions can significantly affect the gaze behavior of the ASD subjects. We also observe reduced complexity in the pupil response dynamics, and lower synchrony between pupil response and fixation pattern in the ASD group.","[{'authorId': '2067177722', 'name': 'Zeeshan Akhtar'}, {'authorId': '1720741', 'name': 'T. Guha'}]",4.0,"{'bibtex': '@Article{Akhtar2019ComputationalAO,\n author = {Zeeshan Akhtar and T. Guha},\n journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {1075-1079},\n title = {Computational Analysis of Gaze Behavior in Autism During Interaction with Virtual Agents},\n year = {2019}\n}\n'}",,"{'pages': '1075-1079', 'name': 'ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}",28.0,Computational Analysis of Gaze Behavior in Autism During Interaction with Virtual Agents,2019.0
2826,fa2d78763a600023e29832cff80862a06f8620b8,"Identification of emotional expressions of a Talking Head (TH) were evaluated and compared to that of natural faces. In addition, the effect of static (pictures) and dynamic (video sequences) stimuli was studied. Natural stimuli consisted of six basic emotional expressions (anger, disgust, fear, happiness, sadness and surprise). Two expression sets were selected from both Ekman-Friesen facial affect pictures [1] and Cohn-Kanade database [2]. In addition, two new natural expression sets were recorded in our laboratory. Synthetic expressions were created by our new TH [3], both with and without facial texture. Preliminary results indicate that the TH expressions, except fear, were identified as expected. Overall level of identification of TH stimuli was below those of natural ones. Of all used stimuli, happiness was identified the best and fear the worst. Natural static and dynamic expressions were identified equally well. However, the dynamic expressions of the TH were identified significantly more accurately than the static ones.","[{'authorId': '48160993', 'name': 'J. Kätsyri'}, {'authorId': '2883014', 'name': 'V. Klucharev'}, {'authorId': '145208327', 'name': 'M. Frydrych'}, {'authorId': '2556968', 'name': 'M. Sams'}]",41.0,"{'bibtex': '@Inproceedings{Kätsyri2003IdentificationOS,\n author = {J. Kätsyri and V. Klucharev and M. Frydrych and M. Sams},\n pages = {239-243},\n title = {Identification of synthetic and natural emotional facial expressions},\n year = {2003}\n}\n'}",,{'pages': '239-243'},14.0,Identification of synthetic and natural emotional facial expressions,2003.0
2827,fa3fa79e49430e156a7cc6cc86045e952883257e,,"[{'authorId': '143747701', 'name': 'C. Carrascosa'}, {'authorId': '1722104', 'name': 'J. Bajo'}, {'authorId': '144890090', 'name': 'V. Julián'}, {'authorId': '1729096', 'name': 'J. Corchado'}, {'authorId': '1686926', 'name': 'V. Botti'}]",113.0,"{'bibtex': '@Article{Carrascosa2008HybridMA,\n author = {C. Carrascosa and J. Bajo and V. Julián and J. Corchado and V. Botti},\n journal = {Expert Syst. Appl.},\n pages = {2-17},\n title = {Hybrid multi-agent architecture as a real-time problem-solving model},\n volume = {34},\n year = {2008}\n}\n'}",,"{'volume': '34', 'pages': '2-17', 'name': 'Expert Syst. Appl.'}",46.0,Hybrid multi-agent architecture as a real-time problem-solving model,2008.0
2828,fa41901489654b1f3292ebb1821187c77f1c9470,,"[{'authorId': '4106384', 'name': 'J. Druckman'}, {'authorId': '153592028', 'name': 'R. McDermott'}]",343.0,"{'bibtex': '@Article{Druckman2008EmotionAT,\n author = {J. Druckman and R. McDermott},\n journal = {Political Behavior},\n pages = {297-321},\n title = {Emotion and the Framing of Risky Choice},\n volume = {30},\n year = {2008}\n}\n'}",,"{'volume': '30', 'pages': '297-321', 'name': 'Political Behavior'}",97.0,Emotion and the Framing of Risky Choice,2008.0
2830,fa55b0b6c37558c985331f05a98f320449e8f9d7,"1. The domain of gesture 2. Visible action as gesture 3. Western interest in gesture from classical antiquity to the eighteenth century 4. Four contributions from the nineteenth century: Andrea de Jorio, Edward Tylor, Garrick Mallery and Wilhelm Wundt 5. Gesture studies in the twentieth century: recession and return 6. Classifying gestures 7. Gesture units, gesture phrases and speech 8. Deployments of gesture in the utterance 9. Gesture and speech in semantic interaction 10. Gesture and referential meaning 11. On pointing 12. Gestures of the 'precision-grip': topic, comment and question markers 13. Two gesture families of the open hand 14. Gesture without speech: the emergence of kinesic codes 15. Gesture and sign on common ground 16. Gesture, culture and the communication economy 17. The status of gesture Appendix I. Transcription conventions Appendix II. The recordings.","[{'authorId': '47985333', 'name': 'A. Kendon'}]",2550.0,"{'bibtex': '@Inproceedings{Kendon2004GestureVA,\n author = {A. Kendon},\n title = {Gesture: Visible Action as Utterance},\n year = {2004}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Gesture: Visible Action as Utterance,2004.0
2831,fa56a969b9fe56d1bd494611ecc2c6d8586549dd,,"[{'authorId': '1737845', 'name': 'R. Atkinson'}, {'authorId': '1819200', 'name': 'R. Mayer'}, {'authorId': '69435153', 'name': 'M. M. Merrill'}]",309.0,"{'bibtex': '@Article{Atkinson2005FosteringSA,\n author = {R. Atkinson and R. Mayer and M. M. Merrill},\n journal = {Contemporary Educational Psychology},\n pages = {117-139},\n title = {Fostering social agency in multimedia learning: Examining the impact of an animated agent’s voice ☆},\n volume = {30},\n year = {2005}\n}\n'}",,"{'volume': '30', 'pages': '117-139', 'name': 'Contemporary Educational Psychology'}",26.0,Fostering social agency in multimedia learning: Examining the impact of an animated agent’s voice ☆,2005.0
2832,faa24a474493ec0485cc1f53f70bc0f15e21239d,"Virtual characters in games and simulations often need to plan visually convincing paths through a crowded environment. This paper describes how crowd density information can be used to guide a large number of characters through a crowded environment. Crowd density information helps characters avoid congested routes that could lead to traffic jams. It also encourages characters to use a wide variety of routes to reach their destination. Our technique measures the desirability of a route by combining distance information with crowd density information. We start by building a navigation mesh for the walkable regions in a polygonal two‐dimensional (2‐D) or multilayered three‐dimensional (3‐D) environment. The skeleton of this navigation mesh is the medial axis. Each walkable region in the navigation mesh maintains an up‐to‐date density value. This density value is equal to the area occupied by all the characters inside a given region divided by the total area of this region. These density values are mapped onto the medial axis to form a weighted graph. An A* search on this graph yields a backbone path for each character, and forces are used to guide the characters through the weighted environment. The characters periodically replan their routes as the density values are updated. Our experiments show that we can compute congestion‐avoiding paths for tens of thousands of characters in real‐time. Copyright © 2012 John Wiley & Sons, Ltd.","[{'authorId': '1778837', 'name': 'W. V. Toll'}, {'authorId': '32509789', 'name': 'Atlas F. Cook'}, {'authorId': '1766759', 'name': 'Roland Geraerts'}]",87.0,"{'bibtex': '@Article{Toll2012RealtimeDC,\n author = {W. V. Toll and Atlas F. Cook and Roland Geraerts},\n journal = {Computer Animation and Virtual Worlds},\n title = {Real‐time density‐based crowd simulation},\n volume = {23},\n year = {2012}\n}\n'}",,"{'volume': '23', 'name': 'Computer Animation and Virtual Worlds'}",34.0,Real‐time density‐based crowd simulation,2012.0
2833,fad58dc0c336d7ff73127328f6659eb2872fc954,"People tend to personify machines. Giving machines the ability to actually produce social information can help improve human-machine interactions. Embodied Conversational Agents (ECAs) are virtual software agents that can process and produce speech, facial expressions, gestures and eye gaze, enabling natural, multimodal, human-machine communication. On the one hand, the field of personality psychology provides insights into how we could describe and measure the virtual personality of ECAs. On the other hand, ECAs provide a method to systematically examine how different factors affect the perception of personality. This paper shows that standardized, validated personality questionnaires can be used to evaluate ECAs psychologically, and that state of the art ECAs can manipulate their perceived personality through appearance and behavior.","[{'authorId': '145649261', 'name': 'Susana Castillo'}, {'authorId': '2057924984', 'name': 'Philipp Hahn'}, {'authorId': '3081238', 'name': 'K. Legde'}, {'authorId': '1790148', 'name': 'D. Cunningham'}]",12.0,"{'bibtex': '@Article{Castillo2018PersonalityAO,\n author = {Susana Castillo and Philipp Hahn and K. Legde and D. Cunningham},\n journal = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},\n title = {Personality Analysis of Embodied Conversational Agents},\n year = {2018}\n}\n'}",,{'name': 'Proceedings of the 18th International Conference on Intelligent Virtual Agents'},24.0,Personality Analysis of Embodied Conversational Agents,2018.0
2834,fadd4f195bc231b55deee6fe5a199af6379b2ef5,,"[{'authorId': '5778922', 'name': 'G. Fricchione'}]",1242.0,"{'bibtex': '@Article{Fricchione1995DescartesEE,\n author = {G. Fricchione},\n journal = {Psychosomatics},\n pages = {151-153},\n title = {Descartes’ Error: Emotion, Reason and the Human Brain},\n volume = {36},\n year = {1995}\n}\n'}",,"{'volume': '36', 'pages': '151-153', 'name': 'Psychosomatics'}",1.0,"Descartes’ Error: Emotion, Reason and the Human Brain",1995.0
2835,fb19d28f3b0d96e65036fde8a0adc9bf1c4bbe46,,"[{'authorId': '2401039', 'name': 'Derrick J. Parkhurst'}, {'authorId': '27685226', 'name': 'Klinton Law'}, {'authorId': '3271571', 'name': 'E. Niebur'}]",1448.0,"{'bibtex': '@Article{Parkhurst2002ModelingTR,\n author = {Derrick J. Parkhurst and Klinton Law and E. Niebur},\n journal = {Vision Research},\n pages = {107-123},\n title = {Modeling the role of salience in the allocation of overt visual attention},\n volume = {42},\n year = {2002}\n}\n'}",,"{'volume': '42', 'pages': '107-123', 'name': 'Vision Research'}",82.0,Modeling the role of salience in the allocation of overt visual attention,2002.0
2836,fb68898a47f6c2d1d639db27df0fd5b1d236e843,"The field of Artificial Intelligence has changed a great deal since the 80s, and arguably no one has played a larger role in that change than Judea Pearl. Judea Pearl's work made probability the prevailing language of modern AI and, perhaps more significantly, it placed the elaboration of crisp and meaningful models, and of effective computational mechanisms, at the center of AI research. This book is a collection of articles in honor of Judea Pearl, written by close colleagues and former students. Its three main parts, heuristics, probabilistic reasoning, and causality, correspond to the titles of the three ground-breaking books authored by Judea, and are followed by a section of short reminiscences. In this volume, leading authors look at the state of the art in the fields of heuristic, probabilistic, and causal reasoning, in light of Judea's seminal contributors. The authors list include Blai Bonet, Eric Hansen, Robert Holte, Jonathan Schaeffer, Ariel Felner, Richard Korf, Austin Parker, Dana Nau, V. S. Subrahmanian, Hector Geffner, Ira Pohl, Adnan Darwiche, Thomas Dean, Rina Dechter, Bozhena Bidyuk, Robert Matescu, Emma Rollon, Michael I. Jordan, Michael Kearns, Daphne Koller, Brian Milch, Stuart Russell, Azaria Paz, David Poole, Ingrid Zukerman, Carlos Brito, Philip Dawid, Felix Elwert, Christopher Winship, Michael Gelfond, Nelson Rushton, Moises Goldszmidt, Sander Greenland, Joseph Y. Halpern, Christopher Hitchcock, David Heckerman, Ross Shachter, Vladimir Lifschitz, Thomas Richardson, James Robins, Yoav Shoham, Peter Spirtes, Clark Glymour, Richard Scheines, Robert Tillman, Wolfgang Spohn, Jian Tian, Ilya Shpitser, Nils Nilsson, Edward T. Purcell, and David Spiegelhalter.","[{'authorId': '1751239', 'name': 'R. Dechter'}, {'authorId': '1806598', 'name': 'Hector Geffner'}, {'authorId': '48118134', 'name': 'J. Halpern'}]",84.0,"{'bibtex': '@Inproceedings{Dechter2010HeuristicsPA,\n author = {R. Dechter and Hector Geffner and J. Halpern},\n title = {Heuristics, Probability and Causality. A Tribute to Judea Pearl},\n year = {2010}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,"Heuristics, Probability and Causality. A Tribute to Judea Pearl",2010.0
2837,fb8007d427c192d8c953890862dcce62ff944dbf,,"[{'authorId': '1383996606', 'name': 'S. D’Mello'}, {'authorId': '1769251', 'name': 'A. Graesser'}]",648.0,"{'bibtex': '@Article{D’Mello2012DynamicsOA,\n author = {S. D’Mello and A. Graesser},\n journal = {Learning and Instruction},\n pages = {145-157},\n title = {Dynamics of affective states during complex learning},\n volume = {22},\n year = {2012}\n}\n'}",,"{'volume': '22', 'pages': '145-157', 'name': 'Learning and Instruction'}",102.0,Dynamics of affective states during complex learning,2012.0
2838,fb8764f0f4e91fbf3d818136b52d52a07fae5d76,,"[{'authorId': '144006457', 'name': 'R. Moreno'}, {'authorId': '2046910', 'name': 'Terri Flowerday'}]",169.0,"{'bibtex': ""@Article{Moreno2006StudentsCO,\n author = {R. Moreno and Terri Flowerday},\n journal = {Contemporary Educational Psychology},\n pages = {186-207},\n title = {Students' choice of animated pedagogical agents in science learning: A test of the similarity-attraction hypothesis on gender and ethnicity},\n volume = {31},\n year = {2006}\n}\n""}",,"{'volume': '31', 'pages': '186-207', 'name': 'Contemporary Educational Psychology'}",66.0,Students' choice of animated pedagogical agents in science learning: A test of the similarity-attraction hypothesis on gender and ethnicity,2006.0
2839,fb981c30d1de57c37df9714305893329fa53f07d,Perspectives on Sexuality Sex Research - an Overview Part 1. Biological Perspectives: Sexual Anatomy 1. Sexual Physiology 2. Human Reproduction 3. Birth Control 4. Abortion Part 2. Developmental Perspectives: Childhood Sexuality 5. Adolescent Sexuality 6. Adult Sexuality 7. Gender Roles Part 3. Psychological Perspectives: Loving and Being Loved 8. Intimacy and Communication Skills 9. Enhancing your Sexual Relationships 10. Sexual Orientation 11. Sexual Behaviour 12. Sexual Variations 13. Coercive Sex - the Varieties of Sexual Assault Part 4. Sexual Health Perspectives: Sexually Transmitted Diseases and Sexual Infections 14. HIV Infection and AIDS 15. Sexual Dysfunctions and Sex Therapy 16. Sexual Disorders and Sexual Health Part 5 Cultural Perspectives: Sex and the Law 17. Religious and Ethical Perspectives and Sexuality,"[{'authorId': '48713852', 'name': 'A. Maslow'}]",21615.0,"{'bibtex': '@Inproceedings{Maslow1954MotivationAP,\n author = {A. Maslow},\n title = {Motivation and Personality},\n year = {1954}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Motivation and Personality,1954.0
2840,fbb671eaf936e0ccf5fde68ab40f31251e2e919e,"Two experiments employed image-based tasks to test the hypothesis that happier moods promote a greater focus on the forest and sadder moods a greater focus on the trees. The hypothesis was based on the idea that in task situations, affective cues may be experienced as task-relevant information, which then influences global versus local attention. Using a serial-reproduction paradigm, Experiment 1 showed that individuals in sad moods were less likely than those in happier moods to use an accessible global concept to guide attempts to reproduce a drawing from memory. Experiment 2 investigated the same hypothesis by assessing the use of global and local attributes to classify geometric figures. As predicted, individuals in sad moods were less likely than those in happier moods to classify figures on the basis of global features.","[{'authorId': '5443453', 'name': 'Karen Gasper'}, {'authorId': '31458494', 'name': 'G. Clore'}]",1065.0,"{'bibtex': '@Article{Gasper2002AttendingTT,\n author = {Karen Gasper and G. Clore},\n journal = {Psychological Science},\n pages = {34 - 40},\n title = {Attending to the Big Picture: Mood and Global Versus Local Processing of Visual Information},\n volume = {13},\n year = {2002}\n}\n'}",,"{'volume': '13', 'pages': '34 - 40', 'name': 'Psychological Science'}",35.0,Attending to the Big Picture: Mood and Global Versus Local Processing of Visual Information,2002.0
2841,fbbaca478fb74e5f35c8594be1d1e3840927db8a,,"[{'authorId': '114052485', 'name': 'T. Antonucci'}]",263.0,"{'bibtex': '@Inproceedings{Antonucci1990SocialSA,\n author = {T. Antonucci},\n pages = {205-226},\n title = {Social Supports, and Social Relation-ships},\n year = {1990}\n}\n'}",,"{'volume': '', 'pages': '205-226', 'name': ''}",0.0,"Social Supports, and Social Relation-ships",1990.0
2842,fbc6562814e08e416e28a268ce7beeaa3d0708c8,,"[{'authorId': '52184096', 'name': 'L. Bottou'}]",5296.0,"{'bibtex': '@Inproceedings{Bottou2010LargeScaleML,\n author = {L. Bottou},\n pages = {177-186},\n title = {Large-Scale Machine Learning with Stochastic Gradient Descent},\n year = {2010}\n}\n'}",,{'pages': '177-186'},25.0,Large-Scale Machine Learning with Stochastic Gradient Descent,2010.0
2843,fbe23d8cbabff474d064b3c859b0fcf62a69b405,"The confidence we have in our assessment of an interaction partner's emotional state can have important consequences for the quality of the interaction. Two studies assessed the hypothesis that immigrants are more confident in their judgment of others' emotional facial expressions if the expresser is a member of their cultural ingroup rather than a member of the host community or another cultural group. In addition, the effects of the perceived familiarity with the type of expression, the length of residence in the host country, the quality of cross-cultural contact, the level of acculturation, and the intensity of the facial expressions were assessed. Overall, the results revealed an ingroup advantage effect for confidence ratings as well as support for the notion that individuals are more confident when judging expressions that they consider as more frequently displayed in everyday life. Furthermore, individuals were more confident when judging happiness expressions as well as more intense expressions in general.","[{'authorId': '5868984', 'name': 'Martin G. Beaupré'}, {'authorId': '3067657', 'name': 'U. Hess'}]",91.0,"{'bibtex': '@Article{Beaupré2006AnIA,\n author = {Martin G. Beaupré and U. Hess},\n journal = {Personality and Social Psychology Bulletin},\n pages = {16 - 26},\n title = {An Ingroup Advantage for Confidence in Emotion Recognition Judgments: The Moderating Effect of Familiarity With the Expressions of Outgroup Members},\n volume = {32},\n year = {2006}\n}\n'}",,"{'volume': '32', 'pages': '16 - 26', 'name': 'Personality and Social Psychology Bulletin'}",39.0,An Ingroup Advantage for Confidence in Emotion Recognition Judgments: The Moderating Effect of Familiarity With the Expressions of Outgroup Members,2006.0
2844,fbe2ba616d50dcbb629357483516af28c21175a7,,"[{'authorId': '2179460', 'name': 'Toni Bloodworth'}, {'authorId': '2725134', 'name': 'Lauren Cairco'}, {'authorId': '1710833', 'name': 'L. Hodges'}, {'authorId': '2165018', 'name': 'N. Meehan'}, {'authorId': '14476154', 'name': 'Arlene Johnson'}]",7.0,"{'bibtex': '@Inproceedings{Bloodworth2014AnET,\n author = {Toni Bloodworth and Lauren Cairco and L. Hodges and N. Meehan and Arlene Johnson},\n pages = {329-338},\n title = {An Eye Tracking Evaluation of a Virtual Pediatric Patient Training System for Nurses},\n year = {2014}\n}\n'}",,{'pages': '329-338'},23.0,An Eye Tracking Evaluation of a Virtual Pediatric Patient Training System for Nurses,2014.0
2845,fc1761014d643028a29b7223fc4b6ac04d07805c,"Children understand early in development that different people know different things, and they are adept at using this information to select appropriate sources of information (Lutz & Keil, 2002). However, in the current digital age, information may be gathered from both humans and technological sources that select and present information as humans do. Using methods designed to study epistemic trust in human informants (e.g., Koenig, Clement, & Harris, 2004), the current study investigates children’s and adults’ selective trust in a technological and human informant. Children (ages 4 and 5) and adults were presented with queries designed to probe their willingness to seek out and accept information from human versus technological informants. The results demonstrate that 4-year-olds prefer to seek information from a human informant, but by age 5, children show an increasing preference for the technological informant. The relationship between children’s trust and their experience with technology is also discussed.","[{'authorId': '2218550', 'name': 'Nicholaus S. Noles'}, {'authorId': '3196314', 'name': 'Judith H. Danovitch'}, {'authorId': '3210220', 'name': 'Patrick Shafto'}]",9.0,"{'bibtex': ""@Article{Noles2015ChildrensTI,\n author = {Nicholaus S. Noles and Judith H. Danovitch and Patrick Shafto},\n journal = {Cognitive Science},\n title = {Children's Trust in Technological and Human Informants},\n year = {2015}\n}\n""}",,"{'volume': '', 'name': 'Cognitive Science'}",18.0,Children's Trust in Technological and Human Informants,2015.0
2846,fc1c03afc7ec328f7567310d0e9bc571d6caf5bb,,"[{'authorId': '145077962', 'name': 'Michael Hermes'}, {'authorId': '3469845', 'name': 'Dirk Hagemann'}]",115.0,"{'bibtex': '@Inproceedings{Hermes2011ExtraversionAI,\n author = {Michael Hermes and Dirk Hagemann},\n title = {Extraversion and its positive emotional core},\n volume = {11},\n year = {2011}\n}\n'}",,"{'volume': '11', 'name': ''}",0.0,Extraversion and its positive emotional core,2011.0
2847,fc27d95c7875813f23e3150bfcd512bbd069a00b,"Three experiments investigated the interpersonal effects of anger and happiness in negotiations. In the course of a computer-mediated negotiation, participants received information about the emotional state (anger, happiness, or none) of their opponent. Consistent with a strategic-choice perspective, Experiment 1 showed that participants conceded more to an angry opponent than to a happy one. Experiment 2 showed that this effect was caused by tracking--participants used the emotion information to infer the other's limit, and they adjusted their demands accordingly. However, this effect was absent when the other made large concessions. Experiment 3 examined the interplay between experienced and communicated emotion and showed that angry communications (unlike happy ones) induced fear and thereby mitigated the effect of the opponent's experienced emotion. These results suggest that negotiators are especially influenced by their opponent's emotions when they are motivated and able to consider them.","[{'authorId': '5980688', 'name': 'Gerben A. van Kleef'}, {'authorId': '8494133', 'name': 'C. D. De Dreu'}, {'authorId': '92736978', 'name': 'A. Manstead'}]",757.0,"{'bibtex': '@Article{Kleef2004TheIE,\n author = {Gerben A. van Kleef and C. D. De Dreu and A. Manstead},\n journal = {Journal of personality and social psychology},\n pages = {\n          57-76\n        },\n title = {The interpersonal effects of anger and happiness in negotiations.},\n volume = {86 1},\n year = {2004}\n}\n'}",,"{'volume': '86 1', 'pages': '\n          57-76\n        ', 'name': 'Journal of personality and social psychology'}",112.0,The interpersonal effects of anger and happiness in negotiations.,2004.0
2851,fc39686337391d94f5044a87d2819019fd583d71,"In this paper, we describe a case study in emotion understanding in stories that leverages the contribution of traumatic brain injured people. In particular, we focus on moral emotions, leveraging the differences in moral functioning that characterizes traumatic brain injured patients. By comparing the understanding of the moral and emotional aspects of characters’ behavior in traumatic brain injured patients and in a control group of neurologically healthy subjects, we observed slight, yet meaningful differences in the two groups. We describe the test methodology and results, discussing their implications for the design of rehabilitation applications that leverage virtual characters.","[{'authorId': '5351581', 'name': 'Eleonora Ceccaldi'}, {'authorId': '2559167', 'name': 'C. Battaglino'}, {'authorId': '144411873', 'name': 'R. Damiano'}, {'authorId': '3792596', 'name': 'V. Galetto'}, {'authorId': '2775305', 'name': 'M. Zettin'}]",1.0,"{'bibtex': '@Inproceedings{Ceccaldi2016ModelingEU,\n author = {Eleonora Ceccaldi and C. Battaglino and R. Damiano and V. Galetto and M. Zettin},\n pages = {45-58},\n title = {Modeling Emotion Understanding in Stories: Insights from Traumatic Brain Injured Patients},\n year = {2016}\n}\n'}",,{'pages': '45-58'},26.0,Modeling Emotion Understanding in Stories: Insights from Traumatic Brain Injured Patients,2016.0
2852,fc5cfedfcfcc89006706655a2b4685d686aebd2f,"This paper presents a census of 147 virtual agents, by examining and reporting on their physical and demographical characteristics. The study shows that the vast majority of agents developed are from a white ethnic background. Overall, female agents tend to be more photo realistic than their male counterparts who are more cartoon like. These findings highlight current stereotypes in relation to agents and contribute to a deeper understanding of virtual worlds.","[{'authorId': '153914091', 'name': 'R. Khan'}, {'authorId': '34919047', 'name': 'A. D. Angeli'}]",12.0,"{'bibtex': '@Inproceedings{Khan2007MappingTD,\n author = {R. Khan and A. D. Angeli},\n pages = {149-152},\n title = {Mapping the demographics of virtual humans},\n year = {2007}\n}\n'}",,{'pages': '149-152'},17.0,Mapping the demographics of virtual humans,2007.0
2853,fc85d88ed4f8d2b1c6bc99e424b7f052244e8708,"Nonverbal behavior is considered critical for indicating intimacy and is important when designing a social virtual agent such as a counselor. One key research question is how to properly express intimate self-disclosure. In this paper we present an extensive study of human nonverbal behavior during intimate self-disclosure. This is an important milestone in creating a virtual counselor. A study of video interactions between human participants demonstrated that people display more head tilts and pauses when they revealed highly intimate information about themselves; they presented more head nods and eye gazes during less intimate sharing. An implementation of these behaviors in a virtual agent suggests that people tend to perceive head tilts, pauses and gaze aversion by the agent as conveying intimate self-disclosure. These findings are important for future research with virtual counselors and other social agents.","[{'authorId': '34728215', 'name': 'Sin-Hwa Kang'}, {'authorId': '145438097', 'name': 'J. Gratch'}, {'authorId': '2668280', 'name': 'C. Sidner'}, {'authorId': '2038490', 'name': 'Ron Artstein'}, {'authorId': '2110799090', 'name': 'Lixing Huang'}, {'authorId': '49933077', 'name': 'Louis-Philippe Morency'}]",45.0,"{'bibtex': '@Inproceedings{Kang2012TowardsBA,\n author = {Sin-Hwa Kang and J. Gratch and C. Sidner and Ron Artstein and Lixing Huang and Louis-Philippe Morency},\n pages = {63-70},\n title = {Towards building a virtual counselor: modeling nonverbal behavior during intimate self-disclosure},\n year = {2012}\n}\n'}",,{'pages': '63-70'},32.0,Towards building a virtual counselor: modeling nonverbal behavior during intimate self-disclosure,2012.0
2855,fca83bc9c768ca236f178ad59a57f07bd56b6875,"This paper introduces a computational model for emotion regulation formalising the model informally described by Gross (1998). The model has been constructed using a highlevel modelling language, and integrates both quantitative aspects (such as levels of emotional response) and qualitative aspects (such as decisions to regulate one’s emotion). A number of simulation experiments have been performed, demonstrating that the computational model successfully reflects the model as described by Gross.","[{'authorId': '145518106', 'name': 'T. Bosse'}, {'authorId': '118596324', 'name': 'Pontier'}, {'authorId': '1726343', 'name': 'Jan Treur'}]",40.0,"{'bibtex': ""@Inproceedings{Bosse2007ADS,\n author = {T. Bosse and Pontier and Jan Treur},\n pages = {187-192},\n title = {A Dynamical System Modelling Approach to Gross' Model of Emotion Regulation},\n year = {2007}\n}\n""}",,"{'volume': '', 'pages': '187-192', 'name': ''}",21.0,A Dynamical System Modelling Approach to Gross' Model of Emotion Regulation,2007.0
2857,fcd377802681d9e70296eba90d520e170f2d3a0b,"Background Research in embodied artificial intelligence (AI) has increasing clinical relevance for therapeutic applications in mental health services. With innovations ranging from ‘virtual psychotherapists’ to social robots in dementia care and autism disorder, to robots for sexual disorders, artificially intelligent virtual and robotic agents are increasingly taking on high-level therapeutic interventions that used to be offered exclusively by highly trained, skilled health professionals. In order to enable responsible clinical implementation, ethical and social implications of the increasing use of embodied AI in mental health need to be identified and addressed. Objective This paper assesses the ethical and social implications of translating embodied AI applications into mental health care across the fields of Psychiatry, Psychology and Psychotherapy. Building on this analysis, it develops a set of preliminary recommendations on how to address ethical and social challenges in current and future applications of embodied AI. Methods Based on a thematic literature search and established principles of medical ethics, an analysis of the ethical and social aspects of currently embodied AI applications was conducted across the fields of Psychiatry, Psychology, and Psychotherapy. To enable a comprehensive evaluation, the analysis was structured around the following three steps: assessment of potential benefits; analysis of overarching ethical issues and concerns; discussion of specific ethical and social issues of the interventions. Results From an ethical perspective, important benefits of embodied AI applications in mental health include new modes of treatment, opportunities to engage hard-to-reach populations, better patient response, and freeing up time for physicians. Overarching ethical issues and concerns include: harm prevention and various questions of data ethics; a lack of guidance on development of AI applications, their clinical integration and training of health professionals; ‘gaps’ in ethical and regulatory frameworks; the potential for misuse including using the technologies to replace established services, thereby potentially exacerbating existing health inequalities. Specific challenges identified and discussed in the application of embodied AI include: matters of risk-assessment, referrals, and supervision; the need to respect and protect patient autonomy; the role of non-human therapy; transparency in the use of algorithms; and specific concerns regarding long-term effects of these applications on understandings of illness and the human condition. Conclusions We argue that embodied AI is a promising approach across the field of mental health; however, further research is needed to address the broader ethical and societal concerns of these technologies to negotiate best research and medical practices in innovative mental health care. We conclude by indicating areas of future research and developing recommendations for high-priority areas in need of concrete ethical guidance.","[{'authorId': '46217950', 'name': 'A. Fiske'}, {'authorId': '6743130', 'name': 'P. Henningsen'}, {'authorId': '5648439', 'name': 'A. Buyx'}]",221.0,"{'bibtex': '@Article{Fiske2018YourRT,\n author = {A. Fiske and P. Henningsen and A. Buyx},\n journal = {Journal of Medical Internet Research},\n title = {Your Robot Therapist Will See You Now: Ethical Implications of Embodied Artificial Intelligence in Psychiatry, Psychology, and Psychotherapy},\n volume = {21},\n year = {2018}\n}\n'}",,"{'volume': '21', 'name': 'Journal of Medical Internet Research'}",102.0,"Your Robot Therapist Will See You Now: Ethical Implications of Embodied Artificial Intelligence in Psychiatry, Psychology, and Psychotherapy",2018.0
2858,fcd9d83de287a9b5fb75b081aa107067a28e4d00,,"[{'authorId': '49584958', 'name': 'N. Frijda'}]",267.0,"{'bibtex': ""@Inproceedings{Frijda2008ThePP,\n author = {N. Frijda},\n pages = {68-87},\n title = {The psychologists' point of view},\n year = {2008}\n}\n""}",,"{'volume': '', 'pages': '68-87', 'name': ''}",0.0,The psychologists' point of view,2008.0
2859,fd0431df6e48db94b35a498b9488611568022df1,,"[{'authorId': '84527386', 'name': 'R. Plutchik'}]",663.0,"{'bibtex': '@Inproceedings{Plutchik2016HumanEH,\n author = {R. Plutchik},\n title = {Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice},\n year = {2016}\n}\n'}",,"{'volume': '', 'name': ''}",11.0,"Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",2016.0
2860,fd1afcaa3d66c5d31d33910a4dcca26ebe49b23f,"The aim of this paper is to review data from my laboratory, which were collected in an attempt to determine whether the facial EMG response is a general component of the emotional reaction. 
 
 
 
In a number of studies it was found that facial reactions: first, are spontaneously elicited and differ according to the kind of emotional stimuli to which sunjects are exposed; second, are sensitive to learning; third, are consistent with how the subject perceive the stimuli and their own specific emotions; fourth, are congruent with autonomic responses; fifth, are more pronoanced for females than for males; and finally, differ among subjects with specific fears. 
 
 
 
These data converge to indicate that facial muscle activity is a general component of the emotional reaction and demonstrate that the facial EMG technique is a sensitve too for measuring emotional ractions.","[{'authorId': '4583182', 'name': 'U. Dimberg'}]",189.0,"{'bibtex': '@Article{Dimberg1990ForDE,\n author = {U. Dimberg},\n journal = {Psychophysiology},\n pages = {481-494},\n title = {For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1988},\n volume = {27},\n year = {1990}\n}\n'}",,"{'volume': '27', 'pages': '481-494', 'name': 'Psychophysiology'}",41.0,"For Distinguished Early Career Contribution to Psychophysiology: Award Address, 1988",1990.0
2861,fd3ae99e4c017df5ed72da49ba892035f842c6e7,,"[{'authorId': '2724558', 'name': 'Dieta Kuchenbrandt'}, {'authorId': '2557354', 'name': 'F. Eyssel'}, {'authorId': '3120498', 'name': 'Simon Bobinger'}, {'authorId': '2074270742', 'name': 'Maria Neufeld'}]",45.0,"{'bibtex': '@Inproceedings{Kuchenbrandt2011MinimalG,\n author = {Dieta Kuchenbrandt and F. Eyssel and Simon Bobinger and Maria Neufeld},\n pages = {104-113},\n title = {Minimal Group - Maximal Effect? Evaluation and Anthropomorphization of the Humanoid Robot NAO},\n year = {2011}\n}\n'}",,{'pages': '104-113'},25.0,Minimal Group - Maximal Effect? Evaluation and Anthropomorphization of the Humanoid Robot NAO,2011.0
2862,fd4ff5e76e17766a0a1ef8c5daba6170d398accc,"Reactive tokens are conversational resources by which a listener co-constructs a speaker's turn at talk. The resources that are available include the forms of the reactive tokens themselves, their duration, and their placement by the listener in the current speaker's turn. The present paper is a contrastive study of the use of these resources by Americans in English, and by Koreans in their native language and in English, and in it we show the ecological relationship between the resources that a language provides and their use in constructing active listenership. Although previous research on English has found listeners use reactive tokens to pass up the opportunity for a full turn at talk, we show that, in Korean, reactive tokens are often elicited by the current speaker and the listener is obligated to provide them. We present evidence that Korean bilinguals transfer some conversational resources from their native language when they take part in conversation in English.","[{'authorId': '2087260302', 'name': 'R. Young'}, {'authorId': '9174234', 'name': 'Jina Lee'}]",59.0,"{'bibtex': '@Article{Young2004IdentifyingUI,\n author = {R. Young and Jina Lee},\n journal = {Journal of Sociolinguistics},\n pages = {380-407},\n title = {Identifying units in interaction: Reactive tokens in Korean and English conversations},\n volume = {8},\n year = {2004}\n}\n'}",,"{'volume': '8', 'pages': '380-407', 'name': 'Journal of Sociolinguistics'}",44.0,Identifying units in interaction: Reactive tokens in Korean and English conversations,2004.0
2863,fdba977c3d073626f432066956b334504bc096d3,"Emotions have a powerful impact on behavior and beliefs. The goal of our research is to create general computational models of this interplay of emotion, cognition and behavior to inform the design of virtual humans. Here, we address an aspect of emotional behavior that has been studied extensively in the psychological literature but largely ignored by computational approaches, emotion-focused coping. Rather than motivating external action, emotion-focused coping strategies alter beliefs in response to strong emotions. For example an individual may alter beliefs about the importance of a goal that is being threatened, thereby reducing their distress. We present a preliminary model of emotion-focused coping and discuss how coping processes, in general, can be coupled to emotions and behavior. The approach is illustrated within a virtual reality training environment where the models are used to create virtual human characters in high-stress social situations.","[{'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '145438097', 'name': 'J. Gratch'}]",136.0,"{'bibtex': '@Inproceedings{Marsella2002AST,\n author = {S. Marsella and J. Gratch},\n pages = {334-341},\n title = {A step toward irrationality: using emotion to change belief},\n year = {2002}\n}\n'}",,{'pages': '334-341'},26.0,A step toward irrationality: using emotion to change belief,2002.0
2865,fddf060c19051aa194be40be5a337d032c90929f,"In this paper, we present first results on applying a personality assessment paradigm to speech input, and comparing human and automatic performance on this task. We cue a professional speaker to produce speech using different personality profiles and encode the resulting vocal personality impressions in terms of the ""Big Five"" NEO-FFI personality traits. We then have human raters, who do not know the speaker, estimate the five factors. We analyze the recordings using signal-based acoustic and prosodic methods and observe high consistency between the acted personalities, the raters' assessments, and initial automatic classification results. This presents a first step towards being able to handle personality traits in speech, which we envision will be used in future voice-based communication between humans and machines.","[{'authorId': '1912566', 'name': 'Tim Polzehl'}, {'authorId': '145733288', 'name': 'S. Möller'}, {'authorId': '1740721', 'name': 'Florian Metze'}]",85.0,"{'bibtex': '@Article{Polzehl2010AutomaticallyAP,\n author = {Tim Polzehl and S. Möller and Florian Metze},\n journal = {2010 IEEE Fourth International Conference on Semantic Computing},\n pages = {134-140},\n title = {Automatically Assessing Personality from Speech},\n year = {2010}\n}\n'}",,"{'pages': '134-140', 'name': '2010 IEEE Fourth International Conference on Semantic Computing'}",13.0,Automatically Assessing Personality from Speech,2010.0
2866,fdfe996e24772ac8b48fc0f7b72140d0d2f366c1,"It is generally thought that individuals with Asperger's syndrome and high-functioning autism (AS/HFA) have deficits in theory of mind. These deficits have been previously linked to problems with social cognition. However, we reasoned that AS/HFA individuals' Theory of Mind deficits also might lead to problems with emotion regulation. To assess emotional functioning in AS/HFA, 27 AS/HFA adults (16 women) and 27 age-, gender-, and education-matched typically developing (TD) participants completed a battery of measures of emotion experience, labeling, and regulation. With respect to emotion experience, individuals with AS/HFA reported higher levels of negative emotions, but similar levels of positive emotions, compared with TD individuals. With respect to emotion labeling, individuals with AS/HFA had greater difficulties identifying and describing their emotions, with approximately two-thirds exceeding the cutoff for alexithymia. With respect to emotion regulation, individuals with AS/HFA used reappraisal less frequently than TD individuals and reported lower levels of reappraisal self-efficacy. Although AS/HFA individuals used suppression more frequently than TD individuals, no difference in suppression self-efficacy was found. It is important to note that these differences in emotion regulation were evident even when controlling for emotion experience and labeling. Implications of these deficits are discussed, and future research directions are proposed.","[{'authorId': '38707445', 'name': 'Andrea C. Samson'}, {'authorId': '48474585', 'name': 'O. Huber'}, {'authorId': '1775321', 'name': 'J. Gross'}]",295.0,"{'bibtex': ""@Article{Samson2012EmotionRI,\n author = {Andrea C. Samson and O. Huber and J. Gross},\n journal = {Emotion},\n pages = {\n          659-65\n        },\n title = {Emotion regulation in Asperger's syndrome and high-functioning autism.},\n volume = {12 4},\n year = {2012}\n}\n""}",,"{'volume': '12 4', 'pages': '\n          659-65\n        ', 'name': 'Emotion'}",56.0,Emotion regulation in Asperger's syndrome and high-functioning autism.,2012.0
2867,fe10d7e580a675c9ef67107366ee084a3da52d33,,"[{'authorId': '40486665', 'name': 'Ancrêt Szpak'}, {'authorId': '3248174', 'name': 'T. Loetscher'}, {'authorId': '4886405', 'name': 'O. Churches'}, {'authorId': '2262597', 'name': 'N. Thomas'}, {'authorId': '46582609', 'name': 'C. Spence'}, {'authorId': '143991815', 'name': 'M. Nicholls'}]",22.0,"{'bibtex': '@Article{Szpak2015KeepingYD,\n author = {Ancrêt Szpak and T. Loetscher and O. Churches and N. Thomas and C. Spence and M. Nicholls},\n journal = {Neuropsychologia},\n pages = {462-467},\n title = {Keeping your distance: attentional withdrawal in individuals who show physiological signs of social discomfort},\n volume = {70},\n year = {2015}\n}\n'}",,"{'volume': '70', 'pages': '462-467', 'name': 'Neuropsychologia'}",50.0,Keeping your distance: attentional withdrawal in individuals who show physiological signs of social discomfort,2015.0
2868,fe1c93fb98fe2fc89e3e17b3c95877b0326ad35b,"Augmented reality emerges as a tool, on which it is necessary to examine its real educational value. This paper shows the results of a bibliometric analysis performed on documents collected from the Web of Science repository, an Internet service that concentrates bibliographic information from more than 7,000 institutions. Our analysis included an overall universe of 12,000 indexed journals and 148,000 conference proceedings. From those, we selected a sample targeting the terms “mobile-learning” or “m-learning” and “augmented reality” as descriptors or components of titles of scientific works. The analysis on journals (n=741)","[{'authorId': '2265886730', 'name': 'Dr. Javier Fombona'}, {'authorId': '2265887271', 'name': 'Dr. Maria-Angeles Pascual-Sevillano'}, {'authorId': '2265887269', 'name': 'Dr. MariCarmen González-Videgaray'}]",30.0,"{'bibtex': '@Inproceedings{Fombona2017MlearningAA,\n author = {Dr. Javier Fombona and Dr. Maria-Angeles Pascual-Sevillano and Dr. MariCarmen González-Videgaray},\n title = {M-learning and Augmented Reality: A Review of the Scientific Literature on the WoS Repository},\n year = {2017}\n}\n'}",,,62.0,M-learning and Augmented Reality: A Review of the Scientific Literature on the WoS Repository,2017.0
2869,fe2484656dec6a895f18d26bad470536db8cb75e,,"[{'authorId': '1691580', 'name': 'R. Trappl'}, {'authorId': '1764052', 'name': 'P. Petta'}]",111.0,"{'bibtex': '@Inproceedings{Trappl1997CreatingPF,\n author = {R. Trappl and P. Petta},\n title = {Creating Personalities for Synthetic Actors},\n volume = {1195},\n year = {1997}\n}\n'}",,{'volume': '1195'},0.0,Creating Personalities for Synthetic Actors,1997.0
2870,fe289440427610dfe935c428cd6735baf9fcafd8,"Strategies are necessary to mitigate the impact of unexpected behavior in collaborative robotics, and research to develop solutions is lacking. Our aim here was to explore the benefits of an affective interaction, as opposed to a more efficient, less error prone but non-communicative one. The experiment took the form of an omelet-making task, with a wide range of participants interacting directly with BERT2, a humanoid robot assistant. Having significant implications for design, results suggest that efficiency is not the most important aspect of performance for users; a personable, expressive robot was found to be preferable over a more efficient one, despite a considerable trade off in time taken to perform the task. Our findings also suggest that a robot exhibiting human-like characteristics may make users reluctant to `hurt its feelings'; they may even lie in order to avoid this.","[{'authorId': '1677007962', 'name': 'Adriana Hamacher'}, {'authorId': '1398541310', 'name': 'N. Bianchi-Berthouze'}, {'authorId': '3859511', 'name': 'A. Pipe'}, {'authorId': '1725137', 'name': 'K. Eder'}]",83.0,"{'bibtex': '@Article{Hamacher2016BelievingIB,\n author = {Adriana Hamacher and N. Bianchi-Berthouze and A. Pipe and K. Eder},\n journal = {2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},\n pages = {493-500},\n title = {Believing in BERT: Using expressive communication to enhance trust and counteract operational error in physical Human-robot interaction},\n year = {2016}\n}\n'}",,"{'pages': '493-500', 'name': '2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)'}",36.0,Believing in BERT: Using expressive communication to enhance trust and counteract operational error in physical Human-robot interaction,2016.0
2871,fe332edfd08a8393696834a69c2d60d38fc896e9,"
 
 
 The development of virtual enterprise is inseparable from the support of partners. Due to information asymmetry, the choice of virtual enterprise partners is blind, and simple matching formula is difficult to meet. At the same time, the psychology of the matching subject is based on the maximization of its own interests rather than the maximization of collective interests. There are interest contradictions and conflicts between subjects, and they may eventually fall into a “prisoner's dilemma”. Therefore, how to carry out the research on emotion regulation based on multi-agent heterogeneous game preference and bilateral partner preference is very important.
 
 
 
 Aiming at the contradictions and interest conflicts between subjects in the process of virtual enterprise partner matching, combined with the basic characteristics of subject language preference evaluation, the game idea is introduced into the process of virtual enterprise partner bilateral matching analysis, a bilateral matching game model of heterogeneous multi-attribute preference and subjects' psychological behavior is proposed, and the influence of the changes of subjects' psychological behavior on the evolution law of bilateral matching game system is analyzed, The Nash equilibrium strategy of bilateral matching is discussed. In order to verify the impact of the algorithm on emotion, this study uses relevant scales to investigate. (1) Positive emotion scale. The Panas emotion scale developed by Watson et al. Is widely used to measure emotion. The scale includes two dimensions: positive emotion and negative emotion. There are 6 questions in this dimension, and Likert scores 5 points (1 means “very inconsistent”, 5 means “very consistent”, the same below). The Cronbach coefficient of this questionnaire is 0.90. (2) Motivation. The problem of measuring motivation is mainly the motivation scale compiled by Phan, which has 8 questions and is scored with Likert 5 points. (3) Social support scale. The scale is adapted from the social support scale compiled by Ye Yuemei et al. It has eight questions, using Likert's five points scoring method. Clonbach α the coefficient of the scale is 0.87. (4) Behavioral propensity scale. Propensity dimension in intention measurement. The scale has 6 questions, and Likert scores 5 points. The clonbach coefficient of the scale is 0.95.
 
 
 
 The results of case analysis show that the model can make full use of the multi-attribute preference information of heterogeneous subjects, describe the mechanism of psychological behavior affecting the evolution of game system, and help virtual enterprises match business partners; This has important value and significance for the establishment of virtual enterprises and their industrial agglomeration effect. In the influence process of cooperative anxiety, emotional response plays an intermediary role, psychological elasticity plays a regulatory role, and emotional response to life events plays an intermediary role. That is, the higher the psychological elasticity, the higher the excessive coping style of the game, and vice versa.
 
 
 
 Various stable matching solutions can be obtained by solving the model, and the conclusion is more suitable for the matching decision-making process of both parties in the actual process, which makes the model can be applied to various scenarios, such as the selection of battery suppliers of new energy vehicles and smart phone screen suppliers, which has greater practical value and significance. It should be pointed out that there are some deficiencies in the setting conditions of the model method, mainly because some individuals in the group cooperate to form an alliance, so as to form a community of interests. In the process of game, the individual experience in the group is constrained by more conditions, which makes the results of the model deviate.
 
 
 
 Zhejiang Provincial Natural Science Foundation (No. LQ20G010005), Zhejiang Provincial Statistical Science Research Project (NO. 21TJQN15) and Natural Science Fund Project of Huzhou City (No. 2018YZ13)
","[{'authorId': '93408810', 'name': 'Hanjie Xiao'}, {'authorId': '104449167', 'name': 'Shuyan Bao'}, {'authorId': '2167863602', 'name': 'Liang Wu'}, {'authorId': '2112779703', 'name': 'Honglei Tang'}, {'authorId': '2153300143', 'name': 'Guosong Wu'}, {'authorId': '1750047336', 'name': 'Jianhua Zhou'}, {'authorId': '2175623845', 'name': 'Jianxin Xu'}]",0.0,"{'bibtex': ""@Article{Xiao2022RESEARCHOT,\n author = {Hanjie Xiao and Shuyan Bao and Liang Wu and Honglei Tang and Guosong Wu and Jianhua Zhou and Jianxin Xu},\n booktitle = {International Journal of Neuropsychopharmacology},\n journal = {International Journal of Neuropsychopharmacology},\n title = {RESEARCH ON THE REGULATION OF MULTI-AGENT'S EMOTION BASED ON MULTI-AGENT'S HETEROGENEOUS GAME PREFERENCE AND BILATERAL PARTNER'S PREFERENCE},\n year = {2022}\n}\n""}",[],{'name': 'International Journal of Neuropsychopharmacology'},0.0,RESEARCH ON THE REGULATION OF MULTI-AGENT'S EMOTION BASED ON MULTI-AGENT'S HETEROGENEOUS GAME PREFERENCE AND BILATERAL PARTNER'S PREFERENCE,2022.0
2872,fe5d8d37d19ede127065b629a8c608f7d59e91ce,,"[{'authorId': '1383996606', 'name': 'S. D’Mello'}, {'authorId': '1769251', 'name': 'A. Graesser'}]",31.0,"{'bibtex': '@Inproceedings{D’Mello2012AdaptiveTF,\n author = {S. D’Mello and A. Graesser},\n title = {Adaptive Technologies for Training and Education: Emotions during Learning with AutoTutor},\n year = {2012}\n}\n'}",,"{'volume': '', 'name': ''}",0.0,Adaptive Technologies for Training and Education: Emotions during Learning with AutoTutor,2012.0
2873,fe5df7c65a2a3b7e0d280ce4abc73d3efd8557ef,"
 
 What characters believe, how they act based on those beliefs,and how their beliefs are updated is an essential element of many stories. State-space narrative planning algorithms treat their search spaces like a set of temporally possible worlds. We present an extension that models character beliefs as epistemically possible worlds and describe how such a space is generated. We also present the results of an experiment that demonstrates that the model meets the expectations of a human audience.
 
","[{'authorId': '144650313', 'name': 'A. Shirvani'}, {'authorId': '34810994', 'name': 'Stephen G. Ware'}, {'authorId': '38518031', 'name': 'Rachelyn Farrell'}]",32.0,"{'bibtex': '@Inproceedings{Shirvani2021APW,\n author = {A. Shirvani and Stephen G. Ware and Rachelyn Farrell},\n pages = {101-107},\n title = {A Possible Worlds Model of Belief for State-Space Narrative Planning},\n year = {2021}\n}\n'}",,{'pages': '101-107'},30.0,A Possible Worlds Model of Belief for State-Space Narrative Planning,2021.0
2874,fe6215fa06cc12f2ce776483c8da2996df7efb2e,"Appraisal theory has become one of the most active aproaches in the domain of emotion psychology. The appraisal process consists of the subjective evaluation that occurs during the individual's encounter with significant events in the environment, thus determining the nature of the emotional reaction and experience. The organism's interpretation of events and situations elicits and differentiates its emotional responses, although the exact processes involved and the limits of the theory are still a matter of debate and are currently the object of active research. This volume is intended to become the primary source of information on appraisal for all those interested in emotion, from beginning graduate students to accomplished researchers in emotion psychology.","[{'authorId': '2462740', 'name': 'K. Scherer'}, {'authorId': '35649425', 'name': 'A. Schorr'}, {'authorId': '30361732', 'name': 'T. Johnstone'}]",1782.0,"{'bibtex': '@Inproceedings{Scherer2001AppraisalPI,\n author = {K. Scherer and A. Schorr and T. Johnstone},\n title = {Appraisal processes in emotion: Theory, methods, research.},\n year = {2001}\n}\n'}",,"{'volume': '', 'name': ''}",1.0,"Appraisal processes in emotion: Theory, methods, research.",2001.0
2875,fe68040b7132b9ae7118fdd517986d13beba4f56,,"[{'authorId': '145009779', 'name': 'J. Tsai'}, {'authorId': '1740910', 'name': 'E. Bowring'}, {'authorId': '1788771', 'name': 'S. Marsella'}, {'authorId': '2059157982', 'name': 'Wendy Wood'}, {'authorId': '143736701', 'name': 'Milind Tambe'}]",47.0,"{'bibtex': '@Inproceedings{Tsai2012ASO,\n author = {J. Tsai and E. Bowring and S. Marsella and Wendy Wood and Milind Tambe},\n pages = {81-88},\n title = {A Study of Emotional Contagion with Virtual Characters},\n year = {2012}\n}\n'}",,{'pages': '81-88'},22.0,A Study of Emotional Contagion with Virtual Characters,2012.0
2879,fe7a6b3f54a2d5ba58b550e4ecaec2578da75850,,"[{'authorId': '1726694', 'name': 'Leila Amgoud'}, {'authorId': '1754782', 'name': 'Srdjan Vesic'}]",28.0,"{'bibtex': '@Inproceedings{Amgoud2011TwoRO,\n author = {Leila Amgoud and Srdjan Vesic},\n pages = {86-97},\n title = {Two Roles of Preferences in Argumentation Frameworks},\n year = {2011}\n}\n'}",,{'pages': '86-97'},21.0,Two Roles of Preferences in Argumentation Frameworks,2011.0
2880,fe9cd9ab8bcc110847a8a4792134e4917fd7e933,"The Tapestry experimental mail system developed at the Xerox Palo Alto Research Center is predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.","[{'authorId': '38111918', 'name': 'David Goldberg'}, {'authorId': '152473861', 'name': 'D. Nichols'}, {'authorId': '1724602', 'name': 'B. Oki'}, {'authorId': '1680763', 'name': 'D. Terry'}]",4280.0,"{'bibtex': '@Article{Goldberg1992UsingCF,\n author = {David Goldberg and D. Nichols and B. Oki and D. Terry},\n journal = {Commun. ACM},\n pages = {61-70},\n title = {Using collaborative filtering to weave an information tapestry},\n volume = {35},\n year = {1992}\n}\n'}",,"{'volume': '35', 'pages': '61-70', 'name': 'Commun. ACM'}",20.0,Using collaborative filtering to weave an information tapestry,1992.0
2881,feac5438d27765f0bda00257f9ce510c3f476438,"This paper reports on an ethological study of 11 depressed hospitalized subjects. Major depression and recovery are described in terms of general behavioral traits, i.e., behavior parameters. The hypothesis, that the primary behavioral feature of major depression is a reduction of social interaction and that secondary features are reduced self occupation and body mobility (posture flexibility) is tested. The behavioral patterns of depression and recovery are described and elucidated by 12 defined behavioral parameters, eight of which show significant changes between the first and the last hospital week. Findings from six of the parameters are consistent with the hypothesis and demonstrate social inhibition during depression; interactions between depression and nonverbal behavior are particularly striking. Findings also confirm that, during depression, self occupation and body mobility are reduced to a less significant degree than social inhibition. Possible relationships between findings and agitated forms of major depression are discussed. A final section examines findings in an evolutionary context and emphasizes their clinical implications.","[{'authorId': '2227379792', 'name': 'Jens Tyge Mørk Schelde'}]",53.0,"{'bibtex': '@Article{Schelde1998MajorDB,\n author = {Jens Tyge Mørk Schelde},\n journal = {The Journal of nervous and mental disease},\n pages = {\n          141-9\n        },\n title = {Major depression: behavioral parameters of depression and recovery.},\n volume = {186 3},\n year = {1998}\n}\n'}",,"{'volume': '186 3', 'pages': '\n          141-9\n        ', 'name': 'The Journal of nervous and mental disease'}",24.0,Major depression: behavioral parameters of depression and recovery.,1998.0
2882,fed2ae5acc2fd9d0795621de3438ee1f08365220,"Consistently exhibited personalities are crucial elements of realistic, engaging, and behavior-rich conversational virtual agents. Both nonverbal and verbal cues help convey these agents’ unseen psychological states, contributing to our effective communication with them. We introduce a comprehensive framework to design conversational agents that express personality through non-verbal behaviors like body movement and facial expressions, as well as verbal behaviors like dialogue selection and voice transformation. We use the OCEAN personality model, which defines personality as a combination of five orthogonal factors of openness, conscientiousness, extraversion, agreeableness, and neuroticism. The framework combines existing personality expression methods with novel ones such as new algorithms to convey Laban Shape and Effort qualities. We perform Amazon Mechanical Turk studies to analyze how different communication modalities influence our perception of virtual agent personalities and compare their individual and combined effects on each personality dimension. The results indicate that our personality-based modifications are perceived as natural, and each additional modality improves perception accuracy, with the best performance achieved when all the modalities are present. We also report some correlations for the perception of conscientiousness with neuroticism and openness with extraversion.","[{'authorId': '1752928392', 'name': 'Sinan Sonlu'}, {'authorId': '1746035', 'name': 'U. Güdükbay'}, {'authorId': '2643744', 'name': 'Funda Durupinar'}]",24.0,"{'bibtex': '@Article{Sonlu2021ACA,\n author = {Sinan Sonlu and U. Güdükbay and Funda Durupinar},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 16},\n title = {A Conversational Agent Framework with Multi-modal Personality Expression},\n volume = {40},\n year = {2021}\n}\n'}",,"{'volume': '40', 'pages': '1 - 16', 'name': 'ACM Transactions on Graphics (TOG)'}",98.0,A Conversational Agent Framework with Multi-modal Personality Expression,2021.0
2883,fed404f61d4d6f23ba71a494597db5ce04140d5d,,"[{'authorId': '2113322333', 'name': 'Zhenzhen Yao'}, {'authorId': '3172102', 'name': 'Guijuan Zhang'}, {'authorId': '7382513', 'name': 'Dianjie Lu'}, {'authorId': '2118902760', 'name': 'Hong Liu'}]",37.0,"{'bibtex': '@Article{Yao2019DatadrivenCE,\n author = {Zhenzhen Yao and Guijuan Zhang and Dianjie Lu and Hong Liu},\n journal = {Neurocomputing},\n pages = {314-327},\n title = {Data-driven crowd evacuation: A reinforcement learning method},\n volume = {366},\n year = {2019}\n}\n'}",,"{'volume': '366', 'pages': '314-327', 'name': 'Neurocomputing'}",51.0,Data-driven crowd evacuation: A reinforcement learning method,2019.0
2884,fede6fe5b167164039762ef44840210dddba991c,"Attempts to add emotion effects to synthesised speech have existed for more than a decade now. Several prototypes and fully operational systems have been built based on different synthesis techniques, and quite a number of smaller studies have been conducted. This paper aims to give an overview of what has been done in this field, pointing out the inherent properties of the various synthesis techniques used, summarising the prosody rules employed, and taking a look at the evaluation paradigms. Finally, an attempt is made to discuss interesting directions for future development.","[{'authorId': '144951065', 'name': 'M. Schröder'}]",424.0,"{'bibtex': '@Inproceedings{Schröder2001EmotionalSS,\n author = {M. Schröder},\n pages = {561-564},\n title = {Emotional speech synthesis: a review},\n year = {2001}\n}\n'}",,{'pages': '561-564'},29.0,Emotional speech synthesis: a review,2001.0
2885,ff0deb44c3d88ca36b8bd67a3ba5cbf1b2613d6d,"Making unconventional emergent plan for dense crowd is one of the critical issues of evacuation simulations. In order to make the behavior of crowd more believable, we present a real-time evacuation route approach based on emotion and geodesic under the influence of individual emotion and multi-hazard circumstances. The proposed emotion model can reflect the dynamic process of individual in group on three factors: individual emotion, perilous field, and crowd emotion. Specifically, we first convert the evacuation scene to Delaunay triangulation representations. Then, we use the optimization-driven geodesic approach to calculate the best evacuation path with user-specified geometric constraints, such as crowd density, obstacle information, and perilous field. Finally, the Smooth Particle Hydrodynamics method is used for local avoidance of collisions with nearby agents in real-time simulation. Extensive experimental results show that our algorithm is efficient and well suited for real-time simulations of crowd evacuation.","[{'authorId': '3236564', 'name': 'Bangquan Liu'}, {'authorId': '46270580', 'name': 'Z. Liu'}, {'authorId': '2865576', 'name': 'Dechao Sun'}, {'authorId': '66052862', 'name': 'Chunyue Bi'}]",14.0,"{'bibtex': '@Article{Liu2018AnER,\n author = {Bangquan Liu and Z. Liu and Dechao Sun and Chunyue Bi},\n journal = {Mathematical Problems in Engineering},\n title = {An Evacuation Route Model of Crowd Based on Emotion and Geodesic},\n year = {2018}\n}\n'}",,{'name': 'Mathematical Problems in Engineering'},43.0,An Evacuation Route Model of Crowd Based on Emotion and Geodesic,2018.0
2886,ff7caf54b65c0fbb4dabbfd9320e5fee51861fe9,,"[{'authorId': '46699409', 'name': 'Cindy Chan'}, {'authorId': '6439210', 'name': 'Cassie Mogilner'}]",4.0,"{'bibtex': '@Article{Chan2014ExperientialGF,\n author = {Cindy Chan and Cassie Mogilner},\n journal = {ACR North American Advances},\n title = {Experiential Gifts Foster Stronger Relationships Than Material Gifts},\n year = {2014}\n}\n'}",,"{'volume': '', 'name': 'ACR North American Advances'}",0.0,Experiential Gifts Foster Stronger Relationships Than Material Gifts,2014.0
2887,ff7e8ce1be15b11ea6a2184652a131d7ba887426,,"[{'authorId': '1726694', 'name': 'Leila Amgoud'}, {'authorId': '1754782', 'name': 'Srdjan Vesic'}]",93.0,"{'bibtex': '@Article{Amgoud2011ANA,\n author = {Leila Amgoud and Srdjan Vesic},\n journal = {Annals of Mathematics and Artificial Intelligence},\n pages = {149-183},\n title = {A new approach for preference-based argumentation frameworks},\n volume = {63},\n year = {2011}\n}\n'}",,"{'volume': '63', 'pages': '149-183', 'name': 'Annals of Mathematics and Artificial Intelligence'}",49.0,A new approach for preference-based argumentation frameworks,2011.0
2888,ffb9c3df35f3b89160f7cd57cd98f508ef5bdf56,,"[{'authorId': '6989922', 'name': 'H. R. Riggio'}]",8.0,"{'bibtex': '@Article{Riggio2020EmotionalE,\n author = {H. R. Riggio},\n journal = {Encyclopedia of Personality and Individual Differences},\n title = {Emotional Expressiveness},\n year = {2020}\n}\n'}",,{'name': 'Encyclopedia of Personality and Individual Differences'},40.0,Emotional Expressiveness,2020.0
2889,ffe0998452a49c164b3704bbc209d97527cbae20,,"[{'authorId': '21112145', 'name': 'Joshua D. Greene'}]",433.0,"{'bibtex': '@Article{Greene2007WhyAV,\n author = {Joshua D. Greene},\n journal = {Trends in Cognitive Sciences},\n pages = {322-323},\n title = {Why are VMPFC patients more utilitarian? A dual-process theory of moral judgment explains},\n volume = {11},\n year = {2007}\n}\n'}",,"{'volume': '11', 'pages': '322-323', 'name': 'Trends in Cognitive Sciences'}",7.0,Why are VMPFC patients more utilitarian? A dual-process theory of moral judgment explains,2007.0
2890,fff6f505aa01e8fe5b27694c80d469458430d203,"Moral norms play an essential role in regulating human interaction. With the growing sophistication and proliferation of robots, it is important to understand how ordinary people apply moral norms to robot agents and make moral judgments about their behavior. We report the first comparison of people's moral judgments (of permissibility, wrongness, and blame) about human and robot agents. Two online experiments (total N =316) found that robots, compared with human agents, were more strongly expected to take an action that sacrifices one person for the good of many (a “utilitarian” choice), and they were blamed more than their human counterparts when they did not make that choice. Though the utilitarian sacrifice was generally seen as permissible for human agents, they were blamed more for choosing this option than for doing nothing. These results provide a first step toward a new field of Moral HRI, which is well placed to help guide the design of social robots. Categories and Subject Descriptors I.2.9 [Artificial Intelligence] Robotics K.4.1 [Computers and Society] Public Policy Issues, Ethics","[{'authorId': '1854509', 'name': 'B. Malle'}, {'authorId': '1793014', 'name': 'Matthias Scheutz'}, {'authorId': '2053868213', 'name': 'Thomas Arnold'}, {'authorId': '2195142', 'name': 'John Voiklis'}, {'authorId': '3391293', 'name': 'Corey J. Cusimano'}]",237.0,"{'bibtex': '@Article{Malle2015SacrificeOF,\n author = {B. Malle and Matthias Scheutz and Thomas Arnold and John Voiklis and Corey J. Cusimano},\n journal = {2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},\n pages = {117-124},\n title = {Sacrifice One For the Good of Many? People Apply Different Moral Norms to Human and Robot Agents},\n year = {2015}\n}\n'}",,"{'pages': '117-124', 'name': '2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)'}",52.0,Sacrifice One For the Good of Many? People Apply Different Moral Norms to Human and Robot Agents,2015.0
2891,,,[],,,,,,The neural correlates of regulating another person's emotions: an exploratory fMRI study,2014.0
